[
  {
    "url": "https://pytorch.org/tutorials/intermediate/realtime_rpi.html",
    "title": "Real Time Inference on Raspberry Pi 4 (30 fps!)\u00b6",
    "code_snippets": [
      "$ python -c \"import torch; print(torch.__version__)\"",
      "import cv2\nfrom PIL import Image\n\ncap = cv2.VideoCapture(0)\ncap.set(cv2.CAP_PROP_FRAME_WIDTH, 224)\ncap.set(cv2.CAP_PROP_FRAME_HEIGHT, 224)\ncap.set(cv2.CAP_PROP_FPS, 36)",
      "from torchvision import transforms\n\npreprocess = transforms.Compose([\n    # convert the frame to a CHW torch tensor for training\n    transforms.ToTensor(),\n    # normalize the colors to the range that mobilenet_v2/3 expect\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\ninput_tensor = preprocess(image)\n# The model can handle multiple images simultaneously so we need to add an\n# empty dimension for the batch.\n# [3, 224, 224] -> [1, 3, 224, 224]\ninput_batch = input_tensor.unsqueeze(0)",
      "import torch\ntorch.backends.quantized.engine = 'qnnpack'",
      "from torchvision import models\nnet = models.quantization.mobilenet_v2(pretrained=True, quantize=True)",
      "net = torch.jit.script(net)",
      "import time\n\nimport torch\nimport numpy as np\nfrom torchvision import models, transforms\n\nimport cv2\nfrom PIL import Image\n\ntorch.backends.quantized.engine = 'qnnpack'\n\ncap = cv2.VideoCapture(0, cv2.CAP_V4L2)\ncap.set(cv2.CAP_PROP_FRAME_WIDTH, 224)\ncap.set(cv2.CAP_PROP_FRAME_HEIGHT, 224)\ncap.set(cv2.CAP_PROP_FPS, 36)\n\npreprocess = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\nnet = models.quantization.mobilenet_v2(pretrained=True, quantize=True)\n# jit model to take it from ~20fps to ~30fps\nnet = torch.jit.script(net)\n\nstarted = time.time()\nlast_logged = time.time()\nframe_count = 0\n\nwith torch.no_grad():\n    while True:\n        # read frame\n        ret, image = cap.read()\n        if not ret:\n            raise RuntimeError(\"failed to read frame\")\n\n        # convert opencv output from BGR to RGB\n        image = image[:, :, [2, 1, 0]]\n        permuted = image\n\n        # preprocess\n        input_tensor = preprocess(image)\n\n        # create a mini-batch as expected by the model\n        input_batch = input_tensor.unsqueeze(0)\n\n        # run model\n        output = net(input_batch)\n        # do something with output ...\n\n        # log model performance\n        frame_count += 1\n        now = time.time()\n        if now - last_logged > 1:\n            print(f\"{frame_count / (now-last_logged)} fps\")\n            last_logged = now\n            frame_count = 0",
      "top = list(enumerate(output[0].softmax(dim=0)))\ntop.sort(key=lambda x: x[1], reverse=True)\nfor idx, val in top[:10]:\n    print(f\"{val.item()*100:.2f}% {classes[idx]}\")",
      "torch.set_num_threads(2)"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/intermediate/process_group_cpp_extension_tutorial.html",
    "title": "Customize Process Group Backends Using Cpp Extensions\u00b6",
    "code_snippets": [
      "// file name: dummy.hpp\n#include <torch/python.h>\n\n#include <torch/csrc/distributed/c10d/Backend.hpp>\n#include <torch/csrc/distributed/c10d/Work.hpp>\n#include <torch/csrc/distributed/c10d/Store.hpp>\n#include <torch/csrc/distributed/c10d/Types.hpp>\n#include <torch/csrc/distributed/c10d/Utils.hpp>\n\n#include <pybind11/chrono.h>\n\nnamespace c10d {\n\nclass BackendDummy : public Backend {\n  public:\n    BackendDummy(int rank, int size);\n\n    c10::intrusive_ptr<Work> allgather(\n        std::vector<std::vector<at::Tensor>>& outputTensors,\n        std::vector<at::Tensor>& inputTensors,\n        const AllgatherOptions& opts = AllgatherOptions()) override;\n\n    c10::intrusive_ptr<Work> allreduce(\n        std::vector<at::Tensor>& tensors,\n        const AllreduceOptions& opts = AllreduceOptions()) override;\n\n    // The collective communication APIs without a custom implementation\n    // will error out if invoked by application code.\n};\n\nclass WorkDummy : public Work {\n  public:\n    WorkDummy(\n      OpType opType,\n      c10::intrusive_ptr<c10::ivalue::Future> future) // future of the output\n      : Work(\n          -1, // rank, only used by recvAnySource, irrelevant in this demo\n          opType),\n      future_(std::move(future)) {}\n    bool isCompleted() override;\n    bool isSuccess() const override;\n    bool wait(std::chrono::milliseconds timeout = kUnsetTimeout) override;\n    virtual c10::intrusive_ptr<c10::ivalue::Future> getFuture() override;\n\n  private:\n    c10::intrusive_ptr<c10::ivalue::Future> future_;\n};\n} // namespace c10d",
      "// file name: dummy.hpp\nclass BackendDummy : public Backend {\n    ...\n    <Step 1 code>\n    ...\n\n    static c10::intrusive_ptr<Backend> createBackendDummy(\n        const c10::intrusive_ptr<::c10d::Store>& store,\n        int rank,\n        int size,\n        const std::chrono::duration<float>& timeout);\n\n    static void BackendDummyConstructor() __attribute__((constructor)) {\n        py::object module = py::module::import(\"torch.distributed\");\n        py::object register_backend =\n            module.attr(\"Backend\").attr(\"register_backend\");\n        // torch.distributed.Backend.register_backend will add `dummy` as a\n        // new valid backend.\n        register_backend(\"dummy\", py::cpp_function(createBackendDummy));\n    }\n}",
      "// file name: dummy.cpp\nc10::intrusive_ptr<Backend> BackendDummy::createBackendDummy(\n        const c10::intrusive_ptr<::c10d::Store>& /* unused */,\n        int rank,\n        int size,\n        const std::chrono::duration<float>& /* unused */) {\n    return c10::make_intrusive<BackendDummy>(rank, size);\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"createBackendDummy\", &BackendDummy::createBackendDummy);\n}",
      "# file name: setup.py\nimport os\nimport sys\nimport torch\nfrom setuptools import setup\nfrom torch.utils import cpp_extension\n\nsources = [\"src/dummy.cpp\"]\ninclude_dirs = [f\"{os.path.dirname(os.path.abspath(__file__))}/include/\"]\n\nif torch.cuda.is_available():\n    module = cpp_extension.CUDAExtension(\n        name = \"dummy_collectives\",\n        sources = sources,\n        include_dirs = include_dirs,\n    )\nelse:\n    module = cpp_extension.CppExtension(\n        name = \"dummy_collectives\",\n        sources = sources,\n        include_dirs = include_dirs,\n    )\n\nsetup(\n    name = \"Dummy-Collectives\",\n    version = \"0.0.1\",\n    ext_modules = [module],\n    cmdclass={'build_ext': cpp_extension.BuildExtension}\n)",
      "import os\n\nimport torch\n# importing dummy_collectives makes torch.distributed recognize `dummy`\n# as a valid backend.\nimport dummy_collectives\n\nimport torch.distributed as dist\n\nos.environ['MASTER_ADDR'] = 'localhost'\nos.environ['MASTER_PORT'] = '29500'\n\n# Alternatively:\n# dist.init_process_group(\"dummy\", rank=0, world_size=1)\ndist.init_process_group(\"cpu:gloo,cuda:dummy\", rank=0, world_size=1)\n\n# this goes through gloo\nx = torch.ones(6)\ndist.all_reduce(x)\nprint(f\"cpu allreduce: {x}\")\n\n# this goes through dummy\nif torch.cuda.is_available():\n    y = x.cuda()\n    dist.all_reduce(y)\n    print(f\"cuda allreduce: {y}\")\n\n    try:\n        dist.broadcast(y, 0)\n    except RuntimeError:\n        print(\"got RuntimeError when calling broadcast\")"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/recipes/torch_logs.html",
    "title": "(beta) Using TORCH_LOGS python API with torch.compile\u00b6",
    "code_snippets": [
      "import logging",
      "import torch\n\n# exit cleanly if we are on a device that doesn't support torch.compile\nif torch.cuda.get_device_capability() < (7, 0):\n    print(\"Skipping because torch.compile is not supported on this device.\")\nelse:\n    @torch.compile()\n    def fn(x, y):\n        z = x + y\n        return z + 2\n\n\n    inputs = (torch.ones(2, 2, device=\"cuda\"), torch.zeros(2, 2, device=\"cuda\"))\n\n\n# print separator and reset dynamo\n# between each example\n    def separator(name):\n        print(f\"==================={name}=========================\")\n        torch._dynamo.reset()\n\n\n    separator(\"Dynamo Tracing\")\n# View dynamo tracing\n# TORCH_LOGS=\"+dynamo\"\n    torch._logging.set_logs(dynamo=logging.DEBUG)\n    fn(*inputs)\n\n    separator(\"Traced Graph\")\n# View traced graph\n# TORCH_LOGS=\"graph\"\n    torch._logging.set_logs(graph=True)\n    fn(*inputs)\n\n    separator(\"Fusion Decisions\")\n# View fusion decisions\n# TORCH_LOGS=\"fusion\"\n    torch._logging.set_logs(fusion=True)\n    fn(*inputs)\n\n    separator(\"Output Code\")\n# View output code generated by inductor\n# TORCH_LOGS=\"output_code\"\n    torch._logging.set_logs(output_code=True)\n    fn(*inputs)\n\n    separator(\"\")",
      "===================Dynamo Tracing=========================\nI0523 19:48:13.436000 23993 torch/_dynamo/utils.py:1603] [0/0] ChromiumEventLogger initialized with id 0bd7cde3-5d06-4b69-a911-4fa57491c783\nV0523 19:48:13.438000 23993 torch/_dynamo/convert_frame.py:1003] [0/0] torchdynamo start compiling fn /var/lib/workspace/recipes_source/torch_logs.py:39, stack (elided 5 frames):\nV0523 19:48:13.438000 23993 torch/_dynamo/convert_frame.py:1003] [0/0]   File \"/usr/local/bin/sphinx-build\", line 8, in <module>\nV0523 19:48:13.438000 23993 torch/_dynamo/convert_frame.py:1003] [0/0]     sys.exit(main())\nV0523 19:48:13.438000 23993 torch/_dynamo/convert_frame.py:1003] [0/0]   File \"/usr/local/lib/python3.10/dist-packages/sphinx/cmd/build.py\", line 288, in main\nV0523 19:48:13.438000 23993 torch/_dynamo/convert_frame.py:1003] [0/0]     return make_main(argv)\nV0523 19:48:13.438000 23993 torch/_dynamo/convert_frame.py:1003] [0/0]   File \"/usr/local/lib/python3.10/dist-packages/sphinx/cmd/build.py\", line 193, in make_main\nV0523 19:48:13.438000 23993 torch/_dynamo/convert_frame.py:1003] [0/0]     return make_mode.run_make_mode(argv[1:])\nV0523 19:48:13.438000 23993 torch/_dynamo/convert_frame.py:1003] [0/0]   File \"/usr/local/lib/python3.10/dist-packages/sphinx/cmd/make_mode.py\", line 160, in run_make_mode\nV0523 19:48:13.438000 23993 torch/_dynamo/convert_frame.py:1003] [0/0]     return make.run_generic_build(args[0])\nV0523 19:48:13.438000 23993 torch/_dynamo/convert_frame.py:1003] [0/0]   File \"/usr/local/lib/python3.10/dist-packages/sphinx/cmd/make_mode.py\", line 148, in run_generic_build\nV0523 19:48:13.438000 23993 torch/_dynamo/convert_frame.py:1003] [0/0]     return build_main(args + opts)\nV0523 19:48:13.438000 23993 torch/_dynamo/convert_frame.py:1003] [0/0]   File \"/usr/local/lib/python3.10/dist-packages/sphinx/cmd/build.py\", line 272, in build_main\nV0523 19:48:13.438000 23993 torch/_dynamo/convert_frame.py:1003] [0/0]     app = Sphinx(args.sourcedir, args.confdir, args.outputdir,\nV0523 19:48:13.438000 23993 torch/_dynamo/convert_frame.py:1003] [0/0]   File \"/usr/local/lib/python3.10/dist-packages/sphinx/application.py\", line 256, in __init__\nV0523 19:48:13.438000 23993 torch/_dynamo/convert_frame.py:1003] [0/0]     self._init_builder()\nV0523 19:48:13.438000 23993 torch/_dynamo/convert_frame.py:1003] [0/0]   File \"/usr/local/lib/python3.10/dist-packages/sphinx/application.py\", line 314, in _init_builder\nV0523 19:48:13.438000 23993 torch/_dynamo/convert_frame.py:1003] [0/0]     self.events.emit('builder-inited')\nV0523 19:48:13.438000 23993 torch/_dynamo/convert_frame.py:1003] [0/0]   File \"/usr/local/lib/python3.10/dist-packages/sphinx/events.py\", line 94, in emit\nV0523 19:48:13.438000 23993 torch/_dynamo/convert_frame.py:1003] [0/0]     results.append(listener.handler(self.app, *args))\nV0523 19:48:13.438000 23993 torch/_dynamo/convert_frame.py:1003] [0/0]   File \"/usr/local/lib/python3.10/dist-packages/sphinx_gallery/gen_gallery.py\", line 491, in generate_gallery_rst\nV0523 19:48:13.438000 23993 torch/_dynamo/convert_frame.py:1003] [0/0]     ) = generate_dir_rst(\nV0523 19:48:13.438000 23993 torch/_dynamo/convert_frame.py:1003] [0/0]   File \"/usr/local/lib/python3.10/dist-packages/sphinx_gallery/gen_rst.py\", line 431, in generate_dir_rst\nV0523 19:48:13.438000 23993 torch/_dynamo/convert_frame.py:1003] [0/0]     intro, title, cost = generate_file_rst(\nV0523 19:48:13.438000 23993 torch/_dynamo/convert_frame.py:1003] [0/0]   File \"/var/lib/workspace/conf.py\", line 79, in wrapper\nV0523 19:48:13.438000 23993 torch/_dynamo/convert_frame.py:1003] [0/0]     p.start()\nV0523 19:48:13.438000 23993 torch/_dynamo/convert_frame.py:1003] [0/0]   File \"/usr/lib/python3.10/multiprocessing/process.py\", line 121, in start\nV0523 19:48:13.438000 23993 torch/_dynamo/convert_frame.py:1003] [0/0]     self._popen = self._Popen(self)\nV0523 19:48:13.438000 23993 torch/_dynamo/convert_frame.py:1003] [0/0]   File \"/usr/lib/python3.10/multiprocessing/context.py\", line 224, in _Popen\nV0523 19:48:13.438000 23993 torch/_dynamo/convert_frame.py:1003] [0/0]     return _default_context.get_context().Process._Popen(process_obj)\nV0523 19:48:13.438000 23993 torch/_dynamo/convert_frame.py:1003] [0/0]   File \"/usr/lib/python3.10/multiprocessing/context.py\", line 281, in _Popen\nV0523 19:48:13.438000 23993 torch/_dynamo/convert_frame.py:1003] [0/0]     return Popen(process_obj)\nV0523 19:48:13.438000 23993 torch/_dynamo/convert_frame.py:1003] [0/0]   File \"/usr/lib/python3.10/multiprocessing/popen_fork.py\", line 19, in __init__\nV0523 19:48:13.438000 23993 torch/_dynamo/convert_frame.py:1003] [0/0]     self._launch(process_obj)\nV0523 19:48:13.438000 23993 torch/_dynamo/convert_frame.py:1003] [0/0]   File \"/usr/lib/python3.10/multiprocessing/popen_fork.py\", line 71, in _launch\nV0523 19:48:13.438000 23993 torch/_dynamo/convert_frame.py:1003] [0/0]     code = process_obj._bootstrap(parent_sentinel=child_r)\nV0523 19:48:13.438000 23993 torch/_dynamo/convert_frame.py:1003] [0/0]   File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\nV0523 19:48:13.438000 23993 torch/_dynamo/convert_frame.py:1003] [0/0]     self.run()\nV0523 19:48:13.438000 23993 torch/_dynamo/convert_frame.py:1003] [0/0]   File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\nV0523 19:48:13.438000 23993 torch/_dynamo/convert_frame.py:1003] [0/0]     self._target(*self._args, **self._kwargs)\nV0523 19:48:13.438000 23993 torch/_dynamo/convert_frame.py:1003] [0/0]   File \"/var/lib/workspace/conf.py\", line 67, in call_fn\nV0523 19:48:13.438000 23993 torch/_dynamo/convert_frame.py:1003] [0/0]     result = func(*args, **kwargs)\nV0523 19:48:13.438000 23993 torch/_dynamo/convert_frame.py:1003] [0/0]   File \"/usr/local/lib/python3.10/dist-packages/sphinx_gallery/gen_rst.py\", line 1027, in generate_file_rst\nV0523 19:48:13.438000 23993 torch/_dynamo/convert_frame.py:1003] [0/0]     output_blocks, time_elapsed = execute_script(script_blocks,\nV0523 19:48:13.438000 23993 torch/_dynamo/convert_frame.py:1003] [0/0]   File \"/usr/local/lib/python3.10/dist-packages/sphinx_gallery/gen_rst.py\", line 945, in execute_script\nV0523 19:48:13.438000 23993 torch/_dynamo/convert_frame.py:1003] [0/0]     output_blocks.append(execute_code_block(\nV0523 19:48:13.438000 23993 torch/_dynamo/convert_frame.py:1003] [0/0]   File \"/usr/local/lib/python3.10/dist-packages/sphinx_gallery/gen_rst.py\", line 810, in execute_code_block\nV0523 19:48:13.438000 23993 torch/_dynamo/convert_frame.py:1003] [0/0]     is_last_expr, mem_max = _exec_and_get_memory(\nV0523 19:48:13.438000 23993 torch/_dynamo/convert_frame.py:1003] [0/0]   File \"/usr/local/lib/python3.10/dist-packages/sphinx_gallery/gen_rst.py\", line 676, in _exec_and_get_memory\nV0523 19:48:13.438000 23993 torch/_dynamo/convert_frame.py:1003] [0/0]     mem_max, _ = gallery_conf['call_memory'](\nV0523 19:48:13.438000 23993 torch/_dynamo/convert_frame.py:1003] [0/0]   File \"/usr/local/lib/python3.10/dist-packages/sphinx_gallery/gen_gallery.py\", line 223, in call_memory\nV0523 19:48:13.438000 23993 torch/_dynamo/convert_frame.py:1003] [0/0]     return 0., func()\nV0523 19:48:13.438000 23993 torch/_dynamo/convert_frame.py:1003] [0/0]   File \"/usr/local/lib/python3.10/dist-packages/sphinx_gallery/gen_rst.py\", line 600, in __call__\nV0523 19:48:13.438000 23993 torch/_dynamo/convert_frame.py:1003] [0/0]     exec(self.code, self.fake_main.__dict__)\nV0523 19:48:13.438000 23993 torch/_dynamo/convert_frame.py:1003] [0/0]   File \"/var/lib/workspace/recipes_source/torch_logs.py\", line 59, in <module>\nV0523 19:48:13.438000 23993 torch/_dynamo/convert_frame.py:1003] [0/0]     fn(*inputs)\nV0523 19:48:13.438000 23993 torch/_dynamo/convert_frame.py:1003] [0/0]\nI0523 19:48:13.442000 23993 torch/_dynamo/symbolic_convert.py:3324] [0/0] Step 1: torchdynamo start tracing fn /var/lib/workspace/recipes_source/torch_logs.py:39\nI0523 19:48:13.442000 23993 torch/fx/experimental/symbolic_shapes.py:3334] [0/0] create_env\nV0523 19:48:13.445000 23993 torch/_dynamo/symbolic_convert.py:1216] [0/0] [__trace_source] TRACE starts_line /var/lib/workspace/recipes_source/torch_logs.py:41 in fn (fn)\nV0523 19:48:13.445000 23993 torch/_dynamo/symbolic_convert.py:1216] [0/0] [__trace_source]             z = x + y\nV0523 19:48:13.447000 23993 torch/_dynamo/symbolic_convert.py:1239] [0/0] [__trace_bytecode] TRACE LOAD_FAST x []\nV0523 19:48:13.447000 23993 torch/_dynamo/symbolic_convert.py:1239] [0/0] [__trace_bytecode] TRACE LOAD_FAST y [LazyVariableTracker()]\nV0523 19:48:13.448000 23993 torch/_dynamo/symbolic_convert.py:1239] [0/0] [__trace_bytecode] TRACE BINARY_ADD None [LazyVariableTracker(), LazyVariableTracker()]\nV0523 19:48:13.449000 23993 torch/_dynamo/variables/builder.py:3025] [0/0] wrap_to_fake L['x'] (2, 2) StatefulSymbolicContext(dynamic_sizes=[<DimDynamic.STATIC: 2>, <DimDynamic.STATIC: 2>], dynamic_strides=[<DimDynamic.INFER_STRIDE: 4>, <DimDynamic.INFER_STRIDE: 4>], constraint_sizes=[None, None], constraint_strides=[None, None], view_base_context=None, tensor_source=LocalSource(local_name='x', is_input=True, dynamism=None, is_derefed_cell_contents=False), shape_env_to_source_to_symbol_cache={}) <class 'torch.Tensor'>\nV0523 19:48:13.450000 23993 torch/_dynamo/output_graph.py:2271] [0/0] create_graph_input L_x_ L['x'] FakeTensor(..., device='cuda:0', size=(2, 2)) at debug_level 0 before=False\nV0523 19:48:13.451000 23993 torch/_dynamo/variables/builder.py:3025] [0/0] wrap_to_fake L['y'] (2, 2) StatefulSymbolicContext(dynamic_sizes=[<DimDynamic.STATIC: 2>, <DimDynamic.STATIC: 2>], dynamic_strides=[<DimDynamic.INFER_STRIDE: 4>, <DimDynamic.INFER_STRIDE: 4>], constraint_sizes=[None, None], constraint_strides=[None, None], view_base_context=None, tensor_source=LocalSource(local_name='y', is_input=True, dynamism=None, is_derefed_cell_contents=False), shape_env_to_source_to_symbol_cache={}) <class 'torch.Tensor'>\nV0523 19:48:13.452000 23993 torch/_dynamo/output_graph.py:2271] [0/0] create_graph_input L_y_ L['y'] FakeTensor(..., device='cuda:0', size=(2, 2)) at debug_level 0 before=False\nV0523 19:48:13.455000 23993 torch/_dynamo/symbolic_convert.py:1239] [0/0] [__trace_bytecode] TRACE STORE_FAST z [TensorVariable()]\nV0523 19:48:13.455000 23993 torch/_dynamo/symbolic_convert.py:1216] [0/0] [__trace_source] TRACE starts_line /var/lib/workspace/recipes_source/torch_logs.py:42 in fn (fn)\nV0523 19:48:13.455000 23993 torch/_dynamo/symbolic_convert.py:1216] [0/0] [__trace_source]             return z + 2\nV0523 19:48:13.455000 23993 torch/_dynamo/symbolic_convert.py:1239] [0/0] [__trace_bytecode] TRACE LOAD_FAST z []\nV0523 19:48:13.456000 23993 torch/_dynamo/symbolic_convert.py:1239] [0/0] [__trace_bytecode] TRACE LOAD_CONST 2 [TensorVariable()]\nV0523 19:48:13.456000 23993 torch/_dynamo/symbolic_convert.py:1239] [0/0] [__trace_bytecode] TRACE BINARY_ADD None [TensorVariable(), ConstantVariable(int: 2)]\nV0523 19:48:13.457000 23993 torch/_dynamo/symbolic_convert.py:1239] [0/0] [__trace_bytecode] TRACE RETURN_VALUE None [TensorVariable()]\nI0523 19:48:13.458000 23993 torch/_dynamo/symbolic_convert.py:3681] [0/0] Step 1: torchdynamo done tracing fn (RETURN_VALUE)\nV0523 19:48:13.458000 23993 torch/_dynamo/symbolic_convert.py:3685] [0/0] RETURN_VALUE triggered compile\nV0523 19:48:13.458000 23993 torch/_dynamo/output_graph.py:1008] [0/0] COMPILING GRAPH due to GraphCompileReason(reason='return_value', user_stack=[<FrameSummary file /var/lib/workspace/recipes_source/torch_logs.py, line 42 in fn>], graph_break=False)\nV0523 19:48:13.460000 23993 torch/_dynamo/output_graph.py:1408] [0/0] [__graph_code] TRACED GRAPH\nV0523 19:48:13.460000 23993 torch/_dynamo/output_graph.py:1408] [0/0] [__graph_code]  ===== __compiled_fn_1 =====\nV0523 19:48:13.460000 23993 torch/_dynamo/output_graph.py:1408] [0/0] [__graph_code]  /usr/local/lib/python3.10/dist-packages/torch/fx/_lazy_graph_module.py class GraphModule(torch.nn.Module):\nV0523 19:48:13.460000 23993 torch/_dynamo/output_graph.py:1408] [0/0] [__graph_code]     def forward(self, L_x_: \"f32[2, 2][2, 1]cuda:0\", L_y_: \"f32[2, 2][2, 1]cuda:0\"):\nV0523 19:48:13.460000 23993 torch/_dynamo/output_graph.py:1408] [0/0] [__graph_code]         l_x_ = L_x_\nV0523 19:48:13.460000 23993 torch/_dynamo/output_graph.py:1408] [0/0] [__graph_code]         l_y_ = L_y_\nV0523 19:48:13.460000 23993 torch/_dynamo/output_graph.py:1408] [0/0] [__graph_code]\nV0523 19:48:13.460000 23993 torch/_dynamo/output_graph.py:1408] [0/0] [__graph_code]          # File: /var/lib/workspace/recipes_source/torch_logs.py:41 in fn, code: z = x + y\nV0523 19:48:13.460000 23993 torch/_dynamo/output_graph.py:1408] [0/0] [__graph_code]         z: \"f32[2, 2][2, 1]cuda:0\" = l_x_ + l_y_;  l_x_ = l_y_ = None\nV0523 19:48:13.460000 23993 torch/_dynamo/output_graph.py:1408] [0/0] [__graph_code]\nV0523 19:48:13.460000 23993 torch/_dynamo/output_graph.py:1408] [0/0] [__graph_code]          # File: /var/lib/workspace/recipes_source/torch_logs.py:42 in fn, code: return z + 2\nV0523 19:48:13.460000 23993 torch/_dynamo/output_graph.py:1408] [0/0] [__graph_code]         add_1: \"f32[2, 2][2, 1]cuda:0\" = z + 2;  z = None\nV0523 19:48:13.460000 23993 torch/_dynamo/output_graph.py:1408] [0/0] [__graph_code]         return (add_1,)\nV0523 19:48:13.460000 23993 torch/_dynamo/output_graph.py:1408] [0/0] [__graph_code]\nV0523 19:48:13.460000 23993 torch/_dynamo/output_graph.py:1408] [0/0] [__graph_code]\nI0523 19:48:13.462000 23993 torch/_dynamo/output_graph.py:1515] [0/0] Step 2: calling compiler function inductor\nI0523 19:48:14.588000 23993 torch/fx/experimental/symbolic_shapes.py:4734] [0/0] produce_guards\nI0523 19:48:14.593000 23993 torch/_dynamo/output_graph.py:1520] [0/0] Step 2: done compiler function inductor\nI0523 19:48:14.595000 23993 torch/fx/experimental/symbolic_shapes.py:4734] [0/0] produce_guards\nV0523 19:48:14.595000 23993 torch/fx/experimental/symbolic_shapes.py:4954] [0/0] track_symint L['x'].size()[0] 2 None\nV0523 19:48:14.596000 23993 torch/fx/experimental/symbolic_shapes.py:4954] [0/0] track_symint L['x'].size()[1] 2 None\nV0523 19:48:14.596000 23993 torch/fx/experimental/symbolic_shapes.py:4954] [0/0] track_symint L['x'].stride()[0] 2 None\nV0523 19:48:14.596000 23993 torch/fx/experimental/symbolic_shapes.py:4954] [0/0] track_symint L['x'].stride()[1] 1 None\nV0523 19:48:14.597000 23993 torch/fx/experimental/symbolic_shapes.py:4954] [0/0] track_symint L['x'].storage_offset() 0 None\nV0523 19:48:14.597000 23993 torch/fx/experimental/symbolic_shapes.py:4954] [0/0] track_symint L['y'].size()[0] 2 None\nV0523 19:48:14.597000 23993 torch/fx/experimental/symbolic_shapes.py:4954] [0/0] track_symint L['y'].size()[1] 2 None\nV0523 19:48:14.598000 23993 torch/fx/experimental/symbolic_shapes.py:4954] [0/0] track_symint L['y'].stride()[0] 2 None\nV0523 19:48:14.598000 23993 torch/fx/experimental/symbolic_shapes.py:4954] [0/0] track_symint L['y'].stride()[1] 1 None\nV0523 19:48:14.598000 23993 torch/fx/experimental/symbolic_shapes.py:4954] [0/0] track_symint L['y'].storage_offset() 0 None\nV0523 19:48:14.599000 23993 torch/fx/experimental/symbolic_shapes.py:5156] [0/0] Skipping guard L['x'].size()[0] == 2\nV0523 19:48:14.599000 23993 torch/fx/experimental/symbolic_shapes.py:5156] [0/0] Skipping guard L['x'].size()[1] == 2\nV0523 19:48:14.599000 23993 torch/fx/experimental/symbolic_shapes.py:5156] [0/0] Skipping guard L['x'].stride()[0] == 2\nV0523 19:48:14.600000 23993 torch/fx/experimental/symbolic_shapes.py:5156] [0/0] Skipping guard L['x'].stride()[1] == 1\nV0523 19:48:14.600000 23993 torch/fx/experimental/symbolic_shapes.py:5156] [0/0] Skipping guard L['x'].storage_offset() == 0\nV0523 19:48:14.600000 23993 torch/fx/experimental/symbolic_shapes.py:5156] [0/0] Skipping guard L['y'].size()[0] == 2\nV0523 19:48:14.600000 23993 torch/fx/experimental/symbolic_shapes.py:5156] [0/0] Skipping guard L['y'].size()[1] == 2\nV0523 19:48:14.601000 23993 torch/fx/experimental/symbolic_shapes.py:5156] [0/0] Skipping guard L['y'].stride()[0] == 2\nV0523 19:48:14.601000 23993 torch/fx/experimental/symbolic_shapes.py:5156] [0/0] Skipping guard L['y'].stride()[1] == 1\nV0523 19:48:14.601000 23993 torch/fx/experimental/symbolic_shapes.py:5156] [0/0] Skipping guard L['y'].storage_offset() == 0\nV0523 19:48:14.602000 23993 torch/_dynamo/guards.py:2557] [0/0] [__guards] GUARDS:\nV0523 19:48:14.602000 23993 torch/_dynamo/guards.py:2495] [0/0] [__guards]\nV0523 19:48:14.602000 23993 torch/_dynamo/guards.py:2495] [0/0] [__guards] TREE_GUARD_MANAGER:\nV0523 19:48:14.602000 23993 torch/_dynamo/guards.py:2495] [0/0] [__guards] +- RootGuardManager\nV0523 19:48:14.602000 23993 torch/_dynamo/guards.py:2495] [0/0] [__guards] | +- DEFAULT_DEVICE: utils_device.CURRENT_DEVICE == None                           # _dynamo/output_graph.py:520 in init_ambient_guards\nV0523 19:48:14.602000 23993 torch/_dynamo/guards.py:2495] [0/0] [__guards] | +- GLOBAL_STATE: ___check_global_state()\nV0523 19:48:14.602000 23993 torch/_dynamo/guards.py:2495] [0/0] [__guards] | +- TORCH_FUNCTION_MODE_STACK: ___check_torch_function_mode_stack()\nV0523 19:48:14.602000 23993 torch/_dynamo/guards.py:2495] [0/0] [__guards] | +- GuardManager: source=L['x'], accessed_by=FrameLocalsGuardAccessor(key='x', framelocals_idx=0)\nV0523 19:48:14.602000 23993 torch/_dynamo/guards.py:2495] [0/0] [__guards] | | +- TENSOR_MATCH: check_tensor(L['x'], Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA), torch.float32, device=0, requires_grad=False, size=[2, 2], stride=[2, 1])  # z = x + y  # ar/lib/workspace/recipes_source/torch_logs.py:41 in fn\nV0523 19:48:14.602000 23993 torch/_dynamo/guards.py:2495] [0/0] [__guards] | | +- NO_HASATTR: hasattr(L['x'], '_dynamo_dynamic_indices') == False           # z = x + y  # ar/lib/workspace/recipes_source/torch_logs.py:41 in fn\nV0523 19:48:14.602000 23993 torch/_dynamo/guards.py:2495] [0/0] [__guards] | | +- NO_TENSOR_ALIASING: check_no_aliasing(L['x'], L['y'])\nV0523 19:48:14.602000 23993 torch/_dynamo/guards.py:2495] [0/0] [__guards] | +- GuardManager: source=L['y'], accessed_by=FrameLocalsGuardAccessor(key='y', framelocals_idx=1)\nV0523 19:48:14.602000 23993 torch/_dynamo/guards.py:2495] [0/0] [__guards] | | +- TENSOR_MATCH: check_tensor(L['y'], Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA), torch.float32, device=0, requires_grad=False, size=[2, 2], stride=[2, 1])  # z = x + y  # ar/lib/workspace/recipes_source/torch_logs.py:41 in fn\nV0523 19:48:14.602000 23993 torch/_dynamo/guards.py:2495] [0/0] [__guards] | | +- NO_HASATTR: hasattr(L['y'], '_dynamo_dynamic_indices') == False           # z = x + y  # ar/lib/workspace/recipes_source/torch_logs.py:41 in fn\nV0523 19:48:14.602000 23993 torch/_dynamo/guards.py:2495] [0/0] [__guards] | | +- NO_TENSOR_ALIASING\nV0523 19:48:14.602000 23993 torch/_dynamo/guards.py:2495] [0/0] [__guards]\nV0523 19:48:14.603000 23993 torch/_dynamo/guards.py:2524] [0/0] [__guards] Guard eval latency = 0.94 us\nI0523 19:48:14.604000 23993 torch/_dynamo/pgo.py:660] [0/0] put_code_state: no cache key, skipping\nI0523 19:48:14.604000 23993 torch/_dynamo/convert_frame.py:1121] [0/0] run_gc_after_compile: running gc\nV0523 19:48:14.607000 23993 torch/_dynamo/convert_frame.py:1395] skipping: _fn (reason: in skipfiles, file: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py)\n===================Traced Graph=========================\nI0523 19:48:14.607000 23993 torch/_dynamo/__init__.py:112] torch._dynamo.reset\nI0523 19:48:14.608000 23993 torch/_dynamo/__init__.py:145] torch._dynamo.reset_code_caches\n===================Fusion Decisions=========================\n===================Output Code=========================\nV0523 19:48:14.698000 23993 torch/_inductor/codecache.py:1093] [0/0] [__output_code] Output code:\nV0523 19:48:14.698000 23993 torch/_inductor/codecache.py:1093] [0/0] [__output_code] # AOT ID: ['0_inference']\nV0523 19:48:14.698000 23993 torch/_inductor/codecache.py:1093] [0/0] [__output_code] from ctypes import c_void_p, c_long, c_int\nV0523 19:48:14.698000 23993 torch/_inductor/codecache.py:1093] [0/0] [__output_code] import torch\nV0523 19:48:14.698000 23993 torch/_inductor/codecache.py:1093] [0/0] [__output_code] import math\nV0523 19:48:14.698000 23993 torch/_inductor/codecache.py:1093] [0/0] [__output_code] import random\nV0523 19:48:14.698000 23993 torch/_inductor/codecache.py:1093] [0/0] [__output_code] import os\nV0523 19:48:14.698000 23993 torch/_inductor/codecache.py:1093] [0/0] [__output_code] import tempfile\nV0523 19:48:14.698000 23993 torch/_inductor/codecache.py:1093] [0/0] [__output_code] from math import inf, nan\nV0523 19:48:14.698000 23993 torch/_inductor/codecache.py:1093] [0/0] [__output_code] from cmath import nanj\nV0523 19:48:14.698000 23993 torch/_inductor/codecache.py:1093] [0/0] [__output_code] from torch._inductor.hooks import run_intermediate_hooks\nV0523 19:48:14.698000 23993 torch/_inductor/codecache.py:1093] [0/0] [__output_code] from torch._inductor.utils import maybe_profile\nV0523 19:48:14.698000 23993 torch/_inductor/codecache.py:1093] [0/0] [__output_code] from torch._inductor.codegen.memory_planning import _align as align\nV0523 19:48:14.698000 23993 torch/_inductor/codecache.py:1093] [0/0] [__output_code] from torch import device, empty_strided\nV0523 19:48:14.698000 23993 torch/_inductor/codecache.py:1093] [0/0] [__output_code] from torch._inductor.async_compile import AsyncCompile\nV0523 19:48:14.698000 23993 torch/_inductor/codecache.py:1093] [0/0] [__output_code] from torch._inductor.select_algorithm import extern_kernels\nV0523 19:48:14.698000 23993 torch/_inductor/codecache.py:1093] [0/0] [__output_code] from torch._inductor.codegen.multi_kernel import MultiKernelCall\nV0523 19:48:14.698000 23993 torch/_inductor/codecache.py:1093] [0/0] [__output_code] import triton\nV0523 19:48:14.698000 23993 torch/_inductor/codecache.py:1093] [0/0] [__output_code] import triton.language as tl\nV0523 19:48:14.698000 23993 torch/_inductor/codecache.py:1093] [0/0] [__output_code] from torch._inductor.runtime.triton_heuristics import start_graph, end_graph\nV0523 19:48:14.698000 23993 torch/_inductor/codecache.py:1093] [0/0] [__output_code] from torch._C import _cuda_getCurrentRawStream as get_raw_stream\nV0523 19:48:14.698000 23993 torch/_inductor/codecache.py:1093] [0/0] [__output_code] from torch._C import _cuda_getCurrentRawStream as get_raw_stream\nV0523 19:48:14.698000 23993 torch/_inductor/codecache.py:1093] [0/0] [__output_code]\nV0523 19:48:14.698000 23993 torch/_inductor/codecache.py:1093] [0/0] [__output_code] aten = torch.ops.aten\nV0523 19:48:14.698000 23993 torch/_inductor/codecache.py:1093] [0/0] [__output_code] inductor_ops = torch.ops.inductor\nV0523 19:48:14.698000 23993 torch/_inductor/codecache.py:1093] [0/0] [__output_code] _quantized = torch.ops._quantized\nV0523 19:48:14.698000 23993 torch/_inductor/codecache.py:1093] [0/0] [__output_code] assert_size_stride = torch._C._dynamo.guards.assert_size_stride\nV0523 19:48:14.698000 23993 torch/_inductor/codecache.py:1093] [0/0] [__output_code] empty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu\nV0523 19:48:14.698000 23993 torch/_inductor/codecache.py:1093] [0/0] [__output_code] empty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\nV0523 19:48:14.698000 23993 torch/_inductor/codecache.py:1093] [0/0] [__output_code] empty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu\nV0523 19:48:14.698000 23993 torch/_inductor/codecache.py:1093] [0/0] [__output_code] reinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor\nV0523 19:48:14.698000 23993 torch/_inductor/codecache.py:1093] [0/0] [__output_code] alloc_from_pool = torch.ops.inductor._alloc_from_pool\nV0523 19:48:14.698000 23993 torch/_inductor/codecache.py:1093] [0/0] [__output_code] async_compile = AsyncCompile()\nV0523 19:48:14.698000 23993 torch/_inductor/codecache.py:1093] [0/0] [__output_code] empty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p\nV0523 19:48:14.698000 23993 torch/_inductor/codecache.py:1093] [0/0] [__output_code]\nV0523 19:48:14.698000 23993 torch/_inductor/codecache.py:1093] [0/0] [__output_code]\nV0523 19:48:14.698000 23993 torch/_inductor/codecache.py:1093] [0/0] [__output_code] # kernel path: /tmp/torchinductor_ci-user/ld/cld7tar7n7kytdxqq7n73fjc5nptwpbw7wqmdbp24zf62axk3q3a.py\nV0523 19:48:14.698000 23993 torch/_inductor/codecache.py:1093] [0/0] [__output_code] # Topologically Sorted Source Nodes: [z, add_1], Original ATen: [aten.add]\nV0523 19:48:14.698000 23993 torch/_inductor/codecache.py:1093] [0/0] [__output_code] # Source node to ATen node mapping:\nV0523 19:48:14.698000 23993 torch/_inductor/codecache.py:1093] [0/0] [__output_code] #   add_1 => add_1\nV0523 19:48:14.698000 23993 torch/_inductor/codecache.py:1093] [0/0] [__output_code] #   z => add\nV0523 19:48:14.698000 23993 torch/_inductor/codecache.py:1093] [0/0] [__output_code] # Graph fragment:\nV0523 19:48:14.698000 23993 torch/_inductor/codecache.py:1093] [0/0] [__output_code] #   %add : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%arg0_1, %arg1_1), kwargs = {})\nV0523 19:48:14.698000 23993 torch/_inductor/codecache.py:1093] [0/0] [__output_code] #   %add_1 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%add, 2), kwargs = {})\nV0523 19:48:14.698000 23993 torch/_inductor/codecache.py:1093] [0/0] [__output_code] triton_poi_fused_add_0 = async_compile.triton('triton_poi_fused_add_0', '''\nV0523 19:48:14.698000 23993 torch/_inductor/codecache.py:1093] [0/0] [__output_code] import triton\nV0523 19:48:14.698000 23993 torch/_inductor/codecache.py:1093] [0/0] [__output_code] import triton.language as tl\nV0523 19:48:14.698000 23993 torch/_inductor/codecache.py:1093] [0/0] [__output_code]\nV0523 19:48:14.698000 23993 torch/_inductor/codecache.py:1093] [0/0] [__output_code] from torch._inductor.runtime import triton_helpers, triton_heuristics\nV0523 19:48:14.698000 23993 torch/_inductor/codecache.py:1093] [0/0] [__output_code] from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\nV0523 19:48:14.698000 23993 torch/_inductor/codecache.py:1093] [0/0] [__output_code] from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties\nV0523 19:48:14.698000 23993 torch/_inductor/codecache.py:1093] [0/0] [__output_code] triton_helpers.set_driver_to_gpu()\nV0523 19:48:14.698000 23993 torch/_inductor/codecache.py:1093] [0/0] [__output_code]\nV0523 19:48:14.698000 23993 torch/_inductor/codecache.py:1093] [0/0] [__output_code] @triton_heuristics.pointwise(\nV0523 19:48:14.698000 23993 torch/_inductor/codecache.py:1093] [0/0] [__output_code]     size_hints={'x': 4},\nV0523 19:48:14.698000 23993 torch/_inductor/codecache.py:1093] [0/0] [__output_code]     filename=__file__,\nV0523 19:48:14.698000 23993 torch/_inductor/codecache.py:1093] [0/0] [__output_code]     triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp32', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=80, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},\nV0523 19:48:14.698000 23993 torch/_inductor/codecache.py:1093] [0/0] [__output_code]     inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_0', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 2, 'num_reduction': 0, 'backend_hash': '1E2C16421D4C3DBA4AD92BFC4278A3CB24C43DEDA6EE7FF9E3FBB1DBB80802DB', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},\nV0523 19:48:14.698000 23993 torch/_inductor/codecache.py:1093] [0/0] [__output_code]     min_elem_per_thread=0\nV0523 19:48:14.698000 23993 torch/_inductor/codecache.py:1093] [0/0] [__output_code] )\nV0523 19:48:14.698000 23993 torch/_inductor/codecache.py:1093] [0/0] [__output_code] @triton.jit\nV0523 19:48:14.698000 23993 torch/_inductor/codecache.py:1093] [0/0] [__output_code] def triton_poi_fused_add_0(in_ptr0, in_ptr1, out_ptr0, xnumel, XBLOCK : tl.constexpr):\nV0523 19:48:14.698000 23993 torch/_inductor/codecache.py:1093] [0/0] [__output_code]     xnumel = 4\nV0523 19:48:14.698000 23993 torch/_inductor/codecache.py:1093] [0/0] [__output_code]     xoffset = tl.program_id(0) * XBLOCK\nV0523 19:48:14.698000 23993 torch/_inductor/codecache.py:1093] [0/0] [__output_code]     xindex = xoffset + tl.arange(0, XBLOCK)[:]\nV0523 19:48:14.698000 23993 torch/_inductor/codecache.py:1093] [0/0] [__output_code]     xmask = xindex < xnumel\nV0523 19:48:14.698000 23993 torch/_inductor/codecache.py:1093] [0/0] [__output_code]     x0 = xindex\nV0523 19:48:14.698000 23993 torch/_inductor/codecache.py:1093] [0/0] [__output_code]     tmp0 = tl.load(in_ptr0 + (x0), xmask)\nV0523 19:48:14.698000 23993 torch/_inductor/codecache.py:1093] [0/0] [__output_code]     tmp1 = tl.load(in_ptr1 + (x0), xmask)\nV0523 19:48:14.698000 23993 torch/_inductor/codecache.py:1093] [0/0] [__output_code]     tmp2 = tmp0 + tmp1\nV0523 19:48:14.698000 23993 torch/_inductor/codecache.py:1093] [0/0] [__output_code]     tmp3 = 2.0\nV0523 19:48:14.698000 23993 torch/_inductor/codecache.py:1093] [0/0] [__output_code]     tmp4 = tmp2 + tmp3\nV0523 19:48:14.698000 23993 torch/_inductor/codecache.py:1093] [0/0] [__output_code]     tl.store(out_ptr0 + (x0), tmp4, xmask)\nV0523 19:48:14.698000 23993 torch/_inductor/codecache.py:1093] [0/0] [__output_code] ''', device_str='cuda')\nV0523 19:48:14.698000 23993 torch/_inductor/codecache.py:1093] [0/0] [__output_code]\nV0523 19:48:14.698000 23993 torch/_inductor/codecache.py:1093] [0/0] [__output_code]\nV0523 19:48:14.698000 23993 torch/_inductor/codecache.py:1093] [0/0] [__output_code] async_compile.wait(globals())\nV0523 19:48:14.698000 23993 torch/_inductor/codecache.py:1093] [0/0] [__output_code] del async_compile\nV0523 19:48:14.698000 23993 torch/_inductor/codecache.py:1093] [0/0] [__output_code]\nV0523 19:48:14.698000 23993 torch/_inductor/codecache.py:1093] [0/0] [__output_code] def call(args):\nV0523 19:48:14.698000 23993 torch/_inductor/codecache.py:1093] [0/0] [__output_code]     arg0_1, arg1_1 = args\nV0523 19:48:14.698000 23993 torch/_inductor/codecache.py:1093] [0/0] [__output_code]     args.clear()\nV0523 19:48:14.698000 23993 torch/_inductor/codecache.py:1093] [0/0] [__output_code]     assert_size_stride(arg0_1, (2, 2), (2, 1))\nV0523 19:48:14.698000 23993 torch/_inductor/codecache.py:1093] [0/0] [__output_code]     assert_size_stride(arg1_1, (2, 2), (2, 1))\nV0523 19:48:14.698000 23993 torch/_inductor/codecache.py:1093] [0/0] [__output_code]     with torch.cuda._DeviceGuard(0):\nV0523 19:48:14.698000 23993 torch/_inductor/codecache.py:1093] [0/0] [__output_code]         torch.cuda.set_device(0)\nV0523 19:48:14.698000 23993 torch/_inductor/codecache.py:1093] [0/0] [__output_code]         buf0 = empty_strided_cuda((2, 2), (2, 1), torch.float32)\nV0523 19:48:14.698000 23993 torch/_inductor/codecache.py:1093] [0/0] [__output_code]         # Topologically Sorted Source Nodes: [z, add_1], Original ATen: [aten.add]\nV0523 19:48:14.698000 23993 torch/_inductor/codecache.py:1093] [0/0] [__output_code]         stream0 = get_raw_stream(0)\nV0523 19:48:14.698000 23993 torch/_inductor/codecache.py:1093] [0/0] [__output_code]         triton_poi_fused_add_0.run(arg0_1, arg1_1, buf0, 4, stream=stream0)\nV0523 19:48:14.698000 23993 torch/_inductor/codecache.py:1093] [0/0] [__output_code]         del arg0_1\nV0523 19:48:14.698000 23993 torch/_inductor/codecache.py:1093] [0/0] [__output_code]         del arg1_1\nV0523 19:48:14.698000 23993 torch/_inductor/codecache.py:1093] [0/0] [__output_code]     return (buf0, )\nV0523 19:48:14.698000 23993 torch/_inductor/codecache.py:1093] [0/0] [__output_code]\nV0523 19:48:14.698000 23993 torch/_inductor/codecache.py:1093] [0/0] [__output_code]\nV0523 19:48:14.698000 23993 torch/_inductor/codecache.py:1093] [0/0] [__output_code] def benchmark_compiled_module(times=10, repeat=10):\nV0523 19:48:14.698000 23993 torch/_inductor/codecache.py:1093] [0/0] [__output_code]     from torch._dynamo.testing import rand_strided\nV0523 19:48:14.698000 23993 torch/_inductor/codecache.py:1093] [0/0] [__output_code]     from torch._inductor.utils import print_performance\nV0523 19:48:14.698000 23993 torch/_inductor/codecache.py:1093] [0/0] [__output_code]     arg0_1 = rand_strided((2, 2), (2, 1), device='cuda:0', dtype=torch.float32)\nV0523 19:48:14.698000 23993 torch/_inductor/codecache.py:1093] [0/0] [__output_code]     arg1_1 = rand_strided((2, 2), (2, 1), device='cuda:0', dtype=torch.float32)\nV0523 19:48:14.698000 23993 torch/_inductor/codecache.py:1093] [0/0] [__output_code]     fn = lambda: call([arg0_1, arg1_1])\nV0523 19:48:14.698000 23993 torch/_inductor/codecache.py:1093] [0/0] [__output_code]     return print_performance(fn, times=times, repeat=repeat)\nV0523 19:48:14.698000 23993 torch/_inductor/codecache.py:1093] [0/0] [__output_code]\nV0523 19:48:14.698000 23993 torch/_inductor/codecache.py:1093] [0/0] [__output_code]\nV0523 19:48:14.698000 23993 torch/_inductor/codecache.py:1093] [0/0] [__output_code] if __name__ == \"__main__\":\nV0523 19:48:14.698000 23993 torch/_inductor/codecache.py:1093] [0/0] [__output_code]     from torch._inductor.wrapper_benchmark import compiled_module_main\nV0523 19:48:14.698000 23993 torch/_inductor/codecache.py:1093] [0/0] [__output_code]     compiled_module_main('None', benchmark_compiled_module)\nV0523 19:48:14.698000 23993 torch/_inductor/codecache.py:1093] [0/0] [__output_code]\nV0523 19:48:14.704000 23993 torch/_inductor/codecache.py:1094] [0/0] [__output_code] Output code written to: /tmp/torchinductor_ci-user/nk/cnk55csixpane7aredk4kvfxz3fx2bb7zgzf4vpzqkzufdznzojb.py\n============================================"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/beginner/nn_tutorial.html",
    "title": "What is torch.nn really?\u00b6",
    "code_snippets": [
      "from pathlib import Path\nimport requests\n\nDATA_PATH = Path(\"data\")\nPATH = DATA_PATH / \"mnist\"\n\nPATH.mkdir(parents=True, exist_ok=True)\n\nURL = \"https://github.com/pytorch/tutorials/raw/main/_static/\"\nFILENAME = \"mnist.pkl.gz\"\n\nif not (PATH / FILENAME).exists():\n        content = requests.get(URL + FILENAME).content\n        (PATH / FILENAME).open(\"wb\").write(content)",
      "import pickle\nimport gzip\n\nwith gzip.open((PATH / FILENAME).as_posix(), \"rb\") as f:\n        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding=\"latin-1\")",
      "from matplotlib import pyplot\nimport numpy as np\n\npyplot.imshow(x_train[0].reshape((28, 28)), cmap=\"gray\")\n# ``pyplot.show()`` only if not on Colab\ntry:\n    import google.colab\nexcept ImportError:\n    pyplot.show()\nprint(x_train.shape)",
      "import torch\n\nx_train, y_train, x_valid, y_valid = map(\n    torch.tensor, (x_train, y_train, x_valid, y_valid)\n)\nn, c = x_train.shape\nprint(x_train, y_train)\nprint(x_train.shape)\nprint(y_train.min(), y_train.max())",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.]]) tensor([5, 0, 4,  ..., 8, 4, 8])\ntorch.Size([50000, 784])\ntensor(0) tensor(9)",
      "import math\n\nweights = torch.randn(784, 10) / math.sqrt(784)\nweights.requires_grad_()\nbias = torch.zeros(10, requires_grad=True)",
      "def log_softmax(x):\n    return x - x.exp().sum(-1).log().unsqueeze(-1)\n\ndef model(xb):\n    return log_softmax(xb @ weights + bias)",
      "tensor([-2.2821, -2.2422, -1.8533, -2.1702, -2.2542, -2.6113, -2.4210, -2.4552,\n        -2.7683, -2.2539], grad_fn=<SelectBackward0>) torch.Size([64, 10])",
      "def nll(input, target):\n    return -input[range(target.shape[0]), target].mean()\n\nloss_func = nll",
      "def accuracy(out, yb):\n    preds = torch.argmax(out, dim=1)\n    return (preds == yb).float().mean()",
      "from IPython.core.debugger import set_trace\n\nlr = 0.5  # learning rate\nepochs = 2  # how many epochs to train for\n\nfor epoch in range(epochs):\n    for i in range((n - 1) // bs + 1):\n        #         set_trace()\n        start_i = i * bs\n        end_i = start_i + bs\n        xb = x_train[start_i:end_i]\n        yb = y_train[start_i:end_i]\n        pred = model(xb)\n        loss = loss_func(pred, yb)\n\n        loss.backward()\n        with torch.no_grad():\n            weights -= weights.grad * lr\n            bias -= bias.grad * lr\n            weights.grad.zero_()\n            bias.grad.zero_()",
      "import torch.nn.functional as F\n\nloss_func = F.cross_entropy\n\ndef model(xb):\n    return xb @ weights + bias",
      "from torch import nn\n\nclass Mnist_Logistic(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.weights = nn.Parameter(torch.randn(784, 10) / math.sqrt(784))\n        self.bias = nn.Parameter(torch.zeros(10))\n\n    def forward(self, xb):\n        return xb @ self.weights + self.bias",
      "with torch.no_grad():\n    weights -= weights.grad * lr\n    bias -= bias.grad * lr\n    weights.grad.zero_()\n    bias.grad.zero_()",
      "with torch.no_grad():\n    for p in model.parameters(): p -= p.grad * lr\n    model.zero_grad()",
      "def fit():\n    for epoch in range(epochs):\n        for i in range((n - 1) // bs + 1):\n            start_i = i * bs\n            end_i = start_i + bs\n            xb = x_train[start_i:end_i]\n            yb = y_train[start_i:end_i]\n            pred = model(xb)\n            loss = loss_func(pred, yb)\n\n            loss.backward()\n            with torch.no_grad():\n                for p in model.parameters():\n                    p -= p.grad * lr\n                model.zero_grad()\n\nfit()",
      "class Mnist_Logistic(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.lin = nn.Linear(784, 10)\n\n    def forward(self, xb):\n        return self.lin(xb)",
      "with torch.no_grad():\n    for p in model.parameters(): p -= p.grad * lr\n    model.zero_grad()",
      "from torch import optim",
      "def get_model():\n    model = Mnist_Logistic()\n    return model, optim.SGD(model.parameters(), lr=lr)\n\nmodel, opt = get_model()\nprint(loss_func(model(xb), yb))\n\nfor epoch in range(epochs):\n    for i in range((n - 1) // bs + 1):\n        start_i = i * bs\n        end_i = start_i + bs\n        xb = x_train[start_i:end_i]\n        yb = y_train[start_i:end_i]\n        pred = model(xb)\n        loss = loss_func(pred, yb)\n\n        loss.backward()\n        opt.step()\n        opt.zero_grad()\n\nprint(loss_func(model(xb), yb))",
      "from torch.utils.data import TensorDataset",
      "from torch.utils.data import DataLoader\n\ntrain_ds = TensorDataset(x_train, y_train)\ntrain_dl = DataLoader(train_ds, batch_size=bs)",
      "model, opt = get_model()\n\nfor epoch in range(epochs):\n    model.train()\n    for xb, yb in train_dl:\n        pred = model(xb)\n        loss = loss_func(pred, yb)\n\n        loss.backward()\n        opt.step()\n        opt.zero_grad()\n\n    model.eval()\n    with torch.no_grad():\n        valid_loss = sum(loss_func(model(xb), yb) for xb, yb in valid_dl)\n\n    print(epoch, valid_loss / len(valid_dl))",
      "def loss_batch(model, loss_func, xb, yb, opt=None):\n    loss = loss_func(model(xb), yb)\n\n    if opt is not None:\n        loss.backward()\n        opt.step()\n        opt.zero_grad()\n\n    return loss.item(), len(xb)",
      "import numpy as np\n\ndef fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n    for epoch in range(epochs):\n        model.train()\n        for xb, yb in train_dl:\n            loss_batch(model, loss_func, xb, yb, opt)\n\n        model.eval()\n        with torch.no_grad():\n            losses, nums = zip(\n                *[loss_batch(model, loss_func, xb, yb) for xb, yb in valid_dl]\n            )\n        val_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)\n\n        print(epoch, val_loss)",
      "def get_data(train_ds, valid_ds, bs):\n    return (\n        DataLoader(train_ds, batch_size=bs, shuffle=True),\n        DataLoader(valid_ds, batch_size=bs * 2),\n    )",
      "class Mnist_CNN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1)\n        self.conv2 = nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1)\n        self.conv3 = nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1)\n\n    def forward(self, xb):\n        xb = xb.view(-1, 1, 28, 28)\n        xb = F.relu(self.conv1(xb))\n        xb = F.relu(self.conv2(xb))\n        xb = F.relu(self.conv3(xb))\n        xb = F.avg_pool2d(xb, 4)\n        return xb.view(-1, xb.size(1))\n\nlr = 0.1",
      "class Lambda(nn.Module):\n    def __init__(self, func):\n        super().__init__()\n        self.func = func\n\n    def forward(self, x):\n        return self.func(x)\n\n\ndef preprocess(x):\n    return x.view(-1, 1, 28, 28)",
      "def preprocess(x, y):\n    return x.view(-1, 1, 28, 28), y\n\n\nclass WrappedDataLoader:\n    def __init__(self, dl, func):\n        self.dl = dl\n        self.func = func\n\n    def __len__(self):\n        return len(self.dl)\n\n    def __iter__(self):\n        for b in self.dl:\n            yield (self.func(*b))\n\ntrain_dl, valid_dl = get_data(train_ds, valid_ds, bs)\ntrain_dl = WrappedDataLoader(train_dl, preprocess)\nvalid_dl = WrappedDataLoader(valid_dl, preprocess)",
      "# If the current accelerator is available, we will use it. Otherwise, we use the CPU.\ndevice = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\nprint(f\"Using {device} device\")",
      "def preprocess(x, y):\n    return x.view(-1, 1, 28, 28).to(device), y.to(device)\n\n\ntrain_dl, valid_dl = get_data(train_ds, valid_ds, bs)\ntrain_dl = WrappedDataLoader(train_dl, preprocess)\nvalid_dl = WrappedDataLoader(valid_dl, preprocess)"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/recipes/deployment_with_flask.html",
    "title": "Deploying with Flask\u00b6",
    "code_snippets": [
      "git clone https://github.com/pytorch/serve\ncp serve/examples/image_classifier/kitten.jpg .\ncp serve/examples/image_classifier/index_to_name.json .",
      "import torchvision.models as models\nimport torchvision.transforms as transforms\nfrom PIL import Image\nfrom flask import Flask, jsonify, request",
      "def transform_image(infile):\n    input_transforms = [transforms.Resize(255),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406],\n            [0.229, 0.224, 0.225])]\n    my_transforms = transforms.Compose(input_transforms)\n    image = Image.open(infile)\n    timg = my_transforms(image)\n    timg.unsqueeze_(0)\n    return timg",
      "def get_prediction(input_tensor):\n    outputs = model.forward(input_tensor)\n    _, y_hat = outputs.max(1)\n    prediction = y_hat.item()\n    return prediction",
      "def render_prediction(prediction_idx):\n    stridx = str(prediction_idx)\n    class_name = 'Unknown'\n    if img_class_map is not None:\n        if stridx in img_class_map is not None:\n            class_name = img_class_map[stridx][1]\n\n    return prediction_idx, class_name",
      "import io\nimport json\nimport os\n\nimport torchvision.models as models\nimport torchvision.transforms as transforms\nfrom PIL import Image\nfrom flask import Flask, jsonify, request\n\n\napp = Flask(__name__)\nmodel = models.densenet121(pretrained=True)               # Trained on 1000 classes from ImageNet\nmodel.eval()                                              # Turns off autograd\n\n\n\nimg_class_map = None\nmapping_file_path = 'index_to_name.json'                  # Human-readable names for Imagenet classes\nif os.path.isfile(mapping_file_path):\n    with open (mapping_file_path) as f:\n        img_class_map = json.load(f)\n\n\n\n# Transform input into the form our model expects\ndef transform_image(infile):\n    input_transforms = [transforms.Resize(255),           # We use multiple TorchVision transforms to ready the image\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406],       # Standard normalization for ImageNet model input\n            [0.229, 0.224, 0.225])]\n    my_transforms = transforms.Compose(input_transforms)\n    image = Image.open(infile)                            # Open the image file\n    timg = my_transforms(image)                           # Transform PIL image to appropriately-shaped PyTorch tensor\n    timg.unsqueeze_(0)                                    # PyTorch models expect batched input; create a batch of 1\n    return timg\n\n\n# Get a prediction\ndef get_prediction(input_tensor):\n    outputs = model.forward(input_tensor)                 # Get likelihoods for all ImageNet classes\n    _, y_hat = outputs.max(1)                             # Extract the most likely class\n    prediction = y_hat.item()                             # Extract the int value from the PyTorch tensor\n    return prediction\n\n# Make the prediction human-readable\ndef render_prediction(prediction_idx):\n    stridx = str(prediction_idx)\n    class_name = 'Unknown'\n    if img_class_map is not None:\n        if stridx in img_class_map is not None:\n            class_name = img_class_map[stridx][1]\n\n    return prediction_idx, class_name\n\n\n@app.route('/', methods=['GET'])\ndef root():\n    return jsonify({'msg' : 'Try POSTing to the /predict endpoint with an RGB image attachment'})\n\n\n@app.route('/predict', methods=['POST'])\ndef predict():\n    if request.method == 'POST':\n        file = request.files['file']\n        if file is not None:\n            input_tensor = transform_image(file)\n            prediction_idx = get_prediction(input_tensor)\n            class_id, class_name = render_prediction(prediction_idx)\n            return jsonify({'class_id': class_id, 'class_name': class_name})\n\n\nif __name__ == '__main__':\n    app.run()",
      "{\"class_id\":285,\"class_name\":\"Egyptian_cat\"}"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/advanced/torch_script_custom_ops.html",
    "title": "Extending TorchScript with Custom C++ Operators\u00b6",
    "code_snippets": [
      "TORCH_LIBRARY(my_ops, m) {\n  m.def(\"warp_perspective\", warp_perspective);\n}",
      "$ mkdir build\n$ cd build\n$ cmake -DCMAKE_PREFIX_PATH=\"$(python -c 'import torch.utils; print(torch.utils.cmake_prefix_path)')\" ..\n-- The C compiler identification is GNU 5.4.0\n-- The CXX compiler identification is GNU 5.4.0\n-- Check for working C compiler: /usr/bin/cc\n-- Check for working C compiler: /usr/bin/cc -- works\n-- Detecting C compiler ABI info\n-- Detecting C compiler ABI info - done\n-- Detecting C compile features\n-- Detecting C compile features - done\n-- Check for working CXX compiler: /usr/bin/c++\n-- Check for working CXX compiler: /usr/bin/c++ -- works\n-- Detecting CXX compiler ABI info\n-- Detecting CXX compiler ABI info - done\n-- Detecting CXX compile features\n-- Detecting CXX compile features - done\n-- Looking for pthread.h\n-- Looking for pthread.h - found\n-- Looking for pthread_create\n-- Looking for pthread_create - not found\n-- Looking for pthread_create in pthreads\n-- Looking for pthread_create in pthreads - not found\n-- Looking for pthread_create in pthread\n-- Looking for pthread_create in pthread - found\n-- Found Threads: TRUE\n-- Found torch: /libtorch/lib/libtorch.so\n-- Configuring done\n-- Generating done\n-- Build files have been written to: /warp_perspective/build\n$ make -j\nScanning dependencies of target warp_perspective\n[ 50%] Building CXX object CMakeFiles/warp_perspective.dir/op.cpp.o\n[100%] Linking CXX shared library libwarp_perspective.so\n[100%] Built target warp_perspective",
      "import torch\ntorch.ops.load_library(\"build/libwarp_perspective.so\")\nprint(torch.ops.my_ops.warp_perspective)",
      "import torch\ntorch.ops.load_library(\"build/libwarp_perspective.so\")\nprint(torch.ops.my_ops.warp_perspective(torch.randn(32, 32), torch.rand(3, 3)))",
      "def compute(x, y, z):\n    return x.matmul(y) + torch.relu(z)",
      "inputs = [torch.randn(4, 8), torch.randn(8, 5), torch.randn(4, 5)]\ntrace = torch.jit.trace(compute, inputs)\nprint(trace.graph)",
      "def compute(x, y, z):\n    x = torch.ops.my_ops.warp_perspective(x, torch.eye(3))\n    return x.matmul(y) + torch.relu(z)",
      "inputs = [torch.randn(4, 8), torch.randn(8, 5), torch.randn(8, 5)]\ntrace = torch.jit.trace(compute, inputs)\nprint(trace.graph)",
      "def compute(x, y):\n  if bool(x[0][0] == 42):\n      z = 5\n  else:\n      z = 10\n  return x.matmul(y) + z",
      "@torch.jit.script\ndef compute(x, y):\n  if bool(x[0][0] == 42):\n      z = 5\n  else:\n      z = 10\n  return x.matmul(y) + z",
      "torch.ops.load_library(\"libwarp_perspective.so\")\n\n@torch.jit.script\ndef compute(x, y):\n  if bool(x[0] == 42):\n      z = 5\n  else:\n      z = 10\n  x = torch.ops.my_ops.warp_perspective(x, torch.eye(3))\n  return x.matmul(y) + z",
      "$ mkdir build\n$ cd build\n$ cmake -DCMAKE_PREFIX_PATH=\"$(python -c 'import torch.utils; print(torch.utils.cmake_prefix_path)')\" ..\n-- The C compiler identification is GNU 5.4.0\n-- The CXX compiler identification is GNU 5.4.0\n-- Check for working C compiler: /usr/bin/cc\n-- Check for working C compiler: /usr/bin/cc -- works\n-- Detecting C compiler ABI info\n-- Detecting C compiler ABI info - done\n-- Detecting C compile features\n-- Detecting C compile features - done\n-- Check for working CXX compiler: /usr/bin/c++\n-- Check for working CXX compiler: /usr/bin/c++ -- works\n-- Detecting CXX compiler ABI info\n-- Detecting CXX compiler ABI info - done\n-- Detecting CXX compile features\n-- Detecting CXX compile features - done\n-- Looking for pthread.h\n-- Looking for pthread.h - found\n-- Looking for pthread_create\n-- Looking for pthread_create - not found\n-- Looking for pthread_create in pthreads\n-- Looking for pthread_create in pthreads - not found\n-- Looking for pthread_create in pthread\n-- Looking for pthread_create in pthread - found\n-- Found Threads: TRUE\n-- Found torch: /libtorch/lib/libtorch.so\n-- Configuring done\n-- Generating done\n-- Build files have been written to: /example_app/build\n$ make -j\nScanning dependencies of target example_app\n[ 50%] Building CXX object CMakeFiles/example_app.dir/main.cpp.o\n[100%] Linking CXX executable example_app\n[100%] Built target example_app",
      "torch.ops.load_library(\"libwarp_perspective.so\")\n\n@torch.jit.script\ndef compute(x, y):\n  if bool(x[0][0] == 42):\n      z = 5\n  else:\n      z = 10\n  x = torch.ops.my_ops.warp_perspective(x, torch.eye(3))\n  return x.matmul(y) + z\n\ncompute.save(\"example.pt\")",
      "$ mkdir build\n$ cd build\n$ cmake -DCMAKE_PREFIX_PATH=\"$(python -c 'import torch.utils; print(torch.utils.cmake_prefix_path)')\" ..\n-- The C compiler identification is GNU 5.4.0\n-- The CXX compiler identification is GNU 5.4.0\n-- Check for working C compiler: /usr/bin/cc\n-- Check for working C compiler: /usr/bin/cc -- works\n-- Detecting C compiler ABI info\n-- Detecting C compiler ABI info - done\n-- Detecting C compile features\n-- Detecting C compile features - done\n-- Check for working CXX compiler: /usr/bin/c++\n-- Check for working CXX compiler: /usr/bin/c++ -- works\n-- Detecting CXX compiler ABI info\n-- Detecting CXX compiler ABI info - done\n-- Detecting CXX compile features\n-- Detecting CXX compile features - done\n-- Looking for pthread.h\n-- Looking for pthread.h - found\n-- Looking for pthread_create\n-- Looking for pthread_create - not found\n-- Looking for pthread_create in pthreads\n-- Looking for pthread_create in pthreads - not found\n-- Looking for pthread_create in pthread\n-- Looking for pthread_create in pthread - found\n-- Found Threads: TRUE\n-- Found torch: /libtorch/lib/libtorch.so\n-- Configuring done\n-- Generating done\n-- Build files have been written to: /warp_perspective/example_app/build\n$ make -j\nScanning dependencies of target warp_perspective\n[ 25%] Building CXX object warp_perspective/CMakeFiles/warp_perspective.dir/op.cpp.o\n[ 50%] Linking CXX shared library libwarp_perspective.so\n[ 50%] Built target warp_perspective\nScanning dependencies of target example_app\n[ 75%] Building CXX object CMakeFiles/example_app.dir/main.cpp.o\n[100%] Linking CXX executable example_app\n[100%] Built target example_app",
      "import torch.utils.cpp_extension\n\ntorch.utils.cpp_extension.load(\n    name=\"warp_perspective\",\n    sources=[\"op.cpp\"],\n    extra_ldflags=[\"-lopencv_core\", \"-lopencv_imgproc\"],\n    is_python_module=False,\n    verbose=True\n)\n\nprint(torch.ops.my_ops.warp_perspective)",
      "import torch\nimport torch.utils.cpp_extension\n\nop_source = \"\"\"\n#include <opencv2/opencv.hpp>\n#include <torch/script.h>\n\ntorch::Tensor warp_perspective(torch::Tensor image, torch::Tensor warp) {\n  cv::Mat image_mat(/*rows=*/image.size(0),\n                    /*cols=*/image.size(1),\n                    /*type=*/CV_32FC1,\n                    /*data=*/image.data<float>());\n  cv::Mat warp_mat(/*rows=*/warp.size(0),\n                   /*cols=*/warp.size(1),\n                   /*type=*/CV_32FC1,\n                   /*data=*/warp.data<float>());\n\n  cv::Mat output_mat;\n  cv::warpPerspective(image_mat, output_mat, warp_mat, /*dsize=*/{64, 64});\n\n  torch::Tensor output =\n    torch::from_blob(output_mat.ptr<float>(), /*sizes=*/{64, 64});\n  return output.clone();\n}\n\nTORCH_LIBRARY(my_ops, m) {\n  m.def(\"warp_perspective\", &warp_perspective);\n}\n\"\"\"\n\ntorch.utils.cpp_extension.load_inline(\n    name=\"warp_perspective\",\n    cpp_sources=op_source,\n    extra_ldflags=[\"-lopencv_core\", \"-lopencv_imgproc\"],\n    is_python_module=False,\n    verbose=True,\n)\n\nprint(torch.ops.my_ops.warp_perspective)",
      "from setuptools import setup\nfrom torch.utils.cpp_extension import BuildExtension, CppExtension\n\nsetup(\n    name=\"warp_perspective\",\n    ext_modules=[\n        CppExtension(\n            \"warp_perspective\",\n            [\"example_app/warp_perspective/op.cpp\"],\n            libraries=[\"opencv_core\", \"opencv_imgproc\"],\n        )\n    ],\n    cmdclass={\"build_ext\": BuildExtension.with_options(no_python_abi_suffix=True)},\n)",
      ">>> import torch\n>>> torch.ops.load_library(\"warp_perspective.so\")\n>>> print(torch.ops.my_ops.warp_perspective)\n<built-in method custom::warp_perspective of PyCapsule object at 0x7ff51c5b7bd0>"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/recipes/recipes/reasoning_about_shapes.html",
    "title": "Reasoning about Shapes in PyTorch\u00b6",
    "code_snippets": [
      "import torch\nimport timeit\n\nt = torch.rand(2, 3, 10, 10, device=\"meta\")\nconv = torch.nn.Conv2d(3, 5, 2, device=\"meta\")\nstart = timeit.default_timer()\nout = conv(t)\nend = timeit.default_timer()\n\nprint(out)\nprint(f\"Time taken: {end-start}\")",
      "t_large = torch.rand(2**10, 3, 2**16, 2**16, device=\"meta\")\nstart = timeit.default_timer()\nout = conv(t_large)\nend = timeit.default_timer()\n\nprint(out)\nprint(f\"Time taken: {end-start}\")",
      "import torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Net(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 6, 5)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = torch.flatten(x, 1) # flatten all dimensions except batch\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x",
      "def fw_hook(module, input, output):\n    print(f\"Shape of output to {module} is {output.shape}.\")\n\n\n# Any tensor created within this torch.device context manager will be\n# on the meta device.\nwith torch.device(\"meta\"):\n    net = Net()\n    inp = torch.randn((1024, 3, 32, 32))\n\nfor name, layer in net.named_modules():\n    layer.register_forward_hook(fw_hook)\n\nout = net(inp)",
      "Shape of output to Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1)) is torch.Size([1024, 6, 28, 28]).\nShape of output to MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) is torch.Size([1024, 6, 14, 14]).\nShape of output to Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1)) is torch.Size([1024, 16, 10, 10]).\nShape of output to MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) is torch.Size([1024, 16, 5, 5]).\nShape of output to Linear(in_features=400, out_features=120, bias=True) is torch.Size([1024, 120]).\nShape of output to Linear(in_features=120, out_features=84, bias=True) is torch.Size([1024, 84]).\nShape of output to Linear(in_features=84, out_features=10, bias=True) is torch.Size([1024, 10]).\nShape of output to Net(\n  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n  (fc1): Linear(in_features=400, out_features=120, bias=True)\n  (fc2): Linear(in_features=120, out_features=84, bias=True)\n  (fc3): Linear(in_features=84, out_features=10, bias=True)\n) is torch.Size([1024, 10])."
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/recipes/compiling_optimizer_lr_scheduler.html",
    "title": "(beta) Running the compiled optimizer with an LR Scheduler\u00b6",
    "code_snippets": [
      "import torch\n\n# Create simple model\nmodel = torch.nn.Sequential(\n    *[torch.nn.Linear(1024, 1024, False, device=\"cuda\") for _ in range(10)]\n)\ninput = torch.rand(1024, device=\"cuda\")\n\n# run forward pass\noutput = model(input)\n\n# run backward to populate the grads for our optimizer below\noutput.sum().backward()",
      "# exit cleanly if we are on a device that doesn't support ``torch.compile``\nif torch.cuda.get_device_capability() < (7, 0):\n    print(\"Exiting because torch.compile is not supported on this device.\")\n    import sys\n    sys.exit(0)\n\n# !!! IMPORTANT !!! Wrap the lr in a Tensor if we are pairing the\n# the optimizer with an LR Scheduler.\n# Without this, torch.compile will recompile as the value of the LR\n# changes.\nopt = torch.optim.Adam(model.parameters(), lr=torch.tensor(0.01))\nsched = torch.optim.lr_scheduler.LinearLR(opt, total_iters=5)\n\n@torch.compile(fullgraph=False)\ndef fn():\n    opt.step()\n    sched.step()\n\n\n# Warmup runs to compile the function\nfor _ in range(5):\n    fn()\n    print(opt.param_groups[0][\"lr\"])",
      "# No longer wrap the LR in a tensor here\nopt = torch.optim.Adam(model.parameters(), lr=0.01)\nsched = torch.optim.lr_scheduler.LinearLR(opt, total_iters=5)\n\n@torch.compile(fullgraph=False)\ndef fn():\n    opt.step()\n    sched.step()\n\n# Setup logging to view recompiles\ntorch._logging.set_logs(recompiles=True)\n\n# Warmup runs to compile the function\n# We will now recompile on each iteration\n# as the value of the lr is mutated.\nfor _ in range(5):\n    fn()"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html",
    "title": "PyTorch Profiler\u00b6",
    "code_snippets": [
      "import torch\nimport torchvision.models as models\nfrom torch.profiler import profile, record_function, ProfilerActivity",
      "model = models.resnet18()\ninputs = torch.randn(5, 3, 224, 224)",
      "if torch.cuda.is_available():\n    device = 'cuda'\nelif torch.xpu.is_available():\n    device = 'xpu'\nelse:\n    print('Neither CUDA nor XPU devices are available to demonstrate profiling on acceleration devices')\n    import sys\n    sys.exit(0)\n\nactivities = [ProfilerActivity.CPU, ProfilerActivity.CUDA, ProfilerActivity.XPU]\nsort_by_keyword = device + \"_time_total\"\n\nmodel = models.resnet18().to(device)\ninputs = torch.randn(5, 3, 224, 224).to(device)\n\nwith profile(activities=activities, record_shapes=True) as prof:\n    with record_function(\"model_inference\"):\n        model(inputs)\n\nprint(prof.key_averages().table(sort_by=sort_by_keyword, row_limit=10))",
      "model = models.resnet18()\ninputs = torch.randn(5, 3, 224, 224)\n\nwith profile(activities=[ProfilerActivity.CPU],\n        profile_memory=True, record_shapes=True) as prof:\n    model(inputs)\n\nprint(prof.key_averages().table(sort_by=\"self_cpu_memory_usage\", row_limit=10))\n\n# (omitting some columns)\n# ---------------------------------  ------------  ------------  ------------\n#                              Name       CPU Mem  Self CPU Mem    # of Calls\n# ---------------------------------  ------------  ------------  ------------\n#                       aten::empty      94.79 Mb      94.79 Mb           121\n#     aten::max_pool2d_with_indices      11.48 Mb      11.48 Mb             1\n#                       aten::addmm      19.53 Kb      19.53 Kb             1\n#               aten::empty_strided         572 b         572 b            25\n#                     aten::resize_         240 b         240 b             6\n#                         aten::abs         480 b         240 b             4\n#                         aten::add         160 b         160 b            20\n#               aten::masked_select         120 b         112 b             1\n#                          aten::ne         122 b          53 b             6\n#                          aten::eq          60 b          30 b             2\n# ---------------------------------  ------------  ------------  ------------\n# Self CPU time total: 53.064ms\n\nprint(prof.key_averages().table(sort_by=\"cpu_memory_usage\", row_limit=10))",
      "device = 'cuda'\n\nactivities = [ProfilerActivity.CPU, ProfilerActivity.CUDA, ProfilerActivity.XPU]\n\nmodel = models.resnet18().to(device)\ninputs = torch.randn(5, 3, 224, 224).to(device)\n\nwith profile(activities=activities) as prof:\n    model(inputs)\n\nprof.export_chrome_trace(\"trace.json\")",
      "from torch.profiler import schedule\n\nmy_schedule = schedule(\n    skip_first=10,\n    wait=5,\n    warmup=1,\n    active=3,\n    repeat=2)",
      "sort_by_keyword = \"self_\" + device + \"_time_total\"\n\ndef trace_handler(p):\n    output = p.key_averages().table(sort_by=sort_by_keyword, row_limit=10)\n    print(output)\n    p.export_chrome_trace(\"/tmp/trace_\" + str(p.step_num) + \".json\")\n\nwith profile(\n    activities=activities,\n    schedule=torch.profiler.schedule(\n        wait=1,\n        warmup=1,\n        active=2),\n    on_trace_ready=trace_handler\n) as p:\n    for idx in range(8):\n        model(inputs)\n        p.step()"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/advanced/extend_dispatcher.html",
    "title": "Extending dispatcher for a new backend in C++\u00b6",
    "code_snippets": [
      "Tensor abs(const Tensor & self); // {\"schema\": \"aten::abs(Tensor self) -> Tensor\", \"dispatch\": \"True\", \"default\": \"True\"}\nTensor & abs_(Tensor & self); // {\"schema\": \"aten::abs_(Tensor(a!) self) -> Tensor(a!)\", \"dispatch\": \"True\", \"default\": \"True\"}\nTensor & abs_out(Tensor & out, const Tensor & self); // {\"schema\": \"aten::abs.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)\", \"dispatch\": \"True\", \"default\": \"False\"}\nTensor absolute(const Tensor & self); // {\"schema\": \"aten::absolute(Tensor self) -> Tensor\", \"dispatch\": \"False\", \"default\": \"False\"}\nTensor & absolute_(Tensor & self); // {\"schema\": \"aten::absolute_(Tensor(a!) self) -> Tensor(a!)\", \"dispatch\": \"False\", \"default\": \"False\"}\nTensor & absolute_out(Tensor & out, const Tensor & self); // {\"schema\": \"aten::absolute.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)\", \"dispatch\": \"False\", \"default\": \"False\"}\nTensor angle(const Tensor & self); // {\"schema\": \"aten::angle(Tensor self) -> Tensor\", \"dispatch\": \"True\", \"default\": \"True\"}\nTensor & angle_out(Tensor & out, const Tensor & self); // {\"schema\": \"aten::angle.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)\", \"dispatch\": \"True\", \"default\": \"False\"}\nTensor sgn(const Tensor & self); // {\"schema\": \"aten::sgn(Tensor self) -> Tensor\", \"dispatch\": \"True\", \"default\": \"True\"}",
      "class MyAddFunction : public torch::autograd::Function<MyAddFunction> {\n  public:\n  static Tensor forward(AutogradContext *ctx, torch::Tensor self, torch::Tensor other) {\n    at::AutoNonVariableTypeMode g;\n    return myadd(self, other);\n  }\n\n  static tensor_list backward(AutogradContext *ctx, tensor_list grad_outputs) {\n    auto grad_output = grad_outputs[0];\n    return {grad_output, grad_output};\n  }\n};\n\nTensor myadd_autograd(const Tensor& self, const Tensor& other) {\n  return MyAddFunction::apply(self, other)[0];\n}\n\n// Register the autograd kernel to AutogradPrivateUse1\nTORCH_LIBRARY_IMPL(aten, AutogradPrivateUse1, m) {\n  m.impl(<myadd_schema>, &myadd_autograd);\n}\n\n// Register the inference kernel to PrivateUse1\nTORCH_LIBRARY_IMPL(aten, PrivateUse1, m) {\n  m.impl(<myadd_schema>, &myadd);\n}",
      "from setuptools import setup\nfrom torch.utils.cpp_extension import BuildExtension, CppExtension\n\nsetup(\n    name='torch_xla',\n    ext_modules=[\n        CppExtension(\n            '_XLAC',\n            torch_xla_sources,\n            include_dirs=include_dirs,\n            extra_compile_args=extra_compile_args,\n            library_dirs=library_dirs,\n            extra_link_args=extra_link_args + \\\n                [make_relative_rpath('torch_xla/lib')],\n        ),\n    ],\n    cmdclass={\n        'build_ext': Build,  # Build is a derived class of BuildExtension\n    }\n    # more configs...\n)"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/recipes/torch_compile_torch_function_modes.html",
    "title": "(beta) Utilizing Torch Function modes with torch.compile\u00b6",
    "code_snippets": [
      "import torch\n\n# exit cleanly if we are on a device that doesn't support ``torch.compile``\nif torch.cuda.get_device_capability() < (7, 0):\n    print(\"Exiting because torch.compile is not supported on this device.\")\n    import sys\n    sys.exit(0)\n\nfrom torch.overrides import BaseTorchFunctionMode\n\n# Define our mode, Note: ``BaseTorchFunctionMode``\n# implements the actual invocation of func(..)\nclass AddToMultiplyMode(BaseTorchFunctionMode):\n    def __torch_function__(self, func, types, args=(), kwargs=None):\n        if func == torch.Tensor.add:\n            func = torch.mul\n\n        return super().__torch_function__(func, types, args, kwargs)\n\n@torch.compile()\ndef test_fn(x, y):\n    return x + y * x # Note: infix operators map to torch.Tensor.* methods\n\nx = torch.rand(2, 2)\ny = torch.rand_like(x)\n\nwith AddToMultiplyMode():\n    z = test_fn(x, y)\n\nassert torch.allclose(z, x * y * x)\n\n# The mode can also be used within the compiled region as well like this:\n\n@torch.compile()\ndef test_fn(x, y):\n    with AddToMultiplyMode():\n        return x + y * x # Note: infix operators map to torch.Tensor.* methods\n\nx = torch.rand(2, 2)\ny = torch.rand_like(x)\nz = test_fn(x, y)\n\nassert torch.allclose(z, x * y * x)"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/recipes/recipes/Captum_Recipe.html",
    "title": "Model Interpretability using Captum\u00b6",
    "code_snippets": [
      "import torchvision\nfrom torchvision import models, transforms\nfrom PIL import Image\nimport requests\nfrom io import BytesIO\n\nmodel = torchvision.models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1).eval()\n\nresponse = requests.get(\"https://image.freepik.com/free-photo/two-beautiful-puppies-cat-dog_58409-6024.jpg\")\nimg = Image.open(BytesIO(response.content))\n\ncenter_crop = transforms.Compose([\n transforms.Resize(256),\n transforms.CenterCrop(224),\n])\n\nnormalize = transforms.Compose([\n    transforms.ToTensor(),               # converts the image to a tensor with values between 0 and 1\n    transforms.Normalize(                # normalize to follow 0-centered imagenet pixel RGB distribution\n     mean=[0.485, 0.456, 0.406],\n     std=[0.229, 0.224, 0.225]\n    )\n])\ninput_img = normalize(center_crop(img)).unsqueeze(0)",
      "from captum.attr import Occlusion\n\nocclusion = Occlusion(model)\n\nstrides = (3, 9, 9)               # smaller = more fine-grained attribution but slower\ntarget=208,                       # Labrador index in ImageNet\nsliding_window_shapes=(3,45, 45)  # choose size enough to change object appearance\nbaselines = 0                     # values to occlude the image with. 0 corresponds to gray\n\nattribution_dog = occlusion.attribute(input_img,\n                                       strides = strides,\n                                       target=target,\n                                       sliding_window_shapes=sliding_window_shapes,\n                                       baselines=baselines)\n\n\ntarget=283,                       # Persian cat index in ImageNet\nattribution_cat = occlusion.attribute(input_img,\n                                       strides = strides,\n                                       target=target,\n                                       sliding_window_shapes=sliding_window_shapes,\n                                       baselines=0)",
      "import numpy as np\nfrom captum.attr import visualization as viz\n\n# Convert the compute attribution tensor into an image-like numpy array\nattribution_dog = np.transpose(attribution_dog.squeeze().cpu().detach().numpy(), (1,2,0))\n\nvis_types = [\"heat_map\", \"original_image\"]\nvis_signs = [\"all\", \"all\"] # \"positive\", \"negative\", or \"all\" to show both\n# positive attribution indicates that the presence of the area increases the prediction score\n# negative attribution indicates distractor areas whose absence increases the score\n\n_ = viz.visualize_image_attr_multiple(attribution_dog,\n                                      np.array(center_crop(img)),\n                                      vis_types,\n                                      vis_signs,\n                                      [\"attribution for dog\", \"image\"],\n                                      show_colorbar = True\n                                     )\n\n\nattribution_cat = np.transpose(attribution_cat.squeeze().cpu().detach().numpy(), (1,2,0))\n\n_ = viz.visualize_image_attr_multiple(attribution_cat,\n                                      np.array(center_crop(img)),\n                                      [\"heat_map\", \"original_image\"],\n                                      [\"all\", \"all\"], # positive/negative attribution or all\n                                      [\"attribution for cat\", \"image\"],\n                                      show_colorbar = True\n                                     )"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/intermediate/compiled_autograd_tutorial.html",
    "title": "Compiled Autograd: Capturing a larger backward graph for torch.compile\u00b6",
    "code_snippets": [
      "import torch\n\nclass Model(torch.nn.Module):\n   def __init__(self):\n      super().__init__()\n      self.linear = torch.nn.Linear(10, 10)\n\n   def forward(self, x):\n      return self.linear(x)",
      "model = Model()\nx = torch.randn(10)\n\ntorch._dynamo.config.compiled_autograd = True\n@torch.compile\ndef train(model, x):\n   loss = model(x).sum()\n   loss.backward()\n\ntrain(model, x)",
      "def train(model, x):\n    model = torch.compile(model)\n    loss = model(x).sum()\n    torch._dynamo.config.compiled_autograd = True\n    torch.compile(lambda: loss.backward(), fullgraph=True)()",
      "def train(model, x):\n   model = torch.compile(model)\n   loss = model(x).sum()\n   with torch._dynamo.compiled_autograd.enable(torch.compile(fullgraph=True)):\n      loss.backward()",
      "@torch.compile(backend=\"aot_eager\")\ndef fn(x):\n   # 1st graph\n   temp = x + 10\n   torch._dynamo.graph_break()\n   # 2nd graph\n   temp = temp + 10\n   torch._dynamo.graph_break()\n   # 3rd graph\n   return temp.sum()\n\nx = torch.randn(10, 10, requires_grad=True)\ntorch._dynamo.utils.counters.clear()\nloss = fn(x)\n\n# 1. base torch.compile\nloss.backward(retain_graph=True)\nassert(torch._dynamo.utils.counters[\"stats\"][\"unique_graphs\"] == 3)\ntorch._dynamo.utils.counters.clear()\n\n# 2. torch.compile with compiled autograd\nwith torch._dynamo.compiled_autograd.enable(torch.compile(backend=\"aot_eager\")):\n   loss.backward()\n\n# single graph for the backward\nassert(torch._dynamo.utils.counters[\"stats\"][\"unique_graphs\"] == 1)",
      "@torch.compile(backend=\"aot_eager\")\ndef fn(x):\n   return x.sum()\n\nx = torch.randn(10, 10, requires_grad=True)\nx.register_hook(lambda grad: grad+10)\nloss = fn(x)\n\nwith torch._dynamo.compiled_autograd.enable(torch.compile(backend=\"aot_eager\")):\n   loss.backward()",
      "torch._dynamo.config.compiled_autograd = True\nx = torch.randn(10, requires_grad=True)\nfor op in [torch.add, torch.sub, torch.mul, torch.div]:\n   loss = op(x, x).sum()\n   torch.compile(lambda: loss.backward(), backend=\"eager\")()",
      "torch._dynamo.config.compiled_autograd = True\nfor i in [10, 100, 10]:\n   x = torch.randn(i, i, requires_grad=True)\n   loss = x.sum()\n   torch.compile(lambda: loss.backward(), backend=\"eager\")()"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/advanced/cpp_custom_ops.html",
    "title": "Custom C++ and CUDA Operators\u00b6",
    "code_snippets": [
      "def mymuladd(a: Tensor, b: Tensor, c: float):\n    return a * b + c",
      "from setuptools import setup, Extension\nfrom torch.utils import cpp_extension\n\nsetup(name=\"extension_cpp\",\n      ext_modules=[\n          cpp_extension.CppExtension(\n            \"extension_cpp\",\n            [\"muladd.cpp\"],\n            # define Py_LIMITED_API with min version 3.9 to expose only the stable\n            # limited API subset from Python.h\n            extra_compile_args={\"cxx\": [\"-DPy_LIMITED_API=0x03090000\"]},\n            py_limited_api=True)],  # Build 1 wheel across multiple Python versions\n      cmdclass={'build_ext': cpp_extension.BuildExtension},\n      options={\"bdist_wheel\": {\"py_limited_api\": \"cp39\"}}  # 3.9 is minimum supported Python version\n)",
      "from setuptools import setup, Extension\nfrom torch.utils import cpp_extension\n\nsetup(name=\"extension_cpp\",\n      ext_modules=[\n          cpp_extension.CppExtension(\n            \"extension_cpp\",\n            [\"muladd.cpp\"])],\n      cmdclass={'build_ext': cpp_extension.BuildExtension},\n)",
      "TORCH_LIBRARY(extension_cpp, m) {\n   // Note that \"float\" in the schema corresponds to the C++ double type\n   // and the Python float type.\n   m.def(\"mymuladd(Tensor a, Tensor b, float c) -> Tensor\");\n }",
      "# Important: the C++ custom operator definitions should be loaded first\n# before calling ``torch.library`` APIs that add registrations for the\n# C++ custom operator(s). The following import loads our\n# C++ custom operator definitions.\n# Note that if you are striving for Python agnosticism, you should use\n# the ``load_library(...)`` API call instead. See the next section for\n# more details.\nfrom . import _C\n\n@torch.library.register_fake(\"extension_cpp::mymuladd\")\ndef _(a, b, c):\n    torch._check(a.shape == b.shape)\n    torch._check(a.dtype == torch.float)\n    torch._check(b.dtype == torch.float)\n    torch._check(a.device == b.device)\n    return torch.empty_like(a)",
      "# in, say, extension/__init__.py\nfrom . import _C",
      "#include <Python.h>\n\nextern \"C\" {\n  /* Creates a dummy empty _C module that can be imported from Python.\n    The import from Python will load the .so consisting of this file\n    in this extension, so that the TORCH_LIBRARY static initializers\n    below are run. */\n  PyObject* PyInit__C(void)\n  {\n      static struct PyModuleDef module_def = {\n          PyModuleDef_HEAD_INIT,\n          \"_C\",   /* name of module */\n          NULL,   /* module documentation, may be NULL */\n          -1,     /* size of per-interpreter state of the module,\n                    or -1 if the module keeps state in global variables. */\n          NULL,   /* methods */\n      };\n      return PyModule_Create(&module_def);\n  }\n}",
      "# in, say, extension/__init__.py\nfrom . import _C",
      "import torch\nfrom pathlib import Path\n\nso_files = list(Path(__file__).parent.glob(\"_C*.so\"))\nassert (\n    len(so_files) == 1\n), f\"Expected one _C*.so file, found {len(so_files)}\"\ntorch.ops.load_library(so_files[0])\n\nfrom . import ops",
      "def _backward(ctx, grad):\n    a, b = ctx.saved_tensors\n    grad_a, grad_b = None, None\n    if ctx.needs_input_grad[0]:\n        grad_a = grad * b\n    if ctx.needs_input_grad[1]:\n        grad_b = grad * a\n    return grad_a, grad_b, None\n\ndef _setup_context(ctx, inputs, output):\n    a, b, c = inputs\n    saved_a, saved_b = None, None\n    if ctx.needs_input_grad[0]:\n        saved_b = b\n    if ctx.needs_input_grad[1]:\n        saved_a = a\n    ctx.save_for_backward(saved_a, saved_b)\n\n# This code adds training support for the operator. You must provide us\n# the backward formula for the operator and a `setup_context` function\n# to save values to be used in the backward.\ntorch.library.register_autograd(\n    \"extension_cpp::mymuladd\", _backward, setup_context=_setup_context)",
      "// New! a mymul_cpu kernel\nat::Tensor mymul_cpu(const at::Tensor& a, const at::Tensor& b) {\n  TORCH_CHECK(a.sizes() == b.sizes());\n  TORCH_CHECK(a.dtype() == at::kFloat);\n  TORCH_CHECK(b.dtype() == at::kFloat);\n  TORCH_CHECK(a.device().type() == at::DeviceType::CPU);\n  TORCH_CHECK(b.device().type() == at::DeviceType::CPU);\n  at::Tensor a_contig = a.contiguous();\n  at::Tensor b_contig = b.contiguous();\n  at::Tensor result = torch::empty(a_contig.sizes(), a_contig.options());\n  const float* a_ptr = a_contig.data_ptr<float>();\n  const float* b_ptr = b_contig.data_ptr<float>();\n  float* result_ptr = result.data_ptr<float>();\n  for (int64_t i = 0; i < result.numel(); i++) {\n    result_ptr[i] = a_ptr[i] * b_ptr[i];\n  }\n  return result;\n}\n\nTORCH_LIBRARY(extension_cpp, m) {\n  m.def(\"mymuladd(Tensor a, Tensor b, float c) -> Tensor\");\n  // New! defining the mymul operator\n  m.def(\"mymul(Tensor a, Tensor b) -> Tensor\");\n}\n\n\nTORCH_LIBRARY_IMPL(extension_cpp, CPU, m) {\n  m.impl(\"mymuladd\", &mymuladd_cpu);\n  // New! registering the cpu kernel for the mymul operator\n  m.impl(\"mymul\", &mymul_cpu);\n}",
      "def _backward(ctx, grad):\n    a, b = ctx.saved_tensors\n    grad_a, grad_b = None, None\n    if ctx.needs_input_grad[0]:\n        grad_a = torch.ops.extension_cpp.mymul.default(grad, b)\n    if ctx.needs_input_grad[1]:\n        grad_b = torch.ops.extension_cpp.mymul.default(grad, a)\n    return grad_a, grad_b, None\n\n\ndef _setup_context(ctx, inputs, output):\n    a, b, c = inputs\n    saved_a, saved_b = None, None\n    if ctx.needs_input_grad[0]:\n        saved_b = b\n    if ctx.needs_input_grad[1]:\n        saved_a = a\n    ctx.save_for_backward(saved_a, saved_b)\n\n\n# This code adds training support for the operator. You must provide us\n# the backward formula for the operator and a `setup_context` function\n# to save values to be used in the backward.\ntorch.library.register_autograd(\n    \"extension_cpp::mymuladd\", _backward, setup_context=_setup_context)",
      "def sample_inputs(device, *, requires_grad=False):\n    def make_tensor(*size):\n        return torch.randn(size, device=device, requires_grad=requires_grad)\n\n    def make_nondiff_tensor(*size):\n        return torch.randn(size, device=device, requires_grad=False)\n\n    return [\n        [make_tensor(3), make_tensor(3), 1],\n        [make_tensor(20), make_tensor(20), 3.14],\n        [make_tensor(20), make_nondiff_tensor(20), -123],\n        [make_nondiff_tensor(2, 3), make_tensor(2, 3), -0.3],\n    ]\n\ndef reference_muladd(a, b, c):\n    return a * b + c\n\nsamples = sample_inputs(device, requires_grad=True)\nsamples.extend(sample_inputs(device, requires_grad=False))\nfor args in samples:\n    # Correctness test\n    result = torch.ops.extension_cpp.mymuladd(*args)\n    expected = reference_muladd(*args)\n    torch.testing.assert_close(result, expected)\n\n    # Use opcheck to check for incorrect usage of operator registration APIs\n    torch.library.opcheck(torch.ops.extension_cpp.mymuladd.default, args)",
      "TORCH_LIBRARY(extension_cpp, m) {\n  m.def(\"mymuladd(Tensor a, Tensor b, float c) -> Tensor\");\n  m.def(\"mymul(Tensor a, Tensor b) -> Tensor\");\n  // New!\n  m.def(\"myadd_out(Tensor a, Tensor b, Tensor(a!) out) -> ()\");\n}\n\nTORCH_LIBRARY_IMPL(extension_cpp, CPU, m) {\n  m.impl(\"mymuladd\", &mymuladd_cpu);\n  m.impl(\"mymul\", &mymul_cpu);\n  // New!\n  m.impl(\"myadd_out\", &myadd_out_cpu);\n}"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/recipes/torch_compile_user_defined_triton_kernel_tutorial.html",
    "title": "Using User-Defined Triton Kernels with torch.compile\u00b6",
    "code_snippets": [
      "import torch\nfrom torch.utils._triton import has_triton",
      "if not has_triton():\n    print(\"Skipping because triton is not supported on this device.\")\nelse:\n    import triton\n    from triton import language as tl\n\n    @triton.jit\n    def add_kernel(\n        in_ptr0,\n        in_ptr1,\n        out_ptr,\n        n_elements,\n        BLOCK_SIZE: \"tl.constexpr\",\n    ):\n        pid = tl.program_id(axis=0)\n        block_start = pid * BLOCK_SIZE\n        offsets = block_start + tl.arange(0, BLOCK_SIZE)\n        mask = offsets < n_elements\n        x = tl.load(in_ptr0 + offsets, mask=mask)\n        y = tl.load(in_ptr1 + offsets, mask=mask)\n        output = x + y\n        tl.store(out_ptr + offsets, output, mask=mask)\n\n    @torch.compile(fullgraph=True)\n    def add_fn(x, y):\n        output = torch.zeros_like(x)\n        n_elements = output.numel()\n        grid = lambda meta: (triton.cdiv(n_elements, meta[\"BLOCK_SIZE\"]),)\n        add_kernel[grid](x, y, output, n_elements, BLOCK_SIZE=4)\n        return output\n\n    x = torch.randn(4, device=\"cuda\")\n    y = torch.randn(4, device=\"cuda\")\n    out = add_fn(x, y)\n    print(f\"Vector addition of\\nX:\\t{x}\\nY:\\t{y}\\nis equal to\\n{out}\")",
      "if not has_triton():\n    print(\"Skipping because triton is not supported on this device.\")\nelse:\n    import triton\n    from triton import language as tl\n\n    @triton.autotune(\n        configs=[\n            triton.Config({\"BLOCK_SIZE\": 4}, num_stages=3, num_warps=8),\n            triton.Config({\"BLOCK_SIZE\": 4}, num_stages=4, num_warps=4),\n            triton.Config({\"BLOCK_SIZE\": 2}, num_stages=3, num_warps=8),\n            triton.Config({\"BLOCK_SIZE\": 2}, num_stages=4, num_warps=4),\n        ],\n        key=[],\n    )\n    @triton.jit\n    def add_kernel_autotuned(\n        in_ptr0,\n        in_ptr1,\n        out_ptr,\n        n_elements,\n        BLOCK_SIZE: \"tl.constexpr\",\n    ):\n        pid = tl.program_id(axis=0)\n        block_start = pid * BLOCK_SIZE\n        offsets = block_start + tl.arange(0, BLOCK_SIZE)\n        mask = offsets < n_elements\n        x = tl.load(in_ptr0 + offsets, mask=mask)\n        y = tl.load(in_ptr1 + offsets, mask=mask)\n        output = x + y\n        tl.store(out_ptr + offsets, output, mask=mask)\n\n    @torch.compile(fullgraph=True)\n    def add_fn(x, y):\n        output = torch.zeros_like(x)\n        n_elements = output.numel()\n        grid = lambda meta: (triton.cdiv(n_elements, meta[\"BLOCK_SIZE\"]),)\n        add_kernel_autotuned[grid](x, y, output, n_elements)\n        return output\n\n    x = torch.randn(4, device=\"cuda\")\n    y = torch.randn(4, device=\"cuda\")\n    out = add_fn(x, y)\n    print(f\"Vector addition of\\nX:\\t{x}\\nY:\\t{y}\\nis equal to\\n{out}\")",
      "from torch.library import triton_op, wrap_triton\n\n@triton_op(\"mylib::mysin\", mutates_args={})\ndef mysin(x: torch.Tensor) -> torch.Tensor:\n    out = torch.empty_like(x)\n    n_elements = x.numel()\n    wrap_triton(sin_kernel)[(n_elements,)](x, out, n_elements, BLOCK_SIZE=4)\n    return out\n\n@triton.jit\ndef sin_kernel(\n    in_ptr0,\n    out_ptr,\n    n_elements,\n    BLOCK_SIZE: \"tl.constexpr\",\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(in_ptr0 + offsets, mask=mask)\n    output = tl.sin(x)\n    tl.store(out_ptr + offsets, output, mask=mask)",
      "x = torch.randn(3, device=\"cuda\")\ny = mysin(x)\nz = torch.ops.mylib.mysin.default(x)\n\nassert torch.allclose(y, x.sin())\nassert torch.allclose(z, x.sin())",
      "y = torch.compile(mysin)(x)\nassert torch.allclose(y, x.sin())",
      "def backward(ctx, grad):\n    x, = ctx.saved_tensors\n    return grad * x.cos()\n\ndef setup_context(ctx, inputs, output):\n    x, = inputs\n    ctx.save_for_backward(x)\n\nmysin.register_autograd(backward, setup_context=setup_context)",
      "@triton.jit\ndef cos_kernel(\n    in_ptr0,\n    out_ptr,\n    n_elements,\n    BLOCK_SIZE: \"tl.constexpr\",\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(in_ptr0 + offsets, mask=mask)\n    output = tl.cos(x)\n    tl.store(out_ptr + offsets, output, mask=mask)\n\n@triton_op(\"mylib::mycos\", mutates_args={})\ndef mycos(x: torch.Tensor) -> torch.Tensor:\n    out = torch.empty_like(x)\n    n_elements = x.numel()\n    wrap_triton(cos_kernel)[(n_elements,)](x, out, n_elements, BLOCK_SIZE=4)\n    return out\n\ndef backward(ctx, grad):\n    x, = ctx.saved_tensors\n    return grad * mycos(x)\n\ndef setup_context(ctx, inputs, output):\n    x, = inputs\n    ctx.save_for_backward(x)\n\nmysin.register_autograd(backward, setup_context=setup_context)",
      "@mysin.register_kernel(\"cpu\")\ndef _(x):\n    return torch.sin(x)\n\nx = torch.randn(3)\ny = mysin(x)\nassert torch.allclose(y, x.sin())",
      "from torch.utils.flop_counter import FlopCounterMode, register_flop_formula\n\n@register_flop_formula(torch.ops.mylib.mysin)\ndef _(x_shape):\n    numel = 1\n    for s in x_shape:\n        numel *= s\n    return numel\n\nx = torch.randn(3, device=\"cuda\")"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/intermediate/scaled_dot_product_attention_tutorial.html",
    "title": "(Beta) Implementing High-Performance Transformers with Scaled Dot Product Attention (SDPA)\u00b6",
    "code_snippets": [
      "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Example Usage:\nquery, key, value = torch.randn(2, 3, 8, device=device), torch.randn(2, 3, 8, device=device), torch.randn(2, 3, 8, device=device)\nF.scaled_dot_product_attention(query, key, value)",
      "# Lets define a helpful benchmarking function:\nimport torch.utils.benchmark as benchmark\ndef benchmark_torch_function_in_microseconds(f, *args, **kwargs):\n    t0 = benchmark.Timer(\n        stmt=\"f(*args, **kwargs)\", globals={\"args\": args, \"kwargs\": kwargs, \"f\": f}\n    )\n    return t0.blocked_autorange().mean * 1e6\n\n# Lets define the hyper-parameters of our input\nbatch_size = 32\nmax_sequence_len = 1024\nnum_heads = 32\nembed_dimension = 32\n\ndtype = torch.float16\n\nquery = torch.rand(batch_size, num_heads, max_sequence_len, embed_dimension, device=device, dtype=dtype)\nkey = torch.rand(batch_size, num_heads, max_sequence_len, embed_dimension, device=device, dtype=dtype)\nvalue = torch.rand(batch_size, num_heads, max_sequence_len, embed_dimension, device=device, dtype=dtype)\n\nprint(f\"The default implementation runs in {benchmark_torch_function_in_microseconds(F.scaled_dot_product_attention, query, key, value):.3f} microseconds\")\n\n# Lets explore the speed of each of the 3 implementations\nfrom torch.nn.attention import SDPBackend, sdpa_kernel\n\n\nwith sdpa_kernel(SDPBackend.MATH):\n    math_time=benchmark_torch_function_in_microseconds(F.scaled_dot_product_attention, query, key, value)\n    print(f\"The math implementation runs in {math_time:.3f} microseconds\")\n\nwith sdpa_kernel(SDPBackend.FLASH_ATTENTION):\n    try:\n        flash_time=benchmark_torch_function_in_microseconds(F.scaled_dot_product_attention, query, key, value)\n        print(f\"The flash attention implementation runs in {flash_time:.3f} microseconds\")\n    except RuntimeError:\n        print(\"FlashAttention is not supported. See warnings for reasons.\")\n\nwith sdpa_kernel(SDPBackend.EFFICIENT_ATTENTION):\n    try:\n        efficient_time=benchmark_torch_function_in_microseconds(F.scaled_dot_product_attention, query, key, value)\n        print(f\"The memory efficient implementation runs in {efficient_time:.3f} microseconds\")\n    except RuntimeError:\n        print(\"EfficientAttention is not supported. See warnings for reasons.\")",
      "The default implementation runs in 2276.634 microseconds\nThe math implementation runs in 87212.281 microseconds\nThe flash attention implementation runs in 2277.302 microseconds\nThe memory efficient implementation runs in 4309.411 microseconds",
      "class CausalSelfAttention(nn.Module):\n\n    def __init__(self, num_heads: int, embed_dimension: int, bias: bool=False, is_causal: bool=False, dropout:float=0.0):\n        super().__init__()\n        assert embed_dimension % num_heads == 0\n        # key, query, value projections for all heads, but in a batch\n        self.c_attn = nn.Linear(embed_dimension, 3 * embed_dimension, bias=bias)\n        # output projection\n        self.c_proj = nn.Linear(embed_dimension, embed_dimension, bias=bias)\n        # regularization\n        self.dropout = dropout\n        self.resid_dropout = nn.Dropout(dropout)\n        self.num_heads = num_heads\n        self.embed_dimension = embed_dimension\n        # Perform causal masking\n        self.is_causal = is_causal\n\n    def forward(self, x):\n        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n        query_projected = self.c_attn(x)\n\n        batch_size = query_projected.size(0)\n        embed_dim = query_projected.size(2)\n        head_dim = embed_dim // (self.num_heads * 3)\n\n        query, key, value = query_projected.chunk(3, -1)\n        query = query.view(batch_size, -1, self.num_heads, head_dim).transpose(1, 2)\n        key = key.view(batch_size, -1, self.num_heads, head_dim).transpose(1, 2)\n        value = value.view(batch_size, -1, self.num_heads, head_dim).transpose(1, 2)\n\n        if self.training:\n            dropout = self.dropout\n            is_causal = self.is_causal\n        else:\n            dropout = 0.0\n            is_causal = False\n\n        y = F.scaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=dropout, is_causal=is_causal)\n        y = y.transpose(1, 2).view(batch_size, -1, self.num_heads * head_dim)\n\n        y = self.resid_dropout(self.c_proj(y))\n        return y\n\n\nnum_heads = 8\nheads_per_dim = 64\nembed_dimension = num_heads * heads_per_dim\ndtype = torch.float16\nmodel = CausalSelfAttention(num_heads=num_heads, embed_dimension=embed_dimension, bias=False, is_causal=True, dropout=0.1).to(\"cuda\").to(dtype).eval()\nprint(model)",
      "import random\ndef generate_rand_batch(\n    batch_size,\n    max_sequence_len,\n    embed_dimension,\n    pad_percentage=None,\n    dtype=torch.float16,\n    device=\"cuda\",\n):\n    if not pad_percentage:\n        return (\n            torch.randn(\n                batch_size,\n                max_sequence_len,\n                embed_dimension,\n                dtype=dtype,\n                device=device,\n            ),\n            None,\n        )\n    # Random sequence lengths\n    seq_len_list = [\n        int(max_sequence_len * (1 - random.gauss(pad_percentage, 0.01)))\n        for _ in range(batch_size)\n    ]\n    # Make random entry in the batch have max sequence length\n    seq_len_list[random.randint(0, batch_size - 1)] = max_sequence_len\n    return (\n        torch.nested.nested_tensor(\n            [\n                torch.randn(seq_len, embed_dimension,\n                            dtype=dtype, device=device)\n                for seq_len in seq_len_list\n            ]\n        ),\n        seq_len_list,\n    )\n\nrandom_nt, _ = generate_rand_batch(32, 512, embed_dimension, pad_percentage=0.5, dtype=dtype, device=device)\nrandom_dense, _ = generate_rand_batch(32, 512, embed_dimension, pad_percentage=None, dtype=dtype, device=device)\n\n# Currently the fused implementations don't support ``NestedTensor`` for training\nmodel.eval()\n\nwith sdpa_kernel(SDPBackend.FLASH_ATTENTION):\n    try:\n        print(f\"Random NT runs in {benchmark_torch_function_in_microseconds(model, random_nt):.3f} microseconds\")\n        print(f\"Random Dense runs in {benchmark_torch_function_in_microseconds(model, random_dense):.3f} microseconds\")\n    except RuntimeError:\n        print(\"FlashAttention is not supported. See warnings for reasons.\")",
      "/usr/local/lib/python3.10/dist-packages/torch/nested/__init__.py:250: UserWarning:\n\nThe PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at /pytorch/aten/src/ATen/NestedTensorImpl.cpp:178.)\n\nRandom NT runs in 607.769 microseconds\nRandom Dense runs in 949.898 microseconds",
      "batch_size = 32\nmax_sequence_len = 256\nx = torch.rand(batch_size, max_sequence_len,\n               embed_dimension, device=device, dtype=dtype)\nprint(\n    f\"The non compiled module runs in  {benchmark_torch_function_in_microseconds(model, x):.3f} microseconds\")\n\n\ncompiled_model = torch.compile(model)\n# Let's compile it\ncompiled_model(x)\nprint(\n    f\"The compiled module runs in  {benchmark_torch_function_in_microseconds(compiled_model, x):.3f} microseconds\")",
      "from torch.profiler import profile, record_function, ProfilerActivity\nactivities = [ProfilerActivity.CPU]\nif device == 'cuda':\n    activities.append(ProfilerActivity.CUDA)\n\nwith profile(activities=activities, record_shapes=False) as prof:\n    with record_function(\" Non-Compilied Causal Attention\"):\n        for _ in range(25):\n            model(x)\nprint(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))\n\n\nwith profile(activities=activities, record_shapes=False) as prof:\n    with record_function(\"Compiled Causal Attention\"):\n        for _ in range(25):\n            compiled_model(x)\nprint(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))\n\n# For even more insights, you can export the trace and use ``chrome://tracing`` to view the results\n#\n# .. code-block:: python\n#\n#    prof.export_chrome_trace(\"compiled_causal_attention_trace.json\").",
      "# As of PyTorch 2.3, we have added a new submodule that contains tensor subclasses.\n# Designed to be used with ``torch.nn.functional.scaled_dot_product_attention``.\n# The module is named ``torch.nn.attention.bias`` and contains the following two\n# utilities for generating causal attention variants:\n#\n# - ``torch.nn.attention.bias.causal_upper_left``\n# - ``torch.nn.attention.bias.causal_lower_right``\n#\n# .. note::\n#    The current argument ``is_causal`` in ``torch.nn.functional.scaled_dot_product_attention``\n#    is the same as using ``torch.nn.attention.bias.causal_upper_left``.\n#\n\nfrom torch.nn.attention.bias import causal_lower_right, causal_upper_left\n\nbatch_size = 32\nsequence_length_q = 2\nsequence_length_kv = 10\nnum_heads = 16\nembed_dimension = 32\n\ndtype = torch.float16\n\nquery = torch.rand(batch_size, num_heads, sequence_length_q, embed_dimension, device=device, dtype=dtype)\nkey = torch.rand(batch_size, num_heads, sequence_length_kv, embed_dimension, device=device, dtype=dtype)\nvalue = torch.rand(batch_size, num_heads, sequence_length_kv, embed_dimension, device=device, dtype=dtype)\n\nupper_left_bias = causal_upper_left(sequence_length_q, sequence_length_kv)\nlower_right_bias = causal_lower_right(sequence_length_q, sequence_length_kv)\n\nprint(type(upper_left_bias))\nprint(type(lower_right_bias))\n\nassert type(upper_left_bias) == type(lower_right_bias)\nassert issubclass(type(upper_left_bias), torch.Tensor)\n\n# As you can see from the previous output, are the same type ``torch.nn.attention.bias.CausalBias``\n# and subclass ``torch.Tensor``\n\n# Lets see what these tensors look like\nprint(upper_left_bias)\nprint(lower_right_bias)\n\n# Upper Left Bias aligns the causal attention mask to the upper left corner of the attention scores matrix.\n# This only has an impact when the attention scores matrix is not square, which is common for decoding use cases.\n# Another way of thinking about this concept is that when you use upper left bias,\n# the 0th token in the query is aligned to the 0th token in the key, while for lower right bias,\n# Assuming the attention score matrix is two dimensional, ``attn_score[0][0]`` is the attention score\n# between the 0th token in the query and the 0th token in the key.\n# For lower right bias, the sequence of q is aligned so that the last token in q is aligned to the last token in k\n# (for example, ``attn_score[-1][-1])`` is all True since the last token in q is at the same position as the last token in k\n# even if the sequence length of q and k are different.\n\n# These objects are intended to be used with sdpa\nout_upper_left = F.scaled_dot_product_attention(query, key, value, upper_left_bias)\nout_lower_right = F.scaled_dot_product_attention(query, key, value, lower_right_bias)\nout_is_causal = F.scaled_dot_product_attention(query, key, value, is_causal=True)\n\nassert torch.allclose(out_upper_left, out_is_causal)\nassert not torch.allclose(out_upper_left, out_lower_right)\n\n# These attention biases should also be compatible with torch.compile\ncompiled_sdpa = torch.compile(F.scaled_dot_product_attention, fullgraph=True)\nout_upper_left = compiled_sdpa(query, key, value, upper_left_bias)",
      "<class 'torch.nn.attention.bias.CausalBias'>\n<class 'torch.nn.attention.bias.CausalBias'>\ntensor([[ True, False, False, False, False, False, False, False, False, False],\n        [ True,  True, False, False, False, False, False, False, False, False]])\ntensor([[ True,  True,  True,  True,  True,  True,  True,  True,  True, False],\n        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]])"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/intermediate/torchrec_intro_tutorial.html",
    "title": "Introduction to TorchRec\u00b6",
    "code_snippets": [
      "!pip3 install --pre torch --index-url https://download.pytorch.org/whl/cu121 -U\n!pip3 install fbgemm_gpu --index-url https://download.pytorch.org/whl/cu121\n!pip3 install torchmetrics==1.0.3\n!pip3 install torchrec --index-url https://download.pytorch.org/whl/cu121",
      "import torch",
      "num_embeddings, embedding_dim = 10, 4\n\n# Initialize our embedding table\nweights = torch.rand(num_embeddings, embedding_dim)\nprint(\"Weights:\", weights)\n\n# Pass in pre-generated weights just for example, typically weights are randomly initialized\nembedding_collection = torch.nn.Embedding(\n    num_embeddings, embedding_dim, _weight=weights\n)\nembedding_bag_collection = torch.nn.EmbeddingBag(\n    num_embeddings, embedding_dim, _weight=weights\n)\n\n# Print out the tables, we should see the same weights as above\nprint(\"Embedding Collection Table: \", embedding_collection.weight)\nprint(\"Embedding Bag Collection Table: \", embedding_bag_collection.weight)\n\n# Lookup rows (ids for embedding ids) from the embedding tables\n# 2D tensor with shape (batch_size, ids for each batch)\nids = torch.tensor([[1, 3]])\nprint(\"Input row IDS: \", ids)\n\nembeddings = embedding_collection(ids)\n\n# Print out the embedding lookups\n# You should see the specific embeddings be the same as the rows (ids) of the embedding tables above\nprint(\"Embedding Collection Results: \")\nprint(embeddings)\nprint(\"Shape: \", embeddings.shape)\n\n# ``nn.EmbeddingBag`` default pooling is mean, so should be mean of batch dimension of values above\npooled_embeddings = embedding_bag_collection(ids)\n\nprint(\"Embedding Bag Collection Results: \")\nprint(pooled_embeddings)\nprint(\"Shape: \", pooled_embeddings.shape)\n\n# ``nn.EmbeddingBag`` is the same as ``nn.Embedding`` but just with pooling (mean, sum, and so on)\n# We can see that the mean of the embeddings of embedding_collection is the same as the output of the embedding_bag_collection\nprint(\"Mean: \", torch.mean(embedding_collection(ids), dim=1))",
      "import torchrec",
      "import inspect\n\n# Let's look at the ``EmbeddingBagCollection`` forward method\n# What is a ``KeyedJaggedTensor`` and ``KeyedTensor``?\nprint(inspect.getsource(ebc.forward))",
      "# Batch Size 2\n# 1 ID in example 1, 2 IDs in example 2\nid_list_feature_lengths = torch.tensor([1, 2])\n\n# Values (IDs) tensor: ID 5 is in example 1, ID 7, 1 is in example 2\nid_list_feature_values = torch.tensor([5, 7, 1])",
      "# Lengths can be converted to offsets for easy indexing of values\nid_list_feature_offsets = torch.cumsum(id_list_feature_lengths, dim=0)\n\nprint(\"Offsets: \", id_list_feature_offsets)\nprint(\"First Batch: \", id_list_feature_values[: id_list_feature_offsets[0]])\nprint(\n    \"Second Batch: \",\n    id_list_feature_values[id_list_feature_offsets[0] : id_list_feature_offsets[1]],\n)\n\nfrom torchrec import JaggedTensor\n\n# ``JaggedTensor`` is just a wrapper around lengths/offsets and values tensors!\njt = JaggedTensor(values=id_list_feature_values, lengths=id_list_feature_lengths)\n\n# Automatically compute offsets from lengths\nprint(\"Offsets: \", jt.offsets())\n\n# Convert to list of values\nprint(\"List of Values: \", jt.to_dense())\n\n# ``__str__`` representation\nprint(jt)\n\nfrom torchrec import KeyedJaggedTensor\n\n# ``JaggedTensor`` represents IDs for 1 feature, but we have multiple features in an ``EmbeddingBagCollection``\n# That's where ``KeyedJaggedTensor`` comes in! ``KeyedJaggedTensor`` is just multiple ``JaggedTensors`` for multiple id_list_feature_offsets\n# From before, we have our two features \"product\" and \"user\". Let's create ``JaggedTensors`` for both!\n\nproduct_jt = JaggedTensor(\n    values=torch.tensor([1, 2, 1, 5]), lengths=torch.tensor([3, 1])\n)\nuser_jt = JaggedTensor(values=torch.tensor([2, 3, 4, 1]), lengths=torch.tensor([2, 2]))\n\n# Q1: How many batches are there, and which values are in the first batch for ``product_jt`` and ``user_jt``?\nkjt = KeyedJaggedTensor.from_jt_dict({\"product\": product_jt, \"user\": user_jt})\n\n# Look at our feature keys for the ``KeyedJaggedTensor``\nprint(\"Keys: \", kjt.keys())\n\n# Look at the overall lengths for the ``KeyedJaggedTensor``\nprint(\"Lengths: \", kjt.lengths())\n\n# Look at all values for ``KeyedJaggedTensor``\nprint(\"Values: \", kjt.values())\n\n# Can convert ``KeyedJaggedTensor`` to dictionary representation\nprint(\"to_dict: \", kjt.to_dict())\n\n# ``KeyedJaggedTensor`` string representation\nprint(kjt)\n\n# Q2: What are the offsets for the ``KeyedJaggedTensor``?\n\n# Now we can run a forward pass on our ``EmbeddingBagCollection`` from before\nresult = ebc(kjt)\nresult\n\n# Result is a ``KeyedTensor``, which contains a list of the feature names and the embedding results\nprint(result.keys())\n\n# The results shape is [2, 128], as batch size of 2. Reread previous section if you need a refresher on how the batch size is determined\n# 128 for dimension of embedding. If you look at where we initialized the ``EmbeddingBagCollection``, we have two tables \"product\" and \"user\" of dimension 64 each\n# meaning embeddings for both features are of size 64. 64 + 64 = 128\nprint(result.values().shape)\n\n# Nice to_dict method to determine the embeddings that belong to each feature\nresult_dict = result.to_dict()\nfor key, embedding in result_dict.items():\n    print(key, embedding.shape)",
      "import os\n\nimport torch.distributed as dist\n\n# Set up environment variables for distributed training\n# RANK is which GPU we are on, default 0\nos.environ[\"RANK\"] = \"0\"\n# How many devices in our \"world\", colab notebook can only handle 1 process\nos.environ[\"WORLD_SIZE\"] = \"1\"\n# Localhost as we are training locally\nos.environ[\"MASTER_ADDR\"] = \"localhost\"\n# Port for distributed training\nos.environ[\"MASTER_PORT\"] = \"29500\"\n\n# nccl backend is for GPUs, gloo is for CPUs\ndist.init_process_group(backend=\"gloo\")\n\nprint(f\"Distributed environment initialized: {dist}\")",
      "ebc\n\nfrom torchrec.distributed.embeddingbag import EmbeddingBagCollectionSharder\nfrom torchrec.distributed.planner import EmbeddingShardingPlanner, Topology\nfrom torchrec.distributed.types import ShardingEnv\n\n# Corresponding sharder for ``EmbeddingBagCollection`` module\nsharder = EmbeddingBagCollectionSharder()\n\n# ``ProcessGroup`` from torch.distributed initialized 2 cells above\npg = dist.GroupMember.WORLD\nassert pg is not None, \"Process group is not initialized\"\n\nprint(f\"Process Group: {pg}\")",
      "# The static plan that was generated\nplan\n\nenv = ShardingEnv.from_process_group(pg)\n\n# Shard the ``EmbeddingBagCollection`` module using the ``EmbeddingBagCollectionSharder``\nsharded_ebc = sharder.shard(ebc, plan.plan[\"\"], env, torch.device(\"cuda\"))\n\nprint(f\"Sharded EBC Module: {sharded_ebc}\")",
      "from typing import List\n\nfrom torchrec.distributed.types import LazyAwaitable\n\n\n# Demonstrate a ``LazyAwaitable`` type:\nclass ExampleAwaitable(LazyAwaitable[torch.Tensor]):\n    def __init__(self, size: List[int]) -> None:\n        super().__init__()\n        self._size = size\n\n    def _wait_impl(self) -> torch.Tensor:\n        return torch.ones(self._size)\n\n\nawaitable = ExampleAwaitable([3, 2])\nawaitable.wait()\n\nkjt = kjt.to(\"cuda\")\noutput = sharded_ebc(kjt)\n# The output of our sharded ``EmbeddingBagCollection`` module is an `Awaitable`?\nprint(output)\n\nkt = output.wait()\n# Now we have our ``KeyedTensor`` after calling ``.wait()``\n# If you are confused as to why we have a ``KeyedTensor ``output,\n# give yourself a refresher on the unsharded ``EmbeddingBagCollection`` module\nprint(type(kt))\n\nprint(kt.keys())\n\nprint(kt.values().shape)\n\n# Same output format as unsharded ``EmbeddingBagCollection``\nresult_dict = kt.to_dict()\nfor key, embedding in result_dict.items():\n    print(key, embedding.shape)",
      "ebc\n\nmodel = torchrec.distributed.DistributedModelParallel(ebc, device=torch.device(\"cuda\"))\n\nout = model(kjt)\nout.wait()\n\nmodel",
      "# Option 1: Passing optimizer kwargs through fused parameters\nfrom torchrec.optim.optimizers import in_backward_optimizer_filter\nfrom fbgemm_gpu.split_embedding_configs import EmbOptimType\n\n\n# We initialize the sharder with\nfused_params = {\n    \"optimizer\": EmbOptimType.EXACT_ROWWISE_ADAGRAD,\n    \"learning_rate\": 0.02,\n    \"eps\": 0.002,\n}\n\n# Initialize sharder with ``fused_params``\nsharder_with_fused_params = EmbeddingBagCollectionSharder(fused_params=fused_params)\n\n# We'll use same plan and unsharded EBC as before but this time with our new sharder\nsharded_ebc_fused_params = sharder_with_fused_params.shard(ebc, plan.plan[\"\"], env, torch.device(\"cuda\"))\n\n# Looking at the optimizer of each, we can see that the learning rate changed, which indicates our optimizer has been applied correctly.\n# If seen, we can also look at the TBE logs of the cell to see that our new optimizer is indeed being applied\nprint(f\"Original Sharded EBC fused optimizer: {sharded_ebc.fused_optimizer}\")\nprint(f\"Sharded EBC with fused parameters fused optimizer: {sharded_ebc_fused_params.fused_optimizer}\")\n\nprint(f\"Type of optimizer: {type(sharded_ebc_fused_params.fused_optimizer)}\")\n\nfrom torch.distributed.optim import _apply_optimizer_in_backward as apply_optimizer_in_backward\nimport copy\n# Option 2: Applying optimizer through apply_optimizer_in_backward\n# Note: we need to call apply_optimizer_in_backward on unsharded model first and then shard it\n\n# We can achieve the same result as we did in the previous\nebc_apply_opt = copy.deepcopy(ebc)\noptimizer_kwargs = {\"lr\": 0.5}\n\nfor name, param in ebc_apply_opt.named_parameters():\n    print(f\"{name=}\")\n    apply_optimizer_in_backward(torch.optim.SGD, [param], optimizer_kwargs)\n\nsharded_ebc_apply_opt = sharder.shard(ebc_apply_opt, plan.plan[\"\"], env, torch.device(\"cuda\"))\n\n# Now when we print the optimizer, we will see our new learning rate, you can verify momentum through the TBE logs as well if outputted\nprint(sharded_ebc_apply_opt.fused_optimizer)\nprint(type(sharded_ebc_apply_opt.fused_optimizer))\n\n# We can also check through the filter other parameters that aren't associated with the \"fused\" optimizer(s)\n# Practically, just non TorchRec module parameters. Since our module is just a TorchRec EBC\n# there are no other parameters that aren't associated with TorchRec\nprint(\"Non Fused Model Parameters:\")\nprint(dict(in_backward_optimizer_filter(sharded_ebc_fused_params.named_parameters())).keys())\n\n# Here we do a dummy backwards call and see that parameter updates for fused\n# optimizers happen as a result of the backward pass\n\nebc_output = sharded_ebc_fused_params(kjt).wait().values()\nloss = torch.sum(torch.ones_like(ebc_output) - ebc_output)\nprint(f\"First Iteration Loss: {loss}\")\n\nloss.backward()\n\nebc_output = sharded_ebc_fused_params(kjt).wait().values()\nloss = torch.sum(torch.ones_like(ebc_output) - ebc_output)\n# We don't call an optimizer.step(), so for the loss to have changed here,\n# that means that the gradients were somehow updated, which is what the\n# fused optimizer automatically handles for us\nprint(f\"Second Iteration Loss: {loss}\")",
      "ebc\n\nclass InferenceModule(torch.nn.Module):\n    def __init__(self, ebc: torchrec.EmbeddingBagCollection):\n        super().__init__()\n        self.ebc_ = ebc\n\n    def forward(self, kjt: KeyedJaggedTensor):\n        return self.ebc_(kjt)\n\nmodule = InferenceModule(ebc)\nfor name, param in module.named_parameters():\n    # Here, the parameters should still be FP32, as we are using a standard EBC\n    # FP32 is default, regularly used for training\n    print(name, param.shape, param.dtype)",
      "from torch import quantization as quant\nfrom torchrec.modules.embedding_configs import QuantConfig\nfrom torchrec.quant.embedding_modules import (\n    EmbeddingBagCollection as QuantEmbeddingBagCollection,\n)\n\n\nquant_dtype = torch.int8\n\n\nqconfig = QuantConfig(\n    # dtype of the result of the embedding lookup, post activation\n    # torch.float generally for compatibility with rest of the model\n    # as rest of the model here usually isn't quantized\n    activation=quant.PlaceholderObserver.with_args(dtype=torch.float),\n    # quantized type for embedding weights, aka parameters to actually quantize\n    weight=quant.PlaceholderObserver.with_args(dtype=quant_dtype),\n)\nqconfig_spec = {\n    # Map of module type to qconfig\n    torchrec.EmbeddingBagCollection: qconfig,\n}\nmapping = {\n    # Map of module type to quantized module type\n    torchrec.EmbeddingBagCollection: QuantEmbeddingBagCollection,\n}\n\n\nmodule = InferenceModule(ebc)\n\n# Quantize the module\nqebc = quant.quantize_dynamic(\n    module,\n    qconfig_spec=qconfig_spec,\n    mapping=mapping,\n    inplace=False,\n)\n\n\nprint(f\"Quantized EBC: {qebc}\")\n\nkjt = kjt.to(\"cpu\")\n\nqebc(kjt)\n\n# Once quantized, goes from parameters -> buffers, as no longer trainable\nfor name, buffer in qebc.named_buffers():\n    # The shapes of the tables should be the same but the dtype should be int8 now\n    # post quantization\n    print(name, buffer.shape, buffer.dtype)",
      "from torchrec import distributed as trec_dist\nfrom torchrec.distributed.shard import _shard_modules\n\n\nsharded_qebc = _shard_modules(\n    module=qebc,\n    device=torch.device(\"cpu\"),\n    env=trec_dist.ShardingEnv.from_local(\n        1,\n        0,\n    ),\n)\n\n\nprint(f\"Sharded Quantized EBC: {sharded_qebc}\")\n\nsharded_qebc(kjt)",
      "from torchrec.fx import Tracer\n\n\ntracer = Tracer(leaf_modules=[\"IntNBitTableBatchedEmbeddingBagsCodegen\"])\n\ngraph = tracer.trace(sharded_qebc)\ngm = torch.fx.GraphModule(sharded_qebc, graph)\n\nprint(\"Graph Module Created!\")\n\nprint(gm.code)\n\nscripted_gm = torch.jit.script(gm)\nprint(\"Scripted Graph Module Created!\")\n\nprint(scripted_gm.code)"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html",
    "title": "Optimizing Model Parameters\u00b6",
    "code_snippets": [
      "import torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets\nfrom torchvision.transforms import ToTensor\n\ntraining_data = datasets.FashionMNIST(\n    root=\"data\",\n    train=True,\n    download=True,\n    transform=ToTensor()\n)\n\ntest_data = datasets.FashionMNIST(\n    root=\"data\",\n    train=False,\n    download=True,\n    transform=ToTensor()\n)\n\ntrain_dataloader = DataLoader(training_data, batch_size=64)\ntest_dataloader = DataLoader(test_data, batch_size=64)\n\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.flatten = nn.Flatten()\n        self.linear_relu_stack = nn.Sequential(\n            nn.Linear(28*28, 512),\n            nn.ReLU(),\n            nn.Linear(512, 512),\n            nn.ReLU(),\n            nn.Linear(512, 10),\n        )\n\n    def forward(self, x):\n        x = self.flatten(x)\n        logits = self.linear_relu_stack(x)\n        return logits\n\nmodel = NeuralNetwork()",
      "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)",
      "def train_loop(dataloader, model, loss_fn, optimizer):\n    size = len(dataloader.dataset)\n    # Set the model to training mode - important for batch normalization and dropout layers\n    # Unnecessary in this situation but added for best practices\n    model.train()\n    for batch, (X, y) in enumerate(dataloader):\n        # Compute prediction and loss\n        pred = model(X)\n        loss = loss_fn(pred, y)\n\n        # Backpropagation\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n        if batch % 100 == 0:\n            loss, current = loss.item(), batch * batch_size + len(X)\n            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n\n\ndef test_loop(dataloader, model, loss_fn):\n    # Set the model to evaluation mode - important for batch normalization and dropout layers\n    # Unnecessary in this situation but added for best practices\n    model.eval()\n    size = len(dataloader.dataset)\n    num_batches = len(dataloader)\n    test_loss, correct = 0, 0\n\n    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n    with torch.no_grad():\n        for X, y in dataloader:\n            pred = model(X)\n            test_loss += loss_fn(pred, y).item()\n            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n\n    test_loss /= num_batches\n    correct /= size\n    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")",
      "loss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n\nepochs = 10\nfor t in range(epochs):\n    print(f\"Epoch {t+1}\\n-------------------------------\")\n    train_loop(train_dataloader, model, loss_fn, optimizer)\n    test_loop(test_dataloader, model, loss_fn)\nprint(\"Done!\")"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/recipes/recipes/dynamic_quantization.html",
    "title": "Dynamic Quantization\u00b6",
    "code_snippets": [
      "# import the modules used here in this recipe\nimport torch\nimport torch.quantization\nimport torch.nn as nn\nimport copy\nimport os\nimport time\n\n# define a very, very simple LSTM for demonstration purposes\n# in this case, we are wrapping ``nn.LSTM``, one layer, no preprocessing or postprocessing\n# inspired by\n# `Sequence Models and Long Short-Term Memory Networks tutorial <https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html`_, by Robert Guthrie\n# and `Dynamic Quanitzation tutorial <https://pytorch.org/tutorials/advanced/dynamic_quantization_tutorial.html>`__.\nclass lstm_for_demonstration(nn.Module):\n  \"\"\"Elementary Long Short Term Memory style model which simply wraps ``nn.LSTM``\n     Not to be used for anything other than demonstration.\n  \"\"\"\n  def __init__(self,in_dim,out_dim,depth):\n     super(lstm_for_demonstration,self).__init__()\n     self.lstm = nn.LSTM(in_dim,out_dim,depth)\n\n  def forward(self,inputs,hidden):\n     out,hidden = self.lstm(inputs,hidden)\n     return out, hidden\n\n\ntorch.manual_seed(29592)  # set the seed for reproducibility\n\n#shape parameters\nmodel_dimension=8\nsequence_length=20\nbatch_size=1\nlstm_depth=1\n\n# random data for input\ninputs = torch.randn(sequence_length,batch_size,model_dimension)\n# hidden is actually is a tuple of the initial hidden state and the initial cell state\nhidden = (torch.randn(lstm_depth,batch_size,model_dimension), torch.randn(lstm_depth,batch_size,model_dimension))",
      "# here is our floating point instance\nfloat_lstm = lstm_for_demonstration(model_dimension, model_dimension,lstm_depth)\n\n# this is the call that does the work\nquantized_lstm = torch.quantization.quantize_dynamic(\n    float_lstm, {nn.LSTM, nn.Linear}, dtype=torch.qint8\n)\n\n# show the changes that were made\nprint('Here is the floating point version of this module:')\nprint(float_lstm)\nprint('')\nprint('and now the quantized version:')\nprint(quantized_lstm)",
      "def print_size_of_model(model, label=\"\"):\n    torch.save(model.state_dict(), \"temp.p\")\n    size=os.path.getsize(\"temp.p\")\n    print(\"model: \",label,' \\t','Size (KB):', size/1e3)\n    os.remove('temp.p')\n    return size\n\n# compare the sizes\nf=print_size_of_model(float_lstm,\"fp32\")\nq=print_size_of_model(quantized_lstm,\"int8\")\nprint(\"{0:.2f} times smaller\".format(f/q))",
      "# run the float model\nout1, hidden1 = float_lstm(inputs, hidden)\nmag1 = torch.mean(abs(out1)).item()\nprint('mean absolute value of output tensor values in the FP32 model is {0:.5f} '.format(mag1))\n\n# run the quantized model\nout2, hidden2 = quantized_lstm(inputs, hidden)\nmag2 = torch.mean(abs(out2)).item()\nprint('mean absolute value of output tensor values in the INT8 model is {0:.5f}'.format(mag2))\n\n# compare them\nmag3 = torch.mean(abs(out1-out2)).item()\nprint('mean absolute value of the difference between the output tensors is {0:.5f} or {1:.2f} percent'.format(mag3,mag3/mag1*100))"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html",
    "title": "Tensors\u00b6",
    "code_snippets": [
      "import torch\nimport numpy as np",
      "data = [[1, 2], [3, 4]]\nx_data = torch.tensor(data)",
      "np_array = np.array(data)\nx_np = torch.from_numpy(np_array)",
      "x_ones = torch.ones_like(x_data) # retains the properties of x_data\nprint(f\"Ones Tensor: \\n {x_ones} \\n\")\n\nx_rand = torch.rand_like(x_data, dtype=torch.float) # overrides the datatype of x_data\nprint(f\"Random Tensor: \\n {x_rand} \\n\")",
      "shape = (2, 3,)\nrand_tensor = torch.rand(shape)\nones_tensor = torch.ones(shape)\nzeros_tensor = torch.zeros(shape)\n\nprint(f\"Random Tensor: \\n {rand_tensor} \\n\")\nprint(f\"Ones Tensor: \\n {ones_tensor} \\n\")\nprint(f\"Zeros Tensor: \\n {zeros_tensor}\")",
      "tensor = torch.rand(3, 4)\n\nprint(f\"Shape of tensor: {tensor.shape}\")\nprint(f\"Datatype of tensor: {tensor.dtype}\")\nprint(f\"Device tensor is stored on: {tensor.device}\")",
      "Shape of tensor: torch.Size([3, 4])\nDatatype of tensor: torch.float32\nDevice tensor is stored on: cpu",
      "# We move our tensor to the GPU if available\nif torch.cuda.is_available():\n  tensor = tensor.to('cuda')\n  print(f\"Device tensor is stored on: {tensor.device}\")",
      "tensor = torch.ones(4, 4)\ntensor[:,1] = 0\nprint(tensor)",
      "t1 = torch.cat([tensor, tensor, tensor], dim=1)\nprint(t1)",
      "t = torch.ones(5)\nprint(f\"t: {t}\")\nn = t.numpy()\nprint(f\"n: {n}\")",
      "n = np.ones(5)\nt = torch.from_numpy(n)",
      "t: tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\nn: [2. 2. 2. 2. 2.]"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/intermediate/TCPStore_libuv_backend.html",
    "title": "Introduction to Libuv TCPStore Backend\u00b6",
    "code_snippets": [
      "import logging\nimport os\n\nfrom time import perf_counter\n\nimport torch\nimport torch.distributed as dist\n\nlogger: logging.Logger = logging.getLogger(__name__)\n\n# Env var are preset when launching the benchmark\nenv_rank = os.environ.get(\"RANK\", 0)\nenv_world_size = os.environ.get(\"WORLD_SIZE\", 1)\nenv_master_addr = os.environ.get(\"MASTER_ADDR\", \"localhost\")\nenv_master_port = os.environ.get(\"MASTER_PORT\", \"23456\")\n\nstart = perf_counter()\ntcp_store = dist.TCPStore(\n    env_master_addr,\n    int(env_master_port),\n    world_size=int(env_world_size),\n    is_master=(int(env_rank) == 0),\n)\nend = perf_counter()\ntime_elapsed = end - start\nlogger.info(\n    f\"Complete TCPStore init with rank={env_rank}, world_size={env_world_size} in {time_elapsed} seconds.\"\n)",
      "import logging\nimport os\nimport time\n\nfrom datetime import timedelta\nfrom time import perf_counter\n\nimport torch\nimport torch.distributed as dist\n\nDistStoreError = torch._C._DistStoreError\nlogger: logging.Logger = logging.getLogger(__name__)\n\n# since dist._store_based_barrier is a private function and cannot be directly called, we need to write a function which does the same\ndef store_based_barrier(\n    rank,\n    store,\n    group_name,\n    rendezvous_count,\n    timeout=dist.constants.default_pg_timeout,\n    logging_interval=timedelta(seconds=10),\n):\n    store_key = f\"store_based_barrier_key:{group_name}\"\n    store.add(store_key, 1)\n\n    world_size = rendezvous_count\n    worker_count = store.add(store_key, 0)\n\n    last_worker_key = f\"{store_key}:last_worker\"\n    if worker_count == world_size:\n        store.set(last_worker_key, \"1\")\n\n    start = time.time()\n    while True:\n        try:\n            # This will throw an exception after the logging_interval in which we print out\n            # the status of the group or time out officially, throwing runtime error\n            store.wait([last_worker_key], logging_interval)\n            break\n        except RuntimeError as e:\n            worker_count = store.add(store_key, 0)\n            # Print status periodically to keep track.\n            logger.info(\n                \"Waiting in store based barrier to initialize process group for \"\n                \"rank: %s, key: %s (world_size=%s, num_workers_joined=%s, timeout=%s)\"\n                \"error: %s\",\n                rank,\n                store_key,\n                world_size,\n                worker_count,\n                timeout,\n                e,\n            )\n\n            if timedelta(seconds=(time.time() - start)) > timeout:\n                raise DistStoreError(\n                    \"Timed out initializing process group in store based barrier on \"\n                    \"rank {}, for key: {} (world_size={}, num_workers_joined={}, timeout={})\".format(\n                        rank, store_key, world_size, worker_count, timeout\n                    )\n                )\n\n    logger.info(\n        \"Rank %s: Completed store-based barrier for key:%s with %s nodes.\",\n        rank,\n        store_key,\n        world_size,\n    )\n\n# Env var are preset when launching the benchmark\nenv_rank = os.environ.get(\"RANK\", 0)\nenv_world_size = os.environ.get(\"WORLD_SIZE\", 1)\nenv_master_addr = os.environ.get(\"MASTER_ADDR\", \"localhost\")\nenv_master_port = os.environ.get(\"MASTER_PORT\", \"23456\")\n\ntcp_store = dist.TCPStore(\n    env_master_addr,\n    int(env_master_port),\n    world_size=int(env_world_size),\n    is_master=(int(env_rank) == 0),\n)\n\n# sync workers\nstore_based_barrier(int(env_rank), tcp_store, \"tcpstore_test\", int(env_world_size))\n\nnumber_runs = 10\nstart = perf_counter()\nfor _ in range(number_runs):\n    store_based_barrier(\n        int(env_rank), tcp_store, \"tcpstore_test\", int(env_world_size)\n    )\nend = perf_counter()\ntime_elapsed = end - start\nlogger.info(\n    f\"Complete {number_runs} TCPStore barrier runs with rank={env_rank}, world_size={env_world_size} in {time_elapsed} seconds.\"\n)",
      "import socket\n\nimport torch\nimport torch.distributed as dist\n\nlisten_sock: socket.socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\nlisten_sock.bind((\"localhost\", 0))\naddr, port, *_ = listen_sock.getsockname()\nlisten_fd = listen_sock.detach()\n\ntcpstore = dist.TCPStore(addr, port, 1, True, master_listen_fd=listen_fd)  # expect NotImplementedError\ntcpstore = dist.TCPStore(addr, port, 1, True, master_listen_fd=listen_fd, use_libuv=False)  # OK. Use legacy backend",
      "import torch\nimport torch.distributed as dist\n\naddr = \"localhost\"\nport = 23456\ndist.init_process_group(\n    backend=\"cpu:gloo,cuda:nccl\",\n    rank=0,\n    world_size=1,\n    init_method=f\"tcp://{addr}:{port}?use_libuv=0\",\n)\ndist.destroy_process_group()",
      "import os\n\nimport torch\nimport torch.distributed as dist\n\naddr = \"localhost\"\nport = 23456\nos.environ[\"USE_LIBUV\"] = \"0\"\ndist.init_process_group(\n    backend=\"cpu:gloo,cuda:nccl\",\n    rank=0,\n    world_size=1,\n    init_method=f\"tcp://{addr}:{port}\",\n)\ndist.destroy_process_group()"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/intermediate/tensorboard_tutorial.html",
    "title": "Visualizing Models, Data, and Training with TensorBoard\u00b6",
    "code_snippets": [
      "# imports\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nimport torch\nimport torchvision\nimport torchvision.transforms as transforms\n\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\n# transforms\ntransform = transforms.Compose(\n    [transforms.ToTensor(),\n    transforms.Normalize((0.5,), (0.5,))])\n\n# datasets\ntrainset = torchvision.datasets.FashionMNIST('./data',\n    download=True,\n    train=True,\n    transform=transform)\ntestset = torchvision.datasets.FashionMNIST('./data',\n    download=True,\n    train=False,\n    transform=transform)\n\n# dataloaders\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n                                        shuffle=True, num_workers=2)\n\n\ntestloader = torch.utils.data.DataLoader(testset, batch_size=4,\n                                        shuffle=False, num_workers=2)\n\n# constant for classes\nclasses = ('T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n        'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot')\n\n# helper function to show an image\n# (used in the `plot_classes_preds` function below)\ndef matplotlib_imshow(img, one_channel=False):\n    if one_channel:\n        img = img.mean(dim=0)\n    img = img / 2 + 0.5     # unnormalize\n    npimg = img.numpy()\n    if one_channel:\n        plt.imshow(npimg, cmap=\"Greys\")\n    else:\n        plt.imshow(np.transpose(npimg, (1, 2, 0)))",
      "class Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 6, 5)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = x.view(-1, 16 * 4 * 4)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\n\nnet = Net()",
      "from torch.utils.tensorboard import SummaryWriter\n\n# default `log_dir` is \"runs\" - we'll be more specific here\nwriter = SummaryWriter('runs/fashion_mnist_experiment_1')",
      "# helper function\ndef select_n_random(data, labels, n=100):\n    '''\n    Selects n random datapoints and their corresponding labels from a dataset\n    '''\n    assert len(data) == len(labels)\n\n    perm = torch.randperm(len(data))\n    return data[perm][:n], labels[perm][:n]\n\n# select random images and their target indices\nimages, labels = select_n_random(trainset.data, trainset.targets)\n\n# get the class labels for each image\nclass_labels = [classes[lab] for lab in labels]\n\n# log embeddings\nfeatures = images.view(-1, 28 * 28)\nwriter.add_embedding(features,\n                    metadata=class_labels,\n                    label_img=images.unsqueeze(1))\nwriter.close()",
      "# helper functions\n\ndef images_to_probs(net, images):\n    '''\n    Generates predictions and corresponding probabilities from a trained\n    network and a list of images\n    '''\n    output = net(images)\n    # convert output probabilities to predicted class\n    _, preds_tensor = torch.max(output, 1)\n    preds = np.squeeze(preds_tensor.numpy())\n    return preds, [F.softmax(el, dim=0)[i].item() for i, el in zip(preds, output)]\n\n\ndef plot_classes_preds(net, images, labels):\n    '''\n    Generates matplotlib Figure using a trained network, along with images\n    and labels from a batch, that shows the network's top prediction along\n    with its probability, alongside the actual label, coloring this\n    information based on whether the prediction was correct or not.\n    Uses the \"images_to_probs\" function.\n    '''\n    preds, probs = images_to_probs(net, images)\n    # plot the images in the batch, along with predicted and true labels\n    fig = plt.figure(figsize=(12, 48))\n    for idx in np.arange(4):\n        ax = fig.add_subplot(1, 4, idx+1, xticks=[], yticks=[])\n        matplotlib_imshow(images[idx], one_channel=True)\n        ax.set_title(\"{0}, {1:.1f}%\\n(label: {2})\".format(\n            classes[preds[idx]],\n            probs[idx] * 100.0,\n            classes[labels[idx]]),\n                    color=(\"green\" if preds[idx]==labels[idx].item() else \"red\"))\n    return fig",
      "running_loss = 0.0\nfor epoch in range(1):  # loop over the dataset multiple times\n\n    for i, data in enumerate(trainloader, 0):\n\n        # get the inputs; data is a list of [inputs, labels]\n        inputs, labels = data\n\n        # zero the parameter gradients\n        optimizer.zero_grad()\n\n        # forward + backward + optimize\n        outputs = net(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        if i % 1000 == 999:    # every 1000 mini-batches...\n\n            # ...log the running loss\n            writer.add_scalar('training loss',\n                            running_loss / 1000,\n                            epoch * len(trainloader) + i)\n\n            # ...log a Matplotlib Figure showing the model's predictions on a\n            # random mini-batch\n            writer.add_figure('predictions vs. actuals',\n                            plot_classes_preds(net, inputs, labels),\n                            global_step=epoch * len(trainloader) + i)\n            running_loss = 0.0\nprint('Finished Training')",
      "# 1. gets the probability predictions in a test_size x num_classes Tensor\n# 2. gets the preds in a test_size Tensor\n# takes ~10 seconds to run\nclass_probs = []\nclass_label = []\nwith torch.no_grad():\n    for data in testloader:\n        images, labels = data\n        output = net(images)\n        class_probs_batch = [F.softmax(el, dim=0) for el in output]\n\n        class_probs.append(class_probs_batch)\n        class_label.append(labels)\n\ntest_probs = torch.cat([torch.stack(batch) for batch in class_probs])\ntest_label = torch.cat(class_label)\n\n# helper function\ndef add_pr_curve_tensorboard(class_index, test_probs, test_label, global_step=0):\n    '''\n    Takes in a \"class_index\" from 0 to 9 and plots the corresponding\n    precision-recall curve\n    '''\n    tensorboard_truth = test_label == class_index\n    tensorboard_probs = test_probs[:, class_index]\n\n    writer.add_pr_curve(classes[class_index],\n                        tensorboard_truth,\n                        tensorboard_probs,\n                        global_step=global_step)\n    writer.close()\n\n# plot all the pr curves\nfor i in range(len(classes)):\n    add_pr_curve_tensorboard(i, test_probs, test_label)"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/beginner/introyt/captumyt.html",
    "title": "Model Understanding with Captum\u00b6",
    "code_snippets": [
      "import torch\nimport torch.nn.functional as F\nimport torchvision.transforms as transforms\nimport torchvision.models as models\n\nimport captum\nfrom captum.attr import IntegratedGradients, Occlusion, LayerGradCam, LayerAttribution\nfrom captum.attr import visualization as viz\n\nimport os, sys\nimport json\n\nimport numpy as np\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import LinearSegmentedColormap",
      "# model expects 224x224 3-color image\ntransform = transforms.Compose([\n transforms.Resize(224),\n transforms.CenterCrop(224),\n transforms.ToTensor()\n])\n\n# standard ImageNet normalization\ntransform_normalize = transforms.Normalize(\n     mean=[0.485, 0.456, 0.406],\n     std=[0.229, 0.224, 0.225]\n )\n\ntransformed_img = transform(test_img)\ninput_img = transform_normalize(transformed_img)\ninput_img = input_img.unsqueeze(0) # the model requires a dummy batch dimension\n\nlabels_path = 'img/imagenet_class_index.json'\nwith open(labels_path) as json_data:\n    idx_to_labels = json.load(json_data)",
      "output = model(input_img)\noutput = F.softmax(output, dim=1)\nprediction_score, pred_label_idx = torch.topk(output, 1)\npred_label_idx.squeeze_()\npredicted_label = idx_to_labels[str(pred_label_idx.item())][1]\nprint('Predicted:', predicted_label, '(', prediction_score.squeeze().item(), ')')",
      "# Initialize the attribution algorithm with the model\nintegrated_gradients = IntegratedGradients(model)\n\n# Ask the algorithm to attribute our output target to\nattributions_ig = integrated_gradients.attribute(input_img, target=pred_label_idx, n_steps=200)\n\n# Show the original image for comparison\n_ = viz.visualize_image_attr(None, np.transpose(transformed_img.squeeze().cpu().detach().numpy(), (1,2,0)),\n                      method=\"original_image\", title=\"Original Image\")\n\ndefault_cmap = LinearSegmentedColormap.from_list('custom blue',\n                                                 [(0, '#ffffff'),\n                                                  (0.25, '#0000ff'),\n                                                  (1, '#0000ff')], N=256)\n\n_ = viz.visualize_image_attr(np.transpose(attributions_ig.squeeze().cpu().detach().numpy(), (1,2,0)),\n                             np.transpose(transformed_img.squeeze().cpu().detach().numpy(), (1,2,0)),\n                             method='heat_map',\n                             cmap=default_cmap,\n                             show_colorbar=True,\n                             sign='positive',\n                             title='Integrated Gradients')",
      "imgs = ['img/cat.jpg', 'img/teapot.jpg', 'img/trilobite.jpg']\n\nfor img in imgs:\n    img = Image.open(img)\n    transformed_img = transform(img)\n    input_img = transform_normalize(transformed_img)\n    input_img = input_img.unsqueeze(0) # the model requires a dummy batch dimension\n\n    output = model(input_img)\n    output = F.softmax(output, dim=1)\n    prediction_score, pred_label_idx = torch.topk(output, 1)\n    pred_label_idx.squeeze_()\n    predicted_label = idx_to_labels[str(pred_label_idx.item())][1]\n    print('Predicted:', predicted_label, '/', pred_label_idx.item(), ' (', prediction_score.squeeze().item(), ')')",
      "from captum.insights import AttributionVisualizer, Batch\nfrom captum.insights.attr_vis.features import ImageFeature\n\n# Baseline is all-zeros input - this may differ depending on your data\ndef baseline_func(input):\n    return input * 0\n\n# merging our image transforms from above\ndef full_img_transform(input):\n    i = Image.open(input)\n    i = transform(i)\n    i = transform_normalize(i)\n    i = i.unsqueeze(0)\n    return i\n\n\ninput_imgs = torch.cat(list(map(lambda i: full_img_transform(i), imgs)), 0)\n\nvisualizer = AttributionVisualizer(\n    models=[model],\n    score_func=lambda o: torch.nn.functional.softmax(o, 1),\n    classes=list(map(lambda k: idx_to_labels[k][1], idx_to_labels.keys())),\n    features=[\n        ImageFeature(\n            \"Photo\",\n            baseline_transforms=[baseline_func],\n            input_transforms=[],\n        )\n    ],\n    dataset=[Batch(input_imgs, labels=[282,849,69])]\n)"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/recipes/recipes/amp_recipe.html",
    "title": "Automatic Mixed Precision\u00b6",
    "code_snippets": [
      "import torch, time, gc\n\n# Timing utilities\nstart_time = None\n\ndef start_timer():\n    global start_time\n    gc.collect()\n    torch.cuda.empty_cache()\n    torch.cuda.reset_max_memory_allocated()\n    torch.cuda.synchronize()\n    start_time = time.time()\n\ndef end_timer_and_print(local_msg):\n    torch.cuda.synchronize()\n    end_time = time.time()\n    print(\"\\n\" + local_msg)\n    print(\"Total execution time = {:.3f} sec\".format(end_time - start_time))\n    print(\"Max memory used by tensors = {} bytes\".format(torch.cuda.max_memory_allocated()))",
      "def make_model(in_size, out_size, num_layers):\n    layers = []\n    for _ in range(num_layers - 1):\n        layers.append(torch.nn.Linear(in_size, in_size))\n        layers.append(torch.nn.ReLU())\n    layers.append(torch.nn.Linear(in_size, out_size))\n    return torch.nn.Sequential(*tuple(layers)).cuda()",
      "batch_size = 512 # Try, for example, 128, 256, 513.\nin_size = 4096\nout_size = 4096\nnum_layers = 3\nnum_batches = 50\nepochs = 3\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\ntorch.set_default_device(device)\n\n# Creates data in default precision.\n# The same data is used for both default and mixed precision trials below.\n# You don't need to manually change inputs' ``dtype`` when enabling mixed precision.\ndata = [torch.randn(batch_size, in_size) for _ in range(num_batches)]\ntargets = [torch.randn(batch_size, out_size) for _ in range(num_batches)]\n\nloss_fn = torch.nn.MSELoss().cuda()",
      "net = make_model(in_size, out_size, num_layers)\nopt = torch.optim.SGD(net.parameters(), lr=0.001)\n\nstart_timer()\nfor epoch in range(epochs):\n    for input, target in zip(data, targets):\n        output = net(input)\n        loss = loss_fn(output, target)\n        loss.backward()\n        opt.step()\n        opt.zero_grad() # set_to_none=True here can modestly improve performance\nend_timer_and_print(\"Default precision:\")",
      "for epoch in range(0): # 0 epochs, this section is for illustration only\n    for input, target in zip(data, targets):\n        # Runs the forward pass under ``autocast``.\n        with torch.autocast(device_type=device, dtype=torch.float16):\n            output = net(input)\n            # output is float16 because linear layers ``autocast`` to float16.\n            assert output.dtype is torch.float16\n\n            loss = loss_fn(output, target)\n            # loss is float32 because ``mse_loss`` layers ``autocast`` to float32.\n            assert loss.dtype is torch.float32\n\n        # Exits ``autocast`` before backward().\n        # Backward passes under ``autocast`` are not recommended.\n        # Backward ops run in the same ``dtype`` ``autocast`` chose for corresponding forward ops.\n        loss.backward()\n        opt.step()\n        opt.zero_grad() # set_to_none=True here can modestly improve performance",
      "# Constructs a ``scaler`` once, at the beginning of the convergence run, using default arguments.\n# If your network fails to converge with default ``GradScaler`` arguments, please file an issue.\n# The same ``GradScaler`` instance should be used for the entire convergence run.\n# If you perform multiple convergence runs in the same script, each run should use\n# a dedicated fresh ``GradScaler`` instance. ``GradScaler`` instances are lightweight.\nscaler = torch.amp.GradScaler(\"cuda\")\n\nfor epoch in range(0): # 0 epochs, this section is for illustration only\n    for input, target in zip(data, targets):\n        with torch.autocast(device_type=device, dtype=torch.float16):\n            output = net(input)\n            loss = loss_fn(output, target)\n\n        # Scales loss. Calls ``backward()`` on scaled loss to create scaled gradients.\n        scaler.scale(loss).backward()\n\n        # ``scaler.step()`` first unscales the gradients of the optimizer's assigned parameters.\n        # If these gradients do not contain ``inf``s or ``NaN``s, optimizer.step() is then called,\n        # otherwise, optimizer.step() is skipped.\n        scaler.step(opt)\n\n        # Updates the scale for next iteration.\n        scaler.update()\n\n        opt.zero_grad() # set_to_none=True here can modestly improve performance",
      "use_amp = True\n\nnet = make_model(in_size, out_size, num_layers)\nopt = torch.optim.SGD(net.parameters(), lr=0.001)\nscaler = torch.amp.GradScaler(\"cuda\" ,enabled=use_amp)\n\nstart_timer()\nfor epoch in range(epochs):\n    for input, target in zip(data, targets):\n        with torch.autocast(device_type=device, dtype=torch.float16, enabled=use_amp):\n            output = net(input)\n            loss = loss_fn(output, target)\n        scaler.scale(loss).backward()\n        scaler.step(opt)\n        scaler.update()\n        opt.zero_grad() # set_to_none=True here can modestly improve performance\nend_timer_and_print(\"Mixed precision:\")",
      "for epoch in range(0): # 0 epochs, this section is for illustration only\n    for input, target in zip(data, targets):\n        with torch.autocast(device_type=device, dtype=torch.float16):\n            output = net(input)\n            loss = loss_fn(output, target)\n        scaler.scale(loss).backward()\n\n        # Unscales the gradients of optimizer's assigned parameters in-place\n        scaler.unscale_(opt)\n\n        # Since the gradients of optimizer's assigned parameters are now unscaled, clips as usual.\n        # You may use the same value for max_norm here as you would without gradient scaling.\n        torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm=0.1)\n\n        scaler.step(opt)\n        scaler.update()\n        opt.zero_grad() # set_to_none=True here can modestly improve performance",
      "checkpoint = {\"model\": net.state_dict(),\n              \"optimizer\": opt.state_dict(),\n              \"scaler\": scaler.state_dict()}\n# Write checkpoint as desired, e.g.,\n# torch.save(checkpoint, \"filename\")",
      "dev = torch.cuda.current_device()\ncheckpoint = torch.load(\"filename\",\n                        map_location = lambda storage, loc: storage.cuda(dev))"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/intermediate/pinmem_nonblock.html",
    "title": "A guide on good usage of non_blocking and pin_memory() in PyTorch\u00b6",
    "code_snippets": [
      "import torch\n\nassert torch.cuda.is_available(), \"A cuda device is required to run this tutorial\"",
      "import contextlib\n\nfrom torch.cuda import Stream\n\n\ns = Stream()\n\ntorch.manual_seed(42)\nt1_cpu_pinned = torch.randn(1024**2 * 5, pin_memory=True)\nt2_cpu_paged = torch.randn(1024**2 * 5, pin_memory=False)\nt3_cuda = torch.randn(1024**2 * 5, device=\"cuda:0\")\n\nassert torch.cuda.is_available()\ndevice = torch.device(\"cuda\", torch.cuda.current_device())\n\n\n# The function we want to profile\ndef inner(pinned: bool, streamed: bool):\n    with torch.cuda.stream(s) if streamed else contextlib.nullcontext():\n        if pinned:\n            t1_cuda = t1_cpu_pinned.to(device, non_blocking=True)\n        else:\n            t2_cuda = t2_cpu_paged.to(device, non_blocking=True)\n        t_star_cuda_h2d_event = s.record_event()\n    # This operation can be executed during the CPU to GPU copy if and only if the tensor is pinned and the copy is\n    #  done in the other stream\n    t3_cuda_mul = t3_cuda * t3_cuda * t3_cuda\n    t3_cuda_h2d_event = torch.cuda.current_stream().record_event()\n    t_star_cuda_h2d_event.synchronize()\n    t3_cuda_h2d_event.synchronize()\n\n\n# Our profiler: profiles the `inner` function and stores the results in a .json file\ndef benchmark_with_profiler(\n    pinned,\n    streamed,\n) -> None:\n    torch._C._profiler._set_cuda_sync_enabled_val(True)\n    wait, warmup, active = 1, 1, 2\n    num_steps = wait + warmup + active\n    rank = 0\n    with torch.profiler.profile(\n        activities=[\n            torch.profiler.ProfilerActivity.CPU,\n            torch.profiler.ProfilerActivity.CUDA,\n        ],\n        schedule=torch.profiler.schedule(\n            wait=wait, warmup=warmup, active=active, repeat=1, skip_first=1\n        ),\n    ) as prof:\n        for step_idx in range(1, num_steps + 1):\n            inner(streamed=streamed, pinned=pinned)\n            if rank is None or rank == 0:\n                prof.step()\n    prof.export_chrome_trace(f\"trace_streamed{int(streamed)}_pinned{int(pinned)}.json\")",
      "import torch\nimport gc\nfrom torch.utils.benchmark import Timer\nimport matplotlib.pyplot as plt\n\n\ndef timer(cmd):\n    median = (\n        Timer(cmd, globals=globals())\n        .adaptive_autorange(min_run_time=1.0, max_run_time=20.0)\n        .median\n        * 1000\n    )\n    print(f\"{cmd}: {median: 4.4f} ms\")\n    return median\n\n\n# A tensor in pageable memory\npageable_tensor = torch.randn(1_000_000)\n\n# A tensor in page-locked (pinned) memory\npinned_tensor = torch.randn(1_000_000, pin_memory=True)\n\n# Runtimes:\npageable_to_device = timer(\"pageable_tensor.to('cuda:0')\")\npinned_to_device = timer(\"pinned_tensor.to('cuda:0')\")\npin_mem = timer(\"pageable_tensor.pin_memory()\")\npin_mem_to_device = timer(\"pageable_tensor.pin_memory().to('cuda:0')\")\n\n# Ratios:\nr1 = pinned_to_device / pageable_to_device\nr2 = pin_mem_to_device / pageable_to_device\n\n# Create a figure with the results\nfig, ax = plt.subplots()\n\nxlabels = [0, 1, 2]\nbar_labels = [\n    \"pageable_tensor.to(device) (1x)\",\n    f\"pinned_tensor.to(device) ({r1:4.2f}x)\",\n    f\"pageable_tensor.pin_memory().to(device) ({r2:4.2f}x)\"\n    f\"\\npin_memory()={100*pin_mem/pin_mem_to_device:.2f}% of runtime.\",\n]\nvalues = [pageable_to_device, pinned_to_device, pin_mem_to_device]\ncolors = [\"tab:blue\", \"tab:red\", \"tab:orange\"]\nax.bar(xlabels, values, label=bar_labels, color=colors)\n\nax.set_ylabel(\"Runtime (ms)\")\nax.set_title(\"Device casting runtime (pin-memory)\")\nax.set_xticks([])\nax.legend()\n\nplt.show()\n\n# Clear tensors\ndel pageable_tensor, pinned_tensor\n_ = gc.collect()",
      "# A simple loop that copies all tensors to cuda\ndef copy_to_device(*tensors):\n    result = []\n    for tensor in tensors:\n        result.append(tensor.to(\"cuda:0\"))\n    return result\n\n\n# A loop that copies all tensors to cuda asynchronously\ndef copy_to_device_nonblocking(*tensors):\n    result = []\n    for tensor in tensors:\n        result.append(tensor.to(\"cuda:0\", non_blocking=True))\n    # We need to synchronize\n    torch.cuda.synchronize()\n    return result\n\n\n# Create a list of tensors\ntensors = [torch.randn(1000) for _ in range(1000)]\nto_device = timer(\"copy_to_device(*tensors)\")\nto_device_nonblocking = timer(\"copy_to_device_nonblocking(*tensors)\")\n\n# Ratio\nr1 = to_device_nonblocking / to_device\n\n# Plot the results\nfig, ax = plt.subplots()\n\nxlabels = [0, 1]\nbar_labels = [f\"to(device) (1x)\", f\"to(device, non_blocking=True) ({r1:4.2f}x)\"]\ncolors = [\"tab:blue\", \"tab:red\"]\nvalues = [to_device, to_device_nonblocking]\n\nax.bar(xlabels, values, label=bar_labels, color=colors)\n\nax.set_ylabel(\"Runtime (ms)\")\nax.set_title(\"Device casting runtime (non-blocking)\")\nax.set_xticks([])\nax.legend()\n\nplt.show()",
      "from torch.profiler import profile, ProfilerActivity\n\n\ndef profile_mem(cmd):\n    with profile(activities=[ProfilerActivity.CPU]) as prof:\n        exec(cmd)\n    print(cmd)\n    print(prof.key_averages().table(row_limit=10))",
      "def pin_copy_to_device(*tensors):\n    result = []\n    for tensor in tensors:\n        result.append(tensor.pin_memory().to(\"cuda:0\"))\n    return result\n\n\ndef pin_copy_to_device_nonblocking(*tensors):\n    result = []\n    for tensor in tensors:\n        result.append(tensor.pin_memory().to(\"cuda:0\", non_blocking=True))\n    # We need to synchronize\n    torch.cuda.synchronize()\n    return result",
      "tensors = [torch.randn(1_000_000) for _ in range(1000)]\npage_copy = timer(\"copy_to_device(*tensors)\")\npage_copy_nb = timer(\"copy_to_device_nonblocking(*tensors)\")\n\ntensors_pinned = [torch.randn(1_000_000, pin_memory=True) for _ in range(1000)]\npinned_copy = timer(\"copy_to_device(*tensors_pinned)\")\npinned_copy_nb = timer(\"copy_to_device_nonblocking(*tensors_pinned)\")\n\npin_and_copy = timer(\"pin_copy_to_device(*tensors)\")\npin_and_copy_nb = timer(\"pin_copy_to_device_nonblocking(*tensors)\")\n\n# Plot\nstrategies = (\"pageable copy\", \"pinned copy\", \"pin and copy\")\nblocking = {\n    \"blocking\": [page_copy, pinned_copy, pin_and_copy],\n    \"non-blocking\": [page_copy_nb, pinned_copy_nb, pin_and_copy_nb],\n}\n\nx = torch.arange(3)\nwidth = 0.25\nmultiplier = 0\n\n\nfig, ax = plt.subplots(layout=\"constrained\")\n\nfor attribute, runtimes in blocking.items():\n    offset = width * multiplier\n    rects = ax.bar(x + offset, runtimes, width, label=attribute)\n    ax.bar_label(rects, padding=3, fmt=\"%.2f\")\n    multiplier += 1\n\n# Add some text for labels, title and custom x-axis tick labels, etc.\nax.set_ylabel(\"Runtime (ms)\")\nax.set_title(\"Runtime (pin-mem and non-blocking)\")\nax.set_xticks([0, 1, 2])\nax.set_xticklabels(strategies)\nplt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\nax.legend(loc=\"upper left\", ncols=3)\n\nplt.show()\n\ndel tensors, tensors_pinned\n_ = gc.collect()",
      "DELAY = 100000000\ntry:\n    i = -1\n    for i in range(100):\n        # Create a tensor in pin-memory\n        cpu_tensor = torch.ones(1024, 1024, pin_memory=True)\n        torch.cuda.synchronize()\n        # Send the tensor to CUDA\n        cuda_tensor = cpu_tensor.to(\"cuda\", non_blocking=True)\n        torch.cuda._sleep(DELAY)\n        # Corrupt the original tensor\n        cpu_tensor.zero_()\n        assert (cuda_tensor == 1).all()\n    print(\"No test failed with non_blocking and pinned tensor\")\nexcept AssertionError:\n    print(f\"{i}th test failed with non_blocking and pinned tensor. Skipping remaining tests\")",
      "i = -1\nfor i in range(100):\n    # Create a tensor in pageable memory\n    cpu_tensor = torch.ones(1024, 1024)\n    torch.cuda.synchronize()\n    # Send the tensor to CUDA\n    cuda_tensor = cpu_tensor.to(\"cuda\", non_blocking=True)\n    torch.cuda._sleep(DELAY)\n    # Corrupt the original tensor\n    cpu_tensor.zero_()\n    assert (cuda_tensor == 1).all()\nprint(\"No test failed with non_blocking and pageable tensor\")",
      "tensor = (\n    torch.arange(1, 1_000_000, dtype=torch.double, device=\"cuda\")\n    .expand(100, 999999)\n    .clone()\n)\ntorch.testing.assert_close(\n    tensor.mean(), torch.tensor(500_000, dtype=torch.double, device=\"cuda\")\n), tensor.mean()\ntry:\n    i = -1\n    for i in range(100):\n        cpu_tensor = tensor.to(\"cpu\", non_blocking=True)\n        torch.testing.assert_close(\n            cpu_tensor.mean(), torch.tensor(500_000, dtype=torch.double)\n        )\n    print(\"No test failed with non_blocking\")\nexcept AssertionError:\n    print(f\"{i}th test failed with non_blocking. Skipping remaining tests\")\ntry:\n    i = -1\n    for i in range(100):\n        cpu_tensor = tensor.to(\"cpu\", non_blocking=True)\n        torch.cuda.synchronize()\n        torch.testing.assert_close(\n            cpu_tensor.mean(), torch.tensor(500_000, dtype=torch.double)\n        )\n    print(\"No test failed with synchronize\")\nexcept AssertionError:\n    print(f\"One test failed with synchronize: {i}th assertion!\")",
      "from tensordict import TensorDict\nimport torch\nfrom torch.utils.benchmark import Timer\nimport matplotlib.pyplot as plt\n\n# Create the dataset\ntd = TensorDict({str(i): torch.randn(1_000_000) for i in range(1000)})\n\n# Runtimes\ncopy_blocking = timer(\"td.to('cuda:0', non_blocking=False)\")\ncopy_non_blocking = timer(\"td.to('cuda:0')\")\ncopy_pin_nb = timer(\"td.to('cuda:0', non_blocking_pin=True, num_threads=0)\")\ncopy_pin_multithread_nb = timer(\"td.to('cuda:0', non_blocking_pin=True, num_threads=4)\")\n\n# Rations\nr1 = copy_non_blocking / copy_blocking\nr2 = copy_pin_nb / copy_blocking\nr3 = copy_pin_multithread_nb / copy_blocking\n\n# Figure\nfig, ax = plt.subplots()\n\nxlabels = [0, 1, 2, 3]\nbar_labels = [\n    \"Blocking copy (1x)\",\n    f\"Non-blocking copy ({r1:4.2f}x)\",\n    f\"Blocking pin, non-blocking copy ({r2:4.2f}x)\",\n    f\"Non-blocking pin, non-blocking copy ({r3:4.2f}x)\",\n]\nvalues = [copy_blocking, copy_non_blocking, copy_pin_nb, copy_pin_multithread_nb]\ncolors = [\"tab:blue\", \"tab:red\", \"tab:orange\", \"tab:green\"]\n\nax.bar(xlabels, values, label=bar_labels, color=colors)\n\nax.set_ylabel(\"Runtime (ms)\")\nax.set_title(\"Device casting runtime\")\nax.set_xticks([])\nax.legend()\n\nplt.show()"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/recipes/amx.html",
    "title": "Leverage Intel\u00ae Advanced Matrix Extensions\u00b6",
    "code_snippets": [
      "model = model.to(memory_format=torch.channels_last)\nwith torch.cpu.amp.autocast():\n   output = model(input)",
      "with torch.backends.mkldnn.verbose(torch.backends.mkldnn.VERBOSE_ON):\n    with torch.cpu.amp.autocast():\n        model(input)",
      "onednn_verbose,info,oneDNN v2.7.3 (commit 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)\nonednn_verbose,info,cpu,runtime:OpenMP,nthr:128\nonednn_verbose,info,cpu,isa:Intel AVX-512 with float16, Intel DL Boost and bfloat16 support and Intel AMX with bfloat16 and 8-bit integer support\nonednn_verbose,info,gpu,runtime:none\nonednn_verbose,info,prim_template:operation,engine,primitive,implementation,prop_kind,memory_descriptors,attributes,auxiliary,problem_desc,exec_time\nonednn_verbose,exec,cpu,reorder,simple:any,undef,src_f32::blocked:a:f0 dst_f32::blocked:a:f0,attr-scratchpad:user ,,2,5.2561\n...\nonednn_verbose,exec,cpu,convolution,jit:avx512_core_amx_bf16,forward_training,src_bf16::blocked:acdb:f0 wei_bf16:p:blocked:ABcd16b16a2b:f0 bia_f32::blocked:a:f0 dst_bf16::blocked:acdb:f0,attr-scratchpad:user ,alg:convolution_direct,mb7_ic2oc1_ih224oh111kh3sh2dh1ph1_iw224ow111kw3sw2dw1pw1,0.628906\n...\nonednn_verbose,exec,cpu,matmul,brg:avx512_core_amx_int8,undef,src_s8::blocked:ab:f0 wei_s8:p:blocked:BA16a64b4a:f0 dst_s8::blocked:ab:f0,attr-scratchpad:user ,,1x30522:30522x768:1x768,7.66382\n..."
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/advanced/rpc_ddp_tutorial.html",
    "title": "Combining Distributed DataParallel with Distributed RPC Framework\u00b6",
    "code_snippets": [
      "def run_worker(rank, world_size):\n    r\"\"\"\n    A wrapper function that initializes RPC, calls the function, and shuts down\n    RPC.\n    \"\"\"\n\n    # We need to use different port numbers in TCP init_method for init_rpc and\n    # init_process_group to avoid port conflicts.\n    rpc_backend_options = TensorPipeRpcBackendOptions()\n    rpc_backend_options.init_method = \"tcp://localhost:29501\"\n\n    # Rank 2 is master, 3 is ps and 0 and 1 are trainers.\n    if rank == 2:\n        rpc.init_rpc(\n            \"master\",\n            rank=rank,\n            world_size=world_size,\n            rpc_backend_options=rpc_backend_options,\n        )\n\n        remote_emb_module = RemoteModule(\n            \"ps\",\n            torch.nn.EmbeddingBag,\n            args=(NUM_EMBEDDINGS, EMBEDDING_DIM),\n            kwargs={\"mode\": \"sum\"},\n        )\n\n        # Run the training loop on trainers.\n        futs = []\n        for trainer_rank in [0, 1]:\n            trainer_name = \"trainer{}\".format(trainer_rank)\n            fut = rpc.rpc_async(\n                trainer_name, _run_trainer, args=(remote_emb_module, trainer_rank)\n            )\n            futs.append(fut)\n\n        # Wait for all training to finish.\n        for fut in futs:\n            fut.wait()\n    elif rank <= 1:\n        # Initialize process group for Distributed DataParallel on trainers.\n        dist.init_process_group(\n            backend=\"gloo\", rank=rank, world_size=2, init_method=\"tcp://localhost:29500\"\n        )\n\n        # Initialize RPC.\n        trainer_name = \"trainer{}\".format(rank)\n        rpc.init_rpc(\n            trainer_name,\n            rank=rank,\n            world_size=world_size,\n            rpc_backend_options=rpc_backend_options,\n        )\n\n        # Trainer just waits for RPCs from master.\n    else:\n        rpc.init_rpc(\n            \"ps\",\n            rank=rank,\n            world_size=world_size,\n            rpc_backend_options=rpc_backend_options,\n        )\n        # parameter server do nothing\n        pass\n\n    # block until all rpcs finish\n    rpc.shutdown()\n\n\nif __name__ == \"__main__\":\n    # 2 trainers, 1 parameter server, 1 master.\n    world_size = 4\n    mp.spawn(run_worker, args=(world_size,), nprocs=world_size, join=True)",
      "class HybridModel(torch.nn.Module):\n    r\"\"\"\n    The model consists of a sparse part and a dense part.\n    1) The dense part is an nn.Linear module that is replicated across all trainers using DistributedDataParallel.\n    2) The sparse part is a Remote Module that holds an nn.EmbeddingBag on the parameter server.\n    This remote model can get a Remote Reference to the embedding table on the parameter server.\n    \"\"\"\n\n    def __init__(self, remote_emb_module, device):\n        super(HybridModel, self).__init__()\n        self.remote_emb_module = remote_emb_module\n        self.fc = DDP(torch.nn.Linear(16, 8).cuda(device), device_ids=[device])\n        self.device = device\n\n    def forward(self, indices, offsets):\n        emb_lookup = self.remote_emb_module.forward(indices, offsets)\n        return self.fc(emb_lookup.cuda(self.device))",
      "def _run_trainer(remote_emb_module, rank):\n    r\"\"\"\n    Each trainer runs a forward pass which involves an embedding lookup on the\n    parameter server and running nn.Linear locally. During the backward pass,\n    DDP is responsible for aggregating the gradients for the dense part\n    (nn.Linear) and distributed autograd ensures gradients updates are\n    propagated to the parameter server.\n    \"\"\"\n\n    # Setup the model.\n    model = HybridModel(remote_emb_module, rank)\n\n    # Retrieve all model parameters as rrefs for DistributedOptimizer.\n\n    # Retrieve parameters for embedding table.\n    model_parameter_rrefs = model.remote_emb_module.remote_parameters()\n\n    # model.fc.parameters() only includes local parameters.\n    # NOTE: Cannot call model.parameters() here,\n    # because this will call remote_emb_module.parameters(),\n    # which supports remote_parameters() but not parameters().\n    for param in model.fc.parameters():\n        model_parameter_rrefs.append(RRef(param))\n\n    # Setup distributed optimizer\n    opt = DistributedOptimizer(\n        optim.SGD,\n        model_parameter_rrefs,\n        lr=0.05,\n    )\n\n    criterion = torch.nn.CrossEntropyLoss()",
      "def get_next_batch(rank):\n        for _ in range(10):\n            num_indices = random.randint(20, 50)\n            indices = torch.LongTensor(num_indices).random_(0, NUM_EMBEDDINGS)\n\n            # Generate offsets.\n            offsets = []\n            start = 0\n            batch_size = 0\n            while start < num_indices:\n                offsets.append(start)\n                start += random.randint(1, 10)\n                batch_size += 1\n\n            offsets_tensor = torch.LongTensor(offsets)\n            target = torch.LongTensor(batch_size).random_(8).cuda(rank)\n            yield indices, offsets_tensor, target\n\n    # Train for 100 epochs\n    for epoch in range(100):\n        # create distributed autograd context\n        for indices, offsets, target in get_next_batch(rank):\n            with dist_autograd.context() as context_id:\n                output = model(indices, offsets)\n                loss = criterion(output, target)\n\n                # Run distributed backward pass\n                dist_autograd.backward(context_id, [loss])\n\n                # Tun distributed optimizer\n                opt.step(context_id)\n\n                # Not necessary to zero grads as each iteration creates a different\n                # distributed autograd context which hosts different grads\n        print(\"Training done for epoch {}\".format(epoch))"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/beginner/onnx/intro_onnx.html",
    "title": "Introduction to ONNX\u00b6",
    "code_snippets": [
      "import torch\nprint(torch.__version__)\n\nimport onnxscript\nprint(onnxscript.__version__)\n\nimport onnxruntime\nprint(onnxruntime.__version__)"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/intermediate/torchserve_with_ipex_2.html",
    "title": "Grokking PyTorch Intel CPU performance from first principles (Part 2)\u00b6",
    "code_snippets": [
      "import torch\nimport torchvision.models as models\nimport time\n\nmodel = models.resnet50(pretrained=False)\nmodel.eval()\nbatch_size = 32\ndata = torch.rand(batch_size, 3, 224, 224)\n\n# warm up\nfor _ in range(100):\n    model(data)\n\n# measure\n# Intel\u00ae VTune Profiler's ITT context manager\nwith torch.autograd.profiler.emit_itt():\n    start = time.time()\n    for i in range(100):\n   # Intel\u00ae VTune Profiler's ITT to annotate each step\n        torch.profiler.itt.range_push('step_{}'.format(i))\n        model(data)\n        torch.profiler.itt.range_pop()\n    end = time.time()\n\nprint('Inference took {:.2f} ms in average'.format((end-start)/100*1000))",
      "cpu_launcher_enable=true\ncpu_launcher_args=--node_id 0 --use_default_allocator",
      "import torch\n\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.conv = torch.nn.Conv2d(16, 33, 3, stride=2)\n        self.relu = torch.nn.ReLU()\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.relu(x)\n        return x\n\nmodel = Model()\nmodel.eval()\ndata = torch.rand(20, 16, 50, 100)\n\n#################### code changes ####################\nimport intel_extension_for_pytorch as ipex\nmodel = ipex.optimize(model)\n######################################################\n\nprint(model)",
      "import torch\n\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.conv = torch.nn.Conv2d(16, 33, 3, stride=2)\n        self.relu = torch.nn.ReLU()\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.relu(x)\n        return x\n\nmodel = Model()\nmodel.eval()\ndata = torch.rand(20, 16, 50, 100)\n\n#################### code changes ####################\nimport intel_extension_for_pytorch as ipex\nmodel = ipex.optimize(model)\n######################################################\n\n# torchscript\nwith torch.no_grad():\n    model = torch.jit.trace(model, data)\n    model = torch.jit.freeze(model)",
      "import torch\n\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.conv = torch.nn.Conv2d(16, 33, 3, stride=2)\n        self.relu = torch.nn.ReLU()\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.relu(x)\n        return x\n\nmodel = Model()\nmodel.eval()\ndata = torch.rand(20, 16, 50, 100)\n\nimport intel_extension_for_pytorch as ipex\n############################### code changes ###############################\nipex.disable_auto_channels_last() # omit this line for channels_last (default)\n############################################################################\nmodel = ipex.optimize(model)\n\nwith torch.no_grad():\n    model = torch.jit.trace(model, data)\n    model = torch.jit.freeze(model)"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/recipes/cuda_rpc.html",
    "title": "Direct Device-to-Device Communication with TensorPipe CUDA RPC\u00b6",
    "code_snippets": [
      "import torch\nimport torch.distributed.autograd as autograd\nimport torch.distributed.rpc as rpc\nimport torch.multiprocessing as mp\nimport torch.nn as nn\n\nimport os\nimport time\n\n\nclass MyModule(nn.Module):\n    def __init__(self, device, comm_mode):\n        super().__init__()\n        self.device = device\n        self.linear = nn.Linear(1000, 1000).to(device)\n        self.comm_mode = comm_mode\n\n    def forward(self, x):\n        # x.to() is a no-op if x is already on self.device\n        y = self.linear(x.to(self.device))\n        return y.cpu() if self.comm_mode == \"cpu\" else y\n\n    def parameter_rrefs(self):\n        return [rpc.RRef(p) for p in self.parameters()]\n\n\ndef measure(comm_mode):\n    # local module on \"worker0/cuda:0\"\n    lm = MyModule(\"cuda:0\", comm_mode)\n    # remote module on \"worker1/cuda:1\"\n    rm = rpc.remote(\"worker1\", MyModule, args=(\"cuda:1\", comm_mode))\n    # prepare random inputs\n    x = torch.randn(1000, 1000).cuda(0)\n\n    tik = time.time()\n    for _ in range(10):\n        with autograd.context() as ctx:\n            y = rm.rpc_sync().forward(lm(x))\n            autograd.backward(ctx, [y.sum()])\n    # synchronize on \"cuda:0\" to make sure that all pending CUDA ops are\n    # included in the measurements\n    torch.cuda.current_stream(\"cuda:0\").synchronize()\n    tok = time.time()\n    print(f\"{comm_mode} RPC total execution time: {tok - tik}\")\n\n\ndef run_worker(rank):\n    os.environ['MASTER_ADDR'] = 'localhost'\n    os.environ['MASTER_PORT'] = '29500'\n    options = rpc.TensorPipeRpcBackendOptions(num_worker_threads=128)\n\n    if rank == 0:\n        options.set_device_map(\"worker1\", {0: 1})\n        rpc.init_rpc(\n            f\"worker{rank}\",\n            rank=rank,\n            world_size=2,\n            rpc_backend_options=options\n        )\n        measure(comm_mode=\"cpu\")\n        measure(comm_mode=\"cuda\")\n    else:\n        rpc.init_rpc(\n            f\"worker{rank}\",\n            rank=rank,\n            world_size=2,\n            rpc_backend_options=options\n        )\n\n    # block until all rpcs finish\n    rpc.shutdown()\n\n\nif __name__==\"__main__\":\n    world_size = 2\n    mp.spawn(run_worker, nprocs=world_size, join=True)"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/recipes/distributed_device_mesh.html",
    "title": "Getting Started with DeviceMesh\u00b6",
    "code_snippets": [
      "import os\n\nimport torch\nimport torch.distributed as dist\n\n# Understand world topology\nrank = int(os.environ[\"RANK\"])\nworld_size = int(os.environ[\"WORLD_SIZE\"])\nprint(f\"Running example on {rank=} in a world with {world_size=}\")\n\n# Create process groups to manage 2-D like parallel pattern\ndist.init_process_group(\"nccl\")\ntorch.cuda.set_device(rank)\n\n# Create shard groups (e.g. (0, 1, 2, 3), (4, 5, 6, 7))\n# and assign the correct shard group to each rank\nnum_node_devices = torch.cuda.device_count()\nshard_rank_lists = list(range(0, num_node_devices // 2)), list(range(num_node_devices // 2, num_node_devices))\nshard_groups = (\n    dist.new_group(shard_rank_lists[0]),\n    dist.new_group(shard_rank_lists[1]),\n)\ncurrent_shard_group = (\n    shard_groups[0] if rank in shard_rank_lists[0] else shard_groups[1]\n)\n\n# Create replicate groups (for example, (0, 4), (1, 5), (2, 6), (3, 7))\n# and assign the correct replicate group to each rank\ncurrent_replicate_group = None\nshard_factor = len(shard_rank_lists[0])\nfor i in range(num_node_devices // 2):\n    replicate_group_ranks = list(range(i, num_node_devices, shard_factor))\n    replicate_group = dist.new_group(replicate_group_ranks)\n    if rank in replicate_group_ranks:\n        current_replicate_group = replicate_group",
      "from torch.distributed.device_mesh import init_device_mesh\nmesh_2d = init_device_mesh(\"cuda\", (2, 4), mesh_dim_names=(\"replicate\", \"shard\"))\n\n# Users can access the underlying process group thru `get_group` API.\nreplicate_group = mesh_2d.get_group(mesh_dim=\"replicate\")\nshard_group = mesh_2d.get_group(mesh_dim=\"shard\")",
      "import torch\nimport torch.nn as nn\n\nfrom torch.distributed.device_mesh import init_device_mesh\nfrom torch.distributed.fsdp import FullyShardedDataParallel as FSDP, ShardingStrategy\n\n\nclass ToyModel(nn.Module):\n    def __init__(self):\n        super(ToyModel, self).__init__()\n        self.net1 = nn.Linear(10, 10)\n        self.relu = nn.ReLU()\n        self.net2 = nn.Linear(10, 5)\n\n    def forward(self, x):\n        return self.net2(self.relu(self.net1(x)))\n\n\n# HSDP: MeshShape(2, 4)\nmesh_2d = init_device_mesh(\"cuda\", (2, 4))\nmodel = FSDP(\n    ToyModel(), device_mesh=mesh_2d, sharding_strategy=ShardingStrategy.HYBRID_SHARD\n)",
      "from torch.distributed.device_mesh import init_device_mesh\nmesh_3d = init_device_mesh(\"cuda\", (2, 2, 2), mesh_dim_names=(\"replicate\", \"shard\", \"tp\"))\n\n# Users can slice child meshes from the parent mesh.\nhsdp_mesh = mesh_3d[\"replicate\", \"shard\"]\ntp_mesh = mesh_3d[\"tp\"]\n\n# Users can access the underlying process group thru `get_group` API.\nreplicate_group = hsdp_mesh[\"replicate\"].get_group()\nshard_group = hsdp_mesh[\"shard\"].get_group()\ntp_group = tp_mesh.get_group()"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/intermediate/rpc_async_execution.html",
    "title": "Implementing Batch RPC Processing Using Asynchronous Executions\u00b6",
    "code_snippets": [
      "import threading\nimport torchvision\nimport torch\nimport torch.distributed.rpc as rpc\nfrom torch import optim\n\nnum_classes, batch_update_size = 30, 5\n\nclass BatchUpdateParameterServer(object):\n    def __init__(self, batch_update_size=batch_update_size):\n        self.model = torchvision.models.resnet50(num_classes=num_classes)\n        self.lock = threading.Lock()\n        self.future_model = torch.futures.Future()\n        self.batch_update_size = batch_update_size\n        self.curr_update_size = 0\n        self.optimizer = optim.SGD(self.model.parameters(), lr=0.001, momentum=0.9)\n        for p in self.model.parameters():\n            p.grad = torch.zeros_like(p)\n\n    def get_model(self):\n        return self.model\n\n    @staticmethod\n    @rpc.functions.async_execution\n    def update_and_fetch_model(ps_rref, grads):\n        # Using the RRef to retrieve the local PS instance\n        self = ps_rref.local_value()\n        with self.lock:\n            self.curr_update_size += 1\n            # accumulate gradients into .grad field\n            for p, g in zip(self.model.parameters(), grads):\n                p.grad += g\n\n            # Save the current future_model and return it to make sure the\n            # returned Future object holds the correct model even if another\n            # thread modifies future_model before this thread returns.\n            fut = self.future_model\n\n            if self.curr_update_size >= self.batch_update_size:\n                # update the model\n                for p in self.model.parameters():\n                    p.grad /= self.batch_update_size\n                self.curr_update_size = 0\n                self.optimizer.step()\n                self.optimizer.zero_grad()\n                # by settiing the result on the Future object, all previous\n                # requests expecting this updated model will be notified and\n                # the their responses will be sent accordingly.\n                fut.set_result(self.model)\n                self.future_model = torch.futures.Future()\n\n        return fut",
      "batch_size, image_w, image_h  = 20, 64, 64\n\nclass Trainer(object):\n    def __init__(self, ps_rref):\n        self.ps_rref, self.loss_fn = ps_rref, torch.nn.MSELoss()\n        self.one_hot_indices = torch.LongTensor(batch_size) \\\n                                    .random_(0, num_classes) \\\n                                    .view(batch_size, 1)\n\n    def get_next_batch(self):\n        for _ in range(6):\n            inputs = torch.randn(batch_size, 3, image_w, image_h)\n            labels = torch.zeros(batch_size, num_classes) \\\n                        .scatter_(1, self.one_hot_indices, 1)\n            yield inputs.cuda(), labels.cuda()\n\n    def train(self):\n        name = rpc.get_worker_info().name\n        # get initial model parameters\n        m = self.ps_rref.rpc_sync().get_model().cuda()\n        # start training\n        for inputs, labels in self.get_next_batch():\n            self.loss_fn(m(inputs), labels).backward()\n            m = rpc.rpc_sync(\n                self.ps_rref.owner(),\n                BatchUpdateParameterServer.update_and_fetch_model,\n                args=(self.ps_rref, [p.grad for p in m.cpu().parameters()]),\n            ).cuda()",
      "import argparse\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nparser = argparse.ArgumentParser(description='PyTorch RPC Batch RL example')\nparser.add_argument('--gamma', type=float, default=1.0, metavar='G',\n                    help='discount factor (default: 1.0)')\nparser.add_argument('--seed', type=int, default=543, metavar='N',\n                    help='random seed (default: 543)')\nparser.add_argument('--num-episode', type=int, default=10, metavar='E',\n                    help='number of episodes (default: 10)')\nargs = parser.parse_args()\n\ntorch.manual_seed(args.seed)\n\nclass Policy(nn.Module):\n    def __init__(self, batch=True):\n        super(Policy, self).__init__()\n        self.affine1 = nn.Linear(4, 128)\n        self.dropout = nn.Dropout(p=0.6)\n        self.affine2 = nn.Linear(128, 2)\n        self.dim = 2 if batch else 1\n\n    def forward(self, x):\n        x = self.affine1(x)\n        x = self.dropout(x)\n        x = F.relu(x)\n        action_scores = self.affine2(x)\n        return F.softmax(action_scores, dim=self.dim)",
      "import gym\nimport torch.distributed.rpc as rpc\n\nclass Observer:\n    def __init__(self, batch=True):\n        self.id = rpc.get_worker_info().id - 1\n        self.env = gym.make('CartPole-v1')\n        self.env.seed(args.seed)\n        self.select_action = Agent.select_action_batch if batch else Agent.select_action",
      "import torch\n\nclass Observer:\n    ...\n\n    def run_episode(self, agent_rref, n_steps):\n        state, ep_reward = self.env.reset(), NUM_STEPS\n        rewards = torch.zeros(n_steps)\n        start_step = 0\n        for step in range(n_steps):\n            state = torch.from_numpy(state).float().unsqueeze(0)\n            # send the state to the agent to get an action\n            action = rpc.rpc_sync(\n                agent_rref.owner(),\n                self.select_action,\n                args=(agent_rref, self.id, state)\n            )\n\n            # apply the action to the environment, and get the reward\n            state, reward, done, _ = self.env.step(action)\n            rewards[step] = reward\n\n            if done or step + 1 >= n_steps:\n                curr_rewards = rewards[start_step:(step + 1)]\n                R = 0\n                for i in range(curr_rewards.numel() -1, -1, -1):\n                    R = curr_rewards[i] + args.gamma * R\n                    curr_rewards[i] = R\n                state = self.env.reset()\n                if start_step == 0:\n                    ep_reward = min(ep_reward, step - start_step + 1)\n                start_step = step + 1\n\n        return [rewards, ep_reward]",
      "import threading\nfrom torch.distributed.rpc import RRef\n\nclass Agent:\n    def __init__(self, world_size, batch=True):\n        self.ob_rrefs = []\n        self.agent_rref = RRef(self)\n        self.rewards = {}\n        self.policy = Policy(batch).cuda()\n        self.optimizer = optim.Adam(self.policy.parameters(), lr=1e-2)\n        self.running_reward = 0\n\n        for ob_rank in range(1, world_size):\n            ob_info = rpc.get_worker_info(OBSERVER_NAME.format(ob_rank))\n            self.ob_rrefs.append(rpc.remote(ob_info, Observer, args=(batch,)))\n            self.rewards[ob_info.id] = []\n\n        self.states = torch.zeros(len(self.ob_rrefs), 1, 4)\n        self.batch = batch\n        self.saved_log_probs = [] if batch else {k:[] for k in range(len(self.ob_rrefs))}\n        self.future_actions = torch.futures.Future()\n        self.lock = threading.Lock()\n        self.pending_states = len(self.ob_rrefs)",
      "from torch.distributions import Categorical\n\nclass Agent:\n    ...\n\n    @staticmethod\n    def select_action(agent_rref, ob_id, state):\n        self = agent_rref.local_value()\n        probs = self.policy(state.cuda())\n        m = Categorical(probs)\n        action = m.sample()\n        self.saved_log_probs[ob_id].append(m.log_prob(action))\n        return action.item()",
      "class Agent:\n    ...\n\n    @staticmethod\n    @rpc.functions.async_execution\n    def select_action_batch(agent_rref, ob_id, state):\n        self = agent_rref.local_value()\n        self.states[ob_id].copy_(state)\n        future_action = self.future_actions.then(\n            lambda future_actions: future_actions.wait()[ob_id].item()\n        )\n\n        with self.lock:\n            self.pending_states -= 1\n            if self.pending_states == 0:\n                self.pending_states = len(self.ob_rrefs)\n                probs = self.policy(self.states.cuda())\n                m = Categorical(probs)\n                actions = m.sample()\n                self.saved_log_probs.append(m.log_prob(actions).t()[0])\n                future_actions = self.future_actions\n                self.future_actions = torch.futures.Future()\n                future_actions.set_result(actions.cpu())\n        return future_action",
      "class Agent:\n    ...\n\n    def run_episode(self, n_steps=0):\n        futs = []\n        for ob_rref in self.ob_rrefs:\n            # make async RPC to kick off an episode on all observers\n            futs.append(ob_rref.rpc_async().run_episode(self.agent_rref, n_steps))\n\n        # wait until all obervers have finished this episode\n        rets = torch.futures.wait_all(futs)\n        rewards = torch.stack([ret[0] for ret in rets]).cuda().t()\n        ep_rewards = sum([ret[1] for ret in rets]) / len(rets)\n\n        # stack saved probs into one tensor\n        if self.batch:\n            probs = torch.stack(self.saved_log_probs)\n        else:\n            probs = [torch.stack(self.saved_log_probs[i]) for i in range(len(rets))]\n            probs = torch.stack(probs)\n\n        policy_loss = -probs * rewards / len(rets)\n        policy_loss.sum().backward()\n        self.optimizer.step()\n        self.optimizer.zero_grad()\n\n        # reset variables\n        self.saved_log_probs = [] if self.batch else {k:[] for k in range(len(self.ob_rrefs))}\n        self.states = torch.zeros(len(self.ob_rrefs), 1, 4)\n\n        # calculate running rewards\n        self.running_reward = 0.5 * ep_rewards + 0.5 * self.running_reward\n        return ep_rewards, self.running_reward",
      "def run_worker(rank, world_size, n_episode, batch, print_log=True):\n    os.environ['MASTER_ADDR'] = 'localhost'\n    os.environ['MASTER_PORT'] = '29500'\n    if rank == 0:\n        # rank0 is the agent\n        rpc.init_rpc(AGENT_NAME, rank=rank, world_size=world_size)\n\n        agent = Agent(world_size, batch)\n        for i_episode in range(n_episode):\n            last_reward, running_reward = agent.run_episode(n_steps=NUM_STEPS)\n\n            if print_log:\n                print('Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}'.format(\n                    i_episode, last_reward, running_reward))\n    else:\n        # other ranks are the observer\n        rpc.init_rpc(OBSERVER_NAME.format(rank), rank=rank, world_size=world_size)\n        # observers passively waiting for instructions from agents\n    rpc.shutdown()\n\n\ndef main():\n    for world_size in range(2, 12):\n        delays = []\n        for batch in [True, False]:\n            tik = time.time()\n            mp.spawn(\n                run_worker,\n                args=(world_size, args.num_episode, batch),\n                nprocs=world_size,\n                join=True\n            )\n            tok = time.time()\n            delays.append(tok - tik)\n\n        print(f\"{world_size}, {delays[0]}, {delays[1]}\")\n\n\nif __name__ == '__main__':\n    main()"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html",
    "title": "Performance Tuning Guide\u00b6",
    "code_snippets": [
      "@torch.compile\ndef gelu(x):\n    return x * 0.5 * (1.0 + torch.erf(x / 1.41421))",
      "# Only this extra line of code is required to use oneDNN Graph\ntorch.jit.enable_onednn_fusion(True)",
      "# sample input should be of the same shape as expected inputs\nsample_input = [torch.rand(32, 3, 224, 224)]\n# Using resnet50 from torchvision in this example for illustrative purposes,\n# but the line below can indeed be modified to use custom models as well.\nmodel = getattr(torchvision.models, \"resnet50\")().eval()\n# Tracing the model with example input\ntraced_model = torch.jit.trace(model, sample_input)\n# Invoking torch.jit.freeze\ntraced_model = torch.jit.freeze(traced_model)",
      "with torch.no_grad():\n    # a couple of warm-up runs\n    traced_model(*sample_input)\n    traced_model(*sample_input)\n    # speedup would be observed after warm-up runs\n    traced_model(*sample_input)",
      "# AMP for JIT mode is enabled by default, and is divergent with its eager mode counterpart\ntorch._C._jit_set_autocast_mode(False)\n\nwith torch.no_grad(), torch.cpu.amp.autocast(cache_enabled=False, dtype=torch.bfloat16):\n    # Conv-BatchNorm folding for CNN-based Vision Models should be done with ``torch.fx.experimental.optimization.fuse`` when AMP is used\n    import torch.fx.experimental.optimization as optimization\n    # Please note that optimization.fuse need not be called when AMP is not used\n    model = optimization.fuse(model)\n    model = torch.jit.trace(model, (example_input))\n    model = torch.jit.freeze(model)\n    # a couple of warm-up runs\n    model(example_input)\n    model(example_input)\n    # speedup would be observed in subsequent runs.\n    model(example_input)",
      "# It can be enabled using\ntorch.compile(m, \"reduce-overhead\")\n# or\ntorch.compile(m, \"max-autotune\")",
      "torch.backends.cudnn.benchmark = True"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/intermediate/reinforcement_ppo.html",
    "title": "Reinforcement Learning (PPO) with TorchRL Tutorial\u00b6",
    "code_snippets": [
      "import warnings\nwarnings.filterwarnings(\"ignore\")\nfrom torch import multiprocessing\n\n\nfrom collections import defaultdict\n\nimport matplotlib.pyplot as plt\nimport torch\nfrom tensordict.nn import TensorDictModule\nfrom tensordict.nn.distributions import NormalParamExtractor\nfrom torch import nn\nfrom torchrl.collectors import SyncDataCollector\nfrom torchrl.data.replay_buffers import ReplayBuffer\nfrom torchrl.data.replay_buffers.samplers import SamplerWithoutReplacement\nfrom torchrl.data.replay_buffers.storages import LazyTensorStorage\nfrom torchrl.envs import (Compose, DoubleToFloat, ObservationNorm, StepCounter,\n                          TransformedEnv)\nfrom torchrl.envs.libs.gym import GymEnv\nfrom torchrl.envs.utils import check_env_specs, ExplorationType, set_exploration_type\nfrom torchrl.modules import ProbabilisticActor, TanhNormal, ValueOperator\nfrom torchrl.objectives import ClipPPOLoss\nfrom torchrl.objectives.value import GAE\nfrom tqdm import tqdm",
      "is_fork = multiprocessing.get_start_method() == \"fork\"\ndevice = (\n    torch.device(0)\n    if torch.cuda.is_available() and not is_fork\n    else torch.device(\"cpu\")\n)\nnum_cells = 256  # number of cells in each layer i.e. output dim.\nlr = 3e-4\nmax_grad_norm = 1.0",
      "normalization constant shape: torch.Size([11])",
      "print(\"observation_spec:\", env.observation_spec)\nprint(\"reward_spec:\", env.reward_spec)\nprint(\"input_spec:\", env.input_spec)\nprint(\"action_spec (as defined by input_spec):\", env.action_spec)",
      "observation_spec: Composite(\n    observation: UnboundedContinuous(\n        shape=torch.Size([11]),\n        space=ContinuousBox(\n            low=Tensor(shape=torch.Size([11]), device=cpu, dtype=torch.float32, contiguous=True),\n            high=Tensor(shape=torch.Size([11]), device=cpu, dtype=torch.float32, contiguous=True)),\n        device=cpu,\n        dtype=torch.float32,\n        domain=continuous),\n    step_count: BoundedDiscrete(\n        shape=torch.Size([1]),\n        space=ContinuousBox(\n            low=Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.int64, contiguous=True),\n            high=Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.int64, contiguous=True)),\n        device=cpu,\n        dtype=torch.int64,\n        domain=discrete),\n    device=cpu,\n    shape=torch.Size([]))\nreward_spec: UnboundedContinuous(\n    shape=torch.Size([1]),\n    space=ContinuousBox(\n        low=Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, contiguous=True),\n        high=Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, contiguous=True)),\n    device=cpu,\n    dtype=torch.float32,\n    domain=continuous)\ninput_spec: Composite(\n    full_state_spec: Composite(\n        step_count: BoundedDiscrete(\n            shape=torch.Size([1]),\n            space=ContinuousBox(\n                low=Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.int64, contiguous=True),\n                high=Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.int64, contiguous=True)),\n            device=cpu,\n            dtype=torch.int64,\n            domain=discrete),\n        device=cpu,\n        shape=torch.Size([])),\n    full_action_spec: Composite(\n        action: BoundedContinuous(\n            shape=torch.Size([1]),\n            space=ContinuousBox(\n                low=Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, contiguous=True),\n                high=Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, contiguous=True)),\n            device=cpu,\n            dtype=torch.float32,\n            domain=continuous),\n        device=cpu,\n        shape=torch.Size([])),\n    device=cpu,\n    shape=torch.Size([]))\naction_spec (as defined by input_spec): BoundedContinuous(\n    shape=torch.Size([1]),\n    space=ContinuousBox(\n        low=Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, contiguous=True),\n        high=Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, contiguous=True)),\n    device=cpu,\n    dtype=torch.float32,\n    domain=continuous)",
      "rollout of three steps: TensorDict(\n    fields={\n        action: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n        done: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n        next: TensorDict(\n            fields={\n                done: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n                observation: Tensor(shape=torch.Size([3, 11]), device=cpu, dtype=torch.float32, is_shared=False),\n                reward: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n                step_count: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.int64, is_shared=False),\n                terminated: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n                truncated: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n            batch_size=torch.Size([3]),\n            device=cpu,\n            is_shared=False),\n        observation: Tensor(shape=torch.Size([3, 11]), device=cpu, dtype=torch.float32, is_shared=False),\n        step_count: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.int64, is_shared=False),\n        terminated: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n        truncated: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n    batch_size=torch.Size([3]),\n    device=cpu,\n    is_shared=False)\nShape of the rollout TensorDict: torch.Size([3])",
      "policy_module = ProbabilisticActor(\n    module=policy_module,\n    spec=env.action_spec,\n    in_keys=[\"loc\", \"scale\"],\n    distribution_class=TanhNormal,\n    distribution_kwargs={\n        \"low\": env.action_spec.space.low,\n        \"high\": env.action_spec.space.high,\n    },\n    return_log_prob=True,\n    # we'll need the log-prob for the numerator of the importance weights\n)",
      "Running policy: TensorDict(\n    fields={\n        action: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, is_shared=False),\n        done: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n        loc: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, is_shared=False),\n        observation: Tensor(shape=torch.Size([11]), device=cpu, dtype=torch.float32, is_shared=False),\n        sample_log_prob: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),\n        scale: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, is_shared=False),\n        step_count: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.int64, is_shared=False),\n        terminated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n        truncated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False)},\n    batch_size=torch.Size([]),\n    device=cpu,\n    is_shared=False)\nRunning value: TensorDict(\n    fields={\n        done: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n        observation: Tensor(shape=torch.Size([11]), device=cpu, dtype=torch.float32, is_shared=False),\n        state_value: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, is_shared=False),\n        step_count: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.int64, is_shared=False),\n        terminated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n        truncated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False)},\n    batch_size=torch.Size([]),\n    device=cpu,\n    is_shared=False)",
      "advantage_module = GAE(\n    gamma=gamma, lmbda=lmbda, value_network=value_module, average_gae=True, device=device,\n)\n\nloss_module = ClipPPOLoss(\n    actor_network=policy_module,\n    critic_network=value_module,\n    clip_epsilon=clip_epsilon,\n    entropy_bonus=bool(entropy_eps),\n    entropy_coef=entropy_eps,\n    # these keys match by default but we set this for completeness\n    critic_coef=1.0,\n    loss_critic_type=\"smooth_l1\",\n)\n\noptim = torch.optim.Adam(loss_module.parameters(), lr)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n    optim, total_frames // frames_per_batch, 0.0\n)",
      "logs = defaultdict(list)\npbar = tqdm(total=total_frames)\neval_str = \"\"\n\n# We iterate over the collector until it reaches the total number of frames it was\n# designed to collect:\nfor i, tensordict_data in enumerate(collector):\n    # we now have a batch of data to work with. Let's learn something from it.\n    for _ in range(num_epochs):\n        # We'll need an \"advantage\" signal to make PPO work.\n        # We re-compute it at each epoch as its value depends on the value\n        # network which is updated in the inner loop.\n        advantage_module(tensordict_data)\n        data_view = tensordict_data.reshape(-1)\n        replay_buffer.extend(data_view.cpu())\n        for _ in range(frames_per_batch // sub_batch_size):\n            subdata = replay_buffer.sample(sub_batch_size)\n            loss_vals = loss_module(subdata.to(device))\n            loss_value = (\n                loss_vals[\"loss_objective\"]\n                + loss_vals[\"loss_critic\"]\n                + loss_vals[\"loss_entropy\"]\n            )\n\n            # Optimization: backward, grad clipping and optimization step\n            loss_value.backward()\n            # this is not strictly mandatory but it's good practice to keep\n            # your gradient norm bounded\n            torch.nn.utils.clip_grad_norm_(loss_module.parameters(), max_grad_norm)\n            optim.step()\n            optim.zero_grad()\n\n    logs[\"reward\"].append(tensordict_data[\"next\", \"reward\"].mean().item())\n    pbar.update(tensordict_data.numel())\n    cum_reward_str = (\n        f\"average reward={logs['reward'][-1]: 4.4f} (init={logs['reward'][0]: 4.4f})\"\n    )\n    logs[\"step_count\"].append(tensordict_data[\"step_count\"].max().item())\n    stepcount_str = f\"step count (max): {logs['step_count'][-1]}\"\n    logs[\"lr\"].append(optim.param_groups[0][\"lr\"])\n    lr_str = f\"lr policy: {logs['lr'][-1]: 4.4f}\"\n    if i % 10 == 0:\n        # We evaluate the policy once every 10 batches of data.\n        # Evaluation is rather simple: execute the policy without exploration\n        # (take the expected value of the action distribution) for a given\n        # number of steps (1000, which is our ``env`` horizon).\n        # The ``rollout`` method of the ``env`` can take a policy as argument:\n        # it will then execute this policy at each step.\n        with set_exploration_type(ExplorationType.DETERMINISTIC), torch.no_grad():\n            # execute a rollout with the trained policy\n            eval_rollout = env.rollout(1000, policy_module)\n            logs[\"eval reward\"].append(eval_rollout[\"next\", \"reward\"].mean().item())\n            logs[\"eval reward (sum)\"].append(\n                eval_rollout[\"next\", \"reward\"].sum().item()\n            )\n            logs[\"eval step_count\"].append(eval_rollout[\"step_count\"].max().item())\n            eval_str = (\n                f\"eval cumulative reward: {logs['eval reward (sum)'][-1]: 4.4f} \"\n                f\"(init: {logs['eval reward (sum)'][0]: 4.4f}), \"\n                f\"eval step-count: {logs['eval step_count'][-1]}\"\n            )\n            del eval_rollout\n    pbar.set_description(\", \".join([eval_str, cum_reward_str, stepcount_str, lr_str]))\n\n    # We're also using a learning rate scheduler. Like the gradient clipping,\n    # this is a nice-to-have but nothing necessary for PPO to work.\n    scheduler.step()"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/intermediate/FSDP_advanced_tutorial.html",
    "title": "Advanced Model Training with Fully Sharded Data Parallel (FSDP)\u00b6",
    "code_snippets": [
      "import os\nimport argparse\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom transformers import AutoTokenizer, GPT2TokenizerFast\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\nimport functools\nfrom torch.optim.lr_scheduler import StepLR\nimport torch.nn.functional as F\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom torch.utils.data.distributed import DistributedSampler\nfrom transformers.models.t5.modeling_t5 import T5Block\n\nfrom torch.distributed.algorithms._checkpoint.checkpoint_wrapper import (\n checkpoint_wrapper,\n CheckpointImpl,\n apply_activation_checkpointing_wrapper)\n\nfrom torch.distributed.fsdp import (\n    FullyShardedDataParallel as FSDP,\n    MixedPrecision,\n    BackwardPrefetch,\n    ShardingStrategy,\n    FullStateDictConfig,\n    StateDictType,\n)\nfrom torch.distributed.fsdp.wrap import (\n    transformer_auto_wrap_policy,\n    enable_wrap,\n    wrap,\n)\nfrom functools import partial\nfrom torch.utils.data import DataLoader\nfrom pathlib import Path\nfrom summarization_dataset import *\nfrom transformers.models.t5.modeling_t5 import T5Block\nfrom typing import Type\nimport time\nimport tqdm\nfrom datetime import datetime",
      "def setup():\n    # initialize the process group\n    dist.init_process_group(\"nccl\")\n\ndef cleanup():\n    dist.destroy_process_group()",
      "def setup_model(model_name):\n    model = T5ForConditionalGeneration.from_pretrained(model_name)\n    tokenizer =  T5Tokenizer.from_pretrained(model_name)\n    return model, tokenizer",
      "def get_date_of_run():\n    \"\"\"create date and time for file save uniqueness\n    example: 2022-05-07-08:31:12_PM'\n    \"\"\"\n    date_of_run = datetime.now().strftime(\"%Y-%m-%d-%I:%M:%S_%p\")\n    print(f\"--> current date and time of run = {date_of_run}\")\n    return date_of_run\n\ndef format_metrics_to_gb(item):\n    \"\"\"quick function to format numbers to gigabyte and round to 4 digit precision\"\"\"\n    metric_num = item / g_gigabyte\n    metric_num = round(metric_num, ndigits=4)\n    return metric_num",
      "def train(args, model, rank, world_size, train_loader, optimizer, epoch, sampler=None):\n    model.train()\n    local_rank = int(os.environ['LOCAL_RANK'])\n    fsdp_loss = torch.zeros(2).to(local_rank)\n\n    if sampler:\n        sampler.set_epoch(epoch)\n    if rank==0:\n        inner_pbar = tqdm.tqdm(\n            range(len(train_loader)), colour=\"blue\", desc=\"r0 Training Epoch\"\n        )\n    for batch in train_loader:\n        for key in batch.keys():\n            batch[key] = batch[key].to(local_rank)\n        optimizer.zero_grad()\n        output = model(input_ids=batch[\"source_ids\"],attention_mask=batch[\"source_mask\"],labels=batch[\"target_ids\"] )\n        loss = output[\"loss\"]\n        loss.backward()\n        optimizer.step()\n        fsdp_loss[0] += loss.item()\n        fsdp_loss[1] += len(batch)\n        if rank==0:\n            inner_pbar.update(1)\n\n    dist.all_reduce(fsdp_loss, op=dist.ReduceOp.SUM)\n    train_accuracy = fsdp_loss[0] / fsdp_loss[1]\n\n\n    if rank == 0:\n        inner_pbar.close()\n        print(\n                f\"Train Epoch: \\t{epoch}, Loss: \\t{train_accuracy:.4f}\"\n            )\n    return train_accuracy",
      "def validation(model, rank, world_size, val_loader):\n    model.eval()\n    correct = 0\n    local_rank = int(os.environ['LOCAL_RANK'])\n    fsdp_loss = torch.zeros(3).to(local_rank)\n    if rank == 0:\n        inner_pbar = tqdm.tqdm(\n            range(len(val_loader)), colour=\"green\", desc=\"Validation Epoch\"\n        )\n    with torch.no_grad():\n        for batch in val_loader:\n            for key in batch.keys():\n                batch[key] = batch[key].to(local_rank)\n            output = model(input_ids=batch[\"source_ids\"],attention_mask=batch[\"source_mask\"],labels=batch[\"target_ids\"])\n            fsdp_loss[0] += output[\"loss\"].item()  # sum up batch loss\n            fsdp_loss[1] += len(batch)\n\n            if rank==0:\n                inner_pbar.update(1)\n\n    dist.all_reduce(fsdp_loss, op=dist.ReduceOp.SUM)\n    val_loss = fsdp_loss[0] / fsdp_loss[1]\n    if rank == 0:\n        inner_pbar.close()\n        print(f\"Validation Loss: {val_loss:.4f}\")\n    return val_loss",
      "def fsdp_main(args):\n\n    model, tokenizer = setup_model(\"t5-base\")\n\n    local_rank = int(os.environ['LOCAL_RANK'])\n    rank = int(os.environ['RANK'])\n    world_size = int(os.environ['WORLD_SIZE'])\n\n\n    dataset = load_dataset('wikihow', 'all', data_dir='data/')\n    print(dataset.keys())\n    print(\"Size of train dataset: \", dataset['train'].shape)\n    print(\"Size of Validation dataset: \", dataset['validation'].shape)\n\n\n    #wikihow(tokenizer, type_path, num_samples, input_length, output_length, print_text=False)\n    train_dataset = wikihow(tokenizer, 'train', 1500, 512, 150, False)\n    val_dataset = wikihow(tokenizer, 'validation', 300, 512, 150, False)\n\n    sampler1 = DistributedSampler(train_dataset, rank=rank, num_replicas=world_size, shuffle=True)\n    sampler2 = DistributedSampler(val_dataset, rank=rank, num_replicas=world_size)\n\n    setup()\n\n\n    train_kwargs = {'batch_size': args.batch_size, 'sampler': sampler1}\n    test_kwargs = {'batch_size': args.test_batch_size, 'sampler': sampler2}\n    cuda_kwargs = {'num_workers': 2,\n                    'pin_memory': True,\n                    'shuffle': False}\n    train_kwargs.update(cuda_kwargs)\n    test_kwargs.update(cuda_kwargs)\n\n    train_loader = torch.utils.data.DataLoader(train_dataset,**train_kwargs)\n    val_loader = torch.utils.data.DataLoader(val_dataset, **test_kwargs)\n\n    t5_auto_wrap_policy = functools.partial(\n        transformer_auto_wrap_policy,\n        transformer_layer_cls={\n            T5Block,\n        },\n    )\n    sharding_strategy: ShardingStrategy = ShardingStrategy.SHARD_GRAD_OP #for Zero2 and FULL_SHARD for Zero3\n    torch.cuda.set_device(local_rank)\n\n\n    #init_start_event = torch.cuda.Event(enable_timing=True)\n    #init_end_event = torch.cuda.Event(enable_timing=True)\n\n    #init_start_event.record()\n\n    bf16_ready = (\n    torch.version.cuda\n    and torch.cuda.is_bf16_supported()\n    and LooseVersion(torch.version.cuda) >= \"11.0\"\n    and dist.is_nccl_available()\n    and nccl.version() >= (2, 10)\n    )\n\n    if bf16_ready:\n        mp_policy = bfSixteen\n    else:\n        mp_policy = None # defaults to fp32\n\n    # model is on CPU before input to FSDP\n    model = FSDP(model,\n        auto_wrap_policy=t5_auto_wrap_policy,\n        mixed_precision=mp_policy,\n        #sharding_strategy=sharding_strategy,\n        device_id=torch.cuda.current_device())\n\n    optimizer = optim.AdamW(model.parameters(), lr=args.lr)\n\n    scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n    best_val_loss = float(\"inf\")\n    curr_val_loss = float(\"inf\")\n    file_save_name = \"T5-model-\"\n\n    if rank == 0:\n        time_of_run = get_date_of_run()\n        dur = []\n        train_acc_tracking = []\n        val_acc_tracking = []\n        training_start_time = time.time()\n\n    if rank == 0 and args.track_memory:\n        mem_alloc_tracker = []\n        mem_reserved_tracker = []\n\n    for epoch in range(1, args.epochs + 1):\n        t0 = time.time()\n        train_accuracy = train(args, model, rank, world_size, train_loader, optimizer, epoch, sampler=sampler1)\n        if args.run_validation:\n            curr_val_loss = validation(model, rank, world_size, val_loader)\n        scheduler.step()\n\n        if rank == 0:\n\n            print(f\"--> epoch {epoch} completed...entering save and stats zone\")\n\n            dur.append(time.time() - t0)\n            train_acc_tracking.append(train_accuracy.item())\n\n            if args.run_validation:\n                val_acc_tracking.append(curr_val_loss.item())\n\n            if args.track_memory:\n                mem_alloc_tracker.append(\n                    format_metrics_to_gb(torch.cuda.memory_allocated())\n                )\n                mem_reserved_tracker.append(\n                    format_metrics_to_gb(torch.cuda.memory_reserved())\n                )\n            print(f\"completed save and stats zone...\")\n\n        if args.save_model and curr_val_loss < best_val_loss:\n\n            # save\n            if rank == 0:\n                print(f\"--> entering save model state\")\n\n            save_policy = FullStateDictConfig(offload_to_cpu=True, rank0_only=True)\n            with FSDP.state_dict_type(\n                model, StateDictType.FULL_STATE_DICT, save_policy\n            ):\n                cpu_state = model.state_dict()\n            #print(f\"saving process: rank {rank}  done w state_dict\")\n\n\n            if rank == 0:\n                print(f\"--> saving model ...\")\n                currEpoch = (\n                    \"-\" + str(epoch) + \"-\" + str(round(curr_val_loss.item(), 4)) + \".pt\"\n                )\n                print(f\"--> attempting to save model prefix {currEpoch}\")\n                save_name = file_save_name + \"-\" + time_of_run + \"-\" + currEpoch\n                print(f\"--> saving as model name {save_name}\")\n\n                torch.save(cpu_state, save_name)\n\n        if curr_val_loss < best_val_loss:\n\n            best_val_loss = curr_val_loss\n            if rank==0:\n                print(f\"-->>>> New Val Loss Record: {best_val_loss}\")\n\n    dist.barrier()\n    cleanup()",
      "if __name__ == '__main__':\n    # Training settings\n    parser = argparse.ArgumentParser(description='PyTorch T5 FSDP Example')\n    parser.add_argument('--batch-size', type=int, default=4, metavar='N',\n                        help='input batch size for training (default: 64)')\n    parser.add_argument('--test-batch-size', type=int, default=4, metavar='N',\n                        help='input batch size for testing (default: 1000)')\n    parser.add_argument('--epochs', type=int, default=2, metavar='N',\n                        help='number of epochs to train (default: 3)')\n    parser.add_argument('--lr', type=float, default=.002, metavar='LR',\n                        help='learning rate (default: .002)')\n    parser.add_argument('--gamma', type=float, default=0.7, metavar='M',\n                        help='Learning rate step gamma (default: 0.7)')\n    parser.add_argument('--no-cuda', action='store_true', default=False,\n                        help='disables CUDA training')\n    parser.add_argument('--seed', type=int, default=1, metavar='S',\n                        help='random seed (default: 1)')\n    parser.add_argument('--track_memory', action='store_false', default=True,\n                        help='track the gpu memory')\n    parser.add_argument('--run_validation', action='store_false', default=True,\n                        help='running the validation')\n    parser.add_argument('--save-model', action='store_false', default=True,\n                        help='For Saving the current Model')\n    args = parser.parse_args()\n\n    torch.manual_seed(args.seed)\n\n    fsdp_main(args)",
      "t5_auto_wrap_policy = functools.partial(\n        transformer_auto_wrap_policy,\n        transformer_layer_cls={\n            T5Block,\n        },\n    )\ntorch.cuda.set_device(local_rank)\n\n\nmodel = FSDP(model,\n    auto_wrap_policy=t5_auto_wrap_policy)",
      "bf16_ready = (\n    torch.version.cuda\n    and torch.cuda.is_bf16_supported()\n    and LooseVersion(torch.version.cuda) >= \"11.0\"\n    and dist.is_nccl_available()\n    and nccl.version() >= (2, 10)\n)",
      "fpSixteen = MixedPrecision(\n    param_dtype=torch.float16,\n    # Gradient communication precision.\n    reduce_dtype=torch.float16,\n    # Buffer precision.\n    buffer_dtype=torch.float16,\n)\n\nbfSixteen = MixedPrecision(\n    param_dtype=torch.bfloat16,\n    # Gradient communication precision.\n    reduce_dtype=torch.bfloat16,\n    # Buffer precision.\n    buffer_dtype=torch.bfloat16,\n)\n\nfp32_policy = MixedPrecision(\n    param_dtype=torch.float32,\n    # Gradient communication precision.\n    reduce_dtype=torch.float32,\n    # Buffer precision.\n    buffer_dtype=torch.float32,\n)",
      "grad_bf16 = MixedPrecision(reduce_dtype=torch.bfloat16)",
      "torch.cuda.set_device(local_rank)\n\n model = FSDP(model,\n        auto_wrap_policy=t5_auto_wrap_policy,\n        mixed_precision=bfSixteen,\n        device_id=torch.cuda.current_device())",
      "torch.cuda.set_device(local_rank)\n\n model = FSDP(model,\n        auto_wrap_policy=t5_auto_wrap_policy,\n        mixed_precision=bfSixteen,\n        device_id=torch.cuda.current_device(),\n        sharding_strategy=ShardingStrategy.SHARD_GRAD_OP # ZERO2)",
      "torch.cuda.set_device(local_rank)\n\n model = FSDP(model,\n        auto_wrap_policy=t5_auto_wrap_policy,\n        mixed_precision=bfSixteen,\n        device_id=torch.cuda.current_device(),\n        backward_prefetch = BackwardPrefetch.BACKWARD_PRE)",
      "save_policy = FullStateDictConfig(offload_to_cpu=True, rank0_only=True)\nwith FSDP.state_dict_type(\n            model, StateDictType.FULL_STATE_DICT, save_policy\n        ):\n            cpu_state = model.state_dict()\nif rank == 0:\n save_name = file_save_name + \"-\" + time_of_run + \"-\" + currEpoch\n torch.save(cpu_state, save_name)"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/intermediate/spatial_transformer_tutorial.html",
    "title": "Spatial Transformer Networks Tutorial\u00b6",
    "code_snippets": [
      "# License: BSD\n# Author: Ghassen Hamrouni\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torchvision\nfrom torchvision import datasets, transforms\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nplt.ion()   # interactive mode",
      "from six.moves import urllib\nopener = urllib.request.build_opener()\nopener.addheaders = [('User-agent', 'Mozilla/5.0')]\nurllib.request.install_opener(opener)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Training dataset\ntrain_loader = torch.utils.data.DataLoader(\n    datasets.MNIST(root='.', train=True, download=True,\n                   transform=transforms.Compose([\n                       transforms.ToTensor(),\n                       transforms.Normalize((0.1307,), (0.3081,))\n                   ])), batch_size=64, shuffle=True, num_workers=4)\n# Test dataset\ntest_loader = torch.utils.data.DataLoader(\n    datasets.MNIST(root='.', train=False, transform=transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.1307,), (0.3081,))\n    ])), batch_size=64, shuffle=True, num_workers=4)",
      "class Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n        self.conv2_drop = nn.Dropout2d()\n        self.fc1 = nn.Linear(320, 50)\n        self.fc2 = nn.Linear(50, 10)\n\n        # Spatial transformer localization-network\n        self.localization = nn.Sequential(\n            nn.Conv2d(1, 8, kernel_size=7),\n            nn.MaxPool2d(2, stride=2),\n            nn.ReLU(True),\n            nn.Conv2d(8, 10, kernel_size=5),\n            nn.MaxPool2d(2, stride=2),\n            nn.ReLU(True)\n        )\n\n        # Regressor for the 3 * 2 affine matrix\n        self.fc_loc = nn.Sequential(\n            nn.Linear(10 * 3 * 3, 32),\n            nn.ReLU(True),\n            nn.Linear(32, 3 * 2)\n        )\n\n        # Initialize the weights/bias with identity transformation\n        self.fc_loc[2].weight.data.zero_()\n        self.fc_loc[2].bias.data.copy_(torch.tensor([1, 0, 0, 0, 1, 0], dtype=torch.float))\n\n    # Spatial transformer network forward function\n    def stn(self, x):\n        xs = self.localization(x)\n        xs = xs.view(-1, 10 * 3 * 3)\n        theta = self.fc_loc(xs)\n        theta = theta.view(-1, 2, 3)\n\n        grid = F.affine_grid(theta, x.size())\n        x = F.grid_sample(x, grid)\n\n        return x\n\n    def forward(self, x):\n        # transform the input\n        x = self.stn(x)\n\n        # Perform the usual forward pass\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n        x = x.view(-1, 320)\n        x = F.relu(self.fc1(x))\n        x = F.dropout(x, training=self.training)\n        x = self.fc2(x)\n        return F.log_softmax(x, dim=1)\n\n\nmodel = Net().to(device)",
      "optimizer = optim.SGD(model.parameters(), lr=0.01)\n\n\ndef train(epoch):\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(device), target.to(device)\n\n        optimizer.zero_grad()\n        output = model(data)\n        loss = F.nll_loss(output, target)\n        loss.backward()\n        optimizer.step()\n        if batch_idx % 500 == 0:\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                epoch, batch_idx * len(data), len(train_loader.dataset),\n                100. * batch_idx / len(train_loader), loss.item()))\n#\n# A simple test procedure to measure the STN performances on MNIST.\n#\n\n\ndef test():\n    with torch.no_grad():\n        model.eval()\n        test_loss = 0\n        correct = 0\n        for data, target in test_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n\n            # sum up batch loss\n            test_loss += F.nll_loss(output, target, size_average=False).item()\n            # get the index of the max log-probability\n            pred = output.max(1, keepdim=True)[1]\n            correct += pred.eq(target.view_as(pred)).sum().item()\n\n        test_loss /= len(test_loader.dataset)\n        print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'\n              .format(test_loss, correct, len(test_loader.dataset),\n                      100. * correct / len(test_loader.dataset)))",
      "def convert_image_np(inp):\n    \"\"\"Convert a Tensor to numpy image.\"\"\"\n    inp = inp.numpy().transpose((1, 2, 0))\n    mean = np.array([0.485, 0.456, 0.406])\n    std = np.array([0.229, 0.224, 0.225])\n    inp = std * inp + mean\n    inp = np.clip(inp, 0, 1)\n    return inp\n\n# We want to visualize the output of the spatial transformers layer\n# after the training, we visualize a batch of input images and\n# the corresponding transformed batch using STN.\n\n\ndef visualize_stn():\n    with torch.no_grad():\n        # Get a batch of training data\n        data = next(iter(test_loader))[0].to(device)\n\n        input_tensor = data.cpu()\n        transformed_input_tensor = model.stn(data).cpu()\n\n        in_grid = convert_image_np(\n            torchvision.utils.make_grid(input_tensor))\n\n        out_grid = convert_image_np(\n            torchvision.utils.make_grid(transformed_input_tensor))\n\n        # Plot the results side-by-side\n        f, axarr = plt.subplots(1, 2)\n        axarr[0].imshow(in_grid)\n        axarr[0].set_title('Dataset Images')\n\n        axarr[1].imshow(out_grid)\n        axarr[1].set_title('Transformed Images')\n\nfor epoch in range(1, 20 + 1):\n    train(epoch)\n    test()\n\n# Visualize the STN transformation on some input batch\nvisualize_stn()\n\nplt.ioff()\nplt.show()"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html",
    "title": "A Gentle Introduction to torch.autograd\u00b6",
    "code_snippets": [
      "import torch\nfrom torchvision.models import resnet18, ResNet18_Weights\nmodel = resnet18(weights=ResNet18_Weights.DEFAULT)\ndata = torch.rand(1, 3, 64, 64)\nlabels = torch.rand(1, 1000)",
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /var/lib/ci-user/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n\n  0%|          | 0.00/44.7M [00:00<?, ?B/s]\n 94%|#########3| 41.9M/44.7M [00:00<00:00, 439MB/s]\n100%|##########| 44.7M/44.7M [00:00<00:00, 437MB/s]",
      "optim = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)",
      "import torch\n\na = torch.tensor([2., 3.], requires_grad=True)\nb = torch.tensor([6., 4.], requires_grad=True)",
      "external_grad = torch.tensor([1., 1.])\nQ.backward(gradient=external_grad)",
      "x = torch.rand(5, 5)\ny = torch.rand(5, 5)\nz = torch.rand((5, 5), requires_grad=True)\n\na = x + y\nprint(f\"Does `a` require gradients?: {a.requires_grad}\")\nb = x + z\nprint(f\"Does `b` require gradients?: {b.requires_grad}\")",
      "from torch import nn, optim\n\nmodel = resnet18(weights=ResNet18_Weights.DEFAULT)\n\n# Freeze all the parameters in the network\nfor param in model.parameters():\n    param.requires_grad = False",
      "# Optimize only the classifier\noptimizer = optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/recipes/recipes/zeroing_out_gradients.html",
    "title": "Zeroing out gradients in PyTorch\u00b6",
    "code_snippets": [
      "import torch\n\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport torch.optim as optim\n\nimport torchvision\nimport torchvision.transforms as transforms",
      "transform = transforms.Compose(\n    [transforms.ToTensor(),\n     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n\ntrainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n                                        download=True, transform=transform)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n                                          shuffle=True, num_workers=2)\n\ntestset = torchvision.datasets.CIFAR10(root='./data', train=False,\n                                       download=True, transform=transform)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=4,\n                                         shuffle=False, num_workers=2)\n\nclasses = ('plane', 'car', 'bird', 'cat',\n           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')",
      "class Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(3, 6, 5)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = x.view(-1, 16 * 5 * 5)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/beginner/basics/saveloadrun_tutorial.html",
    "title": "Save and Load the Model\u00b6",
    "code_snippets": [
      "import torch\nimport torchvision.models as models",
      "model = models.vgg16(weights='IMAGENET1K_V1')\ntorch.save(model.state_dict(), 'model_weights.pth')",
      "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /var/lib/ci-user/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n\n  0%|          | 0.00/528M [00:00<?, ?B/s]\n  8%|8         | 42.6M/528M [00:00<00:01, 446MB/s]\n 16%|#6        | 86.1M/528M [00:00<00:01, 451MB/s]\n 25%|##4       | 130M/528M [00:00<00:00, 455MB/s]\n 33%|###2      | 174M/528M [00:00<00:00, 455MB/s]\n 41%|####1     | 217M/528M [00:00<00:00, 434MB/s]\n 49%|####9     | 259M/528M [00:00<00:00, 428MB/s]\n 57%|#####7    | 303M/528M [00:00<00:00, 439MB/s]\n 66%|######5   | 347M/528M [00:00<00:00, 447MB/s]\n 74%|#######4  | 392M/528M [00:00<00:00, 453MB/s]\n 83%|########2 | 436M/528M [00:01<00:00, 457MB/s]\n 91%|#########1| 481M/528M [00:01<00:00, 460MB/s]\n 99%|#########9| 525M/528M [00:01<00:00, 461MB/s]\n100%|##########| 528M/528M [00:01<00:00, 451MB/s]",
      "model = models.vgg16() # we do not specify ``weights``, i.e. create untrained model\nmodel.load_state_dict(torch.load('model_weights.pth', weights_only=True))\nmodel.eval()",
      "VGG(\n  (features): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU(inplace=True)\n    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU(inplace=True)\n    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (6): ReLU(inplace=True)\n    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (8): ReLU(inplace=True)\n    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (11): ReLU(inplace=True)\n    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (13): ReLU(inplace=True)\n    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (15): ReLU(inplace=True)\n    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (18): ReLU(inplace=True)\n    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (20): ReLU(inplace=True)\n    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (22): ReLU(inplace=True)\n    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (25): ReLU(inplace=True)\n    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (27): ReLU(inplace=True)\n    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (29): ReLU(inplace=True)\n    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n  (classifier): Sequential(\n    (0): Linear(in_features=25088, out_features=4096, bias=True)\n    (1): ReLU(inplace=True)\n    (2): Dropout(p=0.5, inplace=False)\n    (3): Linear(in_features=4096, out_features=4096, bias=True)\n    (4): ReLU(inplace=True)\n    (5): Dropout(p=0.5, inplace=False)\n    (6): Linear(in_features=4096, out_features=1000, bias=True)\n  )\n)",
      "torch.save(model, 'model.pth')",
      "model = torch.load('model.pth', weights_only=False),"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html",
    "title": "Getting Started with Distributed Checkpoint (DCP)\u00b6",
    "code_snippets": [
      "import os\n\nimport torch\nimport torch.distributed as dist\nimport torch.distributed.checkpoint as dcp\nimport torch.multiprocessing as mp\nimport torch.nn as nn\n\nfrom torch.distributed.fsdp import FullyShardedDataParallel as FSDP\nfrom torch.distributed.checkpoint.state_dict import get_state_dict, set_state_dict\nfrom torch.distributed.checkpoint.stateful import Stateful\nfrom torch.distributed.fsdp.fully_sharded_data_parallel import StateDictType\n\nCHECKPOINT_DIR = \"checkpoint\"\n\n\nclass AppState(Stateful):\n    \"\"\"This is a useful wrapper for checkpointing the Application State. Since this object is compliant\n    with the Stateful protocol, DCP will automatically call state_dict/load_stat_dict as needed in the\n    dcp.save/load APIs.\n\n    Note: We take advantage of this wrapper to hande calling distributed state dict methods on the model\n    and optimizer.\n    \"\"\"\n\n    def __init__(self, model, optimizer=None):\n        self.model = model\n        self.optimizer = optimizer\n\n    def state_dict(self):\n        # this line automatically manages FSDP FQN's, as well as sets the default state dict type to FSDP.SHARDED_STATE_DICT\n        model_state_dict, optimizer_state_dict = get_state_dict(self.model, self.optimizer)\n        return {\n            \"model\": model_state_dict,\n            \"optim\": optimizer_state_dict\n        }\n\n    def load_state_dict(self, state_dict):\n        # sets our state dicts on the model and optimizer, now that we've loaded\n        set_state_dict(\n            self.model,\n            self.optimizer,\n            model_state_dict=state_dict[\"model\"],\n            optim_state_dict=state_dict[\"optim\"]\n        )\n\nclass ToyModel(nn.Module):\n    def __init__(self):\n        super(ToyModel, self).__init__()\n        self.net1 = nn.Linear(16, 16)\n        self.relu = nn.ReLU()\n        self.net2 = nn.Linear(16, 8)\n\n    def forward(self, x):\n        return self.net2(self.relu(self.net1(x)))\n\n\ndef setup(rank, world_size):\n    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n    os.environ[\"MASTER_PORT\"] = \"12355 \"\n\n    # initialize the process group\n    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n    torch.cuda.set_device(rank)\n\n\ndef cleanup():\n    dist.destroy_process_group()\n\n\ndef run_fsdp_checkpoint_save_example(rank, world_size):\n    print(f\"Running basic FSDP checkpoint saving example on rank {rank}.\")\n    setup(rank, world_size)\n\n    # create a model and move it to GPU with id rank\n    model = ToyModel().to(rank)\n    model = FSDP(model)\n\n    loss_fn = nn.MSELoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n\n    optimizer.zero_grad()\n    model(torch.rand(8, 16, device=\"cuda\")).sum().backward()\n    optimizer.step()\n\n    state_dict = { \"app\": AppState(model, optimizer) }\n    dcp.save(state_dict, checkpoint_id=CHECKPOINT_DIR)\n\n    cleanup()\n\n\nif __name__ == \"__main__\":\n    world_size = torch.cuda.device_count()\n    print(f\"Running fsdp checkpoint example on {world_size} devices.\")\n    mp.spawn(\n        run_fsdp_checkpoint_save_example,\n        args=(world_size,),\n        nprocs=world_size,\n        join=True,\n    )",
      "import os\n\nimport torch\nimport torch.distributed as dist\nimport torch.distributed.checkpoint as dcp\nfrom torch.distributed.checkpoint.stateful import Stateful\nfrom torch.distributed.checkpoint.state_dict import get_state_dict, set_state_dict\nimport torch.multiprocessing as mp\nimport torch.nn as nn\n\nfrom torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n\nCHECKPOINT_DIR = \"checkpoint\"\n\n\nclass AppState(Stateful):\n    \"\"\"This is a useful wrapper for checkpointing the Application State. Since this object is compliant\n    with the Stateful protocol, DCP will automatically call state_dict/load_stat_dict as needed in the\n    dcp.save/load APIs.\n\n    Note: We take advantage of this wrapper to hande calling distributed state dict methods on the model\n    and optimizer.\n    \"\"\"\n\n    def __init__(self, model, optimizer=None):\n        self.model = model\n        self.optimizer = optimizer\n\n    def state_dict(self):\n        # this line automatically manages FSDP FQN's, as well as sets the default state dict type to FSDP.SHARDED_STATE_DICT\n        model_state_dict, optimizer_state_dict = get_state_dict(self.model, self.optimizer)\n        return {\n            \"model\": model_state_dict,\n            \"optim\": optimizer_state_dict\n        }\n\n    def load_state_dict(self, state_dict):\n        # sets our state dicts on the model and optimizer, now that we've loaded\n        set_state_dict(\n            self.model,\n            self.optimizer,\n            model_state_dict=state_dict[\"model\"],\n            optim_state_dict=state_dict[\"optim\"]\n        )\n\nclass ToyModel(nn.Module):\n    def __init__(self):\n        super(ToyModel, self).__init__()\n        self.net1 = nn.Linear(16, 16)\n        self.relu = nn.ReLU()\n        self.net2 = nn.Linear(16, 8)\n\n    def forward(self, x):\n        return self.net2(self.relu(self.net1(x)))\n\n\ndef setup(rank, world_size):\n    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n    os.environ[\"MASTER_PORT\"] = \"12355 \"\n\n    # initialize the process group\n    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n    torch.cuda.set_device(rank)\n\n\ndef cleanup():\n    dist.destroy_process_group()\n\n\ndef run_fsdp_checkpoint_load_example(rank, world_size):\n    print(f\"Running basic FSDP checkpoint loading example on rank {rank}.\")\n    setup(rank, world_size)\n\n    # create a model and move it to GPU with id rank\n    model = ToyModel().to(rank)\n    model = FSDP(model)\n\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n\n    state_dict = { \"app\": AppState(model, optimizer)}\n    dcp.load(\n        state_dict=state_dict,\n        checkpoint_id=CHECKPOINT_DIR,\n    )\n\n    cleanup()\n\n\nif __name__ == \"__main__\":\n    world_size = torch.cuda.device_count()\n    print(f\"Running fsdp checkpoint example on {world_size} devices.\")\n    mp.spawn(\n        run_fsdp_checkpoint_load_example,\n        args=(world_size,),\n        nprocs=world_size,\n        join=True,\n    )",
      "import os\n\nimport torch\nimport torch.distributed.checkpoint as dcp\nimport torch.nn as nn\n\n\nCHECKPOINT_DIR = \"checkpoint\"\n\n\nclass ToyModel(nn.Module):\n    def __init__(self):\n        super(ToyModel, self).__init__()\n        self.net1 = nn.Linear(16, 16)\n        self.relu = nn.ReLU()\n        self.net2 = nn.Linear(16, 8)\n\n    def forward(self, x):\n        return self.net2(self.relu(self.net1(x)))\n\n\ndef run_checkpoint_load_example():\n    # create the non FSDP-wrapped toy model\n    model = ToyModel()\n    state_dict = {\n        \"model\": model.state_dict(),\n    }\n\n    # since no progress group is initialized, DCP will disable any collectives.\n    dcp.load(\n        state_dict=state_dict,\n        checkpoint_id=CHECKPOINT_DIR,\n    )\n    model.load_state_dict(state_dict[\"model\"])\n\nif __name__ == \"__main__\":\n    print(f\"Running basic DCP checkpoint loading example.\")\n    run_checkpoint_load_example()",
      "python -m torch.distributed.checkpoint.format_utils <mode> <checkpoint location> <location to write formats to>",
      "import os\n\nimport torch\nimport torch.distributed.checkpoint as DCP\nfrom torch.distributed.checkpoint.format_utils import dcp_to_torch_save, torch_save_to_dcp\n\nCHECKPOINT_DIR = \"checkpoint\"\nTORCH_SAVE_CHECKPOINT_DIR = \"torch_save_checkpoint.pth\"\n\n# convert dcp model to torch.save (assumes checkpoint was generated as above)\ndcp_to_torch_save(CHECKPOINT_DIR, TORCH_SAVE_CHECKPOINT_DIR)\n\n# converts the torch.save model back to DCP\ntorch_save_to_dcp(TORCH_SAVE_CHECKPOINT_DIR, f\"{CHECKPOINT_DIR}_new\")"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/advanced/cpp_extension.html",
    "title": "Custom C++ and CUDA Extensions\u00b6",
    "code_snippets": [
      "class LLTM(torch.nn.Module):\n    def __init__(self, input_features, state_size):\n        super(LLTM, self).__init__()\n        self.input_features = input_features\n        self.state_size = state_size\n        # 3 * state_size for input gate, output gate and candidate cell gate.\n        # input_features + state_size because we will multiply with [input, h].\n        self.weights = torch.nn.Parameter(\n            torch.empty(3 * state_size, input_features + state_size))\n        self.bias = torch.nn.Parameter(torch.empty(3 * state_size))\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        stdv = 1.0 / math.sqrt(self.state_size)\n        for weight in self.parameters():\n            weight.data.uniform_(-stdv, +stdv)\n\n    def forward(self, input, state):\n        old_h, old_cell = state\n        X = torch.cat([old_h, input], dim=1)\n\n        # Compute the input, output and candidate cell gates with one MM.\n        gate_weights = F.linear(X, self.weights, self.bias)\n        # Split the combined gate weight matrix into its components.\n        gates = gate_weights.chunk(3, dim=1)\n\n        input_gate = torch.sigmoid(gates[0])\n        output_gate = torch.sigmoid(gates[1])\n        # Here we use an ELU instead of the usual tanh.\n        candidate_cell = F.elu(gates[2])\n\n        # Compute the new cell state.\n        new_cell = old_cell + candidate_cell * input_gate\n        # Compute the new hidden state and output.\n        new_h = torch.tanh(new_cell) * output_gate\n\n        return new_h, new_cell",
      "import torch\n\nX = torch.randn(batch_size, input_features)\nh = torch.randn(batch_size, state_size)\nC = torch.randn(batch_size, state_size)\n\nrnn = LLTM(input_features, state_size)\n\nnew_h, new_C = rnn(X, (h, C))",
      "from setuptools import setup, Extension\nfrom torch.utils import cpp_extension\n\nsetup(name='lltm_cpp',\n      ext_modules=[cpp_extension.CppExtension('lltm_cpp', ['lltm.cpp'])],\n      cmdclass={'build_ext': cpp_extension.BuildExtension})",
      "PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &lltm_forward, \"LLTM forward\");\n  m.def(\"backward\", &lltm_backward, \"LLTM backward\");\n}",
      "In [1]: import torch\nIn [2]: import lltm_cpp\nIn [3]: lltm_cpp.forward\nOut[3]: <function lltm.PyCapsule.forward>",
      "import math\nimport torch\n\n# Our module!\nimport lltm_cpp\n\nclass LLTMFunction(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, input, weights, bias, old_h, old_cell):\n        outputs = lltm_cpp.forward(input, weights, bias, old_h, old_cell)\n        new_h, new_cell = outputs[:2]\n        variables = outputs[1:] + [weights]\n        ctx.save_for_backward(*variables)\n\n        return new_h, new_cell\n\n    @staticmethod\n    def backward(ctx, grad_h, grad_cell):\n        outputs = lltm_cpp.backward(\n            grad_h.contiguous(), grad_cell.contiguous(), *ctx.saved_tensors)\n        d_old_h, d_input, d_weights, d_bias, d_old_cell = outputs\n        return d_input, d_weights, d_bias, d_old_h, d_old_cell\n\n\nclass LLTM(torch.nn.Module):\n    def __init__(self, input_features, state_size):\n        super(LLTM, self).__init__()\n        self.input_features = input_features\n        self.state_size = state_size\n        self.weights = torch.nn.Parameter(\n            torch.empty(3 * state_size, input_features + state_size))\n        self.bias = torch.nn.Parameter(torch.empty(3 * state_size))\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        stdv = 1.0 / math.sqrt(self.state_size)\n        for weight in self.parameters():\n            weight.data.uniform_(-stdv, +stdv)\n\n    def forward(self, input, state):\n        return LLTMFunction.apply(input, self.weights, self.bias, *state)",
      "import time\n\nimport torch\n\nbatch_size = 16\ninput_features = 32\nstate_size = 128\n\nX = torch.randn(batch_size, input_features)\nh = torch.randn(batch_size, state_size)\nC = torch.randn(batch_size, state_size)\n\nrnn = LLTM(input_features, state_size)\n\nforward = 0\nbackward = 0\nfor _ in range(100000):\n    start = time.time()\n    new_h, new_C = rnn(X, (h, C))\n    forward += time.time() - start\n\n    start = time.time()\n    (new_h.sum() + new_C.sum()).backward()\n    backward += time.time() - start\n\nprint('Forward: {:.3f} s | Backward {:.3f} s'.format(forward, backward))",
      "import torch\n\nassert torch.cuda.is_available()\ncuda_device = torch.device(\"cuda\")  # device object representing GPU\n\nbatch_size = 16\ninput_features = 32\nstate_size = 128\n\n# Note the device=cuda_device arguments here\nX = torch.randn(batch_size, input_features, device=cuda_device)\nh = torch.randn(batch_size, state_size, device=cuda_device)\nC = torch.randn(batch_size, state_size, device=cuda_device)\n\nrnn = LLTM(input_features, state_size).to(cuda_device)\n\nforward = 0\nbackward = 0\nfor _ in range(100000):\n    start = time.time()\n    new_h, new_C = rnn(X, (h, C))\n    torch.cuda.synchronize()\n    forward += time.time() - start\n\n    start = time.time()\n    (new_h.sum() + new_C.sum()).backward()\n    torch.cuda.synchronize()\n    backward += time.time() - start\n\nprint('Forward: {:.3f} us | Backward {:.3f} us'.format(forward * 1e6/1e5, backward * 1e6/1e5))",
      "from torch.utils.cpp_extension import load\n\nlltm_cpp = load(name=\"lltm_cpp\", sources=[\"lltm.cpp\"])",
      "#include <torch/extension.h>\n\n#include <vector>\n\n// CUDA forward declarations\n\nstd::vector<torch::Tensor> lltm_cuda_forward(\n    torch::Tensor input,\n    torch::Tensor weights,\n    torch::Tensor bias,\n    torch::Tensor old_h,\n    torch::Tensor old_cell);\n\nstd::vector<torch::Tensor> lltm_cuda_backward(\n    torch::Tensor grad_h,\n    torch::Tensor grad_cell,\n    torch::Tensor new_cell,\n    torch::Tensor input_gate,\n    torch::Tensor output_gate,\n    torch::Tensor candidate_cell,\n    torch::Tensor X,\n    torch::Tensor gate_weights,\n    torch::Tensor weights);\n\n// C++ interface\n\n#define CHECK_CUDA(x) TORCH_CHECK(x.device().is_cuda(), #x \" must be a CUDA tensor\")\n#define CHECK_CONTIGUOUS(x) TORCH_CHECK(x.is_contiguous(), #x \" must be contiguous\")\n#define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n\nstd::vector<torch::Tensor> lltm_forward(\n    torch::Tensor input,\n    torch::Tensor weights,\n    torch::Tensor bias,\n    torch::Tensor old_h,\n    torch::Tensor old_cell) {\n  CHECK_INPUT(input);\n  CHECK_INPUT(weights);\n  CHECK_INPUT(bias);\n  CHECK_INPUT(old_h);\n  CHECK_INPUT(old_cell);\n\n  return lltm_cuda_forward(input, weights, bias, old_h, old_cell);\n}\n\nstd::vector<torch::Tensor> lltm_backward(\n    torch::Tensor grad_h,\n    torch::Tensor grad_cell,\n    torch::Tensor new_cell,\n    torch::Tensor input_gate,\n    torch::Tensor output_gate,\n    torch::Tensor candidate_cell,\n    torch::Tensor X,\n    torch::Tensor gate_weights,\n    torch::Tensor weights) {\n  CHECK_INPUT(grad_h);\n  CHECK_INPUT(grad_cell);\n  CHECK_INPUT(input_gate);\n  CHECK_INPUT(output_gate);\n  CHECK_INPUT(candidate_cell);\n  CHECK_INPUT(X);\n  CHECK_INPUT(gate_weights);\n  CHECK_INPUT(weights);\n\n  return lltm_cuda_backward(\n      grad_h,\n      grad_cell,\n      new_cell,\n      input_gate,\n      output_gate,\n      candidate_cell,\n      X,\n      gate_weights,\n      weights);\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &lltm_forward, \"LLTM forward (CUDA)\");\n  m.def(\"backward\", &lltm_backward, \"LLTM backward (CUDA)\");\n}",
      "from setuptools import setup\nfrom torch.utils.cpp_extension import BuildExtension, CUDAExtension\n\nsetup(\n    name='lltm',\n    ext_modules=[\n        CUDAExtension('lltm_cuda', [\n            'lltm_cuda.cpp',\n            'lltm_cuda_kernel.cu',\n        ])\n    ],\n    cmdclass={\n        'build_ext': BuildExtension\n    })",
      "from torch.utils.cpp_extension import load\n\nlltm = load(name='lltm', sources=['lltm_cuda.cpp', 'lltm_cuda_kernel.cu'])"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/recipes/intel_neural_compressor_for_pytorch.html",
    "title": "Ease-of-use quantization for PyTorch with Intel\u00ae Neural Compressor\u00b6",
    "code_snippets": [
      "# main.py\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# LeNet Model definition\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n        self.conv2_drop = nn.Dropout2d()\n        self.fc1 = nn.Linear(320, 50)\n        self.fc1_drop = nn.Dropout()\n        self.fc2 = nn.Linear(50, 10)\n\n    def forward(self, x):\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n        x = x.reshape(-1, 320)\n        x = F.relu(self.fc1(x))\n        x = self.fc1_drop(x)\n        x = self.fc2(x)\n        return F.log_softmax(x, dim=1)\n\nmodel = Net()\nmodel.load_state_dict(torch.load('./lenet_mnist_model.pth', weights_only=True))",
      "# main.py\nmodel.eval()\n\nfrom torchvision import datasets, transforms\ntest_loader = torch.utils.data.DataLoader(\n    datasets.MNIST('./data', train=False, download=True,\n                   transform=transforms.Compose([\n                       transforms.ToTensor(),\n                   ])),\n    batch_size=1)\n\n# launch code for Intel\u00ae Neural Compressor\nfrom neural_compressor.experimental import Quantization\nquantizer = Quantization(\"./conf.yaml\")\nquantizer.model = model\nquantizer.calib_dataloader = test_loader\nquantizer.eval_dataloader = test_loader\nq_model = quantizer()\nq_model.save('./output')",
      "# main.py\nmodel.eval()\n\nfrom torchvision import datasets, transforms\ntest_loader = torch.utils.data.DataLoader(\n    datasets.MNIST('./data', train=False, download=True,\n                   transform=transforms.Compose([\n                       transforms.ToTensor(),\n                   ])),\n    batch_size=1)\n\n# define a customized metric\nclass Top1Metric(object):\n    def __init__(self):\n        self.correct = 0\n    def update(self, output, label):\n        pred = output.argmax(dim=1, keepdim=True)\n        self.correct += pred.eq(label.view_as(pred)).sum().item()\n    def reset(self):\n        self.correct = 0\n    def result(self):\n        return 100. * self.correct / len(test_loader.dataset)\n\n# launch code for Intel\u00ae Neural Compressor\nfrom neural_compressor.experimental import Quantization\nquantizer = Quantization(\"./conf.yaml\")\nquantizer.model = model\nquantizer.calib_dataloader = test_loader\nquantizer.eval_dataloader = test_loader\nquantizer.metric = Top1Metric()\nq_model = quantizer()\nq_model.save('./output')",
      "# main.py\nmodel.eval()\n\nfrom torchvision import datasets, transforms\ntrain_loader = torch.utils.data.DataLoader(\n    datasets.MNIST('./data', train=True, download=True,\n                   transform=transforms.Compose([\n                       transforms.ToTensor(),\n                       transforms.Normalize((0.1307,), (0.3081,))\n                   ])),\n    batch_size=64, shuffle=True)\ntest_loader = torch.utils.data.DataLoader(\n    datasets.MNIST('./data', train=False, download=True,\n                   transform=transforms.Compose([\n                       transforms.ToTensor(),\n                       transforms.Normalize((0.1307,), (0.3081,))\n                   ])),\n    batch_size=1)\n\nimport torch.optim as optim\noptimizer = optim.SGD(model.parameters(), lr=0.0001, momentum=0.1)\n\ndef training_func(model):\n    model.train()\n    for epoch in range(1, 3):\n        for batch_idx, (data, target) in enumerate(train_loader):\n            optimizer.zero_grad()\n            output = model(data)\n            loss = F.nll_loss(output, target)\n            loss.backward()\n            optimizer.step()\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                  epoch, batch_idx * len(data), len(train_loader.dataset),\n                  100. * batch_idx / len(train_loader), loss.item()))\n\n# launch code for Intel\u00ae Neural Compressor\nfrom neural_compressor.experimental import Quantization\nquantizer = Quantization(\"./conf.yaml\")\nquantizer.model = model\nquantizer.q_func = training_func\nquantizer.eval_dataloader = test_loader\nq_model = quantizer()\nq_model.save('./output')",
      "# main.py\nmodel.eval()\n\n# launch code for Intel\u00ae Neural Compressor\nfrom neural_compressor.experimental import Quantization, common\nfrom neural_compressor.experimental.data.datasets.dummy_dataset import DummyDataset\nquantizer = Quantization(\"./conf.yaml\")\nquantizer.model = model\nquantizer.calib_dataloader = common.DataLoader(DummyDataset([(1, 1, 28, 28)]))\nq_model = quantizer()\nq_model.save('./output')",
      "from neural_compressor.utils.pytorch import load\nint8_model = load('./output', model)"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/intermediate/dist_tuto.html",
    "title": "Writing Distributed Applications with PyTorch\u00b6",
    "code_snippets": [
      "\"\"\"run.py:\"\"\"\n#!/usr/bin/env python\nimport os\nimport sys\nimport torch\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\n\ndef run(rank, size):\n    \"\"\" Distributed function to be implemented later. \"\"\"\n    pass\n\ndef init_process(rank, size, fn, backend='gloo'):\n    \"\"\" Initialize the distributed environment. \"\"\"\n    os.environ['MASTER_ADDR'] = '127.0.0.1'\n    os.environ['MASTER_PORT'] = '29500'\n    dist.init_process_group(backend, rank=rank, world_size=size)\n    fn(rank, size)\n\n\nif __name__ == \"__main__\":\n    world_size = 2\n    processes = []\n    if \"google.colab\" in sys.modules:\n        print(\"Running in Google Colab\")\n        mp.get_context(\"spawn\")\n    else:\n        mp.set_start_method(\"spawn\")\n    for rank in range(world_size):\n        p = mp.Process(target=init_process, args=(rank, world_size, run))\n        p.start()\n        processes.append(p)\n\n    for p in processes:\n        p.join()",
      "\"\"\"Blocking point-to-point communication.\"\"\"\n\ndef run(rank, size):\n    tensor = torch.zeros(1)\n    if rank == 0:\n        tensor += 1\n        # Send the tensor to process 1\n        dist.send(tensor=tensor, dst=1)\n    else:\n        # Receive tensor from process 0\n        dist.recv(tensor=tensor, src=0)\n    print('Rank ', rank, ' has data ', tensor[0])",
      "\"\"\"Non-blocking point-to-point communication.\"\"\"\n\ndef run(rank, size):\n    tensor = torch.zeros(1)\n    req = None\n    if rank == 0:\n        tensor += 1\n        # Send the tensor to process 1\n        req = dist.isend(tensor=tensor, dst=1)\n        print('Rank 0 started sending')\n    else:\n        # Receive tensor from process 0\n        req = dist.irecv(tensor=tensor, src=0)\n        print('Rank 1 started receiving')\n    req.wait()\n    print('Rank ', rank, ' has data ', tensor[0])",
      "\"\"\" All-Reduce example.\"\"\"\ndef run(rank, size):\n    \"\"\" Simple collective communication. \"\"\"\n    group = dist.new_group([0, 1])\n    tensor = torch.ones(1)\n    dist.all_reduce(tensor, op=dist.ReduceOp.SUM, group=group)\n    print('Rank ', rank, ' has data ', tensor[0])",
      "\"\"\" Dataset partitioning helper \"\"\"\nclass Partition(object):\n\n    def __init__(self, data, index):\n        self.data = data\n        self.index = index\n\n    def __len__(self):\n        return len(self.index)\n\n    def __getitem__(self, index):\n        data_idx = self.index[index]\n        return self.data[data_idx]\n\n\nclass DataPartitioner(object):\n\n    def __init__(self, data, sizes=[0.7, 0.2, 0.1], seed=1234):\n        self.data = data\n        self.partitions = []\n        rng = Random()  # from random import Random\n        rng.seed(seed)\n        data_len = len(data)\n        indexes = [x for x in range(0, data_len)]\n        rng.shuffle(indexes)\n\n        for frac in sizes:\n            part_len = int(frac * data_len)\n            self.partitions.append(indexes[0:part_len])\n            indexes = indexes[part_len:]\n\n    def use(self, partition):\n        return Partition(self.data, self.partitions[partition])",
      "\"\"\" Partitioning MNIST \"\"\"\ndef partition_dataset():\n    dataset = datasets.MNIST('./data', train=True, download=True,\n                             transform=transforms.Compose([\n                                 transforms.ToTensor(),\n                                 transforms.Normalize((0.1307,), (0.3081,))\n                             ]))\n    size = dist.get_world_size()\n    bsz = 128 // size\n    partition_sizes = [1.0 / size for _ in range(size)]\n    partition = DataPartitioner(dataset, partition_sizes)\n    partition = partition.use(dist.get_rank())\n    train_set = torch.utils.data.DataLoader(partition,\n                                         batch_size=bsz,\n                                         shuffle=True)\n    return train_set, bsz",
      "\"\"\" Distributed Synchronous SGD Example \"\"\"\ndef run(rank, size):\n    torch.manual_seed(1234)\n    train_set, bsz = partition_dataset()\n    model = Net()\n    optimizer = optim.SGD(model.parameters(),\n                          lr=0.01, momentum=0.5)\n\n    num_batches = ceil(len(train_set.dataset) / float(bsz))\n    for epoch in range(10):\n        epoch_loss = 0.0\n        for data, target in train_set:\n            optimizer.zero_grad()\n            output = model(data)\n            loss = F.nll_loss(output, target)\n            epoch_loss += loss.item()\n            loss.backward()\n            average_gradients(model)\n            optimizer.step()\n        print('Rank ', dist.get_rank(), ', epoch ',\n              epoch, ': ', epoch_loss / num_batches)",
      "\"\"\" Gradient averaging. \"\"\"\ndef average_gradients(model):\n    size = float(dist.get_world_size())\n    for param in model.parameters():\n        dist.all_reduce(param.grad.data, op=dist.ReduceOp.SUM)\n        param.grad.data /= size",
      "\"\"\" Implementation of a ring-reduce with addition. \"\"\"\ndef allreduce(send, recv):\n   rank = dist.get_rank()\n   size = dist.get_world_size()\n   send_buff = send.clone()\n   recv_buff = send.clone()\n   accum = send.clone()\n\n   left = ((rank - 1) + size) % size\n   right = (rank + 1) % size\n\n   for i in range(size - 1):\n       if i % 2 == 0:\n           # Send send_buff\n           send_req = dist.isend(send_buff, right)\n           dist.recv(recv_buff, left)\n           accum[:] += recv_buff[:]\n       else:\n           # Send recv_buff\n           send_req = dist.isend(recv_buff, right)\n           dist.recv(send_buff, left)\n           accum[:] += send_buff[:]\n       send_req.wait()\n   recv[:] = accum[:]"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/advanced/cpp_export.html",
    "title": "Loading a TorchScript Model in C++\u00b6",
    "code_snippets": [
      "import torch\nimport torchvision\n\n# An instance of your model.\nmodel = torchvision.models.resnet18()\n\n# An example input you would normally provide to your model's forward() method.\nexample = torch.rand(1, 3, 224, 224)\n\n# Use torch.jit.trace to generate a torch.jit.ScriptModule via tracing.\ntraced_script_module = torch.jit.trace(model, example)",
      "In[1]: output = traced_script_module(torch.ones(1, 3, 224, 224))\nIn[2]: output[0, :5]\nOut[2]: tensor([-0.2698, -0.0381,  0.4023, -0.3010, -0.0448], grad_fn=<SliceBackward>)",
      "import torch\n\nclass MyModule(torch.nn.Module):\n    def __init__(self, N, M):\n        super(MyModule, self).__init__()\n        self.weight = torch.nn.Parameter(torch.rand(N, M))\n\n    def forward(self, input):\n        if input.sum() > 0:\n          output = self.weight.mv(input)\n        else:\n          output = self.weight + input\n        return output",
      "class MyModule(torch.nn.Module):\n    def __init__(self, N, M):\n        super(MyModule, self).__init__()\n        self.weight = torch.nn.Parameter(torch.rand(N, M))\n\n    def forward(self, input):\n        if input.sum() > 0:\n          output = self.weight.mv(input)\n        else:\n          output = self.weight + input\n        return output\n\nmy_module = MyModule(10,20)\nsm = torch.jit.script(my_module)"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/advanced/python_custom_ops.html",
    "title": "Custom Python Operators\u00b6",
    "code_snippets": [
      "import torch\nfrom torchvision.transforms.functional import to_pil_image, pil_to_tensor\nimport PIL\nimport IPython\nimport matplotlib.pyplot as plt\n\ndef crop(pic, box):\n    img = to_pil_image(pic.cpu())\n    cropped_img = img.crop(box)\n    return pil_to_tensor(cropped_img).to(pic.device) / 255.\n\ndef display(img):\n    plt.imshow(img.numpy().transpose((1, 2, 0)))\n\nimg = torch.ones(3, 64, 64)\nimg *= torch.linspace(0, 1, steps=64) * torch.linspace(0, 1, steps=64).unsqueeze(-1)\ndisplay(img)",
      "@torch.compile(fullgraph=True)\ndef f(img):\n    return crop(img, (10, 10, 50, 50))\n\n# The following raises an error. Uncomment the line to see it.\n# cropped_img = f(img)",
      "from typing import Sequence\n\n# Use torch.library.custom_op to define a new custom operator.\n# If your operator mutates any input Tensors, their names must be specified\n# in the ``mutates_args`` argument.\n@torch.library.custom_op(\"mylib::crop\", mutates_args=())\ndef crop(pic: torch.Tensor, box: Sequence[int]) -> torch.Tensor:\n    img = to_pil_image(pic.cpu())\n    cropped_img = img.crop(box)\n    return (pil_to_tensor(cropped_img) / 255.).to(pic.device, pic.dtype)\n\n# Use register_fake to add a ``FakeTensor`` kernel for the operator\n@crop.register_fake\ndef _(pic, box):\n    channels = pic.shape[0]\n    x0, y0, x1, y1 = box\n    result = pic.new_empty(y1 - y0, x1 - x0, channels).permute(2, 0, 1)\n    # The result should have the same metadata (shape/strides/``dtype``/device)\n    # as running the ``crop`` function above.\n    return result",
      "@torch.compile(fullgraph=True)\ndef f(img):\n    return crop(img, (10, 10, 50, 50))\n\ncropped_img = f(img)\ndisplay(img)",
      "@torch.library.custom_op(\"mylib::paste\", mutates_args=())\ndef paste(im1: torch.Tensor, im2: torch.Tensor, coord: Sequence[int]) -> torch.Tensor:\n    assert im1.device == im2.device\n    assert im1.dtype == im2.dtype\n    im1_pil = to_pil_image(im1.cpu())\n    im2_pil = to_pil_image(im2.cpu())\n    PIL.Image.Image.paste(im1_pil, im2_pil, coord)\n    return (pil_to_tensor(im1_pil) / 255.).to(im1.device, im1.dtype)\n\n@paste.register_fake\ndef _(im1, im2, coord):\n    assert im1.device == im2.device\n    assert im1.dtype == im2.dtype\n    return torch.empty_like(im1)",
      "def backward(ctx, grad_output):\n    grad_input = grad_output.new_zeros(ctx.pic_shape)\n    grad_input = paste(grad_input, grad_output, ctx.coords)\n    return grad_input, None\n\ndef setup_context(ctx, inputs, output):\n    pic, box = inputs\n    ctx.coords = box[:2]\n    ctx.pic_shape = pic.shape\n\ncrop.register_autograd(backward, setup_context=setup_context)",
      "examples = [\n    [torch.randn(3, 64, 64), [0, 0, 10, 10]],\n    [torch.randn(3, 91, 91, requires_grad=True), [10, 0, 20, 10]],\n    [torch.randn(3, 60, 60, dtype=torch.double), [3, 4, 32, 20]],\n    [torch.randn(3, 512, 512, requires_grad=True, dtype=torch.double), [3, 4, 32, 45]],\n]\n\nfor example in examples:\n    torch.library.opcheck(crop, example)",
      "import numpy as np\n\n@torch.library.custom_op(\"mylib::numpy_sin\", mutates_args={\"output\"}, device_types=\"cpu\")\ndef numpy_sin(input: torch.Tensor, output: torch.Tensor) -> None:\n    assert input.device == output.device\n    assert input.device.type == \"cpu\"\n    input_np = input.numpy()\n    output_np = output.numpy()\n    np.sin(input_np, out=output_np)",
      "@torch.compile(fullgraph=True)\ndef f(x):\n    out = torch.empty(3)\n    numpy_sin(x, out)\n    return out\n\nx = torch.randn(3)\ny = f(x)\nassert torch.allclose(y, x.sin())",
      "example_inputs = [\n    [torch.randn(3), torch.empty(3)],\n    [torch.randn(0, 3), torch.empty(0, 3)],\n    [torch.randn(1, 2, 3, 4, dtype=torch.double), torch.empty(1, 2, 3, 4, dtype=torch.double)],\n]\n\nfor example in example_inputs:\n    torch.library.opcheck(numpy_sin, example)"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/recipes/recipes/module_load_state_dict_tips.html",
    "title": "Tips for Loading an nn.Module from a Checkpoint\u00b6",
    "code_snippets": [
      "import torch\nfrom torch import nn\nimport time\n\nclass SomeModule(torch.nn.Module):\n    def __init__(self, size):\n        super().__init__()\n        self.linears = nn.ModuleList([nn.Linear(size, size) for i in range(10)])\n\n    def forward(self, x):\n        return self.linears(x)\n\n\nm = SomeModule(1000)\ntorch.save(m.state_dict(), 'checkpoint.pth')",
      "state_dict = torch.load('checkpoint.pth', mmap=True, weights_only=True)\nwith torch.device('meta'):\n  meta_m = SomeModule(1000)\nmeta_m.load_state_dict(state_dict, assign=True)",
      "state_dict = torch.load('checkpoint.pth', weights_only=True)\nm = SomeModule(1000)\nm.load_state_dict(state_dict)",
      "start_time = time.time()\nstate_dict = torch.load('checkpoint.pth', weights_only=True)\nend_time = time.time()\nprint(f\"loading time without mmap={end_time - start_time}\")",
      "start_time = time.time()\nstate_dict = torch.load('checkpoint.pth', mmap=True, weights_only=True)\nend_time = time.time()\nprint(f\"loading time with mmap={end_time - start_time}\")",
      "def my_special_routine(t, device):\n    # this could be a much fancier operation\n    return t.to(dtype=torch.bfloat16, device=device)\n\ndef my_processing_function(key, device):\n    t = state_dict[key]\n    processed_t = my_special_routine(t, device)\n    del t\n    state_dict[key] = processed_t\n\nfor key in state_dict.keys():\n    device = torch.device('cuda')\n    my_processing_function(key, device)",
      "with torch.device('meta'):\n  new_m = SomeModule(1000)",
      "# As of PyTorch 2.3.0, one can use ``torch.__future__.set_swap_module_params_on_conversion`` to\n# avoid this caveat. This `recipe <https://pytorch.org/tutorials/recipes/recipes/swap_tensors.html>`_\n# provides more details.\n\nnew_m.load_state_dict(state_dict, assign=True)\n# Before 2.3.0, this MUST be done AFTER the load_state_dict with assign.\n# In versions >= 2.3.0, one can consider setting ``torch.__future__.set_swap_module_params_on_conversion``\nopt = torch.optim.SGD(new_m.parameters(), lr=1e-3)"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/intermediate/ddp_tutorial.html",
    "title": "Getting Started with Distributed Data Parallel\u00b6",
    "code_snippets": [
      "import os\nimport sys\nimport tempfile\nimport torch\nimport torch.distributed as dist\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.multiprocessing as mp\n\nfrom torch.nn.parallel import DistributedDataParallel as DDP\n\n# On Windows platform, the torch.distributed package only\n# supports Gloo backend, FileStore and TcpStore.\n# For FileStore, set init_method parameter in init_process_group\n# to a local file. Example as follow:\n# init_method=\"file:///f:/libtmp/some_file\"\n# dist.init_process_group(\n#    \"gloo\",\n#    rank=rank,\n#    init_method=init_method,\n#    world_size=world_size)\n# For TcpStore, same way as on Linux.\n\ndef setup(rank, world_size):\n    os.environ['MASTER_ADDR'] = 'localhost'\n    os.environ['MASTER_PORT'] = '12355'\n\n    # initialize the process group\n    dist.init_process_group(\"gloo\", rank=rank, world_size=world_size)\n\ndef cleanup():\n    dist.destroy_process_group()",
      "class ToyModel(nn.Module):\n    def __init__(self):\n        super(ToyModel, self).__init__()\n        self.net1 = nn.Linear(10, 10)\n        self.relu = nn.ReLU()\n        self.net2 = nn.Linear(10, 5)\n\n    def forward(self, x):\n        return self.net2(self.relu(self.net1(x)))\n\n\ndef demo_basic(rank, world_size):\n    print(f\"Running basic DDP example on rank {rank}.\")\n    setup(rank, world_size)\n\n    # create model and move it to GPU with id rank\n    model = ToyModel().to(rank)\n    ddp_model = DDP(model, device_ids=[rank])\n\n    loss_fn = nn.MSELoss()\n    optimizer = optim.SGD(ddp_model.parameters(), lr=0.001)\n\n    optimizer.zero_grad()\n    outputs = ddp_model(torch.randn(20, 10))\n    labels = torch.randn(20, 5).to(rank)\n    loss_fn(outputs, labels).backward()\n    optimizer.step()\n\n    cleanup()\n    print(f\"Finished running basic DDP example on rank {rank}.\")\n\n\ndef run_demo(demo_fn, world_size):\n    mp.spawn(demo_fn,\n             args=(world_size,),\n             nprocs=world_size,\n             join=True)",
      "def demo_checkpoint(rank, world_size):\n    print(f\"Running DDP checkpoint example on rank {rank}.\")\n    setup(rank, world_size)\n\n    model = ToyModel().to(rank)\n    ddp_model = DDP(model, device_ids=[rank])\n\n\n    CHECKPOINT_PATH = tempfile.gettempdir() + \"/model.checkpoint\"\n    if rank == 0:\n        # All processes should see same parameters as they all start from same\n        # random parameters and gradients are synchronized in backward passes.\n        # Therefore, saving it in one process is sufficient.\n        torch.save(ddp_model.state_dict(), CHECKPOINT_PATH)\n\n    # Use a barrier() to make sure that process 1 loads the model after process\n    # 0 saves it.\n    dist.barrier()\n    # configure map_location properly\n    map_location = {'cuda:%d' % 0: 'cuda:%d' % rank}\n    ddp_model.load_state_dict(\n        torch.load(CHECKPOINT_PATH, map_location=map_location, weights_only=True))\n\n    loss_fn = nn.MSELoss()\n    optimizer = optim.SGD(ddp_model.parameters(), lr=0.001)\n\n    optimizer.zero_grad()\n    outputs = ddp_model(torch.randn(20, 10))\n    labels = torch.randn(20, 5).to(rank)\n\n    loss_fn(outputs, labels).backward()\n    optimizer.step()\n\n    # Not necessary to use a dist.barrier() to guard the file deletion below\n    # as the AllReduce ops in the backward pass of DDP already served as\n    # a synchronization.\n\n    if rank == 0:\n        os.remove(CHECKPOINT_PATH)\n\n    cleanup()\n    print(f\"Finished running DDP checkpoint example on rank {rank}.\")",
      "class ToyMpModel(nn.Module):\n    def __init__(self, dev0, dev1):\n        super(ToyMpModel, self).__init__()\n        self.dev0 = dev0\n        self.dev1 = dev1\n        self.net1 = torch.nn.Linear(10, 10).to(dev0)\n        self.relu = torch.nn.ReLU()\n        self.net2 = torch.nn.Linear(10, 5).to(dev1)\n\n    def forward(self, x):\n        x = x.to(self.dev0)\n        x = self.relu(self.net1(x))\n        x = x.to(self.dev1)\n        return self.net2(x)",
      "def demo_model_parallel(rank, world_size):\n    print(f\"Running DDP with model parallel example on rank {rank}.\")\n    setup(rank, world_size)\n\n    # setup mp_model and devices for this process\n    dev0 = rank * 2\n    dev1 = rank * 2 + 1\n    mp_model = ToyMpModel(dev0, dev1)\n    ddp_mp_model = DDP(mp_model)\n\n    loss_fn = nn.MSELoss()\n    optimizer = optim.SGD(ddp_mp_model.parameters(), lr=0.001)\n\n    optimizer.zero_grad()\n    # outputs will be on dev1\n    outputs = ddp_mp_model(torch.randn(20, 10))\n    labels = torch.randn(20, 5).to(dev1)\n    loss_fn(outputs, labels).backward()\n    optimizer.step()\n\n    cleanup()\n    print(f\"Finished running DDP with model parallel example on rank {rank}.\")\n\n\nif __name__ == \"__main__\":\n    n_gpus = torch.cuda.device_count()\n    assert n_gpus >= 2, f\"Requires at least 2 GPUs to run, but got {n_gpus}\"\n    world_size = n_gpus\n    run_demo(demo_basic, world_size)\n    run_demo(demo_checkpoint, world_size)\n    world_size = n_gpus//2\n    run_demo(demo_model_parallel, world_size)",
      "import torch\nimport torch.distributed as dist\nimport torch.nn as nn\nimport torch.optim as optim\n\nfrom torch.nn.parallel import DistributedDataParallel as DDP\n\nclass ToyModel(nn.Module):\n    def __init__(self):\n        super(ToyModel, self).__init__()\n        self.net1 = nn.Linear(10, 10)\n        self.relu = nn.ReLU()\n        self.net2 = nn.Linear(10, 5)\n\n    def forward(self, x):\n        return self.net2(self.relu(self.net1(x)))\n\n\ndef demo_basic():\n    torch.cuda.set_device(int(os.environ[\"LOCAL_RANK\"]))\n    dist.init_process_group(\"nccl\")\n    rank = dist.get_rank()\n    print(f\"Start running basic DDP example on rank {rank}.\")\n    # create model and move it to GPU with id rank\n    device_id = rank % torch.cuda.device_count()\n    model = ToyModel().to(device_id)\n    ddp_model = DDP(model, device_ids=[device_id])\n    loss_fn = nn.MSELoss()\n    optimizer = optim.SGD(ddp_model.parameters(), lr=0.001)\n\n    optimizer.zero_grad()\n    outputs = ddp_model(torch.randn(20, 10))\n    labels = torch.randn(20, 5).to(device_id)\n    loss_fn(outputs, labels).backward()\n    optimizer.step()\n    dist.destroy_process_group()\n    print(f\"Finished running basic DDP example on rank {rank}.\")\n\nif __name__ == \"__main__\":\n    demo_basic()"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/advanced/torch-script-parallelism.html",
    "title": "Dynamic Parallelism in TorchScript\u00b6",
    "code_snippets": [
      "import torch\n\ndef foo(x):\n    return torch.neg(x)\n\n@torch.jit.script\ndef example(x):\n    # Call `foo` using parallelism:\n    # First, we \"fork\" off a task. This task will run `foo` with argument `x`\n    future = torch.jit.fork(foo, x)\n\n    # Call `foo` normally\n    x_normal = foo(x)\n\n    # Second, we \"wait\" on the task. Since the task may be running in\n    # parallel, we have to \"wait\" for its result to become available.\n    # Notice that by having lines of code between the \"fork()\" and \"wait()\"\n    # call for a given Future, we can overlap computations so that they\n    # run in parallel.\n    x_parallel = torch.jit.wait(future)\n\n    return x_normal, x_parallel\n\nprint(example(torch.ones(1))) # (-1., -1.)",
      "import torch\nfrom typing import List\n\ndef foo(x):\n    return torch.neg(x)\n\n@torch.jit.script\ndef example(x):\n    futures : List[torch.jit.Future[torch.Tensor]] = []\n    for _ in range(100):\n        futures.append(torch.jit.fork(foo, x))\n\n    results = []\n    for future in futures:\n        results.append(torch.jit.wait(future))\n\n    return torch.sum(torch.stack(results))\n\nprint(example(torch.ones([])))",
      "import torch, time\n\n# In RNN parlance, the dimensions we care about are:\n# # of time-steps (T)\n# Batch size (B)\n# Hidden size/number of \"channels\" (C)\nT, B, C = 50, 50, 1024\n\n# A module that defines a single \"bidirectional LSTM\". This is simply two\n# LSTMs applied to the same sequence, but one in reverse\nclass BidirectionalRecurrentLSTM(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.cell_f = torch.nn.LSTM(input_size=C, hidden_size=C)\n        self.cell_b = torch.nn.LSTM(input_size=C, hidden_size=C)\n\n    def forward(self, x : torch.Tensor) -> torch.Tensor:\n        # Forward layer\n        output_f, _ = self.cell_f(x)\n\n        # Backward layer. Flip input in the time dimension (dim 0), apply the\n        # layer, then flip the outputs in the time dimension\n        x_rev = torch.flip(x, dims=[0])\n        output_b, _ = self.cell_b(torch.flip(x, dims=[0]))\n        output_b_rev = torch.flip(output_b, dims=[0])\n\n        return torch.cat((output_f, output_b_rev), dim=2)\n\n\n# An \"ensemble\" of `BidirectionalRecurrentLSTM` modules. The modules in the\n# ensemble are run one-by-one on the same input then their results are\n# stacked and summed together, returning the combined result.\nclass LSTMEnsemble(torch.nn.Module):\n    def __init__(self, n_models):\n        super().__init__()\n        self.n_models = n_models\n        self.models = torch.nn.ModuleList([\n            BidirectionalRecurrentLSTM() for _ in range(self.n_models)])\n\n    def forward(self, x : torch.Tensor) -> torch.Tensor:\n        results = []\n        for model in self.models:\n            results.append(model(x))\n        return torch.stack(results).sum(dim=0)\n\n# For a head-to-head comparison to what we're going to do with fork/wait, let's\n# instantiate the model and compile it with TorchScript\nens = torch.jit.script(LSTMEnsemble(n_models=4))\n\n# Normally you would pull this input out of an embedding table, but for the\n# purpose of this demo let's just use random data.\nx = torch.rand(T, B, C)\n\n# Let's run the model once to warm up things like the memory allocator\nens(x)\n\nx = torch.rand(T, B, C)\n\n# Let's see how fast it runs!\ns = time.time()\nens(x)\nprint('Inference took', time.time() - s, ' seconds')",
      "def forward(self, x : torch.Tensor) -> torch.Tensor:\n    # Forward layer - fork() so this can run in parallel to the backward\n    # layer\n    future_f = torch.jit.fork(self.cell_f, x)\n\n    # Backward layer. Flip input in the time dimension (dim 0), apply the\n    # layer, then flip the outputs in the time dimension\n    x_rev = torch.flip(x, dims=[0])\n    output_b, _ = self.cell_b(torch.flip(x, dims=[0]))\n    output_b_rev = torch.flip(output_b, dims=[0])\n\n    # Retrieve the output from the forward layer. Note this needs to happen\n    # *after* the stuff we want to parallelize with\n    output_f, _ = torch.jit.wait(future_f)\n\n    return torch.cat((output_f, output_b_rev), dim=2)",
      "with torch.autograd.profiler.profile() as prof:\n    ens(x)\nprof.export_chrome_trace('parallel.json')",
      "def forward(self, x : torch.Tensor) -> torch.Tensor:\n    # Launch tasks for each model\n    futures : List[torch.jit.Future[torch.Tensor]] = []\n    for model in self.models:\n        futures.append(torch.jit.fork(model, x))\n\n    # Collect the results from the launched tasks\n    results : List[torch.Tensor] = []\n    for future in futures:\n        results.append(torch.jit.wait(future))\n\n    return torch.stack(results).sum(dim=0)",
      "def forward(self, x : torch.Tensor) -> torch.Tensor:\n    futures = [torch.jit.fork(model, x) for model in self.models]\n    results = [torch.jit.wait(fut) for fut in futures]\n    return torch.stack(results).sum(dim=0)"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html",
    "title": "Automatic Differentiation with torch.autograd\u00b6",
    "code_snippets": [
      "import torch\n\nx = torch.ones(5)  # input tensor\ny = torch.zeros(3)  # expected output\nw = torch.randn(5, 3, requires_grad=True)\nb = torch.randn(3, requires_grad=True)\nz = torch.matmul(x, w)+b\nloss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)",
      "z = torch.matmul(x, w)+b\nprint(z.requires_grad)\n\nwith torch.no_grad():\n    z = torch.matmul(x, w)+b\nprint(z.requires_grad)",
      "z = torch.matmul(x, w)+b\nz_det = z.detach()\nprint(z_det.requires_grad)",
      "inp = torch.eye(4, 5, requires_grad=True)\nout = (inp+1).pow(2).t()\nout.backward(torch.ones_like(out), retain_graph=True)\nprint(f\"First call\\n{inp.grad}\")\nout.backward(torch.ones_like(out), retain_graph=True)\nprint(f\"\\nSecond call\\n{inp.grad}\")\ninp.grad.zero_()\nout.backward(torch.ones_like(out), retain_graph=True)\nprint(f\"\\nCall after zeroing gradients\\n{inp.grad}\")"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/recipes/distributed_async_checkpoint_recipe.html",
    "title": "Asynchronous Saving with Distributed Checkpoint (DCP)\u00b6",
    "code_snippets": [
      "import os\n\nimport torch\nimport torch.distributed as dist\nimport torch.distributed.checkpoint as dcp\nimport torch.multiprocessing as mp\nimport torch.nn as nn\n\nfrom torch.distributed.fsdp import FullyShardedDataParallel as FSDP\nfrom torch.distributed.checkpoint.state_dict import get_state_dict, set_state_dict\nfrom torch.distributed.checkpoint.stateful import Stateful\nfrom torch.distributed.fsdp.fully_sharded_data_parallel import StateDictType\n\nCHECKPOINT_DIR = \"checkpoint\"\n\n\nclass AppState(Stateful):\n    \"\"\"This is a useful wrapper for checkpointing the Application State. Since this object is compliant\n    with the Stateful protocol, DCP will automatically call state_dict/load_stat_dict as needed in the\n    dcp.save/load APIs.\n\n    Note: We take advantage of this wrapper to hande calling distributed state dict methods on the model\n    and optimizer.\n    \"\"\"\n\n    def __init__(self, model, optimizer=None):\n        self.model = model\n        self.optimizer = optimizer\n\n    def state_dict(self):\n        # this line automatically manages FSDP FQN's, as well as sets the default state dict type to FSDP.SHARDED_STATE_DICT\n        model_state_dict, optimizer_state_dict = get_state_dict(model, optimizer)\n        return {\n            \"model\": model_state_dict,\n            \"optim\": optimizer_state_dict\n        }\n\n    def load_state_dict(self, state_dict):\n        # sets our state dicts on the model and optimizer, now that we've loaded\n        set_state_dict(\n            self.model,\n            self.optimizer,\n            model_state_dict=state_dict[\"model\"],\n            optim_state_dict=state_dict[\"optim\"]\n        )\n\nclass ToyModel(nn.Module):\n    def __init__(self):\n        super(ToyModel, self).__init__()\n        self.net1 = nn.Linear(16, 16)\n        self.relu = nn.ReLU()\n        self.net2 = nn.Linear(16, 8)\n\n    def forward(self, x):\n        return self.net2(self.relu(self.net1(x)))\n\n\ndef setup(rank, world_size):\n    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n    os.environ[\"MASTER_PORT\"] = \"12355 \"\n\n    # initialize the process group\n    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n    torch.cuda.set_device(rank)\n\n\ndef cleanup():\n    dist.destroy_process_group()\n\n\ndef run_fsdp_checkpoint_save_example(rank, world_size):\n    print(f\"Running basic FSDP checkpoint saving example on rank {rank}.\")\n    setup(rank, world_size)\n\n    # create a model and move it to GPU with id rank\n    model = ToyModel().to(rank)\n    model = FSDP(model)\n\n    loss_fn = nn.MSELoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n\n    checkpoint_future = None\n    for step in range(10):\n        optimizer.zero_grad()\n        model(torch.rand(8, 16, device=\"cuda\")).sum().backward()\n        optimizer.step()\n\n        # waits for checkpointing to finish if one exists, avoiding queuing more then one checkpoint request at a time\n        if checkpoint_future is not None:\n            checkpoint_future.result()\n\n        state_dict = { \"app\": AppState(model, optimizer) }\n        checkpoint_future = dcp.async_save(state_dict, checkpoint_id=f\"{CHECKPOINT_DIR}_step{step}\")\n\n    cleanup()\n\n\nif __name__ == \"__main__\":\n    world_size = torch.cuda.device_count()\n    print(f\"Running async checkpoint example on {world_size} devices.\")\n    mp.spawn(\n        run_fsdp_checkpoint_save_example,\n        args=(world_size,),\n        nprocs=world_size,\n        join=True,\n    )",
      "import os\n\nimport torch\nimport torch.distributed as dist\nimport torch.distributed.checkpoint as dcp\nimport torch.multiprocessing as mp\nimport torch.nn as nn\n\nfrom torch.distributed.fsdp import FullyShardedDataParallel as FSDP\nfrom torch.distributed.checkpoint.state_dict import get_state_dict, set_state_dict\nfrom torch.distributed.checkpoint.stateful import Stateful\nfrom torch.distributed.fsdp.fully_sharded_data_parallel import StateDictType\nfrom torch.distributed.checkpoint import StorageWriter\n\nCHECKPOINT_DIR = \"checkpoint\"\n\n\nclass AppState(Stateful):\n    \"\"\"This is a useful wrapper for checkpointing the Application State. Since this object is compliant\n    with the Stateful protocol, DCP will automatically call state_dict/load_stat_dict as needed in the\n    dcp.save/load APIs.\n\n    Note: We take advantage of this wrapper to hande calling distributed state dict methods on the model\n    and optimizer.\n    \"\"\"\n\n    def __init__(self, model, optimizer=None):\n        self.model = model\n        self.optimizer = optimizer\n\n    def state_dict(self):\n        # this line automatically manages FSDP FQN's, as well as sets the default state dict type to FSDP.SHARDED_STATE_DICT\n        model_state_dict, optimizer_state_dict = get_state_dict(model, optimizer)\n        return {\n            \"model\": model_state_dict,\n            \"optim\": optimizer_state_dict\n        }\n\n    def load_state_dict(self, state_dict):\n        # sets our state dicts on the model and optimizer, now that we've loaded\n        set_state_dict(\n            self.model,\n            self.optimizer,\n            model_state_dict=state_dict[\"model\"],\n            optim_state_dict=state_dict[\"optim\"]\n        )\n\nclass ToyModel(nn.Module):\n    def __init__(self):\n        super(ToyModel, self).__init__()\n        self.net1 = nn.Linear(16, 16)\n        self.relu = nn.ReLU()\n        self.net2 = nn.Linear(16, 8)\n\n    def forward(self, x):\n        return self.net2(self.relu(self.net1(x)))\n\n\ndef setup(rank, world_size):\n    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n    os.environ[\"MASTER_PORT\"] = \"12355 \"\n\n    # initialize the process group\n    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n    torch.cuda.set_device(rank)\n\n\ndef cleanup():\n    dist.destroy_process_group()\n\n\ndef run_fsdp_checkpoint_save_example(rank, world_size):\n    print(f\"Running basic FSDP checkpoint saving example on rank {rank}.\")\n    setup(rank, world_size)\n\n    # create a model and move it to GPU with id rank\n    model = ToyModel().to(rank)\n    model = FSDP(model)\n\n    loss_fn = nn.MSELoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n\n    # The storage writer defines our 'staging' strategy, where staging is considered the process of copying\n    # checkpoints to in-memory buffers. By setting `cached_state_dict=True`, we enable efficient memory copying\n    # into a persistent buffer with pinned memory enabled.\n    # Note: It's important that the writer persists in between checkpointing requests, since it maintains the\n    # pinned memory buffer.\n    writer = StorageWriter(cached_state_dict=True)\n    checkpoint_future = None\n    for step in range(10):\n        optimizer.zero_grad()\n        model(torch.rand(8, 16, device=\"cuda\")).sum().backward()\n        optimizer.step()\n\n        state_dict = { \"app\": AppState(model, optimizer) }\n        if checkpoint_future is not None:\n            # waits for checkpointing to finish, avoiding queuing more then one checkpoint request at a time\n            checkpoint_future.result()\n        dcp.async_save(state_dict, storage_writer=writer, checkpoint_id=f\"{CHECKPOINT_DIR}_step{step}\")\n\n    cleanup()\n\n\nif __name__ == \"__main__\":\n    world_size = torch.cuda.device_count()\n    print(f\"Running fsdp checkpoint example on {world_size} devices.\")\n    mp.spawn(\n        run_fsdp_checkpoint_save_example,\n        args=(world_size,),\n        nprocs=world_size,\n        join=True,\n    )"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/beginner/basics/transforms_tutorial.html",
    "title": "Transforms\u00b6",
    "code_snippets": [
      "import torch\nfrom torchvision import datasets\nfrom torchvision.transforms import ToTensor, Lambda\n\nds = datasets.FashionMNIST(\n    root=\"data\",\n    train=True,\n    download=True,\n    transform=ToTensor(),\n    target_transform=Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n)",
      "target_transform = Lambda(lambda y: torch.zeros(\n    10, dtype=torch.float).scatter_(dim=0, index=torch.tensor(y), value=1))"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/beginner/fgsm_tutorial.html",
    "title": "Adversarial Example Generation\u00b6",
    "code_snippets": [
      "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nimport numpy as np\nimport matplotlib.pyplot as plt",
      "epsilons = [0, .05, .1, .15, .2, .25, .3]\npretrained_model = \"data/lenet_mnist_model.pth\"\n# Set random seed for reproducibility\ntorch.manual_seed(42)",
      "<torch._C.Generator object at 0x7f0947b8e570>",
      "# LeNet Model definition\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n        self.dropout1 = nn.Dropout(0.25)\n        self.dropout2 = nn.Dropout(0.5)\n        self.fc1 = nn.Linear(9216, 128)\n        self.fc2 = nn.Linear(128, 10)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = F.relu(x)\n        x = self.conv2(x)\n        x = F.relu(x)\n        x = F.max_pool2d(x, 2)\n        x = self.dropout1(x)\n        x = torch.flatten(x, 1)\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.dropout2(x)\n        x = self.fc2(x)\n        output = F.log_softmax(x, dim=1)\n        return output\n\n# MNIST Test dataset and dataloader declaration\ntest_loader = torch.utils.data.DataLoader(\n    datasets.MNIST('../data', train=False, download=True, transform=transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize((0.1307,), (0.3081,)),\n            ])),\n        batch_size=1, shuffle=True)\n\n# We want to be able to train our model on an `accelerator <https://pytorch.org/docs/stable/torch.html#accelerators>`__\n# such as CUDA, MPS, MTIA, or XPU. If the current accelerator is available, we will use it. Otherwise, we use the CPU.\ndevice = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\nprint(f\"Using {device} device\")\n\n# Initialize the network\nmodel = Net().to(device)\n\n# Load the pretrained model\nmodel.load_state_dict(torch.load(pretrained_model, map_location=device, weights_only=True))\n\n# Set the model in evaluation mode. In this case this is for the Dropout layers\nmodel.eval()",
      "# FGSM attack code\ndef fgsm_attack(image, epsilon, data_grad):\n    # Collect the element-wise sign of the data gradient\n    sign_data_grad = data_grad.sign()\n    # Create the perturbed image by adjusting each pixel of the input image\n    perturbed_image = image + epsilon*sign_data_grad\n    # Adding clipping to maintain [0,1] range\n    perturbed_image = torch.clamp(perturbed_image, 0, 1)\n    # Return the perturbed image\n    return perturbed_image\n\n# restores the tensors to their original scale\ndef denorm(batch, mean=[0.1307], std=[0.3081]):\n    \"\"\"\n    Convert a batch of tensors to their original scale.\n\n    Args:\n        batch (torch.Tensor): Batch of normalized tensors.\n        mean (torch.Tensor or list): Mean used for normalization.\n        std (torch.Tensor or list): Standard deviation used for normalization.\n\n    Returns:\n        torch.Tensor: batch of tensors without normalization applied to them.\n    \"\"\"\n    if isinstance(mean, list):\n        mean = torch.tensor(mean).to(device)\n    if isinstance(std, list):\n        std = torch.tensor(std).to(device)\n\n    return batch * std.view(1, -1, 1, 1) + mean.view(1, -1, 1, 1)",
      "def test( model, device, test_loader, epsilon ):\n\n    # Accuracy counter\n    correct = 0\n    adv_examples = []\n\n    # Loop over all examples in test set\n    for data, target in test_loader:\n\n        # Send the data and label to the device\n        data, target = data.to(device), target.to(device)\n\n        # Set requires_grad attribute of tensor. Important for Attack\n        data.requires_grad = True\n\n        # Forward pass the data through the model\n        output = model(data)\n        init_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n\n        # If the initial prediction is wrong, don't bother attacking, just move on\n        if init_pred.item() != target.item():\n            continue\n\n        # Calculate the loss\n        loss = F.nll_loss(output, target)\n\n        # Zero all existing gradients\n        model.zero_grad()\n\n        # Calculate gradients of model in backward pass\n        loss.backward()\n\n        # Collect ``datagrad``\n        data_grad = data.grad.data\n\n        # Restore the data to its original scale\n        data_denorm = denorm(data)\n\n        # Call FGSM Attack\n        perturbed_data = fgsm_attack(data_denorm, epsilon, data_grad)\n\n        # Reapply normalization\n        perturbed_data_normalized = transforms.Normalize((0.1307,), (0.3081,))(perturbed_data)\n\n        # Re-classify the perturbed image\n        output = model(perturbed_data_normalized)\n\n        # Check for success\n        final_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n        if final_pred.item() == target.item():\n            correct += 1\n            # Special case for saving 0 epsilon examples\n            if epsilon == 0 and len(adv_examples) < 5:\n                adv_ex = perturbed_data.squeeze().detach().cpu().numpy()\n                adv_examples.append( (init_pred.item(), final_pred.item(), adv_ex) )\n        else:\n            # Save some adv examples for visualization later\n            if len(adv_examples) < 5:\n                adv_ex = perturbed_data.squeeze().detach().cpu().numpy()\n                adv_examples.append( (init_pred.item(), final_pred.item(), adv_ex) )\n\n    # Calculate final accuracy for this epsilon\n    final_acc = correct/float(len(test_loader))\n    print(f\"Epsilon: {epsilon}\\tTest Accuracy = {correct} / {len(test_loader)} = {final_acc}\")\n\n    # Return the accuracy and an adversarial example\n    return final_acc, adv_examples"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/beginner/pytorch_with_examples.html",
    "title": "Learning PyTorch with Examples\u00b6",
    "code_snippets": [
      "# -*- coding: utf-8 -*-\nimport numpy as np\nimport math\n\n# Create random input and output data\nx = np.linspace(-math.pi, math.pi, 2000)\ny = np.sin(x)\n\n# Randomly initialize weights\na = np.random.randn()\nb = np.random.randn()\nc = np.random.randn()\nd = np.random.randn()\n\nlearning_rate = 1e-6\nfor t in range(2000):\n    # Forward pass: compute predicted y\n    # y = a + b x + c x^2 + d x^3\n    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n\n    # Compute and print loss\n    loss = np.square(y_pred - y).sum()\n    if t % 100 == 99:\n        print(t, loss)\n\n    # Backprop to compute gradients of a, b, c, d with respect to loss\n    grad_y_pred = 2.0 * (y_pred - y)\n    grad_a = grad_y_pred.sum()\n    grad_b = (grad_y_pred * x).sum()\n    grad_c = (grad_y_pred * x ** 2).sum()\n    grad_d = (grad_y_pred * x ** 3).sum()\n\n    # Update weights\n    a -= learning_rate * grad_a\n    b -= learning_rate * grad_b\n    c -= learning_rate * grad_c\n    d -= learning_rate * grad_d\n\nprint(f'Result: y = {a} + {b} x + {c} x^2 + {d} x^3')",
      "# -*- coding: utf-8 -*-\n\nimport torch\nimport math\n\n\ndtype = torch.float\ndevice = torch.device(\"cpu\")\n# device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n\n# Create random input and output data\nx = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\ny = torch.sin(x)\n\n# Randomly initialize weights\na = torch.randn((), device=device, dtype=dtype)\nb = torch.randn((), device=device, dtype=dtype)\nc = torch.randn((), device=device, dtype=dtype)\nd = torch.randn((), device=device, dtype=dtype)\n\nlearning_rate = 1e-6\nfor t in range(2000):\n    # Forward pass: compute predicted y\n    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n\n    # Compute and print loss\n    loss = (y_pred - y).pow(2).sum().item()\n    if t % 100 == 99:\n        print(t, loss)\n\n    # Backprop to compute gradients of a, b, c, d with respect to loss\n    grad_y_pred = 2.0 * (y_pred - y)\n    grad_a = grad_y_pred.sum()\n    grad_b = (grad_y_pred * x).sum()\n    grad_c = (grad_y_pred * x ** 2).sum()\n    grad_d = (grad_y_pred * x ** 3).sum()\n\n    # Update weights using gradient descent\n    a -= learning_rate * grad_a\n    b -= learning_rate * grad_b\n    c -= learning_rate * grad_c\n    d -= learning_rate * grad_d\n\n\nprint(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')",
      "import torch\nimport math\n\n# We want to be able to train our model on an `accelerator <https://pytorch.org/docs/stable/torch.html#accelerators>`__\n# such as CUDA, MPS, MTIA, or XPU. If the current accelerator is available, we will use it. Otherwise, we use the CPU.\n\ndtype = torch.float\ndevice = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\nprint(f\"Using {device} device\")\ntorch.set_default_device(device)\n\n# Create Tensors to hold input and outputs.\n# By default, requires_grad=False, which indicates that we do not need to\n# compute gradients with respect to these Tensors during the backward pass.\nx = torch.linspace(-math.pi, math.pi, 2000, dtype=dtype)\ny = torch.sin(x)\n\n# Create random Tensors for weights. For a third order polynomial, we need\n# 4 weights: y = a + b x + c x^2 + d x^3\n# Setting requires_grad=True indicates that we want to compute gradients with\n# respect to these Tensors during the backward pass.\na = torch.randn((), dtype=dtype, requires_grad=True)\nb = torch.randn((), dtype=dtype, requires_grad=True)\nc = torch.randn((), dtype=dtype, requires_grad=True)\nd = torch.randn((), dtype=dtype, requires_grad=True)\n\nlearning_rate = 1e-6\nfor t in range(2000):\n    # Forward pass: compute predicted y using operations on Tensors.\n    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n\n    # Compute and print loss using operations on Tensors.\n    # Now loss is a Tensor of shape (1,)\n    # loss.item() gets the scalar value held in the loss.\n    loss = (y_pred - y).pow(2).sum()\n    if t % 100 == 99:\n        print(t, loss.item())\n\n    # Use autograd to compute the backward pass. This call will compute the\n    # gradient of loss with respect to all Tensors with requires_grad=True.\n    # After this call a.grad, b.grad. c.grad and d.grad will be Tensors holding\n    # the gradient of the loss with respect to a, b, c, d respectively.\n    loss.backward()\n\n    # Manually update weights using gradient descent. Wrap in torch.no_grad()\n    # because weights have requires_grad=True, but we don't need to track this\n    # in autograd.\n    with torch.no_grad():\n        a -= learning_rate * a.grad\n        b -= learning_rate * b.grad\n        c -= learning_rate * c.grad\n        d -= learning_rate * d.grad\n\n        # Manually zero the gradients after updating weights\n        a.grad = None\n        b.grad = None\n        c.grad = None\n        d.grad = None\n\nprint(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')",
      "import torch\nimport math\n\n\nclass LegendrePolynomial3(torch.autograd.Function):\n    \"\"\"\n    We can implement our own custom autograd Functions by subclassing\n    torch.autograd.Function and implementing the forward and backward passes\n    which operate on Tensors.\n    \"\"\"\n\n    @staticmethod\n    def forward(ctx, input):\n        \"\"\"\n        In the forward pass we receive a Tensor containing the input and return\n        a Tensor containing the output. ctx is a context object that can be used\n        to stash information for backward computation. You can cache arbitrary\n        objects for use in the backward pass using the ctx.save_for_backward method.\n        \"\"\"\n        ctx.save_for_backward(input)\n        return 0.5 * (5 * input ** 3 - 3 * input)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        \"\"\"\n        In the backward pass we receive a Tensor containing the gradient of the loss\n        with respect to the output, and we need to compute the gradient of the loss\n        with respect to the input.\n        \"\"\"\n        input, = ctx.saved_tensors\n        return grad_output * 1.5 * (5 * input ** 2 - 1)\n\n\ndtype = torch.float\ndevice = torch.device(\"cpu\")\n# device = torch.device(\"cuda:0\")  # Uncomment this to run on GPU\n\n# Create Tensors to hold input and outputs.\n# By default, requires_grad=False, which indicates that we do not need to\n# compute gradients with respect to these Tensors during the backward pass.\nx = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\ny = torch.sin(x)\n\n# Create random Tensors for weights. For this example, we need\n# 4 weights: y = a + b * P3(c + d * x), these weights need to be initialized\n# not too far from the correct result to ensure convergence.\n# Setting requires_grad=True indicates that we want to compute gradients with\n# respect to these Tensors during the backward pass.\na = torch.full((), 0.0, device=device, dtype=dtype, requires_grad=True)\nb = torch.full((), -1.0, device=device, dtype=dtype, requires_grad=True)\nc = torch.full((), 0.0, device=device, dtype=dtype, requires_grad=True)\nd = torch.full((), 0.3, device=device, dtype=dtype, requires_grad=True)\n\nlearning_rate = 5e-6\nfor t in range(2000):\n    # To apply our Function, we use Function.apply method. We alias this as 'P3'.\n    P3 = LegendrePolynomial3.apply\n\n    # Forward pass: compute predicted y using operations; we compute\n    # P3 using our custom autograd operation.\n    y_pred = a + b * P3(c + d * x)\n\n    # Compute and print loss\n    loss = (y_pred - y).pow(2).sum()\n    if t % 100 == 99:\n        print(t, loss.item())\n\n    # Use autograd to compute the backward pass.\n    loss.backward()\n\n    # Update weights using gradient descent\n    with torch.no_grad():\n        a -= learning_rate * a.grad\n        b -= learning_rate * b.grad\n        c -= learning_rate * c.grad\n        d -= learning_rate * d.grad\n\n        # Manually zero the gradients after updating weights\n        a.grad = None\n        b.grad = None\n        c.grad = None\n        d.grad = None\n\nprint(f'Result: y = {a.item()} + {b.item()} * P3({c.item()} + {d.item()} x)')",
      "# -*- coding: utf-8 -*-\nimport torch\nimport math\n\n\n# Create Tensors to hold input and outputs.\nx = torch.linspace(-math.pi, math.pi, 2000)\ny = torch.sin(x)\n\n# For this example, the output y is a linear function of (x, x^2, x^3), so\n# we can consider it as a linear layer neural network. Let's prepare the\n# tensor (x, x^2, x^3).\np = torch.tensor([1, 2, 3])\nxx = x.unsqueeze(-1).pow(p)\n\n# In the above code, x.unsqueeze(-1) has shape (2000, 1), and p has shape\n# (3,), for this case, broadcasting semantics will apply to obtain a tensor\n# of shape (2000, 3) \n\n# Use the nn package to define our model as a sequence of layers. nn.Sequential\n# is a Module which contains other Modules, and applies them in sequence to\n# produce its output. The Linear Module computes output from input using a\n# linear function, and holds internal Tensors for its weight and bias.\n# The Flatten layer flatens the output of the linear layer to a 1D tensor,\n# to match the shape of `y`.\nmodel = torch.nn.Sequential(\n    torch.nn.Linear(3, 1),\n    torch.nn.Flatten(0, 1)\n)\n\n# The nn package also contains definitions of popular loss functions; in this\n# case we will use Mean Squared Error (MSE) as our loss function.\nloss_fn = torch.nn.MSELoss(reduction='sum')\n\nlearning_rate = 1e-6\nfor t in range(2000):\n\n    # Forward pass: compute predicted y by passing x to the model. Module objects\n    # override the __call__ operator so you can call them like functions. When\n    # doing so you pass a Tensor of input data to the Module and it produces\n    # a Tensor of output data.\n    y_pred = model(xx)\n\n    # Compute and print loss. We pass Tensors containing the predicted and true\n    # values of y, and the loss function returns a Tensor containing the\n    # loss.\n    loss = loss_fn(y_pred, y)\n    if t % 100 == 99:\n        print(t, loss.item())\n\n    # Zero the gradients before running the backward pass.\n    model.zero_grad()\n\n    # Backward pass: compute gradient of the loss with respect to all the learnable\n    # parameters of the model. Internally, the parameters of each Module are stored\n    # in Tensors with requires_grad=True, so this call will compute gradients for\n    # all learnable parameters in the model.\n    loss.backward()\n\n    # Update the weights using gradient descent. Each parameter is a Tensor, so\n    # we can access its gradients like we did before.\n    with torch.no_grad():\n        for param in model.parameters():\n            param -= learning_rate * param.grad\n\n# You can access the first layer of `model` like accessing the first item of a list\nlinear_layer = model[0]\n\n# For linear layer, its parameters are stored as `weight` and `bias`.\nprint(f'Result: y = {linear_layer.bias.item()} + {linear_layer.weight[:, 0].item()} x + {linear_layer.weight[:, 1].item()} x^2 + {linear_layer.weight[:, 2].item()} x^3')",
      "# -*- coding: utf-8 -*-\nimport torch\nimport math\n\n\n# Create Tensors to hold input and outputs.\nx = torch.linspace(-math.pi, math.pi, 2000)\ny = torch.sin(x)\n\n# Prepare the input tensor (x, x^2, x^3).\np = torch.tensor([1, 2, 3])\nxx = x.unsqueeze(-1).pow(p)\n\n# Use the nn package to define our model and loss function.\nmodel = torch.nn.Sequential(\n    torch.nn.Linear(3, 1),\n    torch.nn.Flatten(0, 1)\n)\nloss_fn = torch.nn.MSELoss(reduction='sum')\n\n# Use the optim package to define an Optimizer that will update the weights of\n# the model for us. Here we will use RMSprop; the optim package contains many other\n# optimization algorithms. The first argument to the RMSprop constructor tells the\n# optimizer which Tensors it should update.\nlearning_rate = 1e-3\noptimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\nfor t in range(2000):\n    # Forward pass: compute predicted y by passing x to the model.\n    y_pred = model(xx)\n\n    # Compute and print loss.\n    loss = loss_fn(y_pred, y)\n    if t % 100 == 99:\n        print(t, loss.item())\n\n    # Before the backward pass, use the optimizer object to zero all of the\n    # gradients for the variables it will update (which are the learnable\n    # weights of the model). This is because by default, gradients are\n    # accumulated in buffers( i.e, not overwritten) whenever .backward()\n    # is called. Checkout docs of torch.autograd.backward for more details.\n    optimizer.zero_grad()\n\n    # Backward pass: compute gradient of the loss with respect to model\n    # parameters\n    loss.backward()\n\n    # Calling the step function on an Optimizer makes an update to its\n    # parameters\n    optimizer.step()\n\n\nlinear_layer = model[0]\nprint(f'Result: y = {linear_layer.bias.item()} + {linear_layer.weight[:, 0].item()} x + {linear_layer.weight[:, 1].item()} x^2 + {linear_layer.weight[:, 2].item()} x^3')",
      "# -*- coding: utf-8 -*-\nimport torch\nimport math\n\n\nclass Polynomial3(torch.nn.Module):\n    def __init__(self):\n        \"\"\"\n        In the constructor we instantiate four parameters and assign them as\n        member parameters.\n        \"\"\"\n        super().__init__()\n        self.a = torch.nn.Parameter(torch.randn(()))\n        self.b = torch.nn.Parameter(torch.randn(()))\n        self.c = torch.nn.Parameter(torch.randn(()))\n        self.d = torch.nn.Parameter(torch.randn(()))\n\n    def forward(self, x):\n        \"\"\"\n        In the forward function we accept a Tensor of input data and we must return\n        a Tensor of output data. We can use Modules defined in the constructor as\n        well as arbitrary operators on Tensors.\n        \"\"\"\n        return self.a + self.b * x + self.c * x ** 2 + self.d * x ** 3\n\n    def string(self):\n        \"\"\"\n        Just like any class in Python, you can also define custom method on PyTorch modules\n        \"\"\"\n        return f'y = {self.a.item()} + {self.b.item()} x + {self.c.item()} x^2 + {self.d.item()} x^3'\n\n\n# Create Tensors to hold input and outputs.\nx = torch.linspace(-math.pi, math.pi, 2000)\ny = torch.sin(x)\n\n# Construct our model by instantiating the class defined above\nmodel = Polynomial3()\n\n# Construct our loss function and an Optimizer. The call to model.parameters()\n# in the SGD constructor will contain the learnable parameters (defined \n# with torch.nn.Parameter) which are members of the model.\ncriterion = torch.nn.MSELoss(reduction='sum')\noptimizer = torch.optim.SGD(model.parameters(), lr=1e-6)\nfor t in range(2000):\n    # Forward pass: Compute predicted y by passing x to the model\n    y_pred = model(x)\n\n    # Compute and print loss\n    loss = criterion(y_pred, y)\n    if t % 100 == 99:\n        print(t, loss.item())\n\n    # Zero gradients, perform a backward pass, and update the weights.\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\nprint(f'Result: {model.string()}')",
      "# -*- coding: utf-8 -*-\nimport random\nimport torch\nimport math\n\n\nclass DynamicNet(torch.nn.Module):\n    def __init__(self):\n        \"\"\"\n        In the constructor we instantiate five parameters and assign them as members.\n        \"\"\"\n        super().__init__()\n        self.a = torch.nn.Parameter(torch.randn(()))\n        self.b = torch.nn.Parameter(torch.randn(()))\n        self.c = torch.nn.Parameter(torch.randn(()))\n        self.d = torch.nn.Parameter(torch.randn(()))\n        self.e = torch.nn.Parameter(torch.randn(()))\n\n    def forward(self, x):\n        \"\"\"\n        For the forward pass of the model, we randomly choose either 4, 5\n        and reuse the e parameter to compute the contribution of these orders.\n\n        Since each forward pass builds a dynamic computation graph, we can use normal\n        Python control-flow operators like loops or conditional statements when\n        defining the forward pass of the model.\n\n        Here we also see that it is perfectly safe to reuse the same parameter many\n        times when defining a computational graph.\n        \"\"\"\n        y = self.a + self.b * x + self.c * x ** 2 + self.d * x ** 3\n        for exp in range(4, random.randint(4, 6)):\n            y = y + self.e * x ** exp\n        return y\n\n    def string(self):\n        \"\"\"\n        Just like any class in Python, you can also define custom method on PyTorch modules\n        \"\"\"\n        return f'y = {self.a.item()} + {self.b.item()} x + {self.c.item()} x^2 + {self.d.item()} x^3 + {self.e.item()} x^4 ? + {self.e.item()} x^5 ?'\n\n\n# Create Tensors to hold input and outputs.\nx = torch.linspace(-math.pi, math.pi, 2000)\ny = torch.sin(x)\n\n# Construct our model by instantiating the class defined above\nmodel = DynamicNet()\n\n# Construct our loss function and an Optimizer. Training this strange model with\n# vanilla stochastic gradient descent is tough, so we use momentum\ncriterion = torch.nn.MSELoss(reduction='sum')\noptimizer = torch.optim.SGD(model.parameters(), lr=1e-8, momentum=0.9)\nfor t in range(30000):\n    # Forward pass: Compute predicted y by passing x to the model\n    y_pred = model(x)\n\n    # Compute and print loss\n    loss = criterion(y_pred, y)\n    if t % 2000 == 1999:\n        print(t, loss.item())\n\n    # Zero gradients, perform a backward pass, and update the weights.\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\nprint(f'Result: {model.string()}')"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/beginner/hta_intro_tutorial.html",
    "title": "Introduction to Holistic Trace Analysis\u00b6",
    "code_snippets": [
      "from hta.trace_analysis import TraceAnalysis\ntrace_dir = \"/path/to/folder/with/traces\"\nanalyzer = TraceAnalysis(trace_dir=trace_dir)"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html",
    "title": "Hyperparameter tuning with Ray Tune\u00b6",
    "code_snippets": [
      "from functools import partial\nimport os\nimport tempfile\nfrom pathlib import Path\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import random_split\nimport torchvision\nimport torchvision.transforms as transforms\nfrom ray import tune\nfrom ray import train\nfrom ray.train import Checkpoint, get_checkpoint\nfrom ray.tune.schedulers import ASHAScheduler\nimport ray.cloudpickle as pickle",
      "def load_data(data_dir=\"./data\"):\n    transform = transforms.Compose(\n        [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n    )\n\n    trainset = torchvision.datasets.CIFAR10(\n        root=data_dir, train=True, download=True, transform=transform\n    )\n\n    testset = torchvision.datasets.CIFAR10(\n        root=data_dir, train=False, download=True, transform=transform\n    )\n\n    return trainset, testset",
      "class Net(nn.Module):\n    def __init__(self, l1=120, l2=84):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(3, 6, 5)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1 = nn.Linear(16 * 5 * 5, l1)\n        self.fc2 = nn.Linear(l1, l2)\n        self.fc3 = nn.Linear(l2, 10)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = torch.flatten(x, 1)  # flatten all dimensions except batch\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x",
      "device = \"cpu\"\nif torch.cuda.is_available():\n    device = \"cuda:0\"\n    if torch.cuda.device_count() > 1:\n        net = nn.DataParallel(net)\nnet.to(device)",
      "def train_cifar(config, data_dir=None):\n    net = Net(config[\"l1\"], config[\"l2\"])\n\n    device = \"cpu\"\n    if torch.cuda.is_available():\n        device = \"cuda:0\"\n        if torch.cuda.device_count() > 1:\n            net = nn.DataParallel(net)\n    net.to(device)\n\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.SGD(net.parameters(), lr=config[\"lr\"], momentum=0.9)\n\n    checkpoint = get_checkpoint()\n    if checkpoint:\n        with checkpoint.as_directory() as checkpoint_dir:\n            data_path = Path(checkpoint_dir) / \"data.pkl\"\n            with open(data_path, \"rb\") as fp:\n                checkpoint_state = pickle.load(fp)\n            start_epoch = checkpoint_state[\"epoch\"]\n            net.load_state_dict(checkpoint_state[\"net_state_dict\"])\n            optimizer.load_state_dict(checkpoint_state[\"optimizer_state_dict\"])\n    else:\n        start_epoch = 0\n\n    trainset, testset = load_data(data_dir)\n\n    test_abs = int(len(trainset) * 0.8)\n    train_subset, val_subset = random_split(\n        trainset, [test_abs, len(trainset) - test_abs]\n    )\n\n    trainloader = torch.utils.data.DataLoader(\n        train_subset, batch_size=int(config[\"batch_size\"]), shuffle=True, num_workers=8\n    )\n    valloader = torch.utils.data.DataLoader(\n        val_subset, batch_size=int(config[\"batch_size\"]), shuffle=True, num_workers=8\n    )\n\n    for epoch in range(start_epoch, 10):  # loop over the dataset multiple times\n        running_loss = 0.0\n        epoch_steps = 0\n        for i, data in enumerate(trainloader, 0):\n            # get the inputs; data is a list of [inputs, labels]\n            inputs, labels = data\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            # zero the parameter gradients\n            optimizer.zero_grad()\n\n            # forward + backward + optimize\n            outputs = net(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            # print statistics\n            running_loss += loss.item()\n            epoch_steps += 1\n            if i % 2000 == 1999:  # print every 2000 mini-batches\n                print(\n                    \"[%d, %5d] loss: %.3f\"\n                    % (epoch + 1, i + 1, running_loss / epoch_steps)\n                )\n                running_loss = 0.0\n\n        # Validation loss\n        val_loss = 0.0\n        val_steps = 0\n        total = 0\n        correct = 0\n        for i, data in enumerate(valloader, 0):\n            with torch.no_grad():\n                inputs, labels = data\n                inputs, labels = inputs.to(device), labels.to(device)\n\n                outputs = net(inputs)\n                _, predicted = torch.max(outputs.data, 1)\n                total += labels.size(0)\n                correct += (predicted == labels).sum().item()\n\n                loss = criterion(outputs, labels)\n                val_loss += loss.cpu().numpy()\n                val_steps += 1\n\n        checkpoint_data = {\n            \"epoch\": epoch,\n            \"net_state_dict\": net.state_dict(),\n            \"optimizer_state_dict\": optimizer.state_dict(),\n        }\n        with tempfile.TemporaryDirectory() as checkpoint_dir:\n            data_path = Path(checkpoint_dir) / \"data.pkl\"\n            with open(data_path, \"wb\") as fp:\n                pickle.dump(checkpoint_data, fp)\n\n            checkpoint = Checkpoint.from_directory(checkpoint_dir)\n            train.report(\n                {\"loss\": val_loss / val_steps, \"accuracy\": correct / total},\n                checkpoint=checkpoint,\n            )\n\n    print(\"Finished Training\")",
      "def test_accuracy(net, device=\"cpu\"):\n    trainset, testset = load_data()\n\n    testloader = torch.utils.data.DataLoader(\n        testset, batch_size=4, shuffle=False, num_workers=2\n    )\n\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for data in testloader:\n            images, labels = data\n            images, labels = images.to(device), labels.to(device)\n            outputs = net(images)\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n    return correct / total",
      "def main(num_samples=10, max_num_epochs=10, gpus_per_trial=2):\n    data_dir = os.path.abspath(\"./data\")\n    load_data(data_dir)\n    config = {\n        \"l1\": tune.choice([2**i for i in range(9)]),\n        \"l2\": tune.choice([2**i for i in range(9)]),\n        \"lr\": tune.loguniform(1e-4, 1e-1),\n        \"batch_size\": tune.choice([2, 4, 8, 16]),\n    }\n    scheduler = ASHAScheduler(\n        metric=\"loss\",\n        mode=\"min\",\n        max_t=max_num_epochs,\n        grace_period=1,\n        reduction_factor=2,\n    )\n    result = tune.run(\n        partial(train_cifar, data_dir=data_dir),\n        resources_per_trial={\"cpu\": 2, \"gpu\": gpus_per_trial},\n        config=config,\n        num_samples=num_samples,\n        scheduler=scheduler,\n    )\n\n    best_trial = result.get_best_trial(\"loss\", \"min\", \"last\")\n    print(f\"Best trial config: {best_trial.config}\")\n    print(f\"Best trial final validation loss: {best_trial.last_result['loss']}\")\n    print(f\"Best trial final validation accuracy: {best_trial.last_result['accuracy']}\")\n\n    best_trained_model = Net(best_trial.config[\"l1\"], best_trial.config[\"l2\"])\n    device = \"cpu\"\n    if torch.cuda.is_available():\n        device = \"cuda:0\"\n        if gpus_per_trial > 1:\n            best_trained_model = nn.DataParallel(best_trained_model)\n    best_trained_model.to(device)\n\n    best_checkpoint = result.get_best_checkpoint(trial=best_trial, metric=\"accuracy\", mode=\"max\")\n    with best_checkpoint.as_directory() as checkpoint_dir:\n        data_path = Path(checkpoint_dir) / \"data.pkl\"\n        with open(data_path, \"rb\") as fp:\n            best_checkpoint_data = pickle.load(fp)\n\n        best_trained_model.load_state_dict(best_checkpoint_data[\"net_state_dict\"])\n        test_acc = test_accuracy(best_trained_model, device)\n        print(\"Best trial test set accuracy: {}\".format(test_acc))\n\n\nif __name__ == \"__main__\":\n    # You can change the number of GPUs per trial here:\n    main(num_samples=10, max_num_epochs=10, gpus_per_trial=0)",
      "0% 0.00/170M [00:00<?, ?B/s]\n  0% 459k/170M [00:00<00:37, 4.59MB/s]\n  4% 6.75M/170M [00:00<00:04, 38.9MB/s]\n  9% 15.5M/170M [00:00<00:02, 61.0MB/s]\n 15% 25.8M/170M [00:00<00:01, 77.3MB/s]\n 20% 34.2M/170M [00:00<00:01, 80.0MB/s]\n 26% 44.8M/170M [00:00<00:01, 88.6MB/s]\n 31% 53.7M/170M [00:00<00:01, 86.3MB/s]\n 38% 64.5M/170M [00:00<00:01, 93.0MB/s]\n 43% 73.8M/170M [00:00<00:01, 89.9MB/s]\n 50% 84.8M/170M [00:01<00:00, 95.9MB/s]\n 55% 94.5M/170M [00:01<00:00, 92.2MB/s]\n 62% 105M/170M [00:01<00:00, 97.2MB/s]\n 68% 115M/170M [00:01<00:00, 93.0MB/s]\n 74% 126M/170M [00:01<00:00, 97.4MB/s]\n 80% 136M/170M [00:01<00:00, 92.9MB/s]\n 86% 147M/170M [00:01<00:00, 97.7MB/s]\n 92% 157M/170M [00:01<00:00, 93.4MB/s]\n 98% 167M/170M [00:01<00:00, 96.9MB/s]\n100% 170M/170M [00:01<00:00, 88.5MB/s]\n2025-05-23 19:40:52,907 WARNING services.py:1889 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 2147467264 bytes available. This will harm performance! You may be able to free up space by deleting files in /dev/shm. If you are inside a Docker container, you can increase /dev/shm size by passing '--shm-size=10.24gb' to 'docker run' (or add it to the run_options list in a Ray cluster config). Make sure to set this to more than 30% of available RAM.\n2025-05-23 19:40:52,963 INFO worker.py:1642 -- Started a local Ray instance.\n2025-05-23 19:40:53,863 INFO tune.py:228 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `tune.run(...)`.\n2025-05-23 19:40:53,864 INFO tune.py:654 -- [output] This will use the new output engine with verbosity 2. To disable the new output and use the legacy output engine, set the environment variable RAY_AIR_NEW_OUTPUT=0. For more information, please see https://github.com/ray-project/ray/issues/36949\n+--------------------------------------------------------------------+\n| Configuration for experiment     train_cifar_2025-05-23_19-40-53   |\n+--------------------------------------------------------------------+\n| Search algorithm                 BasicVariantGenerator             |\n| Scheduler                        AsyncHyperBandScheduler           |\n| Number of trials                 10                                |\n+--------------------------------------------------------------------+\n\nView detailed results here: /var/lib/ci-user/ray_results/train_cifar_2025-05-23_19-40-53\nTo visualize your results with TensorBoard, run: `tensorboard --logdir /var/lib/ci-user/ray_results/train_cifar_2025-05-23_19-40-53`\n\nTrial status: 10 PENDING\nCurrent time: 2025-05-23 19:40:54. Total running time: 0s\nLogical resource usage: 14.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:A10G)\n+-------------------------------------------------------------------------------+\n| Trial name                status       l1     l2            lr     batch_size |\n+-------------------------------------------------------------------------------+\n| train_cifar_d6d9a_00000   PENDING       8     32   0.0665564                2 |\n| train_cifar_d6d9a_00001   PENDING      32    256   0.00750825               8 |\n| train_cifar_d6d9a_00002   PENDING       1      4   0.0100663                4 |\n| train_cifar_d6d9a_00003   PENDING     256      8   0.0262429               16 |\n| train_cifar_d6d9a_00004   PENDING      16     32   0.000239293              2 |\n| train_cifar_d6d9a_00005   PENDING      16     64   0.000370892             16 |\n| train_cifar_d6d9a_00006   PENDING      16      1   0.000174684             16 |\n| train_cifar_d6d9a_00007   PENDING      32     32   0.0208137                8 |\n| train_cifar_d6d9a_00008   PENDING       8      4   0.0474427                8 |\n| train_cifar_d6d9a_00009   PENDING     256     32   0.000463253             16 |\n+-------------------------------------------------------------------------------+\n\nTrial train_cifar_d6d9a_00002 started with configuration:\n+--------------------------------------------------+\n| Trial train_cifar_d6d9a_00002 config             |\n+--------------------------------------------------+\n| batch_size                                     4 |\n| l1                                             1 |\n| l2                                             4 |\n| lr                                       0.01007 |\n+--------------------------------------------------+\n\nTrial train_cifar_d6d9a_00005 started with configuration:\n+--------------------------------------------------+\n| Trial train_cifar_d6d9a_00005 config             |\n+--------------------------------------------------+\n| batch_size                                    16 |\n| l1                                            16 |\n| l2                                            64 |\n| lr                                       0.00037 |\n+--------------------------------------------------+\n\nTrial train_cifar_d6d9a_00004 started with configuration:\n+--------------------------------------------------+\n| Trial train_cifar_d6d9a_00004 config             |\n+--------------------------------------------------+\n| batch_size                                     2 |\n| l1                                            16 |\n| l2                                            32 |\n| lr                                       0.00024 |\n+--------------------------------------------------+\n\nTrial train_cifar_d6d9a_00007 started with configuration:\n+--------------------------------------------------+\n| Trial train_cifar_d6d9a_00007 config             |\n+--------------------------------------------------+\n| batch_size                                     8 |\n| l1                                            32 |\n| l2                                            32 |\n| lr                                       0.02081 |\n+--------------------------------------------------+\n\nTrial train_cifar_d6d9a_00001 started with configuration:\n+--------------------------------------------------+\n| Trial train_cifar_d6d9a_00001 config             |\n+--------------------------------------------------+\n| batch_size                                     8 |\n| l1                                            32 |\n| l2                                           256 |\n| lr                                       0.00751 |\n+--------------------------------------------------+\n\nTrial train_cifar_d6d9a_00000 started with configuration:\n+--------------------------------------------------+\n| Trial train_cifar_d6d9a_00000 config             |\n+--------------------------------------------------+\n| batch_size                                     2 |\n| l1                                             8 |\n| l2                                            32 |\n| lr                                       0.06656 |\n+--------------------------------------------------+\n\nTrial train_cifar_d6d9a_00006 started with configuration:\n+--------------------------------------------------+\n| Trial train_cifar_d6d9a_00006 config             |\n+--------------------------------------------------+\n| batch_size                                    16 |\n| l1                                            16 |\n| l2                                             1 |\n| lr                                       0.00017 |\n+--------------------------------------------------+\n\nTrial train_cifar_d6d9a_00003 started with configuration:\n+--------------------------------------------------+\n| Trial train_cifar_d6d9a_00003 config             |\n+--------------------------------------------------+\n| batch_size                                    16 |\n| l1                                           256 |\n| l2                                             8 |\n| lr                                       0.02624 |\n+--------------------------------------------------+\n(func pid=4878) [1,  2000] loss: 2.302\n\nTrial status: 8 RUNNING | 2 PENDING\nCurrent time: 2025-05-23 19:41:24. Total running time: 30s\nLogical resource usage: 16.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:A10G)\n+-------------------------------------------------------------------------------+\n| Trial name                status       l1     l2            lr     batch_size |\n+-------------------------------------------------------------------------------+\n| train_cifar_d6d9a_00000   RUNNING       8     32   0.0665564                2 |\n| train_cifar_d6d9a_00001   RUNNING      32    256   0.00750825               8 |\n| train_cifar_d6d9a_00002   RUNNING       1      4   0.0100663                4 |\n| train_cifar_d6d9a_00003   RUNNING     256      8   0.0262429               16 |\n| train_cifar_d6d9a_00004   RUNNING      16     32   0.000239293              2 |\n| train_cifar_d6d9a_00005   RUNNING      16     64   0.000370892             16 |\n| train_cifar_d6d9a_00006   RUNNING      16      1   0.000174684             16 |\n| train_cifar_d6d9a_00007   RUNNING      32     32   0.0208137                8 |\n| train_cifar_d6d9a_00008   PENDING       8      4   0.0474427                8 |\n| train_cifar_d6d9a_00009   PENDING     256     32   0.000463253             16 |\n+-------------------------------------------------------------------------------+\n\nTrial train_cifar_d6d9a_00005 finished iteration 1 at 2025-05-23 19:41:24. Total running time: 30s\n+------------------------------------------------------------+\n| Trial train_cifar_d6d9a_00005 result                       |\n+------------------------------------------------------------+\n| checkpoint_dir_name                      checkpoint_000000 |\n| time_this_iter_s                                   26.0594 |\n| time_total_s                                       26.0594 |\n| training_iteration                                       1 |\n| accuracy                                            0.1313 |\n| loss                                               2.29703 |\n+------------------------------------------------------------+\nTrial train_cifar_d6d9a_00005 saved a checkpoint for iteration 1 at: (local)/var/lib/ci-user/ray_results/train_cifar_2025-05-23_19-40-53/train_cifar_d6d9a_00005_5_batch_size=16,l1=16,l2=64,lr=0.0004_2025-05-23_19-40-54/checkpoint_000000\n(func pid=4879) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2025-05-23_19-40-53/train_cifar_d6d9a_00005_5_batch_size=16,l1=16,l2=64,lr=0.0004_2025-05-23_19-40-54/checkpoint_000000)\n\nTrial train_cifar_d6d9a_00006 finished iteration 1 at 2025-05-23 19:41:24. Total running time: 30s\n+------------------------------------------------------------+\n| Trial train_cifar_d6d9a_00006 result                       |\n+------------------------------------------------------------+\n| checkpoint_dir_name                      checkpoint_000000 |\n| time_this_iter_s                                  25.92175 |\n| time_total_s                                      25.92175 |\n| training_iteration                                       1 |\n| accuracy                                            0.1037 |\n| loss                                                2.3221 |\n+------------------------------------------------------------+\n(func pid=4871) [1,  4000] loss: 1.191 [repeated 8x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\nTrial train_cifar_d6d9a_00006 saved a checkpoint for iteration 1 at: (local)/var/lib/ci-user/ray_results/train_cifar_2025-05-23_19-40-53/train_cifar_d6d9a_00006_6_batch_size=16,l1=16,l2=1,lr=0.0002_2025-05-23_19-40-54/checkpoint_000000\n\nTrial train_cifar_d6d9a_00006 completed after 1 iterations at 2025-05-23 19:41:24. Total running time: 30s\n\nTrial train_cifar_d6d9a_00008 started with configuration:\n+--------------------------------------------------+\n| Trial train_cifar_d6d9a_00008 config             |\n+--------------------------------------------------+\n| batch_size                                     8 |\n| l1                                             8 |\n| l2                                             4 |\n| lr                                       0.04744 |\n+--------------------------------------------------+\n\nTrial train_cifar_d6d9a_00003 finished iteration 1 at 2025-05-23 19:41:25. Total running time: 31s\n+------------------------------------------------------------+\n| Trial train_cifar_d6d9a_00003 result                       |\n+------------------------------------------------------------+\n| checkpoint_dir_name                      checkpoint_000000 |\n| time_this_iter_s                                   26.9541 |\n| time_total_s                                       26.9541 |\n| training_iteration                                       1 |\n| accuracy                                            0.3328 |\n| loss                                               1.87587 |\n+------------------------------------------------------------+\nTrial train_cifar_d6d9a_00003 saved a checkpoint for iteration 1 at: (local)/var/lib/ci-user/ray_results/train_cifar_2025-05-23_19-40-53/train_cifar_d6d9a_00003_3_batch_size=16,l1=256,l2=8,lr=0.0262_2025-05-23_19-40-54/checkpoint_000000\n(func pid=4878) [1,  6000] loss: 0.683 [repeated 5x across cluster]\n(func pid=4880) [2,  2000] loss: 1.773 [repeated 5x across cluster]\n\nTrial train_cifar_d6d9a_00001 finished iteration 1 at 2025-05-23 19:41:41. Total running time: 47s\n+------------------------------------------------------------+\n| Trial train_cifar_d6d9a_00001 result                       |\n+------------------------------------------------------------+\n| checkpoint_dir_name                      checkpoint_000000 |\n| time_this_iter_s                                  43.16045 |\n| time_total_s                                      43.16045 |\n| training_iteration                                       1 |\n| accuracy                                            0.4432 |\n| loss                                               1.56667 |\n+------------------------------------------------------------+\nTrial train_cifar_d6d9a_00001 saved a checkpoint for iteration 1 at: (local)/var/lib/ci-user/ray_results/train_cifar_2025-05-23_19-40-53/train_cifar_d6d9a_00001_1_batch_size=8,l1=32,l2=256,lr=0.0075_2025-05-23_19-40-54/checkpoint_000000\n(func pid=4876) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2025-05-23_19-40-53/train_cifar_d6d9a_00001_1_batch_size=8,l1=32,l2=256,lr=0.0075_2025-05-23_19-40-54/checkpoint_000000) [repeated 3x across cluster]\n\nTrial train_cifar_d6d9a_00007 finished iteration 1 at 2025-05-23 19:41:42. Total running time: 48s\n+------------------------------------------------------------+\n| Trial train_cifar_d6d9a_00007 result                       |\n+------------------------------------------------------------+\n| checkpoint_dir_name                      checkpoint_000000 |\n| time_this_iter_s                                  43.64902 |\n| time_total_s                                      43.64902 |\n| training_iteration                                       1 |\n| accuracy                                            0.2248 |\n| loss                                               2.00235 |\n+------------------------------------------------------------+\nTrial train_cifar_d6d9a_00007 saved a checkpoint for iteration 1 at: (local)/var/lib/ci-user/ray_results/train_cifar_2025-05-23_19-40-53/train_cifar_d6d9a_00007_7_batch_size=8,l1=32,l2=32,lr=0.0208_2025-05-23_19-40-54/checkpoint_000000\n(func pid=4877) [1,  8000] loss: 0.577 [repeated 3x across cluster]\n\nTrial train_cifar_d6d9a_00005 finished iteration 2 at 2025-05-23 19:41:47. Total running time: 53s\n+------------------------------------------------------------+\n| Trial train_cifar_d6d9a_00005 result                       |\n+------------------------------------------------------------+\n| checkpoint_dir_name                      checkpoint_000001 |\n| time_this_iter_s                                  23.19744 |\n| time_total_s                                      49.25684 |\n| training_iteration                                       2 |\n| accuracy                                            0.1953 |\n| loss                                               2.17434 |\n+------------------------------------------------------------+\nTrial train_cifar_d6d9a_00005 saved a checkpoint for iteration 2 at: (local)/var/lib/ci-user/ray_results/train_cifar_2025-05-23_19-40-53/train_cifar_d6d9a_00005_5_batch_size=16,l1=16,l2=64,lr=0.0004_2025-05-23_19-40-54/checkpoint_000001\n(func pid=4879) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2025-05-23_19-40-53/train_cifar_d6d9a_00005_5_batch_size=16,l1=16,l2=64,lr=0.0004_2025-05-23_19-40-54/checkpoint_000001) [repeated 2x across cluster]\n\nTrial train_cifar_d6d9a_00003 finished iteration 2 at 2025-05-23 19:41:48. Total running time: 54s\n+------------------------------------------------------------+\n| Trial train_cifar_d6d9a_00003 result                       |\n+------------------------------------------------------------+\n| checkpoint_dir_name                      checkpoint_000001 |\n| time_this_iter_s                                  22.75919 |\n| time_total_s                                      49.71329 |\n| training_iteration                                       2 |\n| accuracy                                            0.3274 |\n| loss                                                1.8141 |\n+------------------------------------------------------------+\nTrial train_cifar_d6d9a_00003 saved a checkpoint for iteration 2 at: (local)/var/lib/ci-user/ray_results/train_cifar_2025-05-23_19-40-53/train_cifar_d6d9a_00003_3_batch_size=16,l1=256,l2=8,lr=0.0262_2025-05-23_19-40-54/checkpoint_000001\n(func pid=4881) [1,  4000] loss: 1.158\n\nTrial status: 8 RUNNING | 1 TERMINATED | 1 PENDING\nCurrent time: 2025-05-23 19:41:54. Total running time: 1min 0s\nLogical resource usage: 16.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:A10G)\n+------------------------------------------------------------------------------------------------------------------------------------+\n| Trial name                status         l1     l2            lr     batch_size     iter     total time (s)      loss     accuracy |\n+------------------------------------------------------------------------------------------------------------------------------------+\n| train_cifar_d6d9a_00000   RUNNING         8     32   0.0665564                2                                                    |\n| train_cifar_d6d9a_00001   RUNNING        32    256   0.00750825               8        1            43.1605   1.56667       0.4432 |\n| train_cifar_d6d9a_00002   RUNNING         1      4   0.0100663                4                                                    |\n| train_cifar_d6d9a_00003   RUNNING       256      8   0.0262429               16        2            49.7133   1.8141        0.3274 |\n| train_cifar_d6d9a_00004   RUNNING        16     32   0.000239293              2                                                    |\n| train_cifar_d6d9a_00005   RUNNING        16     64   0.000370892             16        2            49.2568   2.17434       0.1953 |\n| train_cifar_d6d9a_00007   RUNNING        32     32   0.0208137                8        1            43.649    2.00235       0.2248 |\n| train_cifar_d6d9a_00008   RUNNING         8      4   0.0474427                8                                                    |\n| train_cifar_d6d9a_00006   TERMINATED     16      1   0.000174684             16        1            25.9218   2.3221        0.1037 |\n| train_cifar_d6d9a_00009   PENDING       256     32   0.000463253             16                                                    |\n+------------------------------------------------------------------------------------------------------------------------------------+\n(func pid=4878) [1, 10000] loss: 0.376\n(func pid=4879) [3,  2000] loss: 2.086 [repeated 5x across cluster]\n\nTrial train_cifar_d6d9a_00008 finished iteration 1 at 2025-05-23 19:42:07. Total running time: 1min 13s\n+------------------------------------------------------------+\n| Trial train_cifar_d6d9a_00008 result                       |\n+------------------------------------------------------------+\n| checkpoint_dir_name                      checkpoint_000000 |\n| time_this_iter_s                                  42.71643 |\n| time_total_s                                      42.71643 |\n| training_iteration                                       1 |\n| accuracy                                            0.0968 |\n| loss                                               2.32709 |\n+------------------------------------------------------------+\nTrial train_cifar_d6d9a_00008 saved a checkpoint for iteration 1 at: (local)/var/lib/ci-user/ray_results/train_cifar_2025-05-23_19-40-53/train_cifar_d6d9a_00008_8_batch_size=8,l1=8,l2=4,lr=0.0474_2025-05-23_19-40-54/checkpoint_000000\n\nTrial train_cifar_d6d9a_00008 completed after 1 iterations at 2025-05-23 19:42:07. Total running time: 1min 13s\n\nTrial train_cifar_d6d9a_00009 started with configuration:\n+--------------------------------------------------+\n| Trial train_cifar_d6d9a_00009 config             |\n+--------------------------------------------------+\n| batch_size                                    16 |\n| l1                                           256 |\n| l2                                            32 |\n| lr                                       0.00046 |\n+--------------------------------------------------+\n(func pid=4881) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2025-05-23_19-40-53/train_cifar_d6d9a_00008_8_batch_size=8,l1=8,l2=4,lr=0.0474_2025-05-23_19-40-54/checkpoint_000000) [repeated 2x across cluster]\n(func pid=4882) [2,  4000] loss: 1.066 [repeated 4x across cluster]\n\nTrial train_cifar_d6d9a_00005 finished iteration 3 at 2025-05-23 19:42:11. Total running time: 1min 17s\n+------------------------------------------------------------+\n| Trial train_cifar_d6d9a_00005 result                       |\n+------------------------------------------------------------+\n| checkpoint_dir_name                      checkpoint_000002 |\n| time_this_iter_s                                  23.54535 |\n| time_total_s                                      72.80219 |\n| training_iteration                                       3 |\n| accuracy                                            0.2759 |\n| loss                                               1.97072 |\n+------------------------------------------------------------+\nTrial train_cifar_d6d9a_00005 saved a checkpoint for iteration 3 at: (local)/var/lib/ci-user/ray_results/train_cifar_2025-05-23_19-40-53/train_cifar_d6d9a_00005_5_batch_size=16,l1=16,l2=64,lr=0.0004_2025-05-23_19-40-54/checkpoint_000002\n\nTrial train_cifar_d6d9a_00002 finished iteration 1 at 2025-05-23 19:42:11. Total running time: 1min 18s\n+------------------------------------------------------------+\n| Trial train_cifar_d6d9a_00002 result                       |\n+------------------------------------------------------------+\n| checkpoint_dir_name                      checkpoint_000000 |\n| time_this_iter_s                                  73.39485 |\n| time_total_s                                      73.39485 |\n| training_iteration                                       1 |\n| accuracy                                            0.1011 |\n| loss                                               2.31102 |\n+------------------------------------------------------------+\nTrial train_cifar_d6d9a_00002 saved a checkpoint for iteration 1 at: (local)/var/lib/ci-user/ray_results/train_cifar_2025-05-23_19-40-53/train_cifar_d6d9a_00002_2_batch_size=4,l1=1,l2=4,lr=0.0101_2025-05-23_19-40-54/checkpoint_000000\n\nTrial train_cifar_d6d9a_00002 completed after 1 iterations at 2025-05-23 19:42:12. Total running time: 1min 18s\n\nTrial train_cifar_d6d9a_00003 finished iteration 3 at 2025-05-23 19:42:13. Total running time: 1min 19s\n+------------------------------------------------------------+\n| Trial train_cifar_d6d9a_00003 result                       |\n+------------------------------------------------------------+\n| checkpoint_dir_name                      checkpoint_000002 |\n| time_this_iter_s                                  24.64976 |\n| time_total_s                                      74.36304 |\n| training_iteration                                       3 |\n| accuracy                                            0.3502 |\n| loss                                               1.83237 |\n+------------------------------------------------------------+\nTrial train_cifar_d6d9a_00003 saved a checkpoint for iteration 3 at: (local)/var/lib/ci-user/ray_results/train_cifar_2025-05-23_19-40-53/train_cifar_d6d9a_00003_3_batch_size=16,l1=256,l2=8,lr=0.0262_2025-05-23_19-40-54/checkpoint_000002\n(func pid=4880) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2025-05-23_19-40-53/train_cifar_d6d9a_00003_3_batch_size=16,l1=256,l2=8,lr=0.0262_2025-05-23_19-40-54/checkpoint_000002) [repeated 3x across cluster]\n(func pid=4878) [1, 14000] loss: 0.254 [repeated 2x across cluster]\n\nTrial train_cifar_d6d9a_00001 finished iteration 2 at 2025-05-23 19:42:21. Total running time: 1min 27s\n+------------------------------------------------------------+\n| Trial train_cifar_d6d9a_00001 result                       |\n+------------------------------------------------------------+\n| checkpoint_dir_name                      checkpoint_000001 |\n| time_this_iter_s                                  39.65988 |\n| time_total_s                                      82.82034 |\n| training_iteration                                       2 |\n| accuracy                                            0.4398 |\n| loss                                               1.58023 |\n+------------------------------------------------------------+\nTrial train_cifar_d6d9a_00001 saved a checkpoint for iteration 2 at: (local)/var/lib/ci-user/ray_results/train_cifar_2025-05-23_19-40-53/train_cifar_d6d9a_00001_1_batch_size=8,l1=32,l2=256,lr=0.0075_2025-05-23_19-40-54/checkpoint_000001\n(func pid=4876) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2025-05-23_19-40-53/train_cifar_d6d9a_00001_1_batch_size=8,l1=32,l2=256,lr=0.0075_2025-05-23_19-40-54/checkpoint_000001)\n\nTrial train_cifar_d6d9a_00007 finished iteration 2 at 2025-05-23 19:42:21. Total running time: 1min 27s\n+------------------------------------------------------------+\n| Trial train_cifar_d6d9a_00007 result                       |\n+------------------------------------------------------------+\n| checkpoint_dir_name                      checkpoint_000001 |\n| time_this_iter_s                                  39.34433 |\n| time_total_s                                      82.99335 |\n| training_iteration                                       2 |\n| accuracy                                            0.1919 |\n| loss                                               2.10149 |\n+------------------------------------------------------------+\nTrial train_cifar_d6d9a_00007 saved a checkpoint for iteration 2 at: (local)/var/lib/ci-user/ray_results/train_cifar_2025-05-23_19-40-53/train_cifar_d6d9a_00007_7_batch_size=8,l1=32,l2=32,lr=0.0208_2025-05-23_19-40-54/checkpoint_000001\n\nTrial train_cifar_d6d9a_00007 completed after 2 iterations at 2025-05-23 19:42:21. Total running time: 1min 27s\n(func pid=4882) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2025-05-23_19-40-53/train_cifar_d6d9a_00007_7_batch_size=8,l1=32,l2=32,lr=0.0208_2025-05-23_19-40-54/checkpoint_000001)\n(func pid=4881) [1,  2000] loss: 2.295 [repeated 2x across cluster]\n\nTrial status: 6 RUNNING | 4 TERMINATED\nCurrent time: 2025-05-23 19:42:24. Total running time: 1min 30s\nLogical resource usage: 12.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:A10G)\n+------------------------------------------------------------------------------------------------------------------------------------+\n| Trial name                status         l1     l2            lr     batch_size     iter     total time (s)      loss     accuracy |\n+------------------------------------------------------------------------------------------------------------------------------------+\n| train_cifar_d6d9a_00000   RUNNING         8     32   0.0665564                2                                                    |\n| train_cifar_d6d9a_00001   RUNNING        32    256   0.00750825               8        2            82.8203   1.58023       0.4398 |\n| train_cifar_d6d9a_00003   RUNNING       256      8   0.0262429               16        3            74.363    1.83237       0.3502 |\n| train_cifar_d6d9a_00004   RUNNING        16     32   0.000239293              2                                                    |\n| train_cifar_d6d9a_00005   RUNNING        16     64   0.000370892             16        3            72.8022   1.97072       0.2759 |\n| train_cifar_d6d9a_00009   RUNNING       256     32   0.000463253             16                                                    |\n| train_cifar_d6d9a_00002   TERMINATED      1      4   0.0100663                4        1            73.3948   2.31102       0.1011 |\n| train_cifar_d6d9a_00006   TERMINATED     16      1   0.000174684             16        1            25.9218   2.3221        0.1037 |\n| train_cifar_d6d9a_00007   TERMINATED     32     32   0.0208137                8        2            82.9934   2.10149       0.1919 |\n| train_cifar_d6d9a_00008   TERMINATED      8      4   0.0474427                8        1            42.7164   2.32709       0.0968 |\n+------------------------------------------------------------------------------------------------------------------------------------+\n\nTrial train_cifar_d6d9a_00009 finished iteration 1 at 2025-05-23 19:42:29. Total running time: 1min 35s\n+------------------------------------------------------------+\n| Trial train_cifar_d6d9a_00009 result                       |\n+------------------------------------------------------------+\n| checkpoint_dir_name                      checkpoint_000000 |\n| time_this_iter_s                                  22.12948 |\n| time_total_s                                      22.12948 |\n| training_iteration                                       1 |\n| accuracy                                            0.2146 |\n| loss                                               2.11946 |\n+------------------------------------------------------------+\nTrial train_cifar_d6d9a_00009 saved a checkpoint for iteration 1 at: (local)/var/lib/ci-user/ray_results/train_cifar_2025-05-23_19-40-53/train_cifar_d6d9a_00009_9_batch_size=16,l1=256,l2=32,lr=0.0005_2025-05-23_19-40-54/checkpoint_000000\n(func pid=4881) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2025-05-23_19-40-53/train_cifar_d6d9a_00009_9_batch_size=16,l1=256,l2=32,lr=0.0005_2025-05-23_19-40-54/checkpoint_000000)\n\nTrial train_cifar_d6d9a_00005 finished iteration 4 at 2025-05-23 19:42:30. Total running time: 1min 36s\n+------------------------------------------------------------+\n| Trial train_cifar_d6d9a_00005 result                       |\n+------------------------------------------------------------+\n| checkpoint_dir_name                      checkpoint_000003 |\n| time_this_iter_s                                   19.1335 |\n| time_total_s                                      91.93569 |\n| training_iteration                                       4 |\n| accuracy                                            0.3334 |\n| loss                                               1.77417 |\n+------------------------------------------------------------+\nTrial train_cifar_d6d9a_00005 saved a checkpoint for iteration 4 at: (local)/var/lib/ci-user/ray_results/train_cifar_2025-05-23_19-40-53/train_cifar_d6d9a_00005_5_batch_size=16,l1=16,l2=64,lr=0.0004_2025-05-23_19-40-54/checkpoint_000003\n(func pid=4879) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2025-05-23_19-40-53/train_cifar_d6d9a_00005_5_batch_size=16,l1=16,l2=64,lr=0.0004_2025-05-23_19-40-54/checkpoint_000003)\n(func pid=4876) [3,  2000] loss: 1.526 [repeated 5x across cluster]\n\nTrial train_cifar_d6d9a_00003 finished iteration 4 at 2025-05-23 19:42:33. Total running time: 1min 39s\n+------------------------------------------------------------+\n| Trial train_cifar_d6d9a_00003 result                       |\n+------------------------------------------------------------+\n| checkpoint_dir_name                      checkpoint_000003 |\n| time_this_iter_s                                  19.73991 |\n| time_total_s                                      94.10296 |\n| training_iteration                                       4 |\n| accuracy                                            0.3195 |\n| loss                                                1.8152 |\n+------------------------------------------------------------+\nTrial train_cifar_d6d9a_00003 saved a checkpoint for iteration 4 at: (local)/var/lib/ci-user/ray_results/train_cifar_2025-05-23_19-40-53/train_cifar_d6d9a_00003_3_batch_size=16,l1=256,l2=8,lr=0.0262_2025-05-23_19-40-54/checkpoint_000003\n\nTrial train_cifar_d6d9a_00003 completed after 4 iterations at 2025-05-23 19:42:33. Total running time: 1min 39s\n(func pid=4878) [1, 20000] loss: 0.162 [repeated 3x across cluster]\n\nTrial train_cifar_d6d9a_00005 finished iteration 5 at 2025-05-23 19:42:46. Total running time: 1min 52s\n+------------------------------------------------------------+\n| Trial train_cifar_d6d9a_00005 result                       |\n+------------------------------------------------------------+\n| checkpoint_dir_name                      checkpoint_000004 |\n| time_this_iter_s                                  16.05627 |\n| time_total_s                                     107.99196 |\n| training_iteration                                       5 |\n| accuracy                                            0.4038 |\n| loss                                               1.64771 |\n+------------------------------------------------------------+\nTrial train_cifar_d6d9a_00005 saved a checkpoint for iteration 5 at: (local)/var/lib/ci-user/ray_results/train_cifar_2025-05-23_19-40-53/train_cifar_d6d9a_00005_5_batch_size=16,l1=16,l2=64,lr=0.0004_2025-05-23_19-40-54/checkpoint_000004\n(func pid=4879) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2025-05-23_19-40-53/train_cifar_d6d9a_00005_5_batch_size=16,l1=16,l2=64,lr=0.0004_2025-05-23_19-40-54/checkpoint_000004) [repeated 2x across cluster]\n\nTrial train_cifar_d6d9a_00009 finished iteration 2 at 2025-05-23 19:42:46. Total running time: 1min 52s\n+------------------------------------------------------------+\n| Trial train_cifar_d6d9a_00009 result                       |\n+------------------------------------------------------------+\n| checkpoint_dir_name                      checkpoint_000001 |\n| time_this_iter_s                                  16.99345 |\n| time_total_s                                      39.12293 |\n| training_iteration                                       2 |\n| accuracy                                            0.3542 |\n| loss                                               1.77116 |\n+------------------------------------------------------------+\nTrial train_cifar_d6d9a_00009 saved a checkpoint for iteration 2 at: (local)/var/lib/ci-user/ray_results/train_cifar_2025-05-23_19-40-53/train_cifar_d6d9a_00009_9_batch_size=16,l1=256,l2=32,lr=0.0005_2025-05-23_19-40-54/checkpoint_000001\n\nTrial train_cifar_d6d9a_00001 finished iteration 3 at 2025-05-23 19:42:50. Total running time: 1min 56s\n+------------------------------------------------------------+\n| Trial train_cifar_d6d9a_00001 result                       |\n+------------------------------------------------------------+\n| checkpoint_dir_name                      checkpoint_000002 |\n| time_this_iter_s                                  29.14584 |\n| time_total_s                                     111.96617 |\n| training_iteration                                       3 |\n| accuracy                                            0.4634 |\n| loss                                                 1.512 |\n+------------------------------------------------------------+\nTrial train_cifar_d6d9a_00001 saved a checkpoint for iteration 3 at: (local)/var/lib/ci-user/ray_results/train_cifar_2025-05-23_19-40-53/train_cifar_d6d9a_00001_1_batch_size=8,l1=32,l2=256,lr=0.0075_2025-05-23_19-40-54/checkpoint_000002\n\nTrial status: 5 RUNNING | 5 TERMINATED\nCurrent time: 2025-05-23 19:42:54. Total running time: 2min 0s\nLogical resource usage: 10.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:A10G)\n+------------------------------------------------------------------------------------------------------------------------------------+\n| Trial name                status         l1     l2            lr     batch_size     iter     total time (s)      loss     accuracy |\n+------------------------------------------------------------------------------------------------------------------------------------+\n| train_cifar_d6d9a_00000   RUNNING         8     32   0.0665564                2                                                    |\n| train_cifar_d6d9a_00001   RUNNING        32    256   0.00750825               8        3           111.966    1.512         0.4634 |\n| train_cifar_d6d9a_00004   RUNNING        16     32   0.000239293              2                                                    |\n| train_cifar_d6d9a_00005   RUNNING        16     64   0.000370892             16        5           107.992    1.64771       0.4038 |\n| train_cifar_d6d9a_00009   RUNNING       256     32   0.000463253             16        2            39.1229   1.77116       0.3542 |\n| train_cifar_d6d9a_00002   TERMINATED      1      4   0.0100663                4        1            73.3948   2.31102       0.1011 |\n| train_cifar_d6d9a_00003   TERMINATED    256      8   0.0262429               16        4            94.103    1.8152        0.3195 |\n| train_cifar_d6d9a_00006   TERMINATED     16      1   0.000174684             16        1            25.9218   2.3221        0.1037 |\n| train_cifar_d6d9a_00007   TERMINATED     32     32   0.0208137                8        2            82.9934   2.10149       0.1919 |\n| train_cifar_d6d9a_00008   TERMINATED      8      4   0.0474427                8        1            42.7164   2.32709       0.0968 |\n+------------------------------------------------------------------------------------------------------------------------------------+\n\nTrial train_cifar_d6d9a_00004 finished iteration 1 at 2025-05-23 19:42:55. Total running time: 2min 1s\n+------------------------------------------------------------+\n| Trial train_cifar_d6d9a_00004 result                       |\n+------------------------------------------------------------+\n| checkpoint_dir_name                      checkpoint_000000 |\n| time_this_iter_s                                 116.90967 |\n| time_total_s                                     116.90967 |\n| training_iteration                                       1 |\n| accuracy                                            0.4057 |\n| loss                                               1.62473 |\n+------------------------------------------------------------+\nTrial train_cifar_d6d9a_00004 saved a checkpoint for iteration 1 at: (local)/var/lib/ci-user/ray_results/train_cifar_2025-05-23_19-40-53/train_cifar_d6d9a_00004_4_batch_size=2,l1=16,l2=32,lr=0.0002_2025-05-23_19-40-54/checkpoint_000000\n(func pid=4878) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2025-05-23_19-40-53/train_cifar_d6d9a_00004_4_batch_size=2,l1=16,l2=32,lr=0.0002_2025-05-23_19-40-54/checkpoint_000000) [repeated 3x across cluster]\n\nTrial train_cifar_d6d9a_00000 finished iteration 1 at 2025-05-23 19:42:56. Total running time: 2min 2s\n+------------------------------------------------------------+\n| Trial train_cifar_d6d9a_00000 result                       |\n+------------------------------------------------------------+\n| checkpoint_dir_name                      checkpoint_000000 |\n| time_this_iter_s                                 117.32424 |\n| time_total_s                                     117.32424 |\n| training_iteration                                       1 |\n| accuracy                                            0.0959 |\n| loss                                               2.42625 |\n+------------------------------------------------------------+\nTrial train_cifar_d6d9a_00000 saved a checkpoint for iteration 1 at: (local)/var/lib/ci-user/ray_results/train_cifar_2025-05-23_19-40-53/train_cifar_d6d9a_00000_0_batch_size=2,l1=8,l2=32,lr=0.0666_2025-05-23_19-40-53/checkpoint_000000\n\nTrial train_cifar_d6d9a_00000 completed after 1 iterations at 2025-05-23 19:42:56. Total running time: 2min 2s\n(func pid=4879) [6,  2000] loss: 1.615 [repeated 5x across cluster]\n\nTrial train_cifar_d6d9a_00005 finished iteration 6 at 2025-05-23 19:43:02. Total running time: 2min 8s\n+------------------------------------------------------------+\n| Trial train_cifar_d6d9a_00005 result                       |\n+------------------------------------------------------------+\n| checkpoint_dir_name                      checkpoint_000005 |\n| time_this_iter_s                                  15.51374 |\n| time_total_s                                      123.5057 |\n| training_iteration                                       6 |\n| accuracy                                            0.4216 |\n| loss                                               1.58856 |\n+------------------------------------------------------------+\nTrial train_cifar_d6d9a_00005 saved a checkpoint for iteration 6 at: (local)/var/lib/ci-user/ray_results/train_cifar_2025-05-23_19-40-53/train_cifar_d6d9a_00005_5_batch_size=16,l1=16,l2=64,lr=0.0004_2025-05-23_19-40-54/checkpoint_000005\n(func pid=4879) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2025-05-23_19-40-53/train_cifar_d6d9a_00005_5_batch_size=16,l1=16,l2=64,lr=0.0004_2025-05-23_19-40-54/checkpoint_000005) [repeated 2x across cluster]\n(func pid=4878) [2,  2000] loss: 1.599 [repeated 3x across cluster]\n\nTrial train_cifar_d6d9a_00009 finished iteration 3 at 2025-05-23 19:43:03. Total running time: 2min 9s\n+------------------------------------------------------------+\n| Trial train_cifar_d6d9a_00009 result                       |\n+------------------------------------------------------------+\n| checkpoint_dir_name                      checkpoint_000002 |\n| time_this_iter_s                                  16.61309 |\n| time_total_s                                      55.73601 |\n| training_iteration                                       3 |\n| accuracy                                            0.3981 |\n| loss                                               1.62714 |\n+------------------------------------------------------------+\nTrial train_cifar_d6d9a_00009 saved a checkpoint for iteration 3 at: (local)/var/lib/ci-user/ray_results/train_cifar_2025-05-23_19-40-53/train_cifar_d6d9a_00009_9_batch_size=16,l1=256,l2=32,lr=0.0005_2025-05-23_19-40-54/checkpoint_000002\n(func pid=4876) [4,  4000] loss: 0.758\n(func pid=4878) [2,  4000] loss: 0.781\n\nTrial train_cifar_d6d9a_00001 finished iteration 4 at 2025-05-23 19:43:16. Total running time: 2min 22s\n+------------------------------------------------------------+\n| Trial train_cifar_d6d9a_00001 result                       |\n+------------------------------------------------------------+\n| checkpoint_dir_name                      checkpoint_000003 |\n| time_this_iter_s                                  25.91264 |\n| time_total_s                                     137.87881 |\n| training_iteration                                       4 |\n| accuracy                                            0.4517 |\n| loss                                               1.49898 |\n+------------------------------------------------------------+\nTrial train_cifar_d6d9a_00001 saved a checkpoint for iteration 4 at: (local)/var/lib/ci-user/ray_results/train_cifar_2025-05-23_19-40-53/train_cifar_d6d9a_00001_1_batch_size=8,l1=32,l2=256,lr=0.0075_2025-05-23_19-40-54/checkpoint_000003\n(func pid=4876) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2025-05-23_19-40-53/train_cifar_d6d9a_00001_1_batch_size=8,l1=32,l2=256,lr=0.0075_2025-05-23_19-40-54/checkpoint_000003) [repeated 2x across cluster]\n(func pid=4878) [2,  6000] loss: 0.515 [repeated 3x across cluster]\n\nTrial train_cifar_d6d9a_00005 finished iteration 7 at 2025-05-23 19:43:17. Total running time: 2min 23s\n+------------------------------------------------------------+\n| Trial train_cifar_d6d9a_00005 result                       |\n+------------------------------------------------------------+\n| checkpoint_dir_name                      checkpoint_000006 |\n| time_this_iter_s                                  15.03629 |\n| time_total_s                                     138.54199 |\n| training_iteration                                       7 |\n| accuracy                                            0.4297 |\n| loss                                                1.5628 |\n+------------------------------------------------------------+\nTrial train_cifar_d6d9a_00005 saved a checkpoint for iteration 7 at: (local)/var/lib/ci-user/ray_results/train_cifar_2025-05-23_19-40-53/train_cifar_d6d9a_00005_5_batch_size=16,l1=16,l2=64,lr=0.0004_2025-05-23_19-40-54/checkpoint_000006\n\nTrial train_cifar_d6d9a_00009 finished iteration 4 at 2025-05-23 19:43:19. Total running time: 2min 25s\n+------------------------------------------------------------+\n| Trial train_cifar_d6d9a_00009 result                       |\n+------------------------------------------------------------+\n| checkpoint_dir_name                      checkpoint_000003 |\n| time_this_iter_s                                  16.22163 |\n| time_total_s                                      71.95764 |\n| training_iteration                                       4 |\n| accuracy                                            0.4691 |\n| loss                                               1.47895 |\n+------------------------------------------------------------+\nTrial train_cifar_d6d9a_00009 saved a checkpoint for iteration 4 at: (local)/var/lib/ci-user/ray_results/train_cifar_2025-05-23_19-40-53/train_cifar_d6d9a_00009_9_batch_size=16,l1=256,l2=32,lr=0.0005_2025-05-23_19-40-54/checkpoint_000003\n(func pid=4878) [2,  8000] loss: 0.378\n\nTrial status: 6 TERMINATED | 4 RUNNING\nCurrent time: 2025-05-23 19:43:24. Total running time: 2min 30s\nLogical resource usage: 8.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:A10G)\n+------------------------------------------------------------------------------------------------------------------------------------+\n| Trial name                status         l1     l2            lr     batch_size     iter     total time (s)      loss     accuracy |\n+------------------------------------------------------------------------------------------------------------------------------------+\n| train_cifar_d6d9a_00001   RUNNING        32    256   0.00750825               8        4           137.879    1.49898       0.4517 |\n| train_cifar_d6d9a_00004   RUNNING        16     32   0.000239293              2        1           116.91     1.62473       0.4057 |\n| train_cifar_d6d9a_00005   RUNNING        16     64   0.000370892             16        7           138.542    1.5628        0.4297 |\n| train_cifar_d6d9a_00009   RUNNING       256     32   0.000463253             16        4            71.9576   1.47895       0.4691 |\n| train_cifar_d6d9a_00000   TERMINATED      8     32   0.0665564                2        1           117.324    2.42625       0.0959 |\n| train_cifar_d6d9a_00002   TERMINATED      1      4   0.0100663                4        1            73.3948   2.31102       0.1011 |\n| train_cifar_d6d9a_00003   TERMINATED    256      8   0.0262429               16        4            94.103    1.8152        0.3195 |\n| train_cifar_d6d9a_00006   TERMINATED     16      1   0.000174684             16        1            25.9218   2.3221        0.1037 |\n| train_cifar_d6d9a_00007   TERMINATED     32     32   0.0208137                8        2            82.9934   2.10149       0.1919 |\n| train_cifar_d6d9a_00008   TERMINATED      8      4   0.0474427                8        1            42.7164   2.32709       0.0968 |\n+------------------------------------------------------------------------------------------------------------------------------------+\n(func pid=4876) [5,  2000] loss: 1.507\n(func pid=4881) [5,  2000] loss: 1.453 [repeated 2x across cluster]\n\nTrial train_cifar_d6d9a_00005 finished iteration 8 at 2025-05-23 19:43:31. Total running time: 2min 37s\n+------------------------------------------------------------+\n| Trial train_cifar_d6d9a_00005 result                       |\n+------------------------------------------------------------+\n| checkpoint_dir_name                      checkpoint_000007 |\n| time_this_iter_s                                   14.6815 |\n| time_total_s                                     153.22349 |\n| training_iteration                                       8 |\n| accuracy                                             0.454 |\n| loss                                               1.49853 |\n+------------------------------------------------------------+\nTrial train_cifar_d6d9a_00005 saved a checkpoint for iteration 8 at: (local)/var/lib/ci-user/ray_results/train_cifar_2025-05-23_19-40-53/train_cifar_d6d9a_00005_5_batch_size=16,l1=16,l2=64,lr=0.0004_2025-05-23_19-40-54/checkpoint_000007\n(func pid=4879) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2025-05-23_19-40-53/train_cifar_d6d9a_00005_5_batch_size=16,l1=16,l2=64,lr=0.0004_2025-05-23_19-40-54/checkpoint_000007) [repeated 3x across cluster]\n\nTrial train_cifar_d6d9a_00009 finished iteration 5 at 2025-05-23 19:43:35. Total running time: 2min 41s\n+------------------------------------------------------------+\n| Trial train_cifar_d6d9a_00009 result                       |\n+------------------------------------------------------------+\n| checkpoint_dir_name                      checkpoint_000004 |\n| time_this_iter_s                                  16.02125 |\n| time_total_s                                      87.97889 |\n| training_iteration                                       5 |\n| accuracy                                            0.4858 |\n| loss                                               1.42166 |\n+------------------------------------------------------------+\nTrial train_cifar_d6d9a_00009 saved a checkpoint for iteration 5 at: (local)/var/lib/ci-user/ray_results/train_cifar_2025-05-23_19-40-53/train_cifar_d6d9a_00009_9_batch_size=16,l1=256,l2=32,lr=0.0005_2025-05-23_19-40-54/checkpoint_000004\n(func pid=4878) [2, 12000] loss: 0.249 [repeated 3x across cluster]\n\nTrial train_cifar_d6d9a_00001 finished iteration 5 at 2025-05-23 19:43:41. Total running time: 2min 47s\n+------------------------------------------------------------+\n| Trial train_cifar_d6d9a_00001 result                       |\n+------------------------------------------------------------+\n| checkpoint_dir_name                      checkpoint_000004 |\n| time_this_iter_s                                  25.32665 |\n| time_total_s                                     163.20546 |\n| training_iteration                                       5 |\n| accuracy                                            0.4671 |\n| loss                                               1.52691 |\n+------------------------------------------------------------+\nTrial train_cifar_d6d9a_00001 saved a checkpoint for iteration 5 at: (local)/var/lib/ci-user/ray_results/train_cifar_2025-05-23_19-40-53/train_cifar_d6d9a_00001_1_batch_size=8,l1=32,l2=256,lr=0.0075_2025-05-23_19-40-54/checkpoint_000004\n(func pid=4876) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2025-05-23_19-40-53/train_cifar_d6d9a_00001_1_batch_size=8,l1=32,l2=256,lr=0.0075_2025-05-23_19-40-54/checkpoint_000004) [repeated 2x across cluster]\n(func pid=4878) [2, 14000] loss: 0.209 [repeated 2x across cluster]\n\nTrial train_cifar_d6d9a_00005 finished iteration 9 at 2025-05-23 19:43:46. Total running time: 2min 52s\n+------------------------------------------------------------+\n| Trial train_cifar_d6d9a_00005 result                       |\n+------------------------------------------------------------+\n| checkpoint_dir_name                      checkpoint_000008 |\n| time_this_iter_s                                  14.67706 |\n| time_total_s                                     167.90054 |\n| training_iteration                                       9 |\n| accuracy                                            0.4684 |\n| loss                                                1.4661 |\n+------------------------------------------------------------+\nTrial train_cifar_d6d9a_00005 saved a checkpoint for iteration 9 at: (local)/var/lib/ci-user/ray_results/train_cifar_2025-05-23_19-40-53/train_cifar_d6d9a_00005_5_batch_size=16,l1=16,l2=64,lr=0.0004_2025-05-23_19-40-54/checkpoint_000008\n(func pid=4876) [6,  2000] loss: 1.496 [repeated 2x across cluster]\n\nTrial train_cifar_d6d9a_00009 finished iteration 6 at 2025-05-23 19:43:51. Total running time: 2min 57s\n+------------------------------------------------------------+\n| Trial train_cifar_d6d9a_00009 result                       |\n+------------------------------------------------------------+\n| checkpoint_dir_name                      checkpoint_000005 |\n| time_this_iter_s                                  16.14989 |\n| time_total_s                                     104.12878 |\n| training_iteration                                       6 |\n| accuracy                                            0.4972 |\n| loss                                               1.39604 |\n+------------------------------------------------------------+\nTrial train_cifar_d6d9a_00009 saved a checkpoint for iteration 6 at: (local)/var/lib/ci-user/ray_results/train_cifar_2025-05-23_19-40-53/train_cifar_d6d9a_00009_9_batch_size=16,l1=256,l2=32,lr=0.0005_2025-05-23_19-40-54/checkpoint_000005\n(func pid=4881) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2025-05-23_19-40-53/train_cifar_d6d9a_00009_9_batch_size=16,l1=256,l2=32,lr=0.0005_2025-05-23_19-40-54/checkpoint_000005) [repeated 2x across cluster]\n\nTrial status: 6 TERMINATED | 4 RUNNING\nCurrent time: 2025-05-23 19:43:54. Total running time: 3min 0s\nLogical resource usage: 8.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:A10G)\n+------------------------------------------------------------------------------------------------------------------------------------+\n| Trial name                status         l1     l2            lr     batch_size     iter     total time (s)      loss     accuracy |\n+------------------------------------------------------------------------------------------------------------------------------------+\n| train_cifar_d6d9a_00001   RUNNING        32    256   0.00750825               8        5           163.205    1.52691       0.4671 |\n| train_cifar_d6d9a_00004   RUNNING        16     32   0.000239293              2        1           116.91     1.62473       0.4057 |\n| train_cifar_d6d9a_00005   RUNNING        16     64   0.000370892             16        9           167.901    1.4661        0.4684 |\n| train_cifar_d6d9a_00009   RUNNING       256     32   0.000463253             16        6           104.129    1.39604       0.4972 |\n| train_cifar_d6d9a_00000   TERMINATED      8     32   0.0665564                2        1           117.324    2.42625       0.0959 |\n| train_cifar_d6d9a_00002   TERMINATED      1      4   0.0100663                4        1            73.3948   2.31102       0.1011 |\n| train_cifar_d6d9a_00003   TERMINATED    256      8   0.0262429               16        4            94.103    1.8152        0.3195 |\n| train_cifar_d6d9a_00006   TERMINATED     16      1   0.000174684             16        1            25.9218   2.3221        0.1037 |\n| train_cifar_d6d9a_00007   TERMINATED     32     32   0.0208137                8        2            82.9934   2.10149       0.1919 |\n| train_cifar_d6d9a_00008   TERMINATED      8      4   0.0474427                8        1            42.7164   2.32709       0.0968 |\n+------------------------------------------------------------------------------------------------------------------------------------+\n(func pid=4879) [10,  2000] loss: 1.419 [repeated 2x across cluster]\n\nTrial train_cifar_d6d9a_00005 finished iteration 10 at 2025-05-23 19:44:01. Total running time: 3min 7s\n+------------------------------------------------------------+\n| Trial train_cifar_d6d9a_00005 result                       |\n+------------------------------------------------------------+\n| checkpoint_dir_name                      checkpoint_000009 |\n| time_this_iter_s                                  14.73202 |\n| time_total_s                                     182.63256 |\n| training_iteration                                      10 |\n| accuracy                                            0.4854 |\n| loss                                               1.41427 |\n+------------------------------------------------------------+\nTrial train_cifar_d6d9a_00005 saved a checkpoint for iteration 10 at: (local)/var/lib/ci-user/ray_results/train_cifar_2025-05-23_19-40-53/train_cifar_d6d9a_00005_5_batch_size=16,l1=16,l2=64,lr=0.0004_2025-05-23_19-40-54/checkpoint_000009\n\nTrial train_cifar_d6d9a_00005 completed after 10 iterations at 2025-05-23 19:44:01. Total running time: 3min 7s\n(func pid=4879) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2025-05-23_19-40-53/train_cifar_d6d9a_00005_5_batch_size=16,l1=16,l2=64,lr=0.0004_2025-05-23_19-40-54/checkpoint_000009)\n(func pid=4881) [7,  2000] loss: 1.331 [repeated 3x across cluster]\n\nTrial train_cifar_d6d9a_00001 finished iteration 6 at 2025-05-23 19:44:07. Total running time: 3min 13s\n+------------------------------------------------------------+\n| Trial train_cifar_d6d9a_00001 result                       |\n+------------------------------------------------------------+\n| checkpoint_dir_name                      checkpoint_000005 |\n| time_this_iter_s                                  25.13336 |\n| time_total_s                                     188.33882 |\n| training_iteration                                       6 |\n| accuracy                                            0.4666 |\n| loss                                               1.58691 |\n+------------------------------------------------------------+\nTrial train_cifar_d6d9a_00001 saved a checkpoint for iteration 6 at: (local)/var/lib/ci-user/ray_results/train_cifar_2025-05-23_19-40-53/train_cifar_d6d9a_00001_1_batch_size=8,l1=32,l2=256,lr=0.0075_2025-05-23_19-40-54/checkpoint_000005\n(func pid=4876) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2025-05-23_19-40-53/train_cifar_d6d9a_00001_1_batch_size=8,l1=32,l2=256,lr=0.0075_2025-05-23_19-40-54/checkpoint_000005)\n\nTrial train_cifar_d6d9a_00009 finished iteration 7 at 2025-05-23 19:44:07. Total running time: 3min 13s\n+------------------------------------------------------------+\n| Trial train_cifar_d6d9a_00009 result                       |\n+------------------------------------------------------------+\n| checkpoint_dir_name                      checkpoint_000006 |\n| time_this_iter_s                                  15.72086 |\n| time_total_s                                     119.84963 |\n| training_iteration                                       7 |\n| accuracy                                            0.5058 |\n| loss                                               1.37836 |\n+------------------------------------------------------------+\nTrial train_cifar_d6d9a_00009 saved a checkpoint for iteration 7 at: (local)/var/lib/ci-user/ray_results/train_cifar_2025-05-23_19-40-53/train_cifar_d6d9a_00009_9_batch_size=16,l1=256,l2=32,lr=0.0005_2025-05-23_19-40-54/checkpoint_000006\n(func pid=4876) [7,  2000] loss: 1.494 [repeated 2x across cluster]\n\nTrial train_cifar_d6d9a_00004 finished iteration 2 at 2025-05-23 19:44:16. Total running time: 3min 22s\n+------------------------------------------------------------+\n| Trial train_cifar_d6d9a_00004 result                       |\n+------------------------------------------------------------+\n| checkpoint_dir_name                      checkpoint_000001 |\n| time_this_iter_s                                  80.61134 |\n| time_total_s                                     197.52102 |\n| training_iteration                                       2 |\n| accuracy                                            0.4998 |\n| loss                                               1.38968 |\n+------------------------------------------------------------+\nTrial train_cifar_d6d9a_00004 saved a checkpoint for iteration 2 at: (local)/var/lib/ci-user/ray_results/train_cifar_2025-05-23_19-40-53/train_cifar_d6d9a_00004_4_batch_size=2,l1=16,l2=32,lr=0.0002_2025-05-23_19-40-54/checkpoint_000001\n(func pid=4878) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2025-05-23_19-40-53/train_cifar_d6d9a_00004_4_batch_size=2,l1=16,l2=32,lr=0.0002_2025-05-23_19-40-54/checkpoint_000001) [repeated 2x across cluster]\n\nTrial train_cifar_d6d9a_00009 finished iteration 8 at 2025-05-23 19:44:22. Total running time: 3min 28s\n+------------------------------------------------------------+\n| Trial train_cifar_d6d9a_00009 result                       |\n+------------------------------------------------------------+\n| checkpoint_dir_name                      checkpoint_000007 |\n| time_this_iter_s                                  14.77813 |\n| time_total_s                                     134.62776 |\n| training_iteration                                       8 |\n| accuracy                                            0.5353 |\n| loss                                               1.29696 |\n+------------------------------------------------------------+\nTrial train_cifar_d6d9a_00009 saved a checkpoint for iteration 8 at: (local)/var/lib/ci-user/ray_results/train_cifar_2025-05-23_19-40-53/train_cifar_d6d9a_00009_9_batch_size=16,l1=256,l2=32,lr=0.0005_2025-05-23_19-40-54/checkpoint_000007\n(func pid=4881) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2025-05-23_19-40-53/train_cifar_d6d9a_00009_9_batch_size=16,l1=256,l2=32,lr=0.0005_2025-05-23_19-40-54/checkpoint_000007)\n(func pid=4878) [3,  2000] loss: 1.376 [repeated 2x across cluster]\n\nTrial status: 7 TERMINATED | 3 RUNNING\nCurrent time: 2025-05-23 19:44:24. Total running time: 3min 30s\nLogical resource usage: 6.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:A10G)\n+------------------------------------------------------------------------------------------------------------------------------------+\n| Trial name                status         l1     l2            lr     batch_size     iter     total time (s)      loss     accuracy |\n+------------------------------------------------------------------------------------------------------------------------------------+\n| train_cifar_d6d9a_00001   RUNNING        32    256   0.00750825               8        6           188.339    1.58691       0.4666 |\n| train_cifar_d6d9a_00004   RUNNING        16     32   0.000239293              2        2           197.521    1.38968       0.4998 |\n| train_cifar_d6d9a_00009   RUNNING       256     32   0.000463253             16        8           134.628    1.29696       0.5353 |\n| train_cifar_d6d9a_00000   TERMINATED      8     32   0.0665564                2        1           117.324    2.42625       0.0959 |\n| train_cifar_d6d9a_00002   TERMINATED      1      4   0.0100663                4        1            73.3948   2.31102       0.1011 |\n| train_cifar_d6d9a_00003   TERMINATED    256      8   0.0262429               16        4            94.103    1.8152        0.3195 |\n| train_cifar_d6d9a_00005   TERMINATED     16     64   0.000370892             16       10           182.633    1.41427       0.4854 |\n| train_cifar_d6d9a_00006   TERMINATED     16      1   0.000174684             16        1            25.9218   2.3221        0.1037 |\n| train_cifar_d6d9a_00007   TERMINATED     32     32   0.0208137                8        2            82.9934   2.10149       0.1919 |\n| train_cifar_d6d9a_00008   TERMINATED      8      4   0.0474427                8        1            42.7164   2.32709       0.0968 |\n+------------------------------------------------------------------------------------------------------------------------------------+\n(func pid=4878) [3,  4000] loss: 0.696 [repeated 2x across cluster]\n\nTrial train_cifar_d6d9a_00001 finished iteration 7 at 2025-05-23 19:44:29. Total running time: 3min 35s\n+------------------------------------------------------------+\n| Trial train_cifar_d6d9a_00001 result                       |\n+------------------------------------------------------------+\n| checkpoint_dir_name                      checkpoint_000006 |\n| time_this_iter_s                                   22.7098 |\n| time_total_s                                     211.04862 |\n| training_iteration                                       7 |\n| accuracy                                            0.4687 |\n| loss                                               1.57444 |\n+------------------------------------------------------------+\nTrial train_cifar_d6d9a_00001 saved a checkpoint for iteration 7 at: (local)/var/lib/ci-user/ray_results/train_cifar_2025-05-23_19-40-53/train_cifar_d6d9a_00001_1_batch_size=8,l1=32,l2=256,lr=0.0075_2025-05-23_19-40-54/checkpoint_000006\n(func pid=4876) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2025-05-23_19-40-53/train_cifar_d6d9a_00001_1_batch_size=8,l1=32,l2=256,lr=0.0075_2025-05-23_19-40-54/checkpoint_000006)\n(func pid=4878) [3,  6000] loss: 0.452 [repeated 2x across cluster]\n\nTrial train_cifar_d6d9a_00009 finished iteration 9 at 2025-05-23 19:44:36. Total running time: 3min 42s\n+------------------------------------------------------------+\n| Trial train_cifar_d6d9a_00009 result                       |\n+------------------------------------------------------------+\n| checkpoint_dir_name                      checkpoint_000008 |\n| time_this_iter_s                                  14.16462 |\n| time_total_s                                     148.79239 |\n| training_iteration                                       9 |\n| accuracy                                            0.5541 |\n| loss                                               1.25657 |\n+------------------------------------------------------------+\nTrial train_cifar_d6d9a_00009 saved a checkpoint for iteration 9 at: (local)/var/lib/ci-user/ray_results/train_cifar_2025-05-23_19-40-53/train_cifar_d6d9a_00009_9_batch_size=16,l1=256,l2=32,lr=0.0005_2025-05-23_19-40-54/checkpoint_000008\n(func pid=4881) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2025-05-23_19-40-53/train_cifar_d6d9a_00009_9_batch_size=16,l1=256,l2=32,lr=0.0005_2025-05-23_19-40-54/checkpoint_000008)\n(func pid=4878) [3,  8000] loss: 0.335 [repeated 2x across cluster]\n(func pid=4878) [3, 10000] loss: 0.269 [repeated 3x across cluster]\n\nTrial train_cifar_d6d9a_00009 finished iteration 10 at 2025-05-23 19:44:51. Total running time: 3min 57s\n+------------------------------------------------------------+\n| Trial train_cifar_d6d9a_00009 result                       |\n+------------------------------------------------------------+\n| checkpoint_dir_name                      checkpoint_000009 |\n| time_this_iter_s                                  14.61135 |\n| time_total_s                                     163.40374 |\n| training_iteration                                      10 |\n| accuracy                                            0.5709 |\n| loss                                               1.20364 |\n+------------------------------------------------------------+\nTrial train_cifar_d6d9a_00009 saved a checkpoint for iteration 10 at: (local)/var/lib/ci-user/ray_results/train_cifar_2025-05-23_19-40-53/train_cifar_d6d9a_00009_9_batch_size=16,l1=256,l2=32,lr=0.0005_2025-05-23_19-40-54/checkpoint_000009\n\nTrial train_cifar_d6d9a_00009 completed after 10 iterations at 2025-05-23 19:44:51. Total running time: 3min 57s\n(func pid=4881) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2025-05-23_19-40-53/train_cifar_d6d9a_00009_9_batch_size=16,l1=256,l2=32,lr=0.0005_2025-05-23_19-40-54/checkpoint_000009)\n\nTrial train_cifar_d6d9a_00001 finished iteration 8 at 2025-05-23 19:44:51. Total running time: 3min 57s\n+------------------------------------------------------------+\n| Trial train_cifar_d6d9a_00001 result                       |\n+------------------------------------------------------------+\n| checkpoint_dir_name                      checkpoint_000007 |\n| time_this_iter_s                                  21.70368 |\n| time_total_s                                      232.7523 |\n| training_iteration                                       8 |\n| accuracy                                             0.431 |\n| loss                                               1.63678 |\n+------------------------------------------------------------+\nTrial train_cifar_d6d9a_00001 saved a checkpoint for iteration 8 at: (local)/var/lib/ci-user/ray_results/train_cifar_2025-05-23_19-40-53/train_cifar_d6d9a_00001_1_batch_size=8,l1=32,l2=256,lr=0.0075_2025-05-23_19-40-54/checkpoint_000007\n\nTrial train_cifar_d6d9a_00001 completed after 8 iterations at 2025-05-23 19:44:51. Total running time: 3min 57s\n(func pid=4878) [3, 12000] loss: 0.225\n\nTrial status: 9 TERMINATED | 1 RUNNING\nCurrent time: 2025-05-23 19:44:54. Total running time: 4min 0s\nLogical resource usage: 2.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:A10G)\n+------------------------------------------------------------------------------------------------------------------------------------+\n| Trial name                status         l1     l2            lr     batch_size     iter     total time (s)      loss     accuracy |\n+------------------------------------------------------------------------------------------------------------------------------------+\n| train_cifar_d6d9a_00004   RUNNING        16     32   0.000239293              2        2           197.521    1.38968       0.4998 |\n| train_cifar_d6d9a_00000   TERMINATED      8     32   0.0665564                2        1           117.324    2.42625       0.0959 |\n| train_cifar_d6d9a_00001   TERMINATED     32    256   0.00750825               8        8           232.752    1.63678       0.431  |\n| train_cifar_d6d9a_00002   TERMINATED      1      4   0.0100663                4        1            73.3948   2.31102       0.1011 |\n| train_cifar_d6d9a_00003   TERMINATED    256      8   0.0262429               16        4            94.103    1.8152        0.3195 |\n| train_cifar_d6d9a_00005   TERMINATED     16     64   0.000370892             16       10           182.633    1.41427       0.4854 |\n| train_cifar_d6d9a_00006   TERMINATED     16      1   0.000174684             16        1            25.9218   2.3221        0.1037 |\n| train_cifar_d6d9a_00007   TERMINATED     32     32   0.0208137                8        2            82.9934   2.10149       0.1919 |\n| train_cifar_d6d9a_00008   TERMINATED      8      4   0.0474427                8        1            42.7164   2.32709       0.0968 |\n| train_cifar_d6d9a_00009   TERMINATED    256     32   0.000463253             16       10           163.404    1.20364       0.5709 |\n+------------------------------------------------------------------------------------------------------------------------------------+\n(func pid=4878) [3, 14000] loss: 0.189\n(func pid=4878) [3, 16000] loss: 0.162\n(func pid=4878) [3, 18000] loss: 0.146\n(func pid=4878) [3, 20000] loss: 0.131\n\nTrial train_cifar_d6d9a_00004 finished iteration 3 at 2025-05-23 19:45:21. Total running time: 4min 27s\n+------------------------------------------------------------+\n| Trial train_cifar_d6d9a_00004 result                       |\n+------------------------------------------------------------+\n| checkpoint_dir_name                      checkpoint_000002 |\n| time_this_iter_s                                  65.53513 |\n| time_total_s                                     263.05614 |\n| training_iteration                                       3 |\n| accuracy                                            0.5008 |\n| loss                                                1.3884 |\n+------------------------------------------------------------+\nTrial train_cifar_d6d9a_00004 saved a checkpoint for iteration 3 at: (local)/var/lib/ci-user/ray_results/train_cifar_2025-05-23_19-40-53/train_cifar_d6d9a_00004_4_batch_size=2,l1=16,l2=32,lr=0.0002_2025-05-23_19-40-54/checkpoint_000002\n(func pid=4878) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2025-05-23_19-40-53/train_cifar_d6d9a_00004_4_batch_size=2,l1=16,l2=32,lr=0.0002_2025-05-23_19-40-54/checkpoint_000002) [repeated 2x across cluster]\n\nTrial status: 9 TERMINATED | 1 RUNNING\nCurrent time: 2025-05-23 19:45:24. Total running time: 4min 30s\nLogical resource usage: 2.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:A10G)\n+------------------------------------------------------------------------------------------------------------------------------------+\n| Trial name                status         l1     l2            lr     batch_size     iter     total time (s)      loss     accuracy |\n+------------------------------------------------------------------------------------------------------------------------------------+\n| train_cifar_d6d9a_00004   RUNNING        16     32   0.000239293              2        3           263.056    1.3884        0.5008 |\n| train_cifar_d6d9a_00000   TERMINATED      8     32   0.0665564                2        1           117.324    2.42625       0.0959 |\n| train_cifar_d6d9a_00001   TERMINATED     32    256   0.00750825               8        8           232.752    1.63678       0.431  |\n| train_cifar_d6d9a_00002   TERMINATED      1      4   0.0100663                4        1            73.3948   2.31102       0.1011 |\n| train_cifar_d6d9a_00003   TERMINATED    256      8   0.0262429               16        4            94.103    1.8152        0.3195 |\n| train_cifar_d6d9a_00005   TERMINATED     16     64   0.000370892             16       10           182.633    1.41427       0.4854 |\n| train_cifar_d6d9a_00006   TERMINATED     16      1   0.000174684             16        1            25.9218   2.3221        0.1037 |\n| train_cifar_d6d9a_00007   TERMINATED     32     32   0.0208137                8        2            82.9934   2.10149       0.1919 |\n| train_cifar_d6d9a_00008   TERMINATED      8      4   0.0474427                8        1            42.7164   2.32709       0.0968 |\n| train_cifar_d6d9a_00009   TERMINATED    256     32   0.000463253             16       10           163.404    1.20364       0.5709 |\n+------------------------------------------------------------------------------------------------------------------------------------+\n(func pid=4878) [4,  2000] loss: 1.305\n(func pid=4878) [4,  4000] loss: 0.620\n(func pid=4878) [4,  6000] loss: 0.420\n(func pid=4878) [4,  8000] loss: 0.314\n(func pid=4878) [4, 10000] loss: 0.251\n(func pid=4878) [4, 12000] loss: 0.207\nTrial status: 9 TERMINATED | 1 RUNNING\nCurrent time: 2025-05-23 19:45:54. Total running time: 5min 0s\nLogical resource usage: 2.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:A10G)\n+------------------------------------------------------------------------------------------------------------------------------------+\n| Trial name                status         l1     l2            lr     batch_size     iter     total time (s)      loss     accuracy |\n+------------------------------------------------------------------------------------------------------------------------------------+\n| train_cifar_d6d9a_00004   RUNNING        16     32   0.000239293              2        3           263.056    1.3884        0.5008 |\n| train_cifar_d6d9a_00000   TERMINATED      8     32   0.0665564                2        1           117.324    2.42625       0.0959 |\n| train_cifar_d6d9a_00001   TERMINATED     32    256   0.00750825               8        8           232.752    1.63678       0.431  |\n| train_cifar_d6d9a_00002   TERMINATED      1      4   0.0100663                4        1            73.3948   2.31102       0.1011 |\n| train_cifar_d6d9a_00003   TERMINATED    256      8   0.0262429               16        4            94.103    1.8152        0.3195 |\n| train_cifar_d6d9a_00005   TERMINATED     16     64   0.000370892             16       10           182.633    1.41427       0.4854 |\n| train_cifar_d6d9a_00006   TERMINATED     16      1   0.000174684             16        1            25.9218   2.3221        0.1037 |\n| train_cifar_d6d9a_00007   TERMINATED     32     32   0.0208137                8        2            82.9934   2.10149       0.1919 |\n| train_cifar_d6d9a_00008   TERMINATED      8      4   0.0474427                8        1            42.7164   2.32709       0.0968 |\n| train_cifar_d6d9a_00009   TERMINATED    256     32   0.000463253             16       10           163.404    1.20364       0.5709 |\n+------------------------------------------------------------------------------------------------------------------------------------+\n(func pid=4878) [4, 14000] loss: 0.179\n(func pid=4878) [4, 16000] loss: 0.153\n(func pid=4878) [4, 18000] loss: 0.138\n(func pid=4878) [4, 20000] loss: 0.124\n\nTrial train_cifar_d6d9a_00004 finished iteration 4 at 2025-05-23 19:46:19. Total running time: 5min 25s\n+------------------------------------------------------------+\n| Trial train_cifar_d6d9a_00004 result                       |\n+------------------------------------------------------------+\n| checkpoint_dir_name                      checkpoint_000003 |\n| time_this_iter_s                                  58.05806 |\n| time_total_s                                      321.1142 |\n| training_iteration                                       4 |\n| accuracy                                             0.561 |\n| loss                                               1.22217 |\n+------------------------------------------------------------+\nTrial train_cifar_d6d9a_00004 saved a checkpoint for iteration 4 at: (local)/var/lib/ci-user/ray_results/train_cifar_2025-05-23_19-40-53/train_cifar_d6d9a_00004_4_batch_size=2,l1=16,l2=32,lr=0.0002_2025-05-23_19-40-54/checkpoint_000003\n(func pid=4878) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2025-05-23_19-40-53/train_cifar_d6d9a_00004_4_batch_size=2,l1=16,l2=32,lr=0.0002_2025-05-23_19-40-54/checkpoint_000003)\n\nTrial status: 9 TERMINATED | 1 RUNNING\nCurrent time: 2025-05-23 19:46:24. Total running time: 5min 30s\nLogical resource usage: 2.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:A10G)\n+------------------------------------------------------------------------------------------------------------------------------------+\n| Trial name                status         l1     l2            lr     batch_size     iter     total time (s)      loss     accuracy |\n+------------------------------------------------------------------------------------------------------------------------------------+\n| train_cifar_d6d9a_00004   RUNNING        16     32   0.000239293              2        4           321.114    1.22217       0.561  |\n| train_cifar_d6d9a_00000   TERMINATED      8     32   0.0665564                2        1           117.324    2.42625       0.0959 |\n| train_cifar_d6d9a_00001   TERMINATED     32    256   0.00750825               8        8           232.752    1.63678       0.431  |\n| train_cifar_d6d9a_00002   TERMINATED      1      4   0.0100663                4        1            73.3948   2.31102       0.1011 |\n| train_cifar_d6d9a_00003   TERMINATED    256      8   0.0262429               16        4            94.103    1.8152        0.3195 |\n| train_cifar_d6d9a_00005   TERMINATED     16     64   0.000370892             16       10           182.633    1.41427       0.4854 |\n| train_cifar_d6d9a_00006   TERMINATED     16      1   0.000174684             16        1            25.9218   2.3221        0.1037 |\n| train_cifar_d6d9a_00007   TERMINATED     32     32   0.0208137                8        2            82.9934   2.10149       0.1919 |\n| train_cifar_d6d9a_00008   TERMINATED      8      4   0.0474427                8        1            42.7164   2.32709       0.0968 |\n| train_cifar_d6d9a_00009   TERMINATED    256     32   0.000463253             16       10           163.404    1.20364       0.5709 |\n+------------------------------------------------------------------------------------------------------------------------------------+\n(func pid=4878) [5,  2000] loss: 1.185\n(func pid=4878) [5,  4000] loss: 0.609\n(func pid=4878) [5,  6000] loss: 0.403\n(func pid=4878) [5,  8000] loss: 0.302\n(func pid=4878) [5, 10000] loss: 0.243\n(func pid=4878) [5, 12000] loss: 0.193\nTrial status: 9 TERMINATED | 1 RUNNING\nCurrent time: 2025-05-23 19:46:54. Total running time: 6min 0s\nLogical resource usage: 2.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:A10G)\n+------------------------------------------------------------------------------------------------------------------------------------+\n| Trial name                status         l1     l2            lr     batch_size     iter     total time (s)      loss     accuracy |\n+------------------------------------------------------------------------------------------------------------------------------------+\n| train_cifar_d6d9a_00004   RUNNING        16     32   0.000239293              2        4           321.114    1.22217       0.561  |\n| train_cifar_d6d9a_00000   TERMINATED      8     32   0.0665564                2        1           117.324    2.42625       0.0959 |\n| train_cifar_d6d9a_00001   TERMINATED     32    256   0.00750825               8        8           232.752    1.63678       0.431  |\n| train_cifar_d6d9a_00002   TERMINATED      1      4   0.0100663                4        1            73.3948   2.31102       0.1011 |\n| train_cifar_d6d9a_00003   TERMINATED    256      8   0.0262429               16        4            94.103    1.8152        0.3195 |\n| train_cifar_d6d9a_00005   TERMINATED     16     64   0.000370892             16       10           182.633    1.41427       0.4854 |\n| train_cifar_d6d9a_00006   TERMINATED     16      1   0.000174684             16        1            25.9218   2.3221        0.1037 |\n| train_cifar_d6d9a_00007   TERMINATED     32     32   0.0208137                8        2            82.9934   2.10149       0.1919 |\n| train_cifar_d6d9a_00008   TERMINATED      8      4   0.0474427                8        1            42.7164   2.32709       0.0968 |\n| train_cifar_d6d9a_00009   TERMINATED    256     32   0.000463253             16       10           163.404    1.20364       0.5709 |\n+------------------------------------------------------------------------------------------------------------------------------------+\n(func pid=4878) [5, 14000] loss: 0.172\n(func pid=4878) [5, 16000] loss: 0.149\n(func pid=4878) [5, 18000] loss: 0.132\n(func pid=4878) [5, 20000] loss: 0.118\n\nTrial train_cifar_d6d9a_00004 finished iteration 5 at 2025-05-23 19:47:17. Total running time: 6min 23s\n+------------------------------------------------------------+\n| Trial train_cifar_d6d9a_00004 result                       |\n+------------------------------------------------------------+\n| checkpoint_dir_name                      checkpoint_000004 |\n| time_this_iter_s                                  57.90054 |\n| time_total_s                                     379.01475 |\n| training_iteration                                       5 |\n| accuracy                                            0.5867 |\n| loss                                               1.17232 |\n+------------------------------------------------------------+\nTrial train_cifar_d6d9a_00004 saved a checkpoint for iteration 5 at: (local)/var/lib/ci-user/ray_results/train_cifar_2025-05-23_19-40-53/train_cifar_d6d9a_00004_4_batch_size=2,l1=16,l2=32,lr=0.0002_2025-05-23_19-40-54/checkpoint_000004\n(func pid=4878) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2025-05-23_19-40-53/train_cifar_d6d9a_00004_4_batch_size=2,l1=16,l2=32,lr=0.0002_2025-05-23_19-40-54/checkpoint_000004)\n(func pid=4878) [6,  2000] loss: 1.170\n\nTrial status: 9 TERMINATED | 1 RUNNING\nCurrent time: 2025-05-23 19:47:24. Total running time: 6min 30s\nLogical resource usage: 2.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:A10G)\n+------------------------------------------------------------------------------------------------------------------------------------+\n| Trial name                status         l1     l2            lr     batch_size     iter     total time (s)      loss     accuracy |\n+------------------------------------------------------------------------------------------------------------------------------------+\n| train_cifar_d6d9a_00004   RUNNING        16     32   0.000239293              2        5           379.015    1.17232       0.5867 |\n| train_cifar_d6d9a_00000   TERMINATED      8     32   0.0665564                2        1           117.324    2.42625       0.0959 |\n| train_cifar_d6d9a_00001   TERMINATED     32    256   0.00750825               8        8           232.752    1.63678       0.431  |\n| train_cifar_d6d9a_00002   TERMINATED      1      4   0.0100663                4        1            73.3948   2.31102       0.1011 |\n| train_cifar_d6d9a_00003   TERMINATED    256      8   0.0262429               16        4            94.103    1.8152        0.3195 |\n| train_cifar_d6d9a_00005   TERMINATED     16     64   0.000370892             16       10           182.633    1.41427       0.4854 |\n| train_cifar_d6d9a_00006   TERMINATED     16      1   0.000174684             16        1            25.9218   2.3221        0.1037 |\n| train_cifar_d6d9a_00007   TERMINATED     32     32   0.0208137                8        2            82.9934   2.10149       0.1919 |\n| train_cifar_d6d9a_00008   TERMINATED      8      4   0.0474427                8        1            42.7164   2.32709       0.0968 |\n| train_cifar_d6d9a_00009   TERMINATED    256     32   0.000463253             16       10           163.404    1.20364       0.5709 |\n+------------------------------------------------------------------------------------------------------------------------------------+\n(func pid=4878) [6,  4000] loss: 0.573\n(func pid=4878) [6,  6000] loss: 0.394\n(func pid=4878) [6,  8000] loss: 0.288\n(func pid=4878) [6, 10000] loss: 0.227\n(func pid=4878) [6, 12000] loss: 0.190\n(func pid=4878) [6, 14000] loss: 0.164\nTrial status: 9 TERMINATED | 1 RUNNING\nCurrent time: 2025-05-23 19:47:54. Total running time: 7min 0s\nLogical resource usage: 2.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:A10G)\n+------------------------------------------------------------------------------------------------------------------------------------+\n| Trial name                status         l1     l2            lr     batch_size     iter     total time (s)      loss     accuracy |\n+------------------------------------------------------------------------------------------------------------------------------------+\n| train_cifar_d6d9a_00004   RUNNING        16     32   0.000239293              2        5           379.015    1.17232       0.5867 |\n| train_cifar_d6d9a_00000   TERMINATED      8     32   0.0665564                2        1           117.324    2.42625       0.0959 |\n| train_cifar_d6d9a_00001   TERMINATED     32    256   0.00750825               8        8           232.752    1.63678       0.431  |\n| train_cifar_d6d9a_00002   TERMINATED      1      4   0.0100663                4        1            73.3948   2.31102       0.1011 |\n| train_cifar_d6d9a_00003   TERMINATED    256      8   0.0262429               16        4            94.103    1.8152        0.3195 |\n| train_cifar_d6d9a_00005   TERMINATED     16     64   0.000370892             16       10           182.633    1.41427       0.4854 |\n| train_cifar_d6d9a_00006   TERMINATED     16      1   0.000174684             16        1            25.9218   2.3221        0.1037 |\n| train_cifar_d6d9a_00007   TERMINATED     32     32   0.0208137                8        2            82.9934   2.10149       0.1919 |\n| train_cifar_d6d9a_00008   TERMINATED      8      4   0.0474427                8        1            42.7164   2.32709       0.0968 |\n| train_cifar_d6d9a_00009   TERMINATED    256     32   0.000463253             16       10           163.404    1.20364       0.5709 |\n+------------------------------------------------------------------------------------------------------------------------------------+\n(func pid=4878) [6, 16000] loss: 0.144\n(func pid=4878) [6, 18000] loss: 0.128\n(func pid=4878) [6, 20000] loss: 0.114\n\nTrial train_cifar_d6d9a_00004 finished iteration 6 at 2025-05-23 19:48:15. Total running time: 7min 21s\n+------------------------------------------------------------+\n| Trial train_cifar_d6d9a_00004 result                       |\n+------------------------------------------------------------+\n| checkpoint_dir_name                      checkpoint_000005 |\n| time_this_iter_s                                  58.04098 |\n| time_total_s                                     437.05573 |\n| training_iteration                                       6 |\n| accuracy                                            0.5677 |\n| loss                                               1.23955 |\n+------------------------------------------------------------+\nTrial train_cifar_d6d9a_00004 saved a checkpoint for iteration 6 at: (local)/var/lib/ci-user/ray_results/train_cifar_2025-05-23_19-40-53/train_cifar_d6d9a_00004_4_batch_size=2,l1=16,l2=32,lr=0.0002_2025-05-23_19-40-54/checkpoint_000005\n(func pid=4878) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2025-05-23_19-40-53/train_cifar_d6d9a_00004_4_batch_size=2,l1=16,l2=32,lr=0.0002_2025-05-23_19-40-54/checkpoint_000005)\n(func pid=4878) [7,  2000] loss: 1.092\n\nTrial status: 9 TERMINATED | 1 RUNNING\nCurrent time: 2025-05-23 19:48:24. Total running time: 7min 30s\nLogical resource usage: 2.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:A10G)\n+------------------------------------------------------------------------------------------------------------------------------------+\n| Trial name                status         l1     l2            lr     batch_size     iter     total time (s)      loss     accuracy |\n+------------------------------------------------------------------------------------------------------------------------------------+\n| train_cifar_d6d9a_00004   RUNNING        16     32   0.000239293              2        6           437.056    1.23955       0.5677 |\n| train_cifar_d6d9a_00000   TERMINATED      8     32   0.0665564                2        1           117.324    2.42625       0.0959 |\n| train_cifar_d6d9a_00001   TERMINATED     32    256   0.00750825               8        8           232.752    1.63678       0.431  |\n| train_cifar_d6d9a_00002   TERMINATED      1      4   0.0100663                4        1            73.3948   2.31102       0.1011 |\n| train_cifar_d6d9a_00003   TERMINATED    256      8   0.0262429               16        4            94.103    1.8152        0.3195 |\n| train_cifar_d6d9a_00005   TERMINATED     16     64   0.000370892             16       10           182.633    1.41427       0.4854 |\n| train_cifar_d6d9a_00006   TERMINATED     16      1   0.000174684             16        1            25.9218   2.3221        0.1037 |\n| train_cifar_d6d9a_00007   TERMINATED     32     32   0.0208137                8        2            82.9934   2.10149       0.1919 |\n| train_cifar_d6d9a_00008   TERMINATED      8      4   0.0474427                8        1            42.7164   2.32709       0.0968 |\n| train_cifar_d6d9a_00009   TERMINATED    256     32   0.000463253             16       10           163.404    1.20364       0.5709 |\n+------------------------------------------------------------------------------------------------------------------------------------+\n(func pid=4878) [7,  4000] loss: 0.555\n(func pid=4878) [7,  6000] loss: 0.373\n(func pid=4878) [7,  8000] loss: 0.286\n(func pid=4878) [7, 10000] loss: 0.226\n(func pid=4878) [7, 12000] loss: 0.185\n(func pid=4878) [7, 14000] loss: 0.160\nTrial status: 9 TERMINATED | 1 RUNNING\nCurrent time: 2025-05-23 19:48:54. Total running time: 8min 0s\nLogical resource usage: 2.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:A10G)\n+------------------------------------------------------------------------------------------------------------------------------------+\n| Trial name                status         l1     l2            lr     batch_size     iter     total time (s)      loss     accuracy |\n+------------------------------------------------------------------------------------------------------------------------------------+\n| train_cifar_d6d9a_00004   RUNNING        16     32   0.000239293              2        6           437.056    1.23955       0.5677 |\n| train_cifar_d6d9a_00000   TERMINATED      8     32   0.0665564                2        1           117.324    2.42625       0.0959 |\n| train_cifar_d6d9a_00001   TERMINATED     32    256   0.00750825               8        8           232.752    1.63678       0.431  |\n| train_cifar_d6d9a_00002   TERMINATED      1      4   0.0100663                4        1            73.3948   2.31102       0.1011 |\n| train_cifar_d6d9a_00003   TERMINATED    256      8   0.0262429               16        4            94.103    1.8152        0.3195 |\n| train_cifar_d6d9a_00005   TERMINATED     16     64   0.000370892             16       10           182.633    1.41427       0.4854 |\n| train_cifar_d6d9a_00006   TERMINATED     16      1   0.000174684             16        1            25.9218   2.3221        0.1037 |\n| train_cifar_d6d9a_00007   TERMINATED     32     32   0.0208137                8        2            82.9934   2.10149       0.1919 |\n| train_cifar_d6d9a_00008   TERMINATED      8      4   0.0474427                8        1            42.7164   2.32709       0.0968 |\n| train_cifar_d6d9a_00009   TERMINATED    256     32   0.000463253             16       10           163.404    1.20364       0.5709 |\n+------------------------------------------------------------------------------------------------------------------------------------+\n(func pid=4878) [7, 16000] loss: 0.142\n(func pid=4878) [7, 18000] loss: 0.124\n(func pid=4878) [7, 20000] loss: 0.113\n\nTrial train_cifar_d6d9a_00004 finished iteration 7 at 2025-05-23 19:49:13. Total running time: 8min 19s\n+------------------------------------------------------------+\n| Trial train_cifar_d6d9a_00004 result                       |\n+------------------------------------------------------------+\n| checkpoint_dir_name                      checkpoint_000006 |\n| time_this_iter_s                                  57.53886 |\n| time_total_s                                     494.59459 |\n| training_iteration                                       7 |\n| accuracy                                            0.5966 |\n| loss                                               1.14006 |\n+------------------------------------------------------------+\nTrial train_cifar_d6d9a_00004 saved a checkpoint for iteration 7 at: (local)/var/lib/ci-user/ray_results/train_cifar_2025-05-23_19-40-53/train_cifar_d6d9a_00004_4_batch_size=2,l1=16,l2=32,lr=0.0002_2025-05-23_19-40-54/checkpoint_000006\n(func pid=4878) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2025-05-23_19-40-53/train_cifar_d6d9a_00004_4_batch_size=2,l1=16,l2=32,lr=0.0002_2025-05-23_19-40-54/checkpoint_000006)\n(func pid=4878) [8,  2000] loss: 1.081\n(func pid=4878) [8,  4000] loss: 0.548\n\nTrial status: 9 TERMINATED | 1 RUNNING\nCurrent time: 2025-05-23 19:49:24. Total running time: 8min 30s\nLogical resource usage: 2.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:A10G)\n+------------------------------------------------------------------------------------------------------------------------------------+\n| Trial name                status         l1     l2            lr     batch_size     iter     total time (s)      loss     accuracy |\n+------------------------------------------------------------------------------------------------------------------------------------+\n| train_cifar_d6d9a_00004   RUNNING        16     32   0.000239293              2        7           494.595    1.14006       0.5966 |\n| train_cifar_d6d9a_00000   TERMINATED      8     32   0.0665564                2        1           117.324    2.42625       0.0959 |\n| train_cifar_d6d9a_00001   TERMINATED     32    256   0.00750825               8        8           232.752    1.63678       0.431  |\n| train_cifar_d6d9a_00002   TERMINATED      1      4   0.0100663                4        1            73.3948   2.31102       0.1011 |\n| train_cifar_d6d9a_00003   TERMINATED    256      8   0.0262429               16        4            94.103    1.8152        0.3195 |\n| train_cifar_d6d9a_00005   TERMINATED     16     64   0.000370892             16       10           182.633    1.41427       0.4854 |\n| train_cifar_d6d9a_00006   TERMINATED     16      1   0.000174684             16        1            25.9218   2.3221        0.1037 |\n| train_cifar_d6d9a_00007   TERMINATED     32     32   0.0208137                8        2            82.9934   2.10149       0.1919 |\n| train_cifar_d6d9a_00008   TERMINATED      8      4   0.0474427                8        1            42.7164   2.32709       0.0968 |\n| train_cifar_d6d9a_00009   TERMINATED    256     32   0.000463253             16       10           163.404    1.20364       0.5709 |\n+------------------------------------------------------------------------------------------------------------------------------------+\n(func pid=4878) [8,  6000] loss: 0.371\n(func pid=4878) [8,  8000] loss: 0.271\n(func pid=4878) [8, 10000] loss: 0.223\n(func pid=4878) [8, 12000] loss: 0.185\n(func pid=4878) [8, 14000] loss: 0.159\n(func pid=4878) [8, 16000] loss: 0.136\nTrial status: 9 TERMINATED | 1 RUNNING\nCurrent time: 2025-05-23 19:49:54. Total running time: 9min 0s\nLogical resource usage: 2.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:A10G)\n+------------------------------------------------------------------------------------------------------------------------------------+\n| Trial name                status         l1     l2            lr     batch_size     iter     total time (s)      loss     accuracy |\n+------------------------------------------------------------------------------------------------------------------------------------+\n| train_cifar_d6d9a_00004   RUNNING        16     32   0.000239293              2        7           494.595    1.14006       0.5966 |\n| train_cifar_d6d9a_00000   TERMINATED      8     32   0.0665564                2        1           117.324    2.42625       0.0959 |\n| train_cifar_d6d9a_00001   TERMINATED     32    256   0.00750825               8        8           232.752    1.63678       0.431  |\n| train_cifar_d6d9a_00002   TERMINATED      1      4   0.0100663                4        1            73.3948   2.31102       0.1011 |\n| train_cifar_d6d9a_00003   TERMINATED    256      8   0.0262429               16        4            94.103    1.8152        0.3195 |\n| train_cifar_d6d9a_00005   TERMINATED     16     64   0.000370892             16       10           182.633    1.41427       0.4854 |\n| train_cifar_d6d9a_00006   TERMINATED     16      1   0.000174684             16        1            25.9218   2.3221        0.1037 |\n| train_cifar_d6d9a_00007   TERMINATED     32     32   0.0208137                8        2            82.9934   2.10149       0.1919 |\n| train_cifar_d6d9a_00008   TERMINATED      8      4   0.0474427                8        1            42.7164   2.32709       0.0968 |\n| train_cifar_d6d9a_00009   TERMINATED    256     32   0.000463253             16       10           163.404    1.20364       0.5709 |\n+------------------------------------------------------------------------------------------------------------------------------------+\n(func pid=4878) [8, 18000] loss: 0.123\n(func pid=4878) [8, 20000] loss: 0.107\n\nTrial train_cifar_d6d9a_00004 finished iteration 8 at 2025-05-23 19:50:10. Total running time: 9min 17s\n+------------------------------------------------------------+\n| Trial train_cifar_d6d9a_00004 result                       |\n+------------------------------------------------------------+\n| checkpoint_dir_name                      checkpoint_000007 |\n| time_this_iter_s                                  57.74085 |\n| time_total_s                                     552.33544 |\n| training_iteration                                       8 |\n| accuracy                                             0.599 |\n| loss                                               1.14506 |\n+------------------------------------------------------------+\nTrial train_cifar_d6d9a_00004 saved a checkpoint for iteration 8 at: (local)/var/lib/ci-user/ray_results/train_cifar_2025-05-23_19-40-53/train_cifar_d6d9a_00004_4_batch_size=2,l1=16,l2=32,lr=0.0002_2025-05-23_19-40-54/checkpoint_000007\n(func pid=4878) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2025-05-23_19-40-53/train_cifar_d6d9a_00004_4_batch_size=2,l1=16,l2=32,lr=0.0002_2025-05-23_19-40-54/checkpoint_000007)\n(func pid=4878) [9,  2000] loss: 1.083\n(func pid=4878) [9,  4000] loss: 0.527\n\nTrial status: 9 TERMINATED | 1 RUNNING\nCurrent time: 2025-05-23 19:50:24. Total running time: 9min 30s\nLogical resource usage: 2.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:A10G)\n+------------------------------------------------------------------------------------------------------------------------------------+\n| Trial name                status         l1     l2            lr     batch_size     iter     total time (s)      loss     accuracy |\n+------------------------------------------------------------------------------------------------------------------------------------+\n| train_cifar_d6d9a_00004   RUNNING        16     32   0.000239293              2        8           552.335    1.14506       0.599  |\n| train_cifar_d6d9a_00000   TERMINATED      8     32   0.0665564                2        1           117.324    2.42625       0.0959 |\n| train_cifar_d6d9a_00001   TERMINATED     32    256   0.00750825               8        8           232.752    1.63678       0.431  |\n| train_cifar_d6d9a_00002   TERMINATED      1      4   0.0100663                4        1            73.3948   2.31102       0.1011 |\n| train_cifar_d6d9a_00003   TERMINATED    256      8   0.0262429               16        4            94.103    1.8152        0.3195 |\n| train_cifar_d6d9a_00005   TERMINATED     16     64   0.000370892             16       10           182.633    1.41427       0.4854 |\n| train_cifar_d6d9a_00006   TERMINATED     16      1   0.000174684             16        1            25.9218   2.3221        0.1037 |\n| train_cifar_d6d9a_00007   TERMINATED     32     32   0.0208137                8        2            82.9934   2.10149       0.1919 |\n| train_cifar_d6d9a_00008   TERMINATED      8      4   0.0474427                8        1            42.7164   2.32709       0.0968 |\n| train_cifar_d6d9a_00009   TERMINATED    256     32   0.000463253             16       10           163.404    1.20364       0.5709 |\n+------------------------------------------------------------------------------------------------------------------------------------+\n(func pid=4878) [9,  6000] loss: 0.358\n(func pid=4878) [9,  8000] loss: 0.270\n(func pid=4878) [9, 10000] loss: 0.210\n(func pid=4878) [9, 12000] loss: 0.179\n(func pid=4878) [9, 14000] loss: 0.152\n(func pid=4878) [9, 16000] loss: 0.137\nTrial status: 9 TERMINATED | 1 RUNNING\nCurrent time: 2025-05-23 19:50:54. Total running time: 10min 0s\nLogical resource usage: 2.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:A10G)\n+------------------------------------------------------------------------------------------------------------------------------------+\n| Trial name                status         l1     l2            lr     batch_size     iter     total time (s)      loss     accuracy |\n+------------------------------------------------------------------------------------------------------------------------------------+\n| train_cifar_d6d9a_00004   RUNNING        16     32   0.000239293              2        8           552.335    1.14506       0.599  |\n| train_cifar_d6d9a_00000   TERMINATED      8     32   0.0665564                2        1           117.324    2.42625       0.0959 |\n| train_cifar_d6d9a_00001   TERMINATED     32    256   0.00750825               8        8           232.752    1.63678       0.431  |\n| train_cifar_d6d9a_00002   TERMINATED      1      4   0.0100663                4        1            73.3948   2.31102       0.1011 |\n| train_cifar_d6d9a_00003   TERMINATED    256      8   0.0262429               16        4            94.103    1.8152        0.3195 |\n| train_cifar_d6d9a_00005   TERMINATED     16     64   0.000370892             16       10           182.633    1.41427       0.4854 |\n| train_cifar_d6d9a_00006   TERMINATED     16      1   0.000174684             16        1            25.9218   2.3221        0.1037 |\n| train_cifar_d6d9a_00007   TERMINATED     32     32   0.0208137                8        2            82.9934   2.10149       0.1919 |\n| train_cifar_d6d9a_00008   TERMINATED      8      4   0.0474427                8        1            42.7164   2.32709       0.0968 |\n| train_cifar_d6d9a_00009   TERMINATED    256     32   0.000463253             16       10           163.404    1.20364       0.5709 |\n+------------------------------------------------------------------------------------------------------------------------------------+\n(func pid=4878) [9, 18000] loss: 0.122\n(func pid=4878) [9, 20000] loss: 0.109\n\nTrial train_cifar_d6d9a_00004 finished iteration 9 at 2025-05-23 19:51:08. Total running time: 10min 14s\n+------------------------------------------------------------+\n| Trial train_cifar_d6d9a_00004 result                       |\n+------------------------------------------------------------+\n| checkpoint_dir_name                      checkpoint_000008 |\n| time_this_iter_s                                  57.84528 |\n| time_total_s                                     610.18073 |\n| training_iteration                                       9 |\n| accuracy                                            0.6064 |\n| loss                                               1.11917 |\n+------------------------------------------------------------+\nTrial train_cifar_d6d9a_00004 saved a checkpoint for iteration 9 at: (local)/var/lib/ci-user/ray_results/train_cifar_2025-05-23_19-40-53/train_cifar_d6d9a_00004_4_batch_size=2,l1=16,l2=32,lr=0.0002_2025-05-23_19-40-54/checkpoint_000008\n(func pid=4878) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2025-05-23_19-40-53/train_cifar_d6d9a_00004_4_batch_size=2,l1=16,l2=32,lr=0.0002_2025-05-23_19-40-54/checkpoint_000008)\n(func pid=4878) [10,  2000] loss: 1.060\n(func pid=4878) [10,  4000] loss: 0.523\n(func pid=4878) [10,  6000] loss: 0.355\n\nTrial status: 9 TERMINATED | 1 RUNNING\nCurrent time: 2025-05-23 19:51:25. Total running time: 10min 31s\nLogical resource usage: 2.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:A10G)\n+------------------------------------------------------------------------------------------------------------------------------------+\n| Trial name                status         l1     l2            lr     batch_size     iter     total time (s)      loss     accuracy |\n+------------------------------------------------------------------------------------------------------------------------------------+\n| train_cifar_d6d9a_00004   RUNNING        16     32   0.000239293              2        9           610.181    1.11917       0.6064 |\n| train_cifar_d6d9a_00000   TERMINATED      8     32   0.0665564                2        1           117.324    2.42625       0.0959 |\n| train_cifar_d6d9a_00001   TERMINATED     32    256   0.00750825               8        8           232.752    1.63678       0.431  |\n| train_cifar_d6d9a_00002   TERMINATED      1      4   0.0100663                4        1            73.3948   2.31102       0.1011 |\n| train_cifar_d6d9a_00003   TERMINATED    256      8   0.0262429               16        4            94.103    1.8152        0.3195 |\n| train_cifar_d6d9a_00005   TERMINATED     16     64   0.000370892             16       10           182.633    1.41427       0.4854 |\n| train_cifar_d6d9a_00006   TERMINATED     16      1   0.000174684             16        1            25.9218   2.3221        0.1037 |\n| train_cifar_d6d9a_00007   TERMINATED     32     32   0.0208137                8        2            82.9934   2.10149       0.1919 |\n| train_cifar_d6d9a_00008   TERMINATED      8      4   0.0474427                8        1            42.7164   2.32709       0.0968 |\n| train_cifar_d6d9a_00009   TERMINATED    256     32   0.000463253             16       10           163.404    1.20364       0.5709 |\n+------------------------------------------------------------------------------------------------------------------------------------+\n(func pid=4878) [10,  8000] loss: 0.264\n(func pid=4878) [10, 10000] loss: 0.213\n(func pid=4878) [10, 12000] loss: 0.176\n(func pid=4878) [10, 14000] loss: 0.150\n(func pid=4878) [10, 16000] loss: 0.135\n(func pid=4878) [10, 18000] loss: 0.120\nTrial status: 9 TERMINATED | 1 RUNNING\nCurrent time: 2025-05-23 19:51:55. Total running time: 11min 1s\nLogical resource usage: 2.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:A10G)\n+------------------------------------------------------------------------------------------------------------------------------------+\n| Trial name                status         l1     l2            lr     batch_size     iter     total time (s)      loss     accuracy |\n+------------------------------------------------------------------------------------------------------------------------------------+\n| train_cifar_d6d9a_00004   RUNNING        16     32   0.000239293              2        9           610.181    1.11917       0.6064 |\n| train_cifar_d6d9a_00000   TERMINATED      8     32   0.0665564                2        1           117.324    2.42625       0.0959 |\n| train_cifar_d6d9a_00001   TERMINATED     32    256   0.00750825               8        8           232.752    1.63678       0.431  |\n| train_cifar_d6d9a_00002   TERMINATED      1      4   0.0100663                4        1            73.3948   2.31102       0.1011 |\n| train_cifar_d6d9a_00003   TERMINATED    256      8   0.0262429               16        4            94.103    1.8152        0.3195 |\n| train_cifar_d6d9a_00005   TERMINATED     16     64   0.000370892             16       10           182.633    1.41427       0.4854 |\n| train_cifar_d6d9a_00006   TERMINATED     16      1   0.000174684             16        1            25.9218   2.3221        0.1037 |\n| train_cifar_d6d9a_00007   TERMINATED     32     32   0.0208137                8        2            82.9934   2.10149       0.1919 |\n| train_cifar_d6d9a_00008   TERMINATED      8      4   0.0474427                8        1            42.7164   2.32709       0.0968 |\n| train_cifar_d6d9a_00009   TERMINATED    256     32   0.000463253             16       10           163.404    1.20364       0.5709 |\n+------------------------------------------------------------------------------------------------------------------------------------+\n(func pid=4878) [10, 20000] loss: 0.105\n\nTrial train_cifar_d6d9a_00004 finished iteration 10 at 2025-05-23 19:52:06. Total running time: 11min 12s\n+------------------------------------------------------------+\n| Trial train_cifar_d6d9a_00004 result                       |\n+------------------------------------------------------------+\n| checkpoint_dir_name                      checkpoint_000009 |\n| time_this_iter_s                                  57.39726 |\n| time_total_s                                     667.57798 |\n| training_iteration                                      10 |\n| accuracy                                            0.6078 |\n| loss                                               1.12686 |\n+------------------------------------------------------------+\nTrial train_cifar_d6d9a_00004 saved a checkpoint for iteration 10 at: (local)/var/lib/ci-user/ray_results/train_cifar_2025-05-23_19-40-53/train_cifar_d6d9a_00004_4_batch_size=2,l1=16,l2=32,lr=0.0002_2025-05-23_19-40-54/checkpoint_000009\n\nTrial train_cifar_d6d9a_00004 completed after 10 iterations at 2025-05-23 19:52:06. Total running time: 11min 12s\n\nTrial status: 10 TERMINATED\nCurrent time: 2025-05-23 19:52:06. Total running time: 11min 12s\nLogical resource usage: 2.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:A10G)\n+------------------------------------------------------------------------------------------------------------------------------------+\n| Trial name                status         l1     l2            lr     batch_size     iter     total time (s)      loss     accuracy |\n+------------------------------------------------------------------------------------------------------------------------------------+\n| train_cifar_d6d9a_00000   TERMINATED      8     32   0.0665564                2        1           117.324    2.42625       0.0959 |\n| train_cifar_d6d9a_00001   TERMINATED     32    256   0.00750825               8        8           232.752    1.63678       0.431  |\n| train_cifar_d6d9a_00002   TERMINATED      1      4   0.0100663                4        1            73.3948   2.31102       0.1011 |\n| train_cifar_d6d9a_00003   TERMINATED    256      8   0.0262429               16        4            94.103    1.8152        0.3195 |\n| train_cifar_d6d9a_00004   TERMINATED     16     32   0.000239293              2       10           667.578    1.12686       0.6078 |\n| train_cifar_d6d9a_00005   TERMINATED     16     64   0.000370892             16       10           182.633    1.41427       0.4854 |\n| train_cifar_d6d9a_00006   TERMINATED     16      1   0.000174684             16        1            25.9218   2.3221        0.1037 |\n| train_cifar_d6d9a_00007   TERMINATED     32     32   0.0208137                8        2            82.9934   2.10149       0.1919 |\n| train_cifar_d6d9a_00008   TERMINATED      8      4   0.0474427                8        1            42.7164   2.32709       0.0968 |\n| train_cifar_d6d9a_00009   TERMINATED    256     32   0.000463253             16       10           163.404    1.20364       0.5709 |\n+------------------------------------------------------------------------------------------------------------------------------------+\n\n(func pid=4878) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/var/lib/ci-user/ray_results/train_cifar_2025-05-23_19-40-53/train_cifar_d6d9a_00004_4_batch_size=2,l1=16,l2=32,lr=0.0002_2025-05-23_19-40-54/checkpoint_000009)\nBest trial config: {'l1': 16, 'l2': 32, 'lr': 0.00023929252794779862, 'batch_size': 2}\nBest trial final validation loss: 1.1268595749788917\nBest trial final validation accuracy: 0.6078\nBest trial test set accuracy: 0.5987"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/beginner/knowledge_distillation_tutorial.html",
    "title": "Knowledge Distillation Tutorial\u00b6",
    "code_snippets": [
      "import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.transforms as transforms\nimport torchvision.datasets as datasets\n\n# Check if the current `accelerator <https://pytorch.org/docs/stable/torch.html#accelerators>`__\n# is available, and if not, use the CPU\ndevice = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\nprint(f\"Using {device} device\")",
      "#from torch.utils.data import Subset\n#num_images_to_keep = 2000\n#train_dataset = Subset(train_dataset, range(min(num_images_to_keep, 50_000)))\n#test_dataset = Subset(test_dataset, range(min(num_images_to_keep, 10_000)))",
      "#Dataloaders\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=2)\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=2)",
      "# Deeper neural network class to be used as teacher:\nclass DeepNN(nn.Module):\n    def __init__(self, num_classes=10):\n        super(DeepNN, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 128, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(64, 32, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n        )\n        self.classifier = nn.Sequential(\n            nn.Linear(2048, 512),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(512, num_classes)\n        )\n\n    def forward(self, x):\n        x = self.features(x)\n        x = torch.flatten(x, 1)\n        x = self.classifier(x)\n        return x\n\n# Lightweight neural network class to be used as student:\nclass LightNN(nn.Module):\n    def __init__(self, num_classes=10):\n        super(LightNN, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            nn.Conv2d(16, 16, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n        )\n        self.classifier = nn.Sequential(\n            nn.Linear(1024, 256),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(256, num_classes)\n        )\n\n    def forward(self, x):\n        x = self.features(x)\n        x = torch.flatten(x, 1)\n        x = self.classifier(x)\n        return x",
      "def train(model, train_loader, epochs, learning_rate, device):\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n    model.train()\n\n    for epoch in range(epochs):\n        running_loss = 0.0\n        for inputs, labels in train_loader:\n            # inputs: A collection of batch_size images\n            # labels: A vector of dimensionality batch_size with integers denoting class of each image\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            optimizer.zero_grad()\n            outputs = model(inputs)\n\n            # outputs: Output of the network for the collection of images. A tensor of dimensionality batch_size x num_classes\n            # labels: The actual labels of the images. Vector of dimensionality batch_size\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item()\n\n        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader)}\")\n\ndef test(model, test_loader, device):\n    model.to(device)\n    model.eval()\n\n    correct = 0\n    total = 0\n\n    with torch.no_grad():\n        for inputs, labels in test_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            outputs = model(inputs)\n            _, predicted = torch.max(outputs.data, 1)\n\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n    accuracy = 100 * correct / total\n    print(f\"Test Accuracy: {accuracy:.2f}%\")\n    return accuracy",
      "torch.manual_seed(42)\nnn_deep = DeepNN(num_classes=10).to(device)\ntrain(nn_deep, train_loader, epochs=10, learning_rate=0.001, device=device)\ntest_accuracy_deep = test(nn_deep, test_loader, device)\n\n# Instantiate the lightweight network:\ntorch.manual_seed(42)\nnn_light = LightNN(num_classes=10).to(device)",
      "torch.manual_seed(42)\nnew_nn_light = LightNN(num_classes=10).to(device)",
      "# Print the norm of the first layer of the initial lightweight model\nprint(\"Norm of 1st layer of nn_light:\", torch.norm(nn_light.features[0].weight).item())\n# Print the norm of the first layer of the new lightweight model\nprint(\"Norm of 1st layer of new_nn_light:\", torch.norm(new_nn_light.features[0].weight).item())",
      "def train_knowledge_distillation(teacher, student, train_loader, epochs, learning_rate, T, soft_target_loss_weight, ce_loss_weight, device):\n    ce_loss = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(student.parameters(), lr=learning_rate)\n\n    teacher.eval()  # Teacher set to evaluation mode\n    student.train() # Student to train mode\n\n    for epoch in range(epochs):\n        running_loss = 0.0\n        for inputs, labels in train_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            optimizer.zero_grad()\n\n            # Forward pass with the teacher model - do not save gradients here as we do not change the teacher's weights\n            with torch.no_grad():\n                teacher_logits = teacher(inputs)\n\n            # Forward pass with the student model\n            student_logits = student(inputs)\n\n            #Soften the student logits by applying softmax first and log() second\n            soft_targets = nn.functional.softmax(teacher_logits / T, dim=-1)\n            soft_prob = nn.functional.log_softmax(student_logits / T, dim=-1)\n\n            # Calculate the soft targets loss. Scaled by T**2 as suggested by the authors of the paper \"Distilling the knowledge in a neural network\"\n            soft_targets_loss = torch.sum(soft_targets * (soft_targets.log() - soft_prob)) / soft_prob.size()[0] * (T**2)\n\n            # Calculate the true label loss\n            label_loss = ce_loss(student_logits, labels)\n\n            # Weighted sum of the two losses\n            loss = soft_target_loss_weight * soft_targets_loss + ce_loss_weight * label_loss\n\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item()\n\n        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader)}\")\n\n# Apply ``train_knowledge_distillation`` with a temperature of 2. Arbitrarily set the weights to 0.75 for CE and 0.25 for distillation loss.\ntrain_knowledge_distillation(teacher=nn_deep, student=new_nn_light, train_loader=train_loader, epochs=10, learning_rate=0.001, T=2, soft_target_loss_weight=0.25, ce_loss_weight=0.75, device=device)\ntest_accuracy_light_ce_and_kd = test(new_nn_light, test_loader, device)\n\n# Compare the student test accuracy with and without the teacher, after distillation\nprint(f\"Teacher accuracy: {test_accuracy_deep:.2f}%\")\nprint(f\"Student accuracy without teacher: {test_accuracy_light_ce:.2f}%\")\nprint(f\"Student accuracy with CE + KD: {test_accuracy_light_ce_and_kd:.2f}%\")",
      "class ModifiedDeepNNCosine(nn.Module):\n    def __init__(self, num_classes=10):\n        super(ModifiedDeepNNCosine, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 128, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(64, 32, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n        )\n        self.classifier = nn.Sequential(\n            nn.Linear(2048, 512),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(512, num_classes)\n        )\n\n    def forward(self, x):\n        x = self.features(x)\n        flattened_conv_output = torch.flatten(x, 1)\n        x = self.classifier(flattened_conv_output)\n        flattened_conv_output_after_pooling = torch.nn.functional.avg_pool1d(flattened_conv_output, 2)\n        return x, flattened_conv_output_after_pooling\n\n# Create a similar student class where we return a tuple. We do not apply pooling after flattening.\nclass ModifiedLightNNCosine(nn.Module):\n    def __init__(self, num_classes=10):\n        super(ModifiedLightNNCosine, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            nn.Conv2d(16, 16, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n        )\n        self.classifier = nn.Sequential(\n            nn.Linear(1024, 256),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(256, num_classes)\n        )\n\n    def forward(self, x):\n        x = self.features(x)\n        flattened_conv_output = torch.flatten(x, 1)\n        x = self.classifier(flattened_conv_output)\n        return x, flattened_conv_output\n\n# We do not have to train the modified deep network from scratch of course, we just load its weights from the trained instance\nmodified_nn_deep = ModifiedDeepNNCosine(num_classes=10).to(device)\nmodified_nn_deep.load_state_dict(nn_deep.state_dict())\n\n# Once again ensure the norm of the first layer is the same for both networks\nprint(\"Norm of 1st layer for deep_nn:\", torch.norm(nn_deep.features[0].weight).item())\nprint(\"Norm of 1st layer for modified_deep_nn:\", torch.norm(modified_nn_deep.features[0].weight).item())\n\n# Initialize a modified lightweight network with the same seed as our other lightweight instances. This will be trained from scratch to examine the effectiveness of cosine loss minimization.\ntorch.manual_seed(42)\nmodified_nn_light = ModifiedLightNNCosine(num_classes=10).to(device)\nprint(\"Norm of 1st layer:\", torch.norm(modified_nn_light.features[0].weight).item())",
      "# Create a sample input tensor\nsample_input = torch.randn(128, 3, 32, 32).to(device) # Batch size: 128, Filters: 3, Image size: 32x32\n\n# Pass the input through the student\nlogits, hidden_representation = modified_nn_light(sample_input)\n\n# Print the shapes of the tensors\nprint(\"Student logits shape:\", logits.shape) # batch_size x total_classes\nprint(\"Student hidden representation shape:\", hidden_representation.shape) # batch_size x hidden_representation_size\n\n# Pass the input through the teacher\nlogits, hidden_representation = modified_nn_deep(sample_input)\n\n# Print the shapes of the tensors\nprint(\"Teacher logits shape:\", logits.shape) # batch_size x total_classes\nprint(\"Teacher hidden representation shape:\", hidden_representation.shape) # batch_size x hidden_representation_size",
      "Student logits shape: torch.Size([128, 10])\nStudent hidden representation shape: torch.Size([128, 1024])\nTeacher logits shape: torch.Size([128, 10])\nTeacher hidden representation shape: torch.Size([128, 1024])",
      "def train_cosine_loss(teacher, student, train_loader, epochs, learning_rate, hidden_rep_loss_weight, ce_loss_weight, device):\n    ce_loss = nn.CrossEntropyLoss()\n    cosine_loss = nn.CosineEmbeddingLoss()\n    optimizer = optim.Adam(student.parameters(), lr=learning_rate)\n\n    teacher.to(device)\n    student.to(device)\n    teacher.eval()  # Teacher set to evaluation mode\n    student.train() # Student to train mode\n\n    for epoch in range(epochs):\n        running_loss = 0.0\n        for inputs, labels in train_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            optimizer.zero_grad()\n\n            # Forward pass with the teacher model and keep only the hidden representation\n            with torch.no_grad():\n                _, teacher_hidden_representation = teacher(inputs)\n\n            # Forward pass with the student model\n            student_logits, student_hidden_representation = student(inputs)\n\n            # Calculate the cosine loss. Target is a vector of ones. From the loss formula above we can see that is the case where loss minimization leads to cosine similarity increase.\n            hidden_rep_loss = cosine_loss(student_hidden_representation, teacher_hidden_representation, target=torch.ones(inputs.size(0)).to(device))\n\n            # Calculate the true label loss\n            label_loss = ce_loss(student_logits, labels)\n\n            # Weighted sum of the two losses\n            loss = hidden_rep_loss_weight * hidden_rep_loss + ce_loss_weight * label_loss\n\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item()\n\n        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader)}\")",
      "def test_multiple_outputs(model, test_loader, device):\n    model.to(device)\n    model.eval()\n\n    correct = 0\n    total = 0\n\n    with torch.no_grad():\n        for inputs, labels in test_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            outputs, _ = model(inputs) # Disregard the second tensor of the tuple\n            _, predicted = torch.max(outputs.data, 1)\n\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n    accuracy = 100 * correct / total\n    print(f\"Test Accuracy: {accuracy:.2f}%\")\n    return accuracy",
      "Student's feature extractor output shape:  torch.Size([128, 16, 8, 8])\nTeacher's feature extractor output shape:  torch.Size([128, 32, 8, 8])",
      "class ModifiedDeepNNRegressor(nn.Module):\n    def __init__(self, num_classes=10):\n        super(ModifiedDeepNNRegressor, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 128, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(64, 32, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n        )\n        self.classifier = nn.Sequential(\n            nn.Linear(2048, 512),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(512, num_classes)\n        )\n\n    def forward(self, x):\n        x = self.features(x)\n        conv_feature_map = x\n        x = torch.flatten(x, 1)\n        x = self.classifier(x)\n        return x, conv_feature_map\n\nclass ModifiedLightNNRegressor(nn.Module):\n    def __init__(self, num_classes=10):\n        super(ModifiedLightNNRegressor, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            nn.Conv2d(16, 16, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n        )\n        # Include an extra regressor (in our case linear)\n        self.regressor = nn.Sequential(\n            nn.Conv2d(16, 32, kernel_size=3, padding=1)\n        )\n        self.classifier = nn.Sequential(\n            nn.Linear(1024, 256),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(256, num_classes)\n        )\n\n    def forward(self, x):\n        x = self.features(x)\n        regressor_output = self.regressor(x)\n        x = torch.flatten(x, 1)\n        x = self.classifier(x)\n        return x, regressor_output",
      "def train_mse_loss(teacher, student, train_loader, epochs, learning_rate, feature_map_weight, ce_loss_weight, device):\n    ce_loss = nn.CrossEntropyLoss()\n    mse_loss = nn.MSELoss()\n    optimizer = optim.Adam(student.parameters(), lr=learning_rate)\n\n    teacher.to(device)\n    student.to(device)\n    teacher.eval()  # Teacher set to evaluation mode\n    student.train() # Student to train mode\n\n    for epoch in range(epochs):\n        running_loss = 0.0\n        for inputs, labels in train_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            optimizer.zero_grad()\n\n            # Again ignore teacher logits\n            with torch.no_grad():\n                _, teacher_feature_map = teacher(inputs)\n\n            # Forward pass with the student model\n            student_logits, regressor_feature_map = student(inputs)\n\n            # Calculate the loss\n            hidden_rep_loss = mse_loss(regressor_feature_map, teacher_feature_map)\n\n            # Calculate the true label loss\n            label_loss = ce_loss(student_logits, labels)\n\n            # Weighted sum of the two losses\n            loss = feature_map_weight * hidden_rep_loss + ce_loss_weight * label_loss\n\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item()\n\n        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader)}\")\n\n# Notice how our test function remains the same here with the one we used in our previous case. We only care about the actual outputs because we measure accuracy.\n\n# Initialize a ModifiedLightNNRegressor\ntorch.manual_seed(42)\nmodified_nn_light_reg = ModifiedLightNNRegressor(num_classes=10).to(device)\n\n# We do not have to train the modified deep network from scratch of course, we just load its weights from the trained instance\nmodified_nn_deep_reg = ModifiedDeepNNRegressor(num_classes=10).to(device)\nmodified_nn_deep_reg.load_state_dict(nn_deep.state_dict())\n\n# Train and test once again\ntrain_mse_loss(teacher=modified_nn_deep_reg, student=modified_nn_light_reg, train_loader=train_loader, epochs=10, learning_rate=0.001, feature_map_weight=0.25, ce_loss_weight=0.75, device=device)\ntest_accuracy_light_ce_and_mse_loss = test_multiple_outputs(modified_nn_light_reg, test_loader, device)"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/beginner/flava_finetuning_tutorial.html",
    "title": "TorchMultimodal Tutorial: Finetuning FLAVA\u00b6",
    "code_snippets": [
      "with open(\"data/vocabs/answers_textvqa_more_than_1.txt\") as f:\n  vocab = f.readlines()\n\nanswer_to_idx = {}\nfor idx, entry in enumerate(vocab):\n  answer_to_idx[entry.strip(\"\\n\")] = idx\nprint(len(vocab))\nprint(vocab[:5])\n\nfrom datasets import load_dataset\ndataset = load_dataset(\"textvqa\")",
      "import matplotlib.pyplot as plt\nimport numpy as np\nidx = 5\nprint(\"Question: \", dataset[\"train\"][idx][\"question\"])\nprint(\"Answers: \" ,dataset[\"train\"][idx][\"answers\"])\nim = np.asarray(dataset[\"train\"][idx][\"image\"].resize((500,500)))\nplt.imshow(im)\nplt.show()",
      "import torch\nfrom torchvision import transforms\nfrom collections import defaultdict\nfrom transformers import BertTokenizer\nfrom functools import partial\n\ndef transform(tokenizer, input):\n  batch = {}\n  image_transform = transforms.Compose([transforms.ToTensor(), transforms.Resize([224,224])])\n  image = image_transform(input[\"image\"][0].convert(\"RGB\"))\n  batch[\"image\"] = [image]\n\n  tokenized=tokenizer(input[\"question\"],return_tensors='pt',padding=\"max_length\",max_length=512)\n  batch.update(tokenized)\n\n\n  ans_to_count = defaultdict(int)\n  for ans in input[\"answers\"][0]:\n    ans_to_count[ans] += 1\n  max_value = max(ans_to_count, key=ans_to_count.get)\n  ans_idx = answer_to_idx.get(max_value,0)\n  batch[\"answers\"] = torch.as_tensor([ans_idx])\n  return batch\n\ntokenizer=BertTokenizer.from_pretrained(\"bert-base-uncased\",padding=\"max_length\",max_length=512)\ntransform=partial(transform,tokenizer)\ndataset.set_transform(transform)",
      "from torchmultimodal.models.flava.model import flava_model_for_classification\nmodel = flava_model_for_classification(num_classes=len(vocab))",
      "from torch import nn\nBATCH_SIZE = 2\nMAX_STEPS = 3\nfrom torch.utils.data import DataLoader\n\ntrain_dataloader = DataLoader(dataset[\"train\"], batch_size= BATCH_SIZE)\noptimizer = torch.optim.AdamW(model.parameters())\n\n\nepochs = 1\nfor _ in range(epochs):\n  for idx, batch in enumerate(train_dataloader):\n    optimizer.zero_grad()\n    out = model(text = batch[\"input_ids\"], image = batch[\"image\"], labels = batch[\"answers\"])\n    loss = out.loss\n    loss.backward()\n    optimizer.step()\n    print(f\"Loss at step {idx} = {loss}\")\n    if idx >= MAX_STEPS-1:\n      break"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/intermediate/torchserve_with_ipex.html",
    "title": "Grokking PyTorch Intel CPU performance from first principles\u00b6",
    "code_snippets": [
      "import torch\nimport torchvision.models as models\nimport time\n\nmodel = models.resnet50(pretrained=False)\nmodel.eval()\ndata = torch.rand(1, 3, 224, 224)\n\n# warm up\nfor _ in range(100):\n    model(data)\n\nstart = time.time()\nfor _ in range(100):\n    model(data)\nend = time.time()\nprint('Inference took {:.2f} ms in average'.format((end-start)/100*1000))",
      "torch.set_num_threads(num_physical_cores/num_workers)"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/advanced/privateuseone.html",
    "title": "Facilitating New Backend Integration by PrivateUse1\u00b6",
    "code_snippets": [
      "class CumtomSeluFunction : public torch::autograd::Function<CumtomSeluFunction> {\n  // Implementation of selu kernel in new backend\n}\n\nat::Tensor wrapper_AutogradCumstom__selu(const at::Tensor & self) {\n  return CumtomSeluFunction::apply(self);\n}\n\nTORCH_LIBRARY_IMPL(aten, AutogradPrivateUse1, m) {\n  ...\n  m.impl(\"selu\", TORCH_FN(wrapper_AutogradCustom__selu));\n  ...\n}",
      "torch._register_device_module('npu', torch_npu.npu)",
      "torch.rand((2,2),device='npu:0')\ntorch.rand((2,2),device='privateuse1:0')",
      "torch.rename_privateuse1_backend(\"npu\")",
      "torch.rename_privateuse1_backend(\"npu\")\nunsupported_dtype = [torch.quint8]\ntorch.utils.generate_methods_for_privateuse1_backend(for_tensor=True, for_module=True, for_storage=True, unsupported_dtype=unsupported_dtype)",
      "torch.Tensor.npu()\ntorch.Tensor.is_npu\ntorch.Storage.npu()\ntorch.Storage.is_npu\n..."
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html",
    "title": "(beta) Static Quantization with Eager Mode in PyTorch\u00b6",
    "code_snippets": [
      "import os\nimport sys\nimport time\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\n\nimport torchvision\nfrom torchvision import datasets\nimport torchvision.transforms as transforms\n\n# Set up warnings\nimport warnings\nwarnings.filterwarnings(\n    action='ignore',\n    category=DeprecationWarning,\n    module=r'.*'\n)\nwarnings.filterwarnings(\n    action='default',\n    module=r'torch.ao.quantization'\n)\n\n# Specify random seed for repeatable results\ntorch.manual_seed(191009)",
      "from torch.ao.quantization import QuantStub, DeQuantStub\n\ndef _make_divisible(v, divisor, min_value=None):\n    \"\"\"\n    This function is taken from the original tf repo.\n    It ensures that all layers have a channel number that is divisible by 8\n    It can be seen here:\n    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n    :param v:\n    :param divisor:\n    :param min_value:\n    :return:\n    \"\"\"\n    if min_value is None:\n        min_value = divisor\n    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n    # Make sure that round down does not go down by more than 10%.\n    if new_v < 0.9 * v:\n        new_v += divisor\n    return new_v\n\n\nclass ConvBNReLU(nn.Sequential):\n    def __init__(self, in_planes, out_planes, kernel_size=3, stride=1, groups=1):\n        padding = (kernel_size - 1) // 2\n        super(ConvBNReLU, self).__init__(\n            nn.Conv2d(in_planes, out_planes, kernel_size, stride, padding, groups=groups, bias=False),\n            nn.BatchNorm2d(out_planes, momentum=0.1),\n            # Replace with ReLU\n            nn.ReLU(inplace=False)\n        )\n\n\nclass InvertedResidual(nn.Module):\n    def __init__(self, inp, oup, stride, expand_ratio):\n        super(InvertedResidual, self).__init__()\n        self.stride = stride\n        assert stride in [1, 2]\n\n        hidden_dim = int(round(inp * expand_ratio))\n        self.use_res_connect = self.stride == 1 and inp == oup\n\n        layers = []\n        if expand_ratio != 1:\n            # pw\n            layers.append(ConvBNReLU(inp, hidden_dim, kernel_size=1))\n        layers.extend([\n            # dw\n            ConvBNReLU(hidden_dim, hidden_dim, stride=stride, groups=hidden_dim),\n            # pw-linear\n            nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n            nn.BatchNorm2d(oup, momentum=0.1),\n        ])\n        self.conv = nn.Sequential(*layers)\n        # Replace torch.add with floatfunctional\n        self.skip_add = nn.quantized.FloatFunctional()\n\n    def forward(self, x):\n        if self.use_res_connect:\n            return self.skip_add.add(x, self.conv(x))\n        else:\n            return self.conv(x)\n\n\nclass MobileNetV2(nn.Module):\n    def __init__(self, num_classes=1000, width_mult=1.0, inverted_residual_setting=None, round_nearest=8):\n        \"\"\"\n        MobileNet V2 main class\n        Args:\n            num_classes (int): Number of classes\n            width_mult (float): Width multiplier - adjusts number of channels in each layer by this amount\n            inverted_residual_setting: Network structure\n            round_nearest (int): Round the number of channels in each layer to be a multiple of this number\n            Set to 1 to turn off rounding\n        \"\"\"\n        super(MobileNetV2, self).__init__()\n        block = InvertedResidual\n        input_channel = 32\n        last_channel = 1280\n\n        if inverted_residual_setting is None:\n            inverted_residual_setting = [\n                # t, c, n, s\n                [1, 16, 1, 1],\n                [6, 24, 2, 2],\n                [6, 32, 3, 2],\n                [6, 64, 4, 2],\n                [6, 96, 3, 1],\n                [6, 160, 3, 2],\n                [6, 320, 1, 1],\n            ]\n\n        # only check the first element, assuming user knows t,c,n,s are required\n        if len(inverted_residual_setting) == 0 or len(inverted_residual_setting[0]) != 4:\n            raise ValueError(\"inverted_residual_setting should be non-empty \"\n                             \"or a 4-element list, got {}\".format(inverted_residual_setting))\n\n        # building first layer\n        input_channel = _make_divisible(input_channel * width_mult, round_nearest)\n        self.last_channel = _make_divisible(last_channel * max(1.0, width_mult), round_nearest)\n        features = [ConvBNReLU(3, input_channel, stride=2)]\n        # building inverted residual blocks\n        for t, c, n, s in inverted_residual_setting:\n            output_channel = _make_divisible(c * width_mult, round_nearest)\n            for i in range(n):\n                stride = s if i == 0 else 1\n                features.append(block(input_channel, output_channel, stride, expand_ratio=t))\n                input_channel = output_channel\n        # building last several layers\n        features.append(ConvBNReLU(input_channel, self.last_channel, kernel_size=1))\n        # make it nn.Sequential\n        self.features = nn.Sequential(*features)\n        self.quant = QuantStub()\n        self.dequant = DeQuantStub()\n        # building classifier\n        self.classifier = nn.Sequential(\n            nn.Dropout(0.2),\n            nn.Linear(self.last_channel, num_classes),\n        )\n\n        # weight initialization\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out')\n                if m.bias is not None:\n                    nn.init.zeros_(m.bias)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.ones_(m.weight)\n                nn.init.zeros_(m.bias)\n            elif isinstance(m, nn.Linear):\n                nn.init.normal_(m.weight, 0, 0.01)\n                nn.init.zeros_(m.bias)\n\n    def forward(self, x):\n        x = self.quant(x)\n        x = self.features(x)\n        x = x.mean([2, 3])\n        x = self.classifier(x)\n        x = self.dequant(x)\n        return x\n\n    # Fuse Conv+BN and Conv+BN+Relu modules prior to quantization\n    # This operation does not change the numerics\n    def fuse_model(self, is_qat=False):\n        fuse_modules = torch.ao.quantization.fuse_modules_qat if is_qat else torch.ao.quantization.fuse_modules\n        for m in self.modules():\n            if type(m) == ConvBNReLU:\n                fuse_modules(m, ['0', '1', '2'], inplace=True)\n            if type(m) == InvertedResidual:\n                for idx in range(len(m.conv)):\n                    if type(m.conv[idx]) == nn.Conv2d:\n                        fuse_modules(m.conv, [str(idx), str(idx + 1)], inplace=True)",
      "class AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self, name, fmt=':f'):\n        self.name = name\n        self.fmt = fmt\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n    def __str__(self):\n        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n        return fmtstr.format(**self.__dict__)\n\n\ndef accuracy(output, target, topk=(1,)):\n    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n    with torch.no_grad():\n        maxk = max(topk)\n        batch_size = target.size(0)\n\n        _, pred = output.topk(maxk, 1, True, True)\n        pred = pred.t()\n        correct = pred.eq(target.view(1, -1).expand_as(pred))\n\n        res = []\n        for k in topk:\n            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n            res.append(correct_k.mul_(100.0 / batch_size))\n        return res\n\n\ndef evaluate(model, criterion, data_loader, neval_batches):\n    model.eval()\n    top1 = AverageMeter('Acc@1', ':6.2f')\n    top5 = AverageMeter('Acc@5', ':6.2f')\n    cnt = 0\n    with torch.no_grad():\n        for image, target in data_loader:\n            output = model(image)\n            loss = criterion(output, target)\n            cnt += 1\n            acc1, acc5 = accuracy(output, target, topk=(1, 5))\n            print('.', end = '')\n            top1.update(acc1[0], image.size(0))\n            top5.update(acc5[0], image.size(0))\n            if cnt >= neval_batches:\n                 return top1, top5\n\n    return top1, top5\n\ndef load_model(model_file):\n    model = MobileNetV2()\n    state_dict = torch.load(model_file, weights_only=True)\n    model.load_state_dict(state_dict)\n    model.to('cpu')\n    return model\n\ndef print_size_of_model(model):\n    torch.save(model.state_dict(), \"temp.p\")\n    print('Size (MB):', os.path.getsize(\"temp.p\")/1e6)\n    os.remove('temp.p')",
      "def prepare_data_loaders(data_path):\n    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                     std=[0.229, 0.224, 0.225])\n    dataset = torchvision.datasets.ImageNet(\n        data_path, split=\"train\", transform=transforms.Compose([\n            transforms.RandomResizedCrop(224),\n            transforms.RandomHorizontalFlip(),\n            transforms.ToTensor(),\n            normalize,\n        ]))\n    dataset_test = torchvision.datasets.ImageNet(\n        data_path, split=\"val\", transform=transforms.Compose([\n            transforms.Resize(256),\n            transforms.CenterCrop(224),\n            transforms.ToTensor(),\n            normalize,\n        ]))\n\n    train_sampler = torch.utils.data.RandomSampler(dataset)\n    test_sampler = torch.utils.data.SequentialSampler(dataset_test)\n\n    data_loader = torch.utils.data.DataLoader(\n        dataset, batch_size=train_batch_size,\n        sampler=train_sampler)\n\n    data_loader_test = torch.utils.data.DataLoader(\n        dataset_test, batch_size=eval_batch_size,\n        sampler=test_sampler)\n\n    return data_loader, data_loader_test",
      "num_eval_batches = 1000\n\nprint(\"Size of baseline model\")\nprint_size_of_model(float_model)\n\ntop1, top5 = evaluate(float_model, criterion, data_loader_test, neval_batches=num_eval_batches)\nprint('Evaluation accuracy on %d images, %2.2f'%(num_eval_batches * eval_batch_size, top1.avg))\ntorch.jit.save(torch.jit.script(float_model), saved_model_dir + scripted_float_model_file)",
      "num_calibration_batches = 32\n\nmyModel = load_model(saved_model_dir + float_model_file).to('cpu')\nmyModel.eval()\n\n# Fuse Conv, bn and relu\nmyModel.fuse_model()\n\n# Specify quantization configuration\n# Start with simple min/max range estimation and per-tensor quantization of weights\nmyModel.qconfig = torch.ao.quantization.default_qconfig\nprint(myModel.qconfig)\ntorch.ao.quantization.prepare(myModel, inplace=True)\n\n# Calibrate first\nprint('Post Training Quantization Prepare: Inserting Observers')\nprint('\\n Inverted Residual Block:After observer insertion \\n\\n', myModel.features[1].conv)\n\n# Calibrate with the training set\nevaluate(myModel, criterion, data_loader, neval_batches=num_calibration_batches)\nprint('Post Training Quantization: Calibration done')\n\n# Convert to quantized model\ntorch.ao.quantization.convert(myModel, inplace=True)\n# You may see a user warning about needing to calibrate the model. This warning can be safely ignored.\n# This warning occurs because not all modules are run in each model runs, so some\n# modules may not be calibrated.\nprint('Post Training Quantization: Convert done')\nprint('\\n Inverted Residual Block: After fusion and quantization, note fused modules: \\n\\n',myModel.features[1].conv)\n\nprint(\"Size of model after quantization\")\nprint_size_of_model(myModel)\n\ntop1, top5 = evaluate(myModel, criterion, data_loader_test, neval_batches=num_eval_batches)\nprint('Evaluation accuracy on %d images, %2.2f'%(num_eval_batches * eval_batch_size, top1.avg))",
      "per_channel_quantized_model = load_model(saved_model_dir + float_model_file)\nper_channel_quantized_model.eval()\nper_channel_quantized_model.fuse_model()\n# The old 'fbgemm' is still available but 'x86' is the recommended default.\nper_channel_quantized_model.qconfig = torch.ao.quantization.get_default_qconfig('x86')\nprint(per_channel_quantized_model.qconfig)\n\ntorch.ao.quantization.prepare(per_channel_quantized_model, inplace=True)\nevaluate(per_channel_quantized_model,criterion, data_loader, num_calibration_batches)\ntorch.ao.quantization.convert(per_channel_quantized_model, inplace=True)\ntop1, top5 = evaluate(per_channel_quantized_model, criterion, data_loader_test, neval_batches=num_eval_batches)\nprint('Evaluation accuracy on %d images, %2.2f'%(num_eval_batches * eval_batch_size, top1.avg))\ntorch.jit.save(torch.jit.script(per_channel_quantized_model), saved_model_dir + scripted_quantized_model_file)",
      "def train_one_epoch(model, criterion, optimizer, data_loader, device, ntrain_batches):\n    model.train()\n    top1 = AverageMeter('Acc@1', ':6.2f')\n    top5 = AverageMeter('Acc@5', ':6.2f')\n    avgloss = AverageMeter('Loss', '1.5f')\n\n    cnt = 0\n    for image, target in data_loader:\n        start_time = time.time()\n        print('.', end = '')\n        cnt += 1\n        image, target = image.to(device), target.to(device)\n        output = model(image)\n        loss = criterion(output, target)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        acc1, acc5 = accuracy(output, target, topk=(1, 5))\n        top1.update(acc1[0], image.size(0))\n        top5.update(acc5[0], image.size(0))\n        avgloss.update(loss, image.size(0))\n        if cnt >= ntrain_batches:\n            print('Loss', avgloss.avg)\n\n            print('Training: * Acc@1 {top1.avg:.3f} Acc@5 {top5.avg:.3f}'\n                  .format(top1=top1, top5=top5))\n            return\n\n    print('Full imagenet train set:  * Acc@1 {top1.global_avg:.3f} Acc@5 {top5.global_avg:.3f}'\n          .format(top1=top1, top5=top5))\n    return",
      "qat_model = load_model(saved_model_dir + float_model_file)\nqat_model.fuse_model(is_qat=True)\n\noptimizer = torch.optim.SGD(qat_model.parameters(), lr = 0.0001)\n# The old 'fbgemm' is still available but 'x86' is the recommended default.\nqat_model.qconfig = torch.ao.quantization.get_default_qat_qconfig('x86')",
      "torch.ao.quantization.prepare_qat(qat_model, inplace=True)\nprint('Inverted Residual Block: After preparation for QAT, note fake-quantization modules \\n',qat_model.features[1].conv)",
      "num_train_batches = 20\n\n# QAT takes time and one needs to train over a few epochs.\n# Train and check accuracy after each epoch\nfor nepoch in range(8):\n    train_one_epoch(qat_model, criterion, optimizer, data_loader, torch.device('cpu'), num_train_batches)\n    if nepoch > 3:\n        # Freeze quantizer parameters\n        qat_model.apply(torch.ao.quantization.disable_observer)\n    if nepoch > 2:\n        # Freeze batch norm mean and variance estimates\n        qat_model.apply(torch.nn.intrinsic.qat.freeze_bn_stats)\n\n    # Check the accuracy after each epoch\n    quantized_model = torch.ao.quantization.convert(qat_model.eval(), inplace=False)\n    quantized_model.eval()\n    top1, top5 = evaluate(quantized_model,criterion, data_loader_test, neval_batches=num_eval_batches)\n    print('Epoch %d :Evaluation accuracy on %d images, %2.2f'%(nepoch, num_eval_batches * eval_batch_size, top1.avg))",
      "def run_benchmark(model_file, img_loader):\n    elapsed = 0\n    model = torch.jit.load(model_file)\n    model.eval()\n    num_batches = 5\n    # Run the scripted model on a few batches of images\n    for i, (images, target) in enumerate(img_loader):\n        if i < num_batches:\n            start = time.time()\n            output = model(images)\n            end = time.time()\n            elapsed = elapsed + (end-start)\n        else:\n            break\n    num_images = images.size()[0] * num_batches\n\n    print('Elapsed time: %3.0f ms' % (elapsed/num_images*1000))\n    return elapsed\n\nrun_benchmark(saved_model_dir + scripted_float_model_file, data_loader_test)\n\nrun_benchmark(saved_model_dir + scripted_quantized_model_file, data_loader_test)"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/advanced/dynamic_quantization_tutorial.html",
    "title": "(beta) Dynamic Quantization on an LSTM Word Language Model\u00b6",
    "code_snippets": [
      "# imports\nimport os\nfrom io import open\nimport time\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F",
      "class LSTMModel(nn.Module):\n    \"\"\"Container module with an encoder, a recurrent module, and a decoder.\"\"\"\n\n    def __init__(self, ntoken, ninp, nhid, nlayers, dropout=0.5):\n        super(LSTMModel, self).__init__()\n        self.drop = nn.Dropout(dropout)\n        self.encoder = nn.Embedding(ntoken, ninp)\n        self.rnn = nn.LSTM(ninp, nhid, nlayers, dropout=dropout)\n        self.decoder = nn.Linear(nhid, ntoken)\n\n        self.init_weights()\n\n        self.nhid = nhid\n        self.nlayers = nlayers\n\n    def init_weights(self):\n        initrange = 0.1\n        self.encoder.weight.data.uniform_(-initrange, initrange)\n        self.decoder.bias.data.zero_()\n        self.decoder.weight.data.uniform_(-initrange, initrange)\n\n    def forward(self, input, hidden):\n        emb = self.drop(self.encoder(input))\n        output, hidden = self.rnn(emb, hidden)\n        output = self.drop(output)\n        decoded = self.decoder(output)\n        return decoded, hidden\n\n    def init_hidden(self, bsz):\n        weight = next(self.parameters())\n        return (weight.new_zeros(self.nlayers, bsz, self.nhid),\n                weight.new_zeros(self.nlayers, bsz, self.nhid))",
      "class Dictionary(object):\n    def __init__(self):\n        self.word2idx = {}\n        self.idx2word = []\n\n    def add_word(self, word):\n        if word not in self.word2idx:\n            self.idx2word.append(word)\n            self.word2idx[word] = len(self.idx2word) - 1\n        return self.word2idx[word]\n\n    def __len__(self):\n        return len(self.idx2word)\n\n\nclass Corpus(object):\n    def __init__(self, path):\n        self.dictionary = Dictionary()\n        self.train = self.tokenize(os.path.join(path, 'train.txt'))\n        self.valid = self.tokenize(os.path.join(path, 'valid.txt'))\n        self.test = self.tokenize(os.path.join(path, 'test.txt'))\n\n    def tokenize(self, path):\n        \"\"\"Tokenizes a text file.\"\"\"\n        assert os.path.exists(path)\n        # Add words to the dictionary\n        with open(path, 'r', encoding=\"utf8\") as f:\n            for line in f:\n                words = line.split() + ['<eos>']\n                for word in words:\n                    self.dictionary.add_word(word)\n\n        # Tokenize file content\n        with open(path, 'r', encoding=\"utf8\") as f:\n            idss = []\n            for line in f:\n                words = line.split() + ['<eos>']\n                ids = []\n                for word in words:\n                    ids.append(self.dictionary.word2idx[word])\n                idss.append(torch.tensor(ids).type(torch.int64))\n            ids = torch.cat(idss)\n\n        return ids\n\nmodel_data_filepath = 'data/'\n\ncorpus = Corpus(model_data_filepath + 'wikitext-2')",
      "ntokens = len(corpus.dictionary)\n\nmodel = LSTMModel(\n    ntoken = ntokens,\n    ninp = 512,\n    nhid = 256,\n    nlayers = 5,\n)\n\nmodel.load_state_dict(\n    torch.load(\n        model_data_filepath + 'word_language_model_quantize.pth',\n        map_location=torch.device('cpu'),\n        weights_only=True\n        )\n    )\n\nmodel.eval()\nprint(model)",
      "input_ = torch.randint(ntokens, (1, 1), dtype=torch.long)\nhidden = model.init_hidden(1)\ntemperature = 1.0\nnum_words = 1000\n\nwith open(model_data_filepath + 'out.txt', 'w') as outf:\n    with torch.no_grad():  # no tracking history\n        for i in range(num_words):\n            output, hidden = model(input_, hidden)\n            word_weights = output.squeeze().div(temperature).exp().cpu()\n            word_idx = torch.multinomial(word_weights, 1)[0]\n            input_.fill_(word_idx)\n\n            word = corpus.dictionary.idx2word[word_idx]\n\n            outf.write(str(word.encode('utf-8')) + ('\\n' if i % 20 == 19 else ' '))\n\n            if i % 100 == 0:\n                print('| Generated {}/{} words'.format(i, 1000))\n\nwith open(model_data_filepath + 'out.txt', 'r') as outf:\n    all_output = outf.read()\n    print(all_output)",
      "bptt = 25\ncriterion = nn.CrossEntropyLoss()\neval_batch_size = 1\n\n# create test data set\ndef batchify(data, bsz):\n    # Work out how cleanly we can divide the dataset into ``bsz`` parts.\n    nbatch = data.size(0) // bsz\n    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n    data = data.narrow(0, 0, nbatch * bsz)\n    # Evenly divide the data across the ``bsz`` batches.\n    return data.view(bsz, -1).t().contiguous()\n\ntest_data = batchify(corpus.test, eval_batch_size)\n\n# Evaluation functions\ndef get_batch(source, i):\n    seq_len = min(bptt, len(source) - 1 - i)\n    data = source[i:i+seq_len]\n    target = source[i+1:i+1+seq_len].reshape(-1)\n    return data, target\n\ndef repackage_hidden(h):\n  \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n\n  if isinstance(h, torch.Tensor):\n      return h.detach()\n  else:\n      return tuple(repackage_hidden(v) for v in h)\n\ndef evaluate(model_, data_source):\n    # Turn on evaluation mode which disables dropout.\n    model_.eval()\n    total_loss = 0.\n    hidden = model_.init_hidden(eval_batch_size)\n    with torch.no_grad():\n        for i in range(0, data_source.size(0) - 1, bptt):\n            data, targets = get_batch(data_source, i)\n            output, hidden = model_(data, hidden)\n            hidden = repackage_hidden(hidden)\n            output_flat = output.view(-1, ntokens)\n            total_loss += len(data) * criterion(output_flat, targets).item()\n    return total_loss / (len(data_source) - 1)",
      "import torch.quantization\n\nquantized_model = torch.quantization.quantize_dynamic(\n    model, {nn.LSTM, nn.Linear}, dtype=torch.qint8\n)\nprint(quantized_model)",
      "LSTMModel(\n  (drop): Dropout(p=0.5, inplace=False)\n  (encoder): Embedding(33278, 512)\n  (rnn): DynamicQuantizedLSTM(512, 256, num_layers=5, dropout=0.5)\n  (decoder): DynamicQuantizedLinear(in_features=256, out_features=33278, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n)",
      "def print_size_of_model(model):\n    torch.save(model.state_dict(), \"temp.p\")\n    print('Size (MB):', os.path.getsize(\"temp.p\")/1e6)\n    os.remove('temp.p')\n\nprint_size_of_model(model)\nprint_size_of_model(quantized_model)",
      "torch.set_num_threads(1)\n\ndef time_model_evaluation(model, test_data):\n    s = time.time()\n    loss = evaluate(model, test_data)\n    elapsed = time.time() - s\n    print('''loss: {0:.3f}\\nelapsed time (seconds): {1:.1f}'''.format(loss, elapsed))\n\ntime_model_evaluation(model, test_data)\ntime_model_evaluation(quantized_model, test_data)"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/recipes/quantization.html",
    "title": "Quantization Recipe\u00b6",
    "code_snippets": [
      "import torchvision\nmodel_quantized = torchvision.models.quantization.mobilenet_v2(pretrained=True, quantize=True)",
      "model = torchvision.models.mobilenet_v2(pretrained=True)\n\nimport os\nimport torch\n\ndef print_model_size(mdl):\n    torch.save(mdl.state_dict(), \"tmp.pt\")\n    print(\"%.2f MB\" %(os.path.getsize(\"tmp.pt\")/1e6))\n    os.remove('tmp.pt')\n\nprint_model_size(model)\nprint_model_size(model_quantized)",
      "model_dynamic_quantized = torch.quantization.quantize_dynamic(\n    model, qconfig_spec={torch.nn.Linear}, dtype=torch.qint8\n)",
      "backend = \"qnnpack\"\nmodel.qconfig = torch.quantization.get_default_qconfig(backend)\ntorch.backends.quantized.engine = backend\nmodel_static_quantized = torch.quantization.prepare(model, inplace=False)\nmodel_static_quantized = torch.quantization.convert(model_static_quantized, inplace=False)",
      "self.quant = torch.quantization.QuantStub()\nself.dequant = torch.quantization.DeQuantStub()",
      "model.qconfig = torch.quantization.get_default_qat_qconfig(backend)\nmodel_qat = torch.quantization.prepare_qat(model, inplace=False)\n# quantization aware training goes here\nmodel_qat = torch.quantization.convert(model_qat.eval(), inplace=False)"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/intermediate/rpc_tutorial.html",
    "title": "Getting Started with Distributed RPC Framework\u00b6",
    "code_snippets": [
      "import torch.nn as nn\nimport torch.nn.functional as F\n\nclass Policy(nn.Module):\n\n    def __init__(self):\n        super(Policy, self).__init__()\n        self.affine1 = nn.Linear(4, 128)\n        self.dropout = nn.Dropout(p=0.6)\n        self.affine2 = nn.Linear(128, 2)\n\n    def forward(self, x):\n        x = self.affine1(x)\n        x = self.dropout(x)\n        x = F.relu(x)\n        action_scores = self.affine2(x)\n        return F.softmax(action_scores, dim=1)",
      "import argparse\nimport gym\nimport torch.distributed.rpc as rpc\n\nparser = argparse.ArgumentParser(\n    description=\"RPC Reinforcement Learning Example\",\n    formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n)\n\nparser.add_argument('--world_size', default=2, type=int, metavar='W',\n                    help='number of workers')\nparser.add_argument('--log_interval', type=int, default=10, metavar='N',\n                    help='interval between training status logs')\nparser.add_argument('--gamma', type=float, default=0.99, metavar='G',\n                    help='how much to value future rewards')\nparser.add_argument('--seed', type=int, default=1, metavar='S',\n                    help='random seed  for reproducibility')\nargs = parser.parse_args()\n\nclass Observer:\n\n    def __init__(self):\n        self.id = rpc.get_worker_info().id\n        self.env = gym.make('CartPole-v1')\n        self.env.seed(args.seed)\n\n    def run_episode(self, agent_rref):\n        state, ep_reward = self.env.reset(), 0\n        for _ in range(10000):\n            # send the state to the agent to get an action\n            action = agent_rref.rpc_sync().select_action(self.id, state)\n\n            # apply the action to the environment, and get the reward\n            state, reward, done, _ = self.env.step(action)\n\n            # report the reward to the agent for training purpose\n            agent_rref.rpc_sync().report_reward(self.id, reward)\n\n            # finishes after the number of self.env._max_episode_steps\n            if done:\n                break",
      "import gym\nimport numpy as np\n\nimport torch\nimport torch.distributed.rpc as rpc\nimport torch.optim as optim\nfrom torch.distributed.rpc import RRef, rpc_async, remote\nfrom torch.distributions import Categorical\n\nclass Agent:\n    def __init__(self, world_size):\n        self.ob_rrefs = []\n        self.agent_rref = RRef(self)\n        self.rewards = {}\n        self.saved_log_probs = {}\n        self.policy = Policy()\n        self.optimizer = optim.Adam(self.policy.parameters(), lr=1e-2)\n        self.eps = np.finfo(np.float32).eps.item()\n        self.running_reward = 0\n        self.reward_threshold = gym.make('CartPole-v1').spec.reward_threshold\n        for ob_rank in range(1, world_size):\n            ob_info = rpc.get_worker_info(OBSERVER_NAME.format(ob_rank))\n            self.ob_rrefs.append(remote(ob_info, Observer))\n            self.rewards[ob_info.id] = []\n            self.saved_log_probs[ob_info.id] = []",
      "class Agent:\n    ...\n    def select_action(self, ob_id, state):\n        state = torch.from_numpy(state).float().unsqueeze(0)\n        probs = self.policy(state)\n        m = Categorical(probs)\n        action = m.sample()\n        self.saved_log_probs[ob_id].append(m.log_prob(action))\n        return action.item()\n\n    def report_reward(self, ob_id, reward):\n        self.rewards[ob_id].append(reward)",
      "class Agent:\n    ...\n    def run_episode(self):\n        futs = []\n        for ob_rref in self.ob_rrefs:\n            # make async RPC to kick off an episode on all observers\n            futs.append(\n                rpc_async(\n                    ob_rref.owner(),\n                    ob_rref.rpc_sync().run_episode,\n                    args=(self.agent_rref,)\n                )\n            )\n\n        # wait until all obervers have finished this episode\n        for fut in futs:\n            fut.wait()",
      "class Agent:\n    ...\n    def finish_episode(self):\n      # joins probs and rewards from different observers into lists\n      R, probs, rewards = 0, [], []\n      for ob_id in self.rewards:\n          probs.extend(self.saved_log_probs[ob_id])\n          rewards.extend(self.rewards[ob_id])\n\n      # use the minimum observer reward to calculate the running reward\n      min_reward = min([sum(self.rewards[ob_id]) for ob_id in self.rewards])\n      self.running_reward = 0.05 * min_reward + (1 - 0.05) * self.running_reward\n\n      # clear saved probs and rewards\n      for ob_id in self.rewards:\n          self.rewards[ob_id] = []\n          self.saved_log_probs[ob_id] = []\n\n      policy_loss, returns = [], []\n      for r in rewards[::-1]:\n          R = r + args.gamma * R\n          returns.insert(0, R)\n      returns = torch.tensor(returns)\n      returns = (returns - returns.mean()) / (returns.std() + self.eps)\n      for log_prob, R in zip(probs, returns):\n          policy_loss.append(-log_prob * R)\n      self.optimizer.zero_grad()\n      policy_loss = torch.cat(policy_loss).sum()\n      policy_loss.backward()\n      self.optimizer.step()\n      return min_reward",
      "import os\nfrom itertools import count\n\nimport torch.multiprocessing as mp\n\nAGENT_NAME = \"agent\"\nOBSERVER_NAME=\"obs{}\"\n\ndef run_worker(rank, world_size):\n    os.environ['MASTER_ADDR'] = 'localhost'\n    os.environ['MASTER_PORT'] = '29500'\n    if rank == 0:\n        # rank0 is the agent\n        rpc.init_rpc(AGENT_NAME, rank=rank, world_size=world_size)\n\n        agent = Agent(world_size)\n        print(f\"This will run until reward threshold of {agent.reward_threshold}\"\n                \" is reached. Ctrl+C to exit.\")\n        for i_episode in count(1):\n            agent.run_episode()\n            last_reward = agent.finish_episode()\n\n            if i_episode % args.log_interval == 0:\n                print(f\"Episode {i_episode}\\tLast reward: {last_reward:.2f}\\tAverage reward: \"\n                    f\"{agent.running_reward:.2f}\")\n            if agent.running_reward > agent.reward_threshold:\n                print(f\"Solved! Running reward is now {agent.running_reward}!\")\n                break\n    else:\n        # other ranks are the observer\n        rpc.init_rpc(OBSERVER_NAME.format(rank), rank=rank, world_size=world_size)\n        # observers passively waiting for instructions from the agent\n\n    # block until all rpcs finish, and shutdown the RPC instance\n    rpc.shutdown()\n\n\nmp.spawn(\n    run_worker,\n    args=(args.world_size, ),\n    nprocs=args.world_size,\n    join=True\n)",
      "class EmbeddingTable(nn.Module):\n    r\"\"\"\n    Encoding layers of the RNNModel\n    \"\"\"\n    def __init__(self, ntoken, ninp, dropout):\n        super(EmbeddingTable, self).__init__()\n        self.drop = nn.Dropout(dropout)\n        self.encoder = nn.Embedding(ntoken, ninp).cuda()\n        self.encoder.weight.data.uniform_(-0.1, 0.1)\n\n    def forward(self, input):\n        return self.drop(self.encoder(input.cuda()).cpu()\n\n\nclass Decoder(nn.Module):\n    def __init__(self, ntoken, nhid, dropout):\n        super(Decoder, self).__init__()\n        self.drop = nn.Dropout(dropout)\n        self.decoder = nn.Linear(nhid, ntoken)\n        self.decoder.bias.data.zero_()\n        self.decoder.weight.data.uniform_(-0.1, 0.1)\n\n    def forward(self, output):\n        return self.decoder(self.drop(output))",
      "class RNNModel(nn.Module):\n    def __init__(self, ps, ntoken, ninp, nhid, nlayers, dropout=0.5):\n        super(RNNModel, self).__init__()\n\n        # setup embedding table remotely\n        self.emb_table_rref = rpc.remote(ps, EmbeddingTable, args=(ntoken, ninp, dropout))\n        # setup LSTM locally\n        self.rnn = nn.LSTM(ninp, nhid, nlayers, dropout=dropout)\n        # setup decoder remotely\n        self.decoder_rref = rpc.remote(ps, Decoder, args=(ntoken, nhid, dropout))\n\n    def forward(self, input, hidden):\n        # pass input to the remote embedding table and fetch emb tensor back\n        emb = _remote_method(EmbeddingTable.forward, self.emb_table_rref, input)\n        output, hidden = self.rnn(emb, hidden)\n        # pass output to the rremote decoder and get the decoded output back\n        decoded = _remote_method(Decoder.forward, self.decoder_rref, output)\n        return decoded, hidden",
      "def _parameter_rrefs(module):\n    param_rrefs = []\n    for param in module.parameters():\n        param_rrefs.append(RRef(param))\n    return param_rrefs",
      "class RNNModel(nn.Module):\n    ...\n    def parameter_rrefs(self):\n        remote_params = []\n        # get RRefs of embedding table\n        remote_params.extend(_remote_method(_parameter_rrefs, self.emb_table_rref))\n        # create RRefs for local parameters\n        remote_params.extend(_parameter_rrefs(self.rnn))\n        # get RRefs of decoder\n        remote_params.extend(_remote_method(_parameter_rrefs, self.decoder_rref))\n        return remote_params",
      "def run_trainer():\n    batch = 5\n    ntoken = 10\n    ninp = 2\n\n    nhid = 3\n    nindices = 3\n    nlayers = 4\n    hidden = (\n        torch.randn(nlayers, nindices, nhid),\n        torch.randn(nlayers, nindices, nhid)\n    )\n\n    model = rnn.RNNModel('ps', ntoken, ninp, nhid, nlayers)\n\n    # setup distributed optimizer\n    opt = DistributedOptimizer(\n        optim.SGD,\n        model.parameter_rrefs(),\n        lr=0.05,\n    )\n\n    criterion = torch.nn.CrossEntropyLoss()\n\n    def get_next_batch():\n        for _ in range(5):\n            data = torch.LongTensor(batch, nindices) % ntoken\n            target = torch.LongTensor(batch, ntoken) % nindices\n            yield data, target\n\n    # train for 10 iterations\n    for epoch in range(10):\n        for data, target in get_next_batch():\n            # create distributed autograd context\n            with dist_autograd.context() as context_id:\n                hidden[0].detach_()\n                hidden[1].detach_()\n                output, hidden = model(data, hidden)\n                loss = criterion(output, target)\n                # run distributed backward pass\n                dist_autograd.backward(context_id, [loss])\n                # run distributed optimizer\n                opt.step(context_id)\n                # not necessary to zero grads since they are\n                # accumulated into the distributed autograd context\n                # which is reset every iteration.\n        print(\"Training epoch {}\".format(epoch))",
      "def run_worker(rank, world_size):\n    os.environ['MASTER_ADDR'] = 'localhost'\n    os.environ['MASTER_PORT'] = '29500'\n    if rank == 1:\n        rpc.init_rpc(\"trainer\", rank=rank, world_size=world_size)\n        _run_trainer()\n    else:\n        rpc.init_rpc(\"ps\", rank=rank, world_size=world_size)\n        # parameter server do nothing\n        pass\n\n    # block until all rpcs finish\n    rpc.shutdown()\n\n\nif __name__==\"__main__\":\n    world_size = 2\n    mp.spawn(run_worker, args=(world_size, ), nprocs=world_size, join=True)"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/intermediate/tiatoolbox_tutorial.html",
    "title": "Whole Slide Image Classification Using PyTorch and TIAToolbox\u00b6",
    "code_snippets": [
      "\"\"\"Import modules required to run the Jupyter notebook.\"\"\"\nfrom __future__ import annotations\n\n# Configure logging\nimport logging\nimport warnings\nif logging.getLogger().hasHandlers():\n    logging.getLogger().handlers.clear()\nwarnings.filterwarnings(\"ignore\", message=\".*The 'nopython' keyword.*\")\n\n# Downloading data and files\nimport shutil\nfrom pathlib import Path\nfrom zipfile import ZipFile\n\n# Data processing and visualization\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import cm\nimport PIL\nimport contextlib\nimport io\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\n# TIAToolbox for WSI loading and processing\nfrom tiatoolbox import logger\nfrom tiatoolbox.models.architecture import vanilla\nfrom tiatoolbox.models.engine.patch_predictor import (\n    IOPatchPredictorConfig,\n    PatchPredictor,\n)\nfrom tiatoolbox.utils.misc import download_data, grab_files_from_dir\nfrom tiatoolbox.utils.visualization import overlay_prediction_mask\nfrom tiatoolbox.wsicore.wsireader import WSIReader\n\n# Torch-related\nimport torch\nfrom torchvision import transforms\n\n# Configure plotting\nmpl.rcParams[\"figure.dpi\"] = 160  # for high resolution figure in notebook\nmpl.rcParams[\"figure.facecolor\"] = \"white\"  # To make sure text is visible in dark mode\n\n# If you are not using GPU, change ON_GPU to False\nON_GPU = True\n\n# Function to suppress console output for overly verbose code blocks\ndef suppress_console_output():\n    return contextlib.redirect_stderr(io.StringIO())",
      "warnings.filterwarnings(\"ignore\")\nglobal_save_dir = Path(\"./tmp/\")\n\n\ndef rmdir(dir_path: str | Path) -> None:\n    \"\"\"Helper function to delete directory.\"\"\"\n    if Path(dir_path).is_dir():\n        shutil.rmtree(dir_path)\n        logger.info(\"Removing directory %s\", dir_path)\n\n\nrmdir(global_save_dir)  # remove  directory if it exists from previous runs\nglobal_save_dir.mkdir()\nlogger.info(\"Creating new directory %s\", global_save_dir)",
      "wsi_path = global_save_dir / \"sample_wsi.svs\"\npatches_path = global_save_dir / \"kather100k-validation-sample.zip\"\nweights_path = global_save_dir / \"resnet18-kather100k.pth\"\n\nlogger.info(\"Download has started. Please wait...\")\n\n# Downloading and unzip a sample whole-slide image\ndownload_data(\n    \"https://tiatoolbox.dcs.warwick.ac.uk/sample_wsis/TCGA-3L-AA1B-01Z-00-DX1.8923A151-A690-40B7-9E5A-FCBEDFC2394F.svs\",\n    wsi_path,\n)\n\n# Download and unzip a sample of the validation set used to train the Kather 100K dataset\ndownload_data(\n    \"https://tiatoolbox.dcs.warwick.ac.uk/datasets/kather100k-validation-sample.zip\",\n    patches_path,\n)\nwith ZipFile(patches_path, \"r\") as zipfile:\n    zipfile.extractall(path=global_save_dir)\n\n# Download pretrained model weights for WSI classification using ResNet18 architecture\ndownload_data(\n    \"https://tiatoolbox.dcs.warwick.ac.uk/models/pc/resnet18-kather100k.pth\",\n    weights_path,\n)\n\nlogger.info(\"Download is complete.\")",
      "# Read the patch data and create a list of patches and a list of corresponding labels\ndataset_path = global_save_dir / \"kather100k-validation-sample\"\n\n# Set the path to the dataset\nimage_ext = \".tif\"  # file extension of each image\n\n# Obtain the mapping between the label ID and the class name\nlabel_dict = {\n    \"BACK\": 0, # Background (empty glass region)\n    \"NORM\": 1, # Normal colon mucosa\n    \"DEB\": 2,  # Debris\n    \"TUM\": 3,  # Colorectal adenocarcinoma epithelium\n    \"ADI\": 4,  # Adipose\n    \"MUC\": 5,  # Mucus\n    \"MUS\": 6,  # Smooth muscle\n    \"STR\": 7,  # Cancer-associated stroma\n    \"LYM\": 8,  # Lymphocytes\n}\n\nclass_names = list(label_dict.keys())\nclass_labels = list(label_dict.values())\n\n# Generate a list of patches and generate the label from the filename\npatch_list = []\nlabel_list = []\nfor class_name, label in label_dict.items():\n    dataset_class_path = dataset_path / class_name\n    patch_list_single_class = grab_files_from_dir(\n        dataset_class_path,\n        file_types=\"*\" + image_ext,\n    )\n    patch_list.extend(patch_list_single_class)\n    label_list.extend([label] * len(patch_list_single_class))\n\n# Show some dataset statistics\nplt.bar(class_names, [label_list.count(label) for label in class_labels])\nplt.xlabel(\"Patch types\")\nplt.ylabel(\"Number of patches\")\n\n# Count the number of examples per class\nfor class_name, label in label_dict.items():\n    logger.info(\n        \"Class ID: %d -- Class Name: %s -- Number of images: %d\",\n        label,\n        class_name,\n        label_list.count(label),\n    )\n\n# Overall dataset statistics\nlogger.info(\"Total number of patches: %d\", (len(patch_list)))",
      "# Importing a pretrained PyTorch model from TIAToolbox\npredictor = PatchPredictor(pretrained_model='resnet18-kather100k', batch_size=32)\n\n# Users can load any PyTorch model architecture instead using the following script\nmodel = vanilla.CNNModel(backbone=\"resnet18\", num_classes=9) # Importing model from torchvision.models.resnet18\nmodel.load_state_dict(torch.load(weights_path, map_location=\"cpu\", weights_only=True), strict=True)\ndef preproc_func(img):\n    img = PIL.Image.fromarray(img)\n    img = transforms.ToTensor()(img)\n    return img.permute(1, 2, 0)\nmodel.preproc_func = preproc_func\npredictor = PatchPredictor(model=model, batch_size=32)",
      "with suppress_console_output():\n    output = predictor.predict(imgs=patch_list, mode=\"patch\", on_gpu=ON_GPU)\n\nacc = accuracy_score(label_list, output[\"predictions\"])\nlogger.info(\"Classification accuracy: %f\", acc)\n\n# Creating and visualizing the confusion matrix for patch classification results\nconf = confusion_matrix(label_list, output[\"predictions\"], normalize=\"true\")\ndf_cm = pd.DataFrame(conf, index=class_names, columns=class_names)\ndf_cm",
      "# Visualization of whole-slide image patch-level prediction\n# first set up a label to color mapping\nlabel_color_dict = {}\nlabel_color_dict[0] = (\"empty\", (0, 0, 0))\ncolors = cm.get_cmap(\"Set1\").colors\nfor class_name, label in label_dict.items():\n    label_color_dict[label + 1] = (class_name, 255 * np.array(colors[label]))\n\npred_map = predictor.merge_predictions(\n    wsi_path,\n    wsi_output[0],\n    resolution=overview_resolution,\n    units=overview_unit,\n)\noverlay = overlay_prediction_mask(\n    wsi_overview,\n    pred_map,\n    alpha=0.5,\n    label_info=label_color_dict,\n    return_ax=True,\n)\nplt.show()",
      "# Import some extra modules\nimport histoencoder.functional as F\nimport torch.nn as nn\n\nfrom tiatoolbox.models.engine.semantic_segmentor import DeepFeatureExtractor, IOSegmentorConfig\nfrom tiatoolbox.models.models_abc import ModelABC\nimport umap",
      "class HistoEncWrapper(ModelABC):\n    \"\"\"Wrapper for HistoEnc model that conforms to tiatoolbox ModelABC interface.\"\"\"\n\n    def __init__(self: HistoEncWrapper, encoder) -> None:\n        super().__init__()\n        self.feat_extract = encoder\n\n    def forward(self: HistoEncWrapper, imgs: torch.Tensor) -> torch.Tensor:\n        \"\"\"Pass input data through the model.\n\n        Args:\n            imgs (torch.Tensor):\n                Model input.\n\n        \"\"\"\n        out = F.extract_features(self.feat_extract, imgs, num_blocks=2, avg_pool=True)\n        return out\n\n    @staticmethod\n    def infer_batch(\n        model: nn.Module,\n        batch_data: torch.Tensor,\n        *,\n        on_gpu: bool,\n    ) -> list[np.ndarray]:\n        \"\"\"Run inference on an input batch.\n\n        Contains logic for forward operation as well as i/o aggregation.\n\n        Args:\n            model (nn.Module):\n                PyTorch defined model.\n            batch_data (torch.Tensor):\n                A batch of data generated by\n                `torch.utils.data.DataLoader`.\n            on_gpu (bool):\n                Whether to run inference on a GPU.\n\n        \"\"\"\n        img_patches_device = batch_data.to('cuda') if on_gpu else batch_data\n        model.eval()\n        # Do not compute the gradient (not training)\n        with torch.inference_mode():\n            output = model(img_patches_device)\n        return [output.cpu().numpy()]",
      "# First we define a function to calculate the umap reduction\ndef umap_reducer(x, dims=3, nns=10):\n    \"\"\"UMAP reduction of the input data.\"\"\"\n    reducer = umap.UMAP(n_neighbors=nns, n_components=dims, metric=\"manhattan\", spread=0.5, random_state=2)\n    reduced = reducer.fit_transform(x)\n    reduced -= reduced.min(axis=0)\n    reduced /= reduced.max(axis=0)\n    return reduced\n\n# load the features output by our feature extractor\npos = np.load(global_save_dir / \"wsi_features\" / \"0.position.npy\")\nfeats = np.load(global_save_dir / \"wsi_features\" / \"0.features.0.npy\")\npos = pos / 8 # as we extracted at 0.5mpp, and we are overlaying on a thumbnail at 4mpp\n\n# reduce the features into 3 dimensional (rgb) space\nreduced = umap_reducer(feats)\n\n# plot the prediction map the classifier again\noverlay = overlay_prediction_mask(\n    wsi_overview,\n    pred_map,\n    alpha=0.5,\n    label_info=label_color_dict,\n    return_ax=True,\n)\n\n# plot the feature map reduction\nplt.figure()\nplt.imshow(wsi_overview)\nplt.scatter(pos[:,0], pos[:,1], c=reduced, s=1, alpha=0.5)\nplt.axis(\"off\")\nplt.title(\"UMAP reduction of HistoEnc features\")\nplt.show()"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/intermediate/dynamic_quantization_bert_tutorial.html",
    "title": "(beta) Dynamic Quantization on BERT\u00b6",
    "code_snippets": [
      "yes y | pip uninstall torch torchvision\nyes y | pip install --pre torch -f https://download.pytorch.org/whl/nightly/cu101/torch_nightly.html",
      "import logging\nimport numpy as np\nimport os\nimport random\nimport sys\nimport time\nimport torch\n\nfrom argparse import Namespace\nfrom torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n                              TensorDataset)\nfrom tqdm import tqdm\nfrom transformers import (BertConfig, BertForSequenceClassification, BertTokenizer,)\nfrom transformers import glue_compute_metrics as compute_metrics\nfrom transformers import glue_output_modes as output_modes\nfrom transformers import glue_processors as processors\nfrom transformers import glue_convert_examples_to_features as convert_examples_to_features\n\n# Setup logging\nlogger = logging.getLogger(__name__)\nlogging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n                    datefmt = '%m/%d/%Y %H:%M:%S',\n                    level = logging.WARN)\n\nlogging.getLogger(\"transformers.modeling_utils\").setLevel(\n   logging.WARN)  # Reduce logging\n\nprint(torch.__version__)",
      "torch.set_num_threads(1)\nprint(torch.__config__.parallel_info())",
      "configs = Namespace()\n\n# The output directory for the fine-tuned model, $OUT_DIR.\nconfigs.output_dir = \"./MRPC/\"\n\n# The data directory for the MRPC task in the GLUE benchmark, $GLUE_DIR/$TASK_NAME.\nconfigs.data_dir = \"./glue_data/MRPC\"\n\n# The model name or path for the pre-trained model.\nconfigs.model_name_or_path = \"bert-base-uncased\"\n# The maximum length of an input sequence\nconfigs.max_seq_length = 128\n\n# Prepare GLUE task.\nconfigs.task_name = \"MRPC\".lower()\nconfigs.processor = processors[configs.task_name]()\nconfigs.output_mode = output_modes[configs.task_name]\nconfigs.label_list = configs.processor.get_labels()\nconfigs.model_type = \"bert\".lower()\nconfigs.do_lower_case = True\n\n# Set the device, batch size, topology, and caching flags.\nconfigs.device = \"cpu\"\nconfigs.per_gpu_eval_batch_size = 8\nconfigs.n_gpu = 0\nconfigs.local_rank = -1\nconfigs.overwrite_cache = False\n\n\n# Set random seed for reproducibility.\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\nset_seed(42)",
      "# coding=utf-8\n# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\ndef evaluate(args, model, tokenizer, prefix=\"\"):\n    # Loop to handle MNLI double evaluation (matched, mis-matched)\n    eval_task_names = (\"mnli\", \"mnli-mm\") if args.task_name == \"mnli\" else (args.task_name,)\n    eval_outputs_dirs = (args.output_dir, args.output_dir + '-MM') if args.task_name == \"mnli\" else (args.output_dir,)\n\n    results = {}\n    for eval_task, eval_output_dir in zip(eval_task_names, eval_outputs_dirs):\n        eval_dataset = load_and_cache_examples(args, eval_task, tokenizer, evaluate=True)\n\n        if not os.path.exists(eval_output_dir) and args.local_rank in [-1, 0]:\n            os.makedirs(eval_output_dir)\n\n        args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n        # Note that DistributedSampler samples randomly\n        eval_sampler = SequentialSampler(eval_dataset) if args.local_rank == -1 else DistributedSampler(eval_dataset)\n        eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)\n\n        # multi-gpu eval\n        if args.n_gpu > 1:\n            model = torch.nn.DataParallel(model)\n\n        # Eval!\n        logger.info(\"***** Running evaluation {} *****\".format(prefix))\n        logger.info(\"  Num examples = %d\", len(eval_dataset))\n        logger.info(\"  Batch size = %d\", args.eval_batch_size)\n        eval_loss = 0.0\n        nb_eval_steps = 0\n        preds = None\n        out_label_ids = None\n        for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n            model.eval()\n            batch = tuple(t.to(args.device) for t in batch)\n\n            with torch.no_grad():\n                inputs = {'input_ids':      batch[0],\n                          'attention_mask': batch[1],\n                          'labels':         batch[3]}\n                if args.model_type != 'distilbert':\n                    inputs['token_type_ids'] = batch[2] if args.model_type in ['bert', 'xlnet'] else None  # XLM, DistilBERT and RoBERTa don't use segment_ids\n                outputs = model(**inputs)\n                tmp_eval_loss, logits = outputs[:2]\n\n                eval_loss += tmp_eval_loss.mean().item()\n            nb_eval_steps += 1\n            if preds is None:\n                preds = logits.detach().cpu().numpy()\n                out_label_ids = inputs['labels'].detach().cpu().numpy()\n            else:\n                preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n                out_label_ids = np.append(out_label_ids, inputs['labels'].detach().cpu().numpy(), axis=0)\n\n        eval_loss = eval_loss / nb_eval_steps\n        if args.output_mode == \"classification\":\n            preds = np.argmax(preds, axis=1)\n        elif args.output_mode == \"regression\":\n            preds = np.squeeze(preds)\n        result = compute_metrics(eval_task, preds, out_label_ids)\n        results.update(result)\n\n        output_eval_file = os.path.join(eval_output_dir, prefix, \"eval_results.txt\")\n        with open(output_eval_file, \"w\") as writer:\n            logger.info(\"***** Eval results {} *****\".format(prefix))\n            for key in sorted(result.keys()):\n                logger.info(\"  %s = %s\", key, str(result[key]))\n                writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n\n    return results\n\n\ndef load_and_cache_examples(args, task, tokenizer, evaluate=False):\n    if args.local_rank not in [-1, 0] and not evaluate:\n        torch.distributed.barrier()  # Make sure only the first process in distributed training process the dataset, and the others will use the cache\n\n    processor = processors[task]()\n    output_mode = output_modes[task]\n    # Load data features from cache or dataset file\n    cached_features_file = os.path.join(args.data_dir, 'cached_{}_{}_{}_{}'.format(\n        'dev' if evaluate else 'train',\n        list(filter(None, args.model_name_or_path.split('/'))).pop(),\n        str(args.max_seq_length),\n        str(task)))\n    if os.path.exists(cached_features_file) and not args.overwrite_cache:\n        logger.info(\"Loading features from cached file %s\", cached_features_file)\n        features = torch.load(cached_features_file)\n    else:\n        logger.info(\"Creating features from dataset file at %s\", args.data_dir)\n        label_list = processor.get_labels()\n        if task in ['mnli', 'mnli-mm'] and args.model_type in ['roberta']:\n            # HACK(label indices are swapped in RoBERTa pretrained model)\n            label_list[1], label_list[2] = label_list[2], label_list[1]\n        examples = processor.get_dev_examples(args.data_dir) if evaluate else processor.get_train_examples(args.data_dir)\n        features = convert_examples_to_features(examples,\n                                                tokenizer,\n                                                label_list=label_list,\n                                                max_length=args.max_seq_length,\n                                                output_mode=output_mode,\n                                                pad_on_left=bool(args.model_type in ['xlnet']),                 # pad on the left for xlnet\n                                                pad_token=tokenizer.convert_tokens_to_ids([tokenizer.pad_token])[0],\n                                                pad_token_segment_id=4 if args.model_type in ['xlnet'] else 0,\n        )\n        if args.local_rank in [-1, 0]:\n            logger.info(\"Saving features into cached file %s\", cached_features_file)\n            torch.save(features, cached_features_file)\n\n    if args.local_rank == 0 and not evaluate:\n        torch.distributed.barrier()  # Make sure only the first process in distributed training process the dataset, and the others will use the cache\n\n    # Convert to Tensors and build dataset\n    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n    all_attention_mask = torch.tensor([f.attention_mask for f in features], dtype=torch.long)\n    all_token_type_ids = torch.tensor([f.token_type_ids for f in features], dtype=torch.long)\n    if output_mode == \"classification\":\n        all_labels = torch.tensor([f.label for f in features], dtype=torch.long)\n    elif output_mode == \"regression\":\n        all_labels = torch.tensor([f.label for f in features], dtype=torch.float)\n\n    dataset = TensorDataset(all_input_ids, all_attention_mask, all_token_type_ids, all_labels)\n    return dataset",
      "quantized_model = torch.quantization.quantize_dynamic(\n    model, {torch.nn.Linear}, dtype=torch.qint8\n)\nprint(quantized_model)",
      "def print_size_of_model(model):\n    torch.save(model.state_dict(), \"temp.p\")\n    print('Size (MB):', os.path.getsize(\"temp.p\")/1e6)\n    os.remove('temp.p')\n\nprint_size_of_model(model)\nprint_size_of_model(quantized_model)",
      "def time_model_evaluation(model, configs, tokenizer):\n    eval_start_time = time.time()\n    result = evaluate(configs, model, tokenizer, prefix=\"\")\n    eval_end_time = time.time()\n    eval_duration_time = eval_end_time - eval_start_time\n    print(result)\n    print(\"Evaluate total time (seconds): {0:.1f}\".format(eval_duration_time))\n\n# Evaluate the original FP32 BERT model\ntime_model_evaluation(model, configs, tokenizer)\n\n# Evaluate the INT8 BERT model after the dynamic quantization\ntime_model_evaluation(quantized_model, configs, tokenizer)",
      "def ids_tensor(shape, vocab_size):\n    #  Creates a random int32 tensor of the shape within the vocab size\n    return torch.randint(0, vocab_size, shape=shape, dtype=torch.int, device='cpu')\n\ninput_ids = ids_tensor([8, 128], 2)\ntoken_type_ids = ids_tensor([8, 128], 2)\nattention_mask = ids_tensor([8, 128], vocab_size=2)\ndummy_input = (input_ids, attention_mask, token_type_ids)\ntraced_model = torch.jit.trace(quantized_model, dummy_input)\ntorch.jit.save(traced_model, \"bert_traced_eager_quant.pt\")",
      "loaded_quantized_model = torch.jit.load(\"bert_traced_eager_quant.pt\")"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/beginner/vt_tutorial.html",
    "title": "Optimizing Vision Transformer Model for Deployment\u00b6",
    "code_snippets": [
      "from PIL import Image\nimport torch\nimport timm\nimport requests\nimport torchvision.transforms as transforms\nfrom timm.data.constants import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n\nprint(torch.__version__)\n# should be 1.8.0\n\n\nmodel = torch.hub.load('facebookresearch/deit:main', 'deit_base_patch16_224', pretrained=True)\nmodel.eval()\n\ntransform = transforms.Compose([\n    transforms.Resize(256, interpolation=3),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD),\n])\n\nimg = Image.open(requests.get(\"https://raw.githubusercontent.com/pytorch/ios-demo-app/master/HelloWorld/HelloWorld/HelloWorld/image.png\", stream=True).raw)\nimg = transform(img)[None,]\nout = model(img)\nclsidx = torch.argmax(out)\nprint(clsidx.item())",
      "2.7.0+cu126\nDownloading: \"https://github.com/facebookresearch/deit/zipball/main\" to /var/lib/ci-user/.cache/torch/hub/main.zip\n/usr/local/lib/python3.10/dist-packages/timm/models/registry.py:4: FutureWarning:\n\nImporting from timm.models.registry is deprecated, please import via timm.models\n\n/usr/local/lib/python3.10/dist-packages/timm/models/layers/__init__.py:48: FutureWarning:\n\nImporting from timm.models.layers is deprecated, please import via timm.layers\n\n/var/lib/ci-user/.cache/torch/hub/facebookresearch_deit_main/models.py:63: UserWarning:\n\nOverwriting deit_tiny_patch16_224 in registry with models.deit_tiny_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n\n/var/lib/ci-user/.cache/torch/hub/facebookresearch_deit_main/models.py:78: UserWarning:\n\nOverwriting deit_small_patch16_224 in registry with models.deit_small_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n\n/var/lib/ci-user/.cache/torch/hub/facebookresearch_deit_main/models.py:93: UserWarning:\n\nOverwriting deit_base_patch16_224 in registry with models.deit_base_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n\n/var/lib/ci-user/.cache/torch/hub/facebookresearch_deit_main/models.py:108: UserWarning:\n\nOverwriting deit_tiny_distilled_patch16_224 in registry with models.deit_tiny_distilled_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n\n/var/lib/ci-user/.cache/torch/hub/facebookresearch_deit_main/models.py:123: UserWarning:\n\nOverwriting deit_small_distilled_patch16_224 in registry with models.deit_small_distilled_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n\n/var/lib/ci-user/.cache/torch/hub/facebookresearch_deit_main/models.py:138: UserWarning:\n\nOverwriting deit_base_distilled_patch16_224 in registry with models.deit_base_distilled_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n\n/var/lib/ci-user/.cache/torch/hub/facebookresearch_deit_main/models.py:153: UserWarning:\n\nOverwriting deit_base_patch16_384 in registry with models.deit_base_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n\n/var/lib/ci-user/.cache/torch/hub/facebookresearch_deit_main/models.py:168: UserWarning:\n\nOverwriting deit_base_distilled_patch16_384 in registry with models.deit_base_distilled_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n\nDownloading: \"https://dl.fbaipublicfiles.com/deit/deit_base_patch16_224-b5f2ef4d.pth\" to /var/lib/ci-user/.cache/torch/hub/checkpoints/deit_base_patch16_224-b5f2ef4d.pth\n\n  0%|          | 0.00/330M [00:00<?, ?B/s]\n  5%|4         | 15.2M/330M [00:00<00:02, 160MB/s]\n 11%|#1        | 37.9M/330M [00:00<00:01, 205MB/s]\n 19%|#8        | 62.8M/330M [00:00<00:01, 229MB/s]\n 26%|##6       | 86.1M/330M [00:00<00:01, 235MB/s]\n 34%|###3      | 111M/330M [00:00<00:00, 245MB/s]\n 41%|####      | 135M/330M [00:00<00:00, 245MB/s]\n 48%|####7     | 158M/330M [00:00<00:00, 234MB/s]\n 55%|#####4    | 181M/330M [00:00<00:00, 237MB/s]\n 62%|######1   | 204M/330M [00:00<00:00, 223MB/s]\n 68%|######8   | 226M/330M [00:01<00:00, 223MB/s]\n 75%|#######4  | 247M/330M [00:01<00:00, 208MB/s]\n 82%|########1 | 270M/330M [00:01<00:00, 218MB/s]\n 88%|########8 | 292M/330M [00:01<00:00, 220MB/s]\n 95%|#########4| 313M/330M [00:01<00:00, 218MB/s]\n100%|##########| 330M/330M [00:01<00:00, 223MB/s]\n269",
      "model = torch.hub.load('facebookresearch/deit:main', 'deit_base_patch16_224', pretrained=True)\nmodel.eval()\nscripted_model = torch.jit.script(model)\nscripted_model.save(\"fbdeit_scripted.pt\")",
      "# Use 'x86' for server inference (the old 'fbgemm' is still available but 'x86' is the recommended default) and ``qnnpack`` for mobile inference.\nbackend = \"x86\" # replaced with ``qnnpack`` causing much worse inference speed for quantized model on this notebook\nmodel.qconfig = torch.quantization.get_default_qconfig(backend)\ntorch.backends.quantized.engine = backend\n\nquantized_model = torch.quantization.quantize_dynamic(model, qconfig_spec={torch.nn.Linear}, dtype=torch.qint8)\nscripted_quantized_model = torch.jit.script(quantized_model)\nscripted_quantized_model.save(\"fbdeit_scripted_quantized.pt\")",
      "out = scripted_quantized_model(img)\nclsidx = torch.argmax(out)\nprint(clsidx.item())\n# The same output 269 should be printed",
      "from torch.utils.mobile_optimizer import optimize_for_mobile\noptimized_scripted_quantized_model = optimize_for_mobile(scripted_quantized_model)\noptimized_scripted_quantized_model.save(\"fbdeit_optimized_scripted_quantized.pt\")",
      "out = optimized_scripted_quantized_model(img)\nclsidx = torch.argmax(out)\nprint(clsidx.item())\n# Again, the same output 269 should be printed",
      "optimized_scripted_quantized_model._save_for_lite_interpreter(\"fbdeit_optimized_scripted_quantized_lite.ptl\")\nptl = torch.jit.load(\"fbdeit_optimized_scripted_quantized_lite.ptl\")",
      "with torch.autograd.profiler.profile(use_cuda=False) as prof1:\n    out = model(img)\nwith torch.autograd.profiler.profile(use_cuda=False) as prof2:\n    out = scripted_model(img)\nwith torch.autograd.profiler.profile(use_cuda=False) as prof3:\n    out = scripted_quantized_model(img)\nwith torch.autograd.profiler.profile(use_cuda=False) as prof4:\n    out = optimized_scripted_quantized_model(img)\nwith torch.autograd.profiler.profile(use_cuda=False) as prof5:\n    out = ptl(img)\n\nprint(\"original model: {:.2f}ms\".format(prof1.self_cpu_time_total/1000))\nprint(\"scripted model: {:.2f}ms\".format(prof2.self_cpu_time_total/1000))\nprint(\"scripted & quantized model: {:.2f}ms\".format(prof3.self_cpu_time_total/1000))\nprint(\"scripted & quantized & optimized model: {:.2f}ms\".format(prof4.self_cpu_time_total/1000))\nprint(\"lite model: {:.2f}ms\".format(prof5.self_cpu_time_total/1000))",
      "import pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({'Model': ['original model','scripted model', 'scripted & quantized model', 'scripted & quantized & optimized model', 'lite model']})\ndf = pd.concat([df, pd.DataFrame([\n    [\"{:.2f}ms\".format(prof1.self_cpu_time_total/1000), \"0%\"],\n    [\"{:.2f}ms\".format(prof2.self_cpu_time_total/1000),\n     \"{:.2f}%\".format((prof1.self_cpu_time_total-prof2.self_cpu_time_total)/prof1.self_cpu_time_total*100)],\n    [\"{:.2f}ms\".format(prof3.self_cpu_time_total/1000),\n     \"{:.2f}%\".format((prof1.self_cpu_time_total-prof3.self_cpu_time_total)/prof1.self_cpu_time_total*100)],\n    [\"{:.2f}ms\".format(prof4.self_cpu_time_total/1000),\n     \"{:.2f}%\".format((prof1.self_cpu_time_total-prof4.self_cpu_time_total)/prof1.self_cpu_time_total*100)],\n    [\"{:.2f}ms\".format(prof5.self_cpu_time_total/1000),\n     \"{:.2f}%\".format((prof1.self_cpu_time_total-prof5.self_cpu_time_total)/prof1.self_cpu_time_total*100)]],\n    columns=['Inference Time', 'Reduction'])], axis=1)\n\nprint(df)\n\n\"\"\"\n        Model                             Inference Time    Reduction\n0   original model                             1236.69ms           0%\n1   scripted model                             1226.72ms        0.81%\n2   scripted & quantized model                  593.19ms       52.03%\n3   scripted & quantized & optimized model      598.01ms       51.64%\n4   lite model                                  600.72ms       51.43%\n\"\"\""
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/recipes/profile_with_itt.html",
    "title": "Profiling PyTorch workloads with The Instrumentation and Tracing Technology (ITT) API\u00b6",
    "code_snippets": [
      "with torch.autograd.profiler.emit_itt():\n  <code-to-be-profiled...>",
      "# sample.py\n\nimport torch\nimport torch.nn as nn\n\nclass ITTSample(nn.Module):\n  def __init__(self):\n    super(ITTSample, self).__init__()\n    self.conv = nn.Conv2d(3, 5, 3)\n    self.linear = nn.Linear(292820, 1000)\n\n  def forward(self, x):\n    x = self.conv(x)\n    x = x.view(x.shape[0], -1)\n    x = self.linear(x)\n    return x\n\ndef main():\n  m = ITTSample\n  # unmark below code for XPU\n  # m = m.to(\"xpu\")\n  x = torch.rand(10, 3, 244, 244)\n  # unmark below code for XPU\n  # x = x.to(\"xpu\")\n  with torch.autograd.profiler.emit_itt():\n    for i in range(3)\n      # Labeling a region with pair of range_push and range_pop\n      #torch.profiler.itt.range_push(f'iteration_{i}')\n      #m(x)\n      #torch.profiler.itt.range_pop()\n\n      # Labeling a region with range scope\n      with torch.profiler.itt.range(f'iteration_{i}'):\n        m(x)\n\nif __name__ == '__main__':\n  main()"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/intermediate/fx_profiling_tutorial.html",
    "title": "(beta) Building a Simple CPU Performance Profiler with FX\u00b6",
    "code_snippets": [
      "import torch\nimport torch.fx\nimport torchvision.models as models\n\nrn18 = models.resnet18()\nrn18.eval()",
      "input = torch.randn(5, 3, 224, 224)\noutput = rn18(input)",
      "import statistics, tabulate, time\nfrom typing import Any, Dict, List\nfrom torch.fx import Interpreter",
      "traced_rn18 = torch.fx.symbolic_trace(rn18)\nprint(traced_rn18.graph)",
      "graph():\n    %x : torch.Tensor [num_users=1] = placeholder[target=x]\n    %conv1 : [num_users=1] = call_module[target=conv1](args = (%x,), kwargs = {})\n    %bn1 : [num_users=1] = call_module[target=bn1](args = (%conv1,), kwargs = {})\n    %relu : [num_users=1] = call_module[target=relu](args = (%bn1,), kwargs = {})\n    %maxpool : [num_users=2] = call_module[target=maxpool](args = (%relu,), kwargs = {})\n    %layer1_0_conv1 : [num_users=1] = call_module[target=layer1.0.conv1](args = (%maxpool,), kwargs = {})\n    %layer1_0_bn1 : [num_users=1] = call_module[target=layer1.0.bn1](args = (%layer1_0_conv1,), kwargs = {})\n    %layer1_0_relu : [num_users=1] = call_module[target=layer1.0.relu](args = (%layer1_0_bn1,), kwargs = {})\n    %layer1_0_conv2 : [num_users=1] = call_module[target=layer1.0.conv2](args = (%layer1_0_relu,), kwargs = {})\n    %layer1_0_bn2 : [num_users=1] = call_module[target=layer1.0.bn2](args = (%layer1_0_conv2,), kwargs = {})\n    %add : [num_users=1] = call_function[target=operator.add](args = (%layer1_0_bn2, %maxpool), kwargs = {})\n    %layer1_0_relu_1 : [num_users=2] = call_module[target=layer1.0.relu](args = (%add,), kwargs = {})\n    %layer1_1_conv1 : [num_users=1] = call_module[target=layer1.1.conv1](args = (%layer1_0_relu_1,), kwargs = {})\n    %layer1_1_bn1 : [num_users=1] = call_module[target=layer1.1.bn1](args = (%layer1_1_conv1,), kwargs = {})\n    %layer1_1_relu : [num_users=1] = call_module[target=layer1.1.relu](args = (%layer1_1_bn1,), kwargs = {})\n    %layer1_1_conv2 : [num_users=1] = call_module[target=layer1.1.conv2](args = (%layer1_1_relu,), kwargs = {})\n    %layer1_1_bn2 : [num_users=1] = call_module[target=layer1.1.bn2](args = (%layer1_1_conv2,), kwargs = {})\n    %add_1 : [num_users=1] = call_function[target=operator.add](args = (%layer1_1_bn2, %layer1_0_relu_1), kwargs = {})\n    %layer1_1_relu_1 : [num_users=2] = call_module[target=layer1.1.relu](args = (%add_1,), kwargs = {})\n    %layer2_0_conv1 : [num_users=1] = call_module[target=layer2.0.conv1](args = (%layer1_1_relu_1,), kwargs = {})\n    %layer2_0_bn1 : [num_users=1] = call_module[target=layer2.0.bn1](args = (%layer2_0_conv1,), kwargs = {})\n    %layer2_0_relu : [num_users=1] = call_module[target=layer2.0.relu](args = (%layer2_0_bn1,), kwargs = {})\n    %layer2_0_conv2 : [num_users=1] = call_module[target=layer2.0.conv2](args = (%layer2_0_relu,), kwargs = {})\n    %layer2_0_bn2 : [num_users=1] = call_module[target=layer2.0.bn2](args = (%layer2_0_conv2,), kwargs = {})\n    %layer2_0_downsample_0 : [num_users=1] = call_module[target=layer2.0.downsample.0](args = (%layer1_1_relu_1,), kwargs = {})\n    %layer2_0_downsample_1 : [num_users=1] = call_module[target=layer2.0.downsample.1](args = (%layer2_0_downsample_0,), kwargs = {})\n    %add_2 : [num_users=1] = call_function[target=operator.add](args = (%layer2_0_bn2, %layer2_0_downsample_1), kwargs = {})\n    %layer2_0_relu_1 : [num_users=2] = call_module[target=layer2.0.relu](args = (%add_2,), kwargs = {})\n    %layer2_1_conv1 : [num_users=1] = call_module[target=layer2.1.conv1](args = (%layer2_0_relu_1,), kwargs = {})\n    %layer2_1_bn1 : [num_users=1] = call_module[target=layer2.1.bn1](args = (%layer2_1_conv1,), kwargs = {})\n    %layer2_1_relu : [num_users=1] = call_module[target=layer2.1.relu](args = (%layer2_1_bn1,), kwargs = {})\n    %layer2_1_conv2 : [num_users=1] = call_module[target=layer2.1.conv2](args = (%layer2_1_relu,), kwargs = {})\n    %layer2_1_bn2 : [num_users=1] = call_module[target=layer2.1.bn2](args = (%layer2_1_conv2,), kwargs = {})\n    %add_3 : [num_users=1] = call_function[target=operator.add](args = (%layer2_1_bn2, %layer2_0_relu_1), kwargs = {})\n    %layer2_1_relu_1 : [num_users=2] = call_module[target=layer2.1.relu](args = (%add_3,), kwargs = {})\n    %layer3_0_conv1 : [num_users=1] = call_module[target=layer3.0.conv1](args = (%layer2_1_relu_1,), kwargs = {})\n    %layer3_0_bn1 : [num_users=1] = call_module[target=layer3.0.bn1](args = (%layer3_0_conv1,), kwargs = {})\n    %layer3_0_relu : [num_users=1] = call_module[target=layer3.0.relu](args = (%layer3_0_bn1,), kwargs = {})\n    %layer3_0_conv2 : [num_users=1] = call_module[target=layer3.0.conv2](args = (%layer3_0_relu,), kwargs = {})\n    %layer3_0_bn2 : [num_users=1] = call_module[target=layer3.0.bn2](args = (%layer3_0_conv2,), kwargs = {})\n    %layer3_0_downsample_0 : [num_users=1] = call_module[target=layer3.0.downsample.0](args = (%layer2_1_relu_1,), kwargs = {})\n    %layer3_0_downsample_1 : [num_users=1] = call_module[target=layer3.0.downsample.1](args = (%layer3_0_downsample_0,), kwargs = {})\n    %add_4 : [num_users=1] = call_function[target=operator.add](args = (%layer3_0_bn2, %layer3_0_downsample_1), kwargs = {})\n    %layer3_0_relu_1 : [num_users=2] = call_module[target=layer3.0.relu](args = (%add_4,), kwargs = {})\n    %layer3_1_conv1 : [num_users=1] = call_module[target=layer3.1.conv1](args = (%layer3_0_relu_1,), kwargs = {})\n    %layer3_1_bn1 : [num_users=1] = call_module[target=layer3.1.bn1](args = (%layer3_1_conv1,), kwargs = {})\n    %layer3_1_relu : [num_users=1] = call_module[target=layer3.1.relu](args = (%layer3_1_bn1,), kwargs = {})\n    %layer3_1_conv2 : [num_users=1] = call_module[target=layer3.1.conv2](args = (%layer3_1_relu,), kwargs = {})\n    %layer3_1_bn2 : [num_users=1] = call_module[target=layer3.1.bn2](args = (%layer3_1_conv2,), kwargs = {})\n    %add_5 : [num_users=1] = call_function[target=operator.add](args = (%layer3_1_bn2, %layer3_0_relu_1), kwargs = {})\n    %layer3_1_relu_1 : [num_users=2] = call_module[target=layer3.1.relu](args = (%add_5,), kwargs = {})\n    %layer4_0_conv1 : [num_users=1] = call_module[target=layer4.0.conv1](args = (%layer3_1_relu_1,), kwargs = {})\n    %layer4_0_bn1 : [num_users=1] = call_module[target=layer4.0.bn1](args = (%layer4_0_conv1,), kwargs = {})\n    %layer4_0_relu : [num_users=1] = call_module[target=layer4.0.relu](args = (%layer4_0_bn1,), kwargs = {})\n    %layer4_0_conv2 : [num_users=1] = call_module[target=layer4.0.conv2](args = (%layer4_0_relu,), kwargs = {})\n    %layer4_0_bn2 : [num_users=1] = call_module[target=layer4.0.bn2](args = (%layer4_0_conv2,), kwargs = {})\n    %layer4_0_downsample_0 : [num_users=1] = call_module[target=layer4.0.downsample.0](args = (%layer3_1_relu_1,), kwargs = {})\n    %layer4_0_downsample_1 : [num_users=1] = call_module[target=layer4.0.downsample.1](args = (%layer4_0_downsample_0,), kwargs = {})\n    %add_6 : [num_users=1] = call_function[target=operator.add](args = (%layer4_0_bn2, %layer4_0_downsample_1), kwargs = {})\n    %layer4_0_relu_1 : [num_users=2] = call_module[target=layer4.0.relu](args = (%add_6,), kwargs = {})\n    %layer4_1_conv1 : [num_users=1] = call_module[target=layer4.1.conv1](args = (%layer4_0_relu_1,), kwargs = {})\n    %layer4_1_bn1 : [num_users=1] = call_module[target=layer4.1.bn1](args = (%layer4_1_conv1,), kwargs = {})\n    %layer4_1_relu : [num_users=1] = call_module[target=layer4.1.relu](args = (%layer4_1_bn1,), kwargs = {})\n    %layer4_1_conv2 : [num_users=1] = call_module[target=layer4.1.conv2](args = (%layer4_1_relu,), kwargs = {})\n    %layer4_1_bn2 : [num_users=1] = call_module[target=layer4.1.bn2](args = (%layer4_1_conv2,), kwargs = {})\n    %add_7 : [num_users=1] = call_function[target=operator.add](args = (%layer4_1_bn2, %layer4_0_relu_1), kwargs = {})\n    %layer4_1_relu_1 : [num_users=1] = call_module[target=layer4.1.relu](args = (%add_7,), kwargs = {})\n    %avgpool : [num_users=1] = call_module[target=avgpool](args = (%layer4_1_relu_1,), kwargs = {})\n    %flatten : [num_users=1] = call_function[target=torch.flatten](args = (%avgpool, 1), kwargs = {})\n    %fc : [num_users=1] = call_module[target=fc](args = (%flatten,), kwargs = {})\n    return fc",
      "class ProfilingInterpreter(Interpreter):\n    def __init__(self, mod : torch.nn.Module):\n        # Rather than have the user symbolically trace their model,\n        # we're going to do it in the constructor. As a result, the\n        # user can pass in any ``Module`` without having to worry about\n        # symbolic tracing APIs\n        gm = torch.fx.symbolic_trace(mod)\n        super().__init__(gm)\n\n        # We are going to store away two things here:\n        #\n        # 1. A list of total runtimes for ``mod``. In other words, we are\n        #    storing away the time ``mod(...)`` took each time this\n        #    interpreter is called.\n        self.total_runtime_sec : List[float] = []\n        # 2. A map from ``Node`` to a list of times (in seconds) that\n        #    node took to run. This can be seen as similar to (1) but\n        #    for specific sub-parts of the model.\n        self.runtimes_sec : Dict[torch.fx.Node, List[float]] = {}\n\n    ######################################################################\n    # Next, let's override our first method: ``run()``. ``Interpreter``'s ``run``\n    # method is the top-level entry point for execution of the model. We will\n    # want to intercept this so that we can record the total runtime of the\n    # model.\n\n    def run(self, *args) -> Any:\n        # Record the time we started running the model\n        t_start = time.time()\n        # Run the model by delegating back into Interpreter.run()\n        return_val = super().run(*args)\n        # Record the time we finished running the model\n        t_end = time.time()\n        # Store the total elapsed time this model execution took in the\n        # ``ProfilingInterpreter``\n        self.total_runtime_sec.append(t_end - t_start)\n        return return_val\n\n    ######################################################################\n    # Now, let's override ``run_node``. ``Interpreter`` calls ``run_node`` each\n    # time it executes a single node. We will intercept this so that we\n    # can measure and record the time taken for each individual call in\n    # the model.\n\n    def run_node(self, n : torch.fx.Node) -> Any:\n        # Record the time we started running the op\n        t_start = time.time()\n        # Run the op by delegating back into Interpreter.run_node()\n        return_val = super().run_node(n)\n        # Record the time we finished running the op\n        t_end = time.time()\n        # If we don't have an entry for this node in our runtimes_sec\n        # data structure, add one with an empty list value.\n        self.runtimes_sec.setdefault(n, [])\n        # Record the total elapsed time for this single invocation\n        # in the runtimes_sec data structure\n        self.runtimes_sec[n].append(t_end - t_start)\n        return return_val\n\n    ######################################################################\n    # Finally, we are going to define a method (one which doesn't override\n    # any ``Interpreter`` method) that provides us a nice, organized view of\n    # the data we have collected.\n\n    def summary(self, should_sort : bool = False) -> str:\n        # Build up a list of summary information for each node\n        node_summaries : List[List[Any]] = []\n        # Calculate the mean runtime for the whole network. Because the\n        # network may have been called multiple times during profiling,\n        # we need to summarize the runtimes. We choose to use the\n        # arithmetic mean for this.\n        mean_total_runtime = statistics.mean(self.total_runtime_sec)\n\n        # For each node, record summary statistics\n        for node, runtimes in self.runtimes_sec.items():\n            # Similarly, compute the mean runtime for ``node``\n            mean_runtime = statistics.mean(runtimes)\n            # For easier understanding, we also compute the percentage\n            # time each node took with respect to the whole network.\n            pct_total = mean_runtime / mean_total_runtime * 100\n            # Record the node's type, name of the node, mean runtime, and\n            # percent runtime.\n            node_summaries.append(\n                [node.op, str(node), mean_runtime, pct_total])\n\n        # One of the most important questions to answer when doing performance\n        # profiling is \"Which op(s) took the longest?\". We can make this easy\n        # to see by providing sorting functionality in our summary view\n        if should_sort:\n            node_summaries.sort(key=lambda s: s[2], reverse=True)\n\n        # Use the ``tabulate`` library to create a well-formatted table\n        # presenting our summary information\n        headers : List[str] = [\n            'Op type', 'Op', 'Average runtime (s)', 'Pct total runtime'\n        ]\n        return tabulate.tabulate(node_summaries, headers=headers)"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/advanced/super_resolution_with_onnxruntime.html",
    "title": "(optional) Exporting a Model from PyTorch to ONNX and Running it using ONNX Runtime\u00b6",
    "code_snippets": [
      "# Some standard imports\nimport numpy as np\n\nfrom torch import nn\nimport torch.utils.model_zoo as model_zoo\nimport torch.onnx",
      "# Super Resolution model definition in PyTorch\nimport torch.nn as nn\nimport torch.nn.init as init\n\n\nclass SuperResolutionNet(nn.Module):\n    def __init__(self, upscale_factor, inplace=False):\n        super(SuperResolutionNet, self).__init__()\n\n        self.relu = nn.ReLU(inplace=inplace)\n        self.conv1 = nn.Conv2d(1, 64, (5, 5), (1, 1), (2, 2))\n        self.conv2 = nn.Conv2d(64, 64, (3, 3), (1, 1), (1, 1))\n        self.conv3 = nn.Conv2d(64, 32, (3, 3), (1, 1), (1, 1))\n        self.conv4 = nn.Conv2d(32, upscale_factor ** 2, (3, 3), (1, 1), (1, 1))\n        self.pixel_shuffle = nn.PixelShuffle(upscale_factor)\n\n        self._initialize_weights()\n\n    def forward(self, x):\n        x = self.relu(self.conv1(x))\n        x = self.relu(self.conv2(x))\n        x = self.relu(self.conv3(x))\n        x = self.pixel_shuffle(self.conv4(x))\n        return x\n\n    def _initialize_weights(self):\n        init.orthogonal_(self.conv1.weight, init.calculate_gain('relu'))\n        init.orthogonal_(self.conv2.weight, init.calculate_gain('relu'))\n        init.orthogonal_(self.conv3.weight, init.calculate_gain('relu'))\n        init.orthogonal_(self.conv4.weight)\n\n# Create the super-resolution model by using the above model definition.\ntorch_model = SuperResolutionNet(upscale_factor=3)",
      "# Load pretrained model weights\nmodel_url = 'https://s3.amazonaws.com/pytorch/test_data/export/superres_epoch100-44c6958e.pth'\nbatch_size = 64    # just a random number\n\n# Initialize model with the pretrained weights\nmap_location = lambda storage, loc: storage\nif torch.cuda.is_available():\n    map_location = None\ntorch_model.load_state_dict(model_zoo.load_url(model_url, map_location=map_location))\n\n# set the model to inference mode\ntorch_model.eval()",
      "# Input to the model\nx = torch.randn(batch_size, 1, 224, 224, requires_grad=True)\ntorch_out = torch_model(x)\n\n# Export the model\ntorch.onnx.export(torch_model,               # model being run\n                  x,                         # model input (or a tuple for multiple inputs)\n                  \"super_resolution.onnx\",   # where to save the model (can be a file or file-like object)\n                  export_params=True,        # store the trained parameter weights inside the model file\n                  opset_version=10,          # the ONNX version to export the model to\n                  do_constant_folding=True,  # whether to execute constant folding for optimization\n                  input_names = ['input'],   # the model's input names\n                  output_names = ['output'], # the model's output names\n                  dynamic_axes={'input' : {0 : 'batch_size'},    # variable length axes\n                                'output' : {0 : 'batch_size'}})",
      "import onnx\n\nonnx_model = onnx.load(\"super_resolution.onnx\")\nonnx.checker.check_model(onnx_model)",
      "import onnxruntime\n\nort_session = onnxruntime.InferenceSession(\"super_resolution.onnx\", providers=[\"CPUExecutionProvider\"])\n\ndef to_numpy(tensor):\n    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n\n# compute ONNX Runtime output prediction\nort_inputs = {ort_session.get_inputs()[0].name: to_numpy(x)}\nort_outs = ort_session.run(None, ort_inputs)\n\n# compare ONNX Runtime and PyTorch results\nnp.testing.assert_allclose(to_numpy(torch_out), ort_outs[0], rtol=1e-03, atol=1e-05)\n\nprint(\"Exported model has been tested with ONNXRuntime, and the result looks good!\")",
      "import time\n\nx = torch.randn(batch_size, 1, 224, 224, requires_grad=True)\n\nstart = time.time()\ntorch_out = torch_model(x)\nend = time.time()\nprint(f\"Inference of Pytorch model used {end - start} seconds\")\n\nort_inputs = {ort_session.get_inputs()[0].name: to_numpy(x)}\nstart = time.time()\nort_outs = ort_session.run(None, ort_inputs)\nend = time.time()\nprint(f\"Inference of ONNX model used {end - start} seconds\")",
      "from PIL import Image\nimport torchvision.transforms as transforms\n\nimg = Image.open(\"./_static/img/cat.jpg\")\n\nresize = transforms.Resize([224, 224])\nimg = resize(img)\n\nimg_ycbcr = img.convert('YCbCr')\nimg_y, img_cb, img_cr = img_ycbcr.split()\n\nto_tensor = transforms.ToTensor()\nimg_y = to_tensor(img_y)\nimg_y.unsqueeze_(0)"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/intermediate/quantized_transfer_learning_tutorial.html",
    "title": "(beta) Quantized Transfer Learning for Computer Vision Tutorial\u00b6",
    "code_snippets": [
      "# Imports\nimport copy\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport time\n\nplt.ion()",
      "pip install numpy\npip install --pre torch torchvision -f https://download.pytorch.org/whl/nightly/cpu/torch_nightly.html\n# For CUDA support use https://download.pytorch.org/whl/nightly/cu101/torch_nightly.html",
      "import torch\nfrom torchvision import transforms, datasets\n\n# Data augmentation and normalization for training\n# Just normalization for validation\ndata_transforms = {\n    'train': transforms.Compose([\n        transforms.Resize(224),\n        transforms.RandomCrop(224),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n    'val': transforms.Compose([\n        transforms.Resize(224),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n}\n\ndata_dir = 'data/hymenoptera_data'\nimage_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n                                          data_transforms[x])\n                  for x in ['train', 'val']}\ndataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=16,\n                                              shuffle=True, num_workers=8)\n              for x in ['train', 'val']}\ndataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\nclass_names = image_datasets['train'].classes\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")",
      "import torchvision\n\ndef imshow(inp, title=None, ax=None, figsize=(5, 5)):\n  \"\"\"Imshow for Tensor.\"\"\"\n  inp = inp.numpy().transpose((1, 2, 0))\n  mean = np.array([0.485, 0.456, 0.406])\n  std = np.array([0.229, 0.224, 0.225])\n  inp = std * inp + mean\n  inp = np.clip(inp, 0, 1)\n  if ax is None:\n    fig, ax = plt.subplots(1, figsize=figsize)\n  ax.imshow(inp)\n  ax.set_xticks([])\n  ax.set_yticks([])\n  if title is not None:\n    ax.set_title(title)\n\n# Get a batch of training data\ninputs, classes = next(iter(dataloaders['train']))\n\n# Make a grid from batch\nout = torchvision.utils.make_grid(inputs, nrow=4)\n\nfig, ax = plt.subplots(1, figsize=(10, 10))\nimshow(out, title=[class_names[x] for x in classes], ax=ax)",
      "def train_model(model, criterion, optimizer, scheduler, num_epochs=25, device='cpu'):\n  \"\"\"\n  Support function for model training.\n\n  Args:\n    model: Model to be trained\n    criterion: Optimization criterion (loss)\n    optimizer: Optimizer to use for training\n    scheduler: Instance of ``torch.optim.lr_scheduler``\n    num_epochs: Number of epochs\n    device: Device to run the training on. Must be 'cpu' or 'cuda'\n  \"\"\"\n  since = time.time()\n\n  best_model_wts = copy.deepcopy(model.state_dict())\n  best_acc = 0.0\n\n  for epoch in range(num_epochs):\n    print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n    print('-' * 10)\n\n    # Each epoch has a training and validation phase\n    for phase in ['train', 'val']:\n      if phase == 'train':\n        model.train()  # Set model to training mode\n      else:\n        model.eval()   # Set model to evaluate mode\n\n      running_loss = 0.0\n      running_corrects = 0\n\n      # Iterate over data.\n      for inputs, labels in dataloaders[phase]:\n        inputs = inputs.to(device)\n        labels = labels.to(device)\n\n        # zero the parameter gradients\n        optimizer.zero_grad()\n\n        # forward\n        # track history if only in train\n        with torch.set_grad_enabled(phase == 'train'):\n          outputs = model(inputs)\n          _, preds = torch.max(outputs, 1)\n          loss = criterion(outputs, labels)\n\n          # backward + optimize only if in training phase\n          if phase == 'train':\n            loss.backward()\n            optimizer.step()\n\n        # statistics\n        running_loss += loss.item() * inputs.size(0)\n        running_corrects += torch.sum(preds == labels.data)\n      if phase == 'train':\n        scheduler.step()\n\n      epoch_loss = running_loss / dataset_sizes[phase]\n      epoch_acc = running_corrects.double() / dataset_sizes[phase]\n\n      print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n        phase, epoch_loss, epoch_acc))\n\n      # deep copy the model\n      if phase == 'val' and epoch_acc > best_acc:\n        best_acc = epoch_acc\n        best_model_wts = copy.deepcopy(model.state_dict())\n\n    print()\n\n  time_elapsed = time.time() - since\n  print('Training complete in {:.0f}m {:.0f}s'.format(\n    time_elapsed // 60, time_elapsed % 60))\n  print('Best val Acc: {:4f}'.format(best_acc))\n\n  # load best model weights\n  model.load_state_dict(best_model_wts)\n  return model",
      "def visualize_model(model, rows=3, cols=3):\n  was_training = model.training\n  model.eval()\n  current_row = current_col = 0\n  fig, ax = plt.subplots(rows, cols, figsize=(cols*2, rows*2))\n\n  with torch.no_grad():\n    for idx, (imgs, lbls) in enumerate(dataloaders['val']):\n      imgs = imgs.cpu()\n      lbls = lbls.cpu()\n\n      outputs = model(imgs)\n      _, preds = torch.max(outputs, 1)\n\n      for jdx in range(imgs.size()[0]):\n        imshow(imgs.data[jdx], ax=ax[current_row, current_col])\n        ax[current_row, current_col].axis('off')\n        ax[current_row, current_col].set_title('predicted: {}'.format(class_names[preds[jdx]]))\n\n        current_col += 1\n        if current_col >= cols:\n          current_row += 1\n          current_col = 0\n        if current_row >= rows:\n          model.train(mode=was_training)\n          return\n    model.train(mode=was_training)",
      "import torchvision.models.quantization as models\n\n# You will need the number of filters in the `fc` for future use.\n# Here the size of each output sample is set to 2.\n# Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\nmodel_fe = models.resnet18(pretrained=True, progress=True, quantize=True)\nnum_ftrs = model_fe.fc.in_features",
      "from torch import nn\n\ndef create_combined_model(model_fe):\n  # Step 1. Isolate the feature extractor.\n  model_fe_features = nn.Sequential(\n    model_fe.quant,  # Quantize the input\n    model_fe.conv1,\n    model_fe.bn1,\n    model_fe.relu,\n    model_fe.maxpool,\n    model_fe.layer1,\n    model_fe.layer2,\n    model_fe.layer3,\n    model_fe.layer4,\n    model_fe.avgpool,\n    model_fe.dequant,  # Dequantize the output\n  )\n\n  # Step 2. Create a new \"head\"\n  new_head = nn.Sequential(\n    nn.Dropout(p=0.5),\n    nn.Linear(num_ftrs, 2),\n  )\n\n  # Step 3. Combine, and don't forget the quant stubs.\n  new_model = nn.Sequential(\n    model_fe_features,\n    nn.Flatten(1),\n    new_head,\n  )\n  return new_model",
      "import torch.optim as optim\nnew_model = create_combined_model(model_fe)\nnew_model = new_model.to('cpu')\n\ncriterion = nn.CrossEntropyLoss()\n\n# Note that we are only training the head.\noptimizer_ft = optim.SGD(new_model.parameters(), lr=0.01, momentum=0.9)\n\n# Decay LR by a factor of 0.1 every 7 epochs\nexp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)",
      "# notice `quantize=False`\nmodel = models.resnet18(pretrained=True, progress=True, quantize=False)\nnum_ftrs = model.fc.in_features\n\n# Step 1\nmodel.train()\nmodel.fuse_model()\n# Step 2\nmodel_ft = create_combined_model(model)\nmodel_ft[0].qconfig = torch.quantization.default_qat_qconfig  # Use default QAT configuration\n# Step 3\nmodel_ft = torch.quantization.prepare_qat(model_ft, inplace=True)",
      "from torch.quantization import convert\nmodel_ft_tuned.cpu()\n\nmodel_quantized_and_trained = convert(model_ft_tuned, inplace=False)"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/recipes/torch_export_challenges_solutions.html",
    "title": "Demonstration of torch.export flow, common challenges and the solutions to address them\u00b6",
    "code_snippets": [
      "import numpy as np\nimport torch\nfrom torchvision.models.video import MViT_V1_B_Weights, mvit_v1_b\nimport traceback as tb\n\nmodel = mvit_v1_b(weights=MViT_V1_B_Weights.DEFAULT)\n\n# Create a batch of 2 videos, each with 16 frames of shape 224x224x3.\ninput_frames = torch.randn(2,16, 224, 224, 3)\n# Transpose to get [1, 3, num_clips, height, width].\ninput_frames = np.transpose(input_frames, (0, 4, 1, 2, 3))\n\n# Export the model.\nexported_program = torch.export.export(\n    model,\n    (input_frames,),\n)\n\n# Create a batch of 4 videos, each with 16 frames of shape 224x224x3.\ninput_frames = torch.randn(4,16, 224, 224, 3)\ninput_frames = np.transpose(input_frames, (0, 4, 1, 2, 3))\ntry:\n    exported_program.module()(input_frames)\nexcept Exception:\n    tb.print_exc()",
      "import numpy as np\nimport torch\nfrom torchvision.models.video import MViT_V1_B_Weights, mvit_v1_b\nimport traceback as tb\n\n\nmodel = mvit_v1_b(weights=MViT_V1_B_Weights.DEFAULT)\n\n# Create a batch of 2 videos, each with 16 frames of shape 224x224x3.\ninput_frames = torch.randn(2,16, 224, 224, 3)\n\n# Transpose to get [1, 3, num_clips, height, width].\ninput_frames = np.transpose(input_frames, (0, 4, 1, 2, 3))\n\n# Export the model.\nbatch_dim = torch.export.Dim(\"batch\", min=2, max=16)\nexported_program = torch.export.export(\n    model,\n    (input_frames,),\n    # Specify the first dimension of the input x as dynamic\n    dynamic_shapes={\"x\": {0: batch_dim}},\n)\n\n# Create a batch of 4 videos, each with 16 frames of shape 224x224x3.\ninput_frames = torch.randn(4,16, 224, 224, 3)\ninput_frames = np.transpose(input_frames, (0, 4, 1, 2, 3))\ntry:\n    exported_program.module()(input_frames)\nexcept Exception:\n    tb.print_exc()",
      "import torch\nfrom transformers import WhisperProcessor, WhisperForConditionalGeneration\nfrom datasets import load_dataset\n\n# load model\nmodel = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny\")\n\n# dummy inputs for exporting the model\ninput_features = torch.randn(1,80, 3000)\nattention_mask = torch.ones(1, 3000)\ndecoder_input_ids = torch.tensor([[1, 1, 1 , 1]]) * model.config.decoder_start_token_id\n\nmodel.eval()\n\nexported_program: torch.export.ExportedProgram= torch.export.export(model, args=(input_features, attention_mask, decoder_input_ids,))",
      "torch._dynamo.exc.InternalTorchDynamoError: AttributeError: 'DynamicCache' object has no attribute 'key_cache'",
      "import torch\nfrom transformers import WhisperProcessor, WhisperForConditionalGeneration\nfrom datasets import load_dataset\n\n# load model\nmodel = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny\")\n\n# dummy inputs for exporting the model\ninput_features = torch.randn(1,80, 3000)\nattention_mask = torch.ones(1, 3000)\ndecoder_input_ids = torch.tensor([[1, 1, 1 , 1]]) * model.config.decoder_start_token_id\n\nmodel.eval()\n\nexported_program: torch.export.ExportedProgram= torch.export.export(model, args=(input_features, attention_mask, decoder_input_ids,), strict=False)",
      "import torch\nfrom models.blip import blip_decoder\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nimage_size = 384\nimage = torch.randn(1, 3,384,384).to(device)\ncaption_input = \"\"\n\nmodel_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_capfilt_large.pth'\nmodel = blip_decoder(pretrained=model_url, image_size=image_size, vit='base')\nmodel.eval()\nmodel = model.to(device)\n\nexported_program: torch.export.ExportedProgram= torch.export.export(model, args=(image,caption_input,), strict=False)",
      "File \"/BLIP/models/blip.py\", line 112, in forward\n    text.input_ids[:,0] = self.tokenizer.bos_token_id\n  File \"/anaconda3/envs/export/lib/python3.10/site-packages/torch/_subclasses/functional_tensor.py\", line 545, in __torch_dispatch__\n    outs_unwrapped = func._op_dk(\nRuntimeError: cannot mutate tensors with frozen storage",
      "ep = torch.export.export(\n    self._predict,\n    args=(unnorm_coords, labels, unnorm_box, mask_input, multimask_output),\n    kwargs={\"return_logits\": return_logits},\n    strict=False,\n)",
      "Traceback (most recent call last):\n  File \"/sam2/image_predict.py\", line 20, in <module>\n    masks, scores, _ = predictor.predict(\n  File \"/sam2/sam2/sam2_image_predictor.py\", line 312, in predict\n    ep = torch.export.export(\n  File \"python3.10/site-packages/torch/export/__init__.py\", line 359, in export\n    raise ValueError(\nValueError: Expected `mod` to be an instance of `torch.nn.Module`, got <class 'method'>.",
      "class ExportHelper(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(_, *args, **kwargs):\n        return self._predict(*args, **kwargs)\n\n model_to_export = ExportHelper()\n ep = torch.export.export(\n      model_to_export,\n      args=(unnorm_coords, labels, unnorm_box, mask_input,  multimask_output),\n      kwargs={\"return_logits\": return_logits},\n      strict=False,\n      )"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/intermediate/per_sample_grads.html",
    "title": "Per-sample-gradients\u00b6",
    "code_snippets": [
      "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\ntorch.manual_seed(0)\n\n# Here's a simple CNN and loss function:\n\nclass SimpleCNN(nn.Module):\n    def __init__(self):\n        super(SimpleCNN, self).__init__()\n        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n        self.fc1 = nn.Linear(9216, 128)\n        self.fc2 = nn.Linear(128, 10)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = F.relu(x)\n        x = self.conv2(x)\n        x = F.relu(x)\n        x = F.max_pool2d(x, 2)\n        x = torch.flatten(x, 1)\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.fc2(x)\n        output = F.log_softmax(x, dim=1)\n        return output\n\ndef loss_fn(predictions, targets):\n    return F.nll_loss(predictions, targets)",
      "device = 'cuda'\n\nnum_models = 10\nbatch_size = 64\ndata = torch.randn(batch_size, 1, 28, 28, device=device)\n\ntargets = torch.randint(10, (64,), device=device)",
      "def compute_grad(sample, target):\n    sample = sample.unsqueeze(0)  # prepend batch dimension for processing\n    target = target.unsqueeze(0)\n\n    prediction = model(sample)\n    loss = loss_fn(prediction, target)\n\n    return torch.autograd.grad(loss, list(model.parameters()))\n\n\ndef compute_sample_grads(data, targets):\n    \"\"\" manually process each sample with per sample gradient \"\"\"\n    sample_grads = [compute_grad(data[i], targets[i]) for i in range(batch_size)]\n    sample_grads = zip(*sample_grads)\n    sample_grads = [torch.stack(shards) for shards in sample_grads]\n    return sample_grads\n\nper_sample_grads = compute_sample_grads(data, targets)",
      "torch.Size([64, 32, 1, 3, 3])",
      "from torch.func import functional_call, vmap, grad\n\nparams = {k: v.detach() for k, v in model.named_parameters()}\nbuffers = {k: v.detach() for k, v in model.named_buffers()}",
      "def compute_loss(params, buffers, sample, target):\n    batch = sample.unsqueeze(0)\n    targets = target.unsqueeze(0)\n\n    predictions = functional_call(model, (params, buffers), (batch,))\n    loss = loss_fn(predictions, targets)\n    return loss",
      "for per_sample_grad, ft_per_sample_grad in zip(per_sample_grads, ft_per_sample_grads.values()):\n    assert torch.allclose(per_sample_grad, ft_per_sample_grad, atol=3e-3, rtol=1e-5)",
      "def get_perf(first, first_descriptor, second, second_descriptor):\n    \"\"\"takes torch.benchmark objects and compares delta of second vs first.\"\"\"\n    second_res = second.times[0]\n    first_res = first.times[0]\n\n    gain = (first_res-second_res)/first_res\n    if gain < 0: gain *=-1\n    final_gain = gain*100\n\n    print(f\"Performance delta: {final_gain:.4f} percent improvement with {first_descriptor} \")\n\nfrom torch.utils.benchmark import Timer\n\nwithout_vmap = Timer(stmt=\"compute_sample_grads(data, targets)\", globals=globals())\nwith_vmap = Timer(stmt=\"ft_compute_sample_grad(params, buffers, data, targets)\",globals=globals())\nno_vmap_timing = without_vmap.timeit(100)\nwith_vmap_timing = with_vmap.timeit(100)\n\nprint(f'Per-sample-grads without vmap {no_vmap_timing}')\nprint(f'Per-sample-grads with vmap {with_vmap_timing}')\n\nget_perf(with_vmap_timing, \"vmap\", no_vmap_timing, \"no vmap\")",
      "Per-sample-grads without vmap <torch.utils.benchmark.utils.common.Measurement object at 0x7f81d7c5ae90>\ncompute_sample_grads(data, targets)\n  62.00 ms\n  1 measurement, 100 runs , 1 thread\nPer-sample-grads with vmap <torch.utils.benchmark.utils.common.Measurement object at 0x7f81b97bada0>\nft_compute_sample_grad(params, buffers, data, targets)\n  3.40 ms\n  1 measurement, 100 runs , 1 thread\nPerformance delta: 1721.9124 percent improvement with vmap"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/intermediate/fx_conv_bn_fuser.html",
    "title": "(beta) Building a Convolution/Batch Norm fuser in FX\u00b6",
    "code_snippets": [
      "from typing import Type, Dict, Any, Tuple, Iterable\nimport copy\nimport torch.fx as fx\nimport torch\nimport torch.nn as nn",
      "class WrappedBatchNorm(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.mod = nn.BatchNorm2d(1)\n    def forward(self, x):\n        return self.mod(x)\n\nclass M(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 1, 1)\n        self.bn1 = nn.BatchNorm2d(1)\n        self.conv2 = nn.Conv2d(1, 1, 1)\n        self.nested = nn.Sequential(\n            nn.BatchNorm2d(1),\n            nn.Conv2d(1, 1, 1),\n        )\n        self.wrapped = WrappedBatchNorm()\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.conv2(x)\n        x = self.nested(x)\n        x = self.wrapped(x)\n        return x\n\nmodel = M()\n\nmodel.eval()",
      "traced_model = torch.fx.symbolic_trace(model)\nprint(traced_model.graph)",
      "def fuse_conv_bn_eval(conv, bn):\n    \"\"\"\n    Given a conv Module `A` and an batch_norm module `B`, returns a conv\n    module `C` such that C(x) == B(A(x)) in inference mode.\n    \"\"\"\n    assert(not (conv.training or bn.training)), \"Fusion only for eval!\"\n    fused_conv = copy.deepcopy(conv)\n\n    fused_conv.weight, fused_conv.bias = \\\n        fuse_conv_bn_weights(fused_conv.weight, fused_conv.bias,\n                             bn.running_mean, bn.running_var, bn.eps, bn.weight, bn.bias)\n\n    return fused_conv\n\ndef fuse_conv_bn_weights(conv_w, conv_b, bn_rm, bn_rv, bn_eps, bn_w, bn_b):\n    if conv_b is None:\n        conv_b = torch.zeros_like(bn_rm)\n    if bn_w is None:\n        bn_w = torch.ones_like(bn_rm)\n    if bn_b is None:\n        bn_b = torch.zeros_like(bn_rm)\n    bn_var_rsqrt = torch.rsqrt(bn_rv + bn_eps)\n\n    conv_w = conv_w * (bn_w * bn_var_rsqrt).reshape([-1] + [1] * (len(conv_w.shape) - 1))\n    conv_b = (conv_b - bn_rm) * bn_var_rsqrt * bn_w + bn_b\n\n    return torch.nn.Parameter(conv_w), torch.nn.Parameter(conv_b)",
      "def _parent_name(target : str) -> Tuple[str, str]:\n    \"\"\"\n    Splits a ``qualname`` into parent path and last atom.\n    For example, `foo.bar.baz` -> (`foo.bar`, `baz`)\n    \"\"\"\n    *parent, name = target.rsplit('.', 1)\n    return parent[0] if parent else '', name\n\ndef replace_node_module(node: fx.Node, modules: Dict[str, Any], new_module: torch.nn.Module):\n    assert(isinstance(node.target, str))\n    parent_name, name = _parent_name(node.target)\n    setattr(modules[parent_name], name, new_module)\n\n\ndef fuse(model: torch.nn.Module) -> torch.nn.Module:\n    model = copy.deepcopy(model)\n    # The first step of most FX passes is to symbolically trace our model to\n    # obtain a `GraphModule`. This is a representation of our original model\n    # that is functionally identical to our original model, except that we now\n    # also have a graph representation of our forward pass.\n    fx_model: fx.GraphModule = fx.symbolic_trace(model)\n    modules = dict(fx_model.named_modules())\n\n    # The primary representation for working with FX are the `Graph` and the\n    # `Node`. Each `GraphModule` has a `Graph` associated with it - this\n    # `Graph` is also what generates `GraphModule.code`.\n    # The `Graph` itself is represented as a list of `Node` objects. Thus, to\n    # iterate through all of the operations in our graph, we iterate over each\n    # `Node` in our `Graph`.\n    for node in fx_model.graph.nodes:\n        # The FX IR contains several types of nodes, which generally represent\n        # call sites to modules, functions, or methods. The type of node is\n        # determined by `Node.op`.\n        if node.op != 'call_module': # If our current node isn't calling a Module then we can ignore it.\n            continue\n        # For call sites, `Node.target` represents the module/function/method\n        # that's being called. Here, we check `Node.target` to see if it's a\n        # batch norm module, and then check `Node.args[0].target` to see if the\n        # input `Node` is a convolution.\n        if type(modules[node.target]) is nn.BatchNorm2d and type(modules[node.args[0].target]) is nn.Conv2d:\n            if len(node.args[0].users) > 1:  # Output of conv is used by other nodes\n                continue\n            conv = modules[node.args[0].target]\n            bn = modules[node.target]\n            fused_conv = fuse_conv_bn_eval(conv, bn)\n            replace_node_module(node.args[0], modules, fused_conv)\n            # As we've folded the batch nor into the conv, we need to replace all uses\n            # of the batch norm with the conv.\n            node.replace_all_uses_with(node.args[0])\n            # Now that all uses of the batch norm have been replaced, we can\n            # safely remove the batch norm.\n            fx_model.graph.erase_node(node)\n    fx_model.graph.lint()\n    # After we've modified our graph, we need to recompile our graph in order\n    # to keep the generated code in sync.\n    fx_model.recompile()\n    return fx_model",
      "fused_model = fuse(model)\nprint(fused_model.code)\ninp = torch.randn(5, 1, 1, 1)\ntorch.testing.assert_allclose(fused_model(inp), model(inp))",
      "import torchvision.models as models\nimport time\n\nrn18 = models.resnet18()\nrn18.eval()\n\ninp = torch.randn(10, 3, 224, 224)\noutput = rn18(inp)\n\ndef benchmark(model, iters=20):\n    for _ in range(10):\n        model(inp)\n    begin = time.time()\n    for _ in range(iters):\n        model(inp)\n    return str(time.time()-begin)\n\nfused_rn18 = fuse(rn18)\nprint(\"Unfused time: \", benchmark(rn18))\nprint(\"Fused time: \", benchmark(fused_rn18))",
      "jit_rn18 = torch.jit.script(fused_rn18)\nprint(\"jit time: \", benchmark(jit_rn18))\n\n\n############\n# Conclusion\n# ----------\n# As we can see, using FX we can easily write static graph transformations on\n# PyTorch code.\n#\n# Since FX is still in beta, we would be happy to hear any\n# feedback you have about using it. Please feel free to use the\n# PyTorch Forums (https://discuss.pytorch.org/) and the issue tracker\n# (https://github.com/pytorch/pytorch/issues) to provide any feedback\n# you might have."
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/recipes/zero_redundancy_optimizer.html",
    "title": "Shard Optimizer States with ZeroRedundancyOptimizer\u00b6",
    "code_snippets": [
      "import os\nimport torch\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.distributed.optim import ZeroRedundancyOptimizer\nfrom torch.nn.parallel import DistributedDataParallel as DDP\n\ndef print_peak_memory(prefix, device):\n    if device == 0:\n        print(f\"{prefix}: {torch.cuda.max_memory_allocated(device) // 1e6}MB \")\n\ndef example(rank, world_size, use_zero):\n    torch.manual_seed(0)\n    torch.cuda.manual_seed(0)\n    os.environ['MASTER_ADDR'] = 'localhost'\n    os.environ['MASTER_PORT'] = '29500'\n    # create default process group\n    dist.init_process_group(\"gloo\", rank=rank, world_size=world_size)\n\n    # create local model\n    model = nn.Sequential(*[nn.Linear(2000, 2000).to(rank) for _ in range(20)])\n    print_peak_memory(\"Max memory allocated after creating local model\", rank)\n\n    # construct DDP model\n    ddp_model = DDP(model, device_ids=[rank])\n    print_peak_memory(\"Max memory allocated after creating DDP\", rank)\n\n    # define loss function and optimizer\n    loss_fn = nn.MSELoss()\n    if use_zero:\n        optimizer = ZeroRedundancyOptimizer(\n            ddp_model.parameters(),\n            optimizer_class=torch.optim.Adam,\n            lr=0.01\n        )\n    else:\n        optimizer = torch.optim.Adam(ddp_model.parameters(), lr=0.01)\n\n    # forward pass\n    outputs = ddp_model(torch.randn(20, 2000).to(rank))\n    labels = torch.randn(20, 2000).to(rank)\n    # backward pass\n    loss_fn(outputs, labels).backward()\n\n    # update parameters\n    print_peak_memory(\"Max memory allocated before optimizer step()\", rank)\n    optimizer.step()\n    print_peak_memory(\"Max memory allocated after optimizer step()\", rank)\n\n    print(f\"params sum is: {sum(model.parameters()).sum()}\")\n\n\n\ndef main():\n    world_size = 2\n    print(\"=== Using ZeroRedundancyOptimizer ===\")\n    mp.spawn(example,\n        args=(world_size, True),\n        nprocs=world_size,\n        join=True)\n\n    print(\"=== Not Using ZeroRedundancyOptimizer ===\")\n    mp.spawn(example,\n        args=(world_size, False),\n        nprocs=world_size,\n        join=True)\n\nif __name__==\"__main__\":\n    main()"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/intermediate/inductor_debug_cpu.html",
    "title": "Inductor CPU backend debugging and profiling\u00b6",
    "code_snippets": [
      "import torch\n\ndef foo1(x1, x2):\n    a = torch.neg(x1)\n    b = torch.maximum(x2, a)\n    y = torch.cat([b], dim=0)\n    return y\n\nx1 = torch.randint(256, (1, 8), dtype=torch.uint8)\nx2 = torch.randint(256, (8390, 8), dtype=torch.uint8)\n\ncompiled_foo1 = torch.compile(foo1)\nresult = compiled_foo1(x1, x2)",
      "def neg1(x):\n    return f\"decltype({x})(-{x})\"",
      "torch._inductor.debug: [WARNING] model___20 debug trace: /tmp/torchinductor_root/rx/crxfi2ybd7yp5sbj2pnhw33wfhtdw7wumvrobyp5sjvdui5ktjc2.debug",
      "def forward1(self, arg0_1, arg1_1):\n    neg = torch.ops.aten.neg.default(arg0_1);  arg0_1 = None\n    maximum = torch.ops.aten.maximum.default(arg1_1, neg);  arg1_1 = neg = None\n    clone = torch.ops.aten.clone.default(maximum);  maximum = None\n    return (clone,)",
      "import torch\nfrom torch._inductor.async_compile import AsyncCompile\nasync_compile = AsyncCompile()\n\ncpp_fused_cat_maximum_neg_0 = async_compile.cpp('''\n#include \"/tmp/torchinductor_root/gv/cgv6n5aotqjo5w4vknjibhengeycuattfto532hkxpozszcgxr3x.h\"\nextern \"C\" void kernel(const unsigned char* in_ptr0,\n                       const unsigned char* in_ptr1,\n                       unsigned char* out_ptr0)\n{\n    {\n        #pragma GCC ivdep\n        for(long i0=static_cast<long>(0L); i0<static_cast<long>(8390L); i0+=static_cast<long>(1L))\n        {\n            #pragma GCC ivdep\n            for(long i1=static_cast<long>(0L); i1<static_cast<long>(8L); i1+=static_cast<long>(1L))\n            {\n                auto tmp0 = in_ptr0[static_cast<long>(i1 + (8L*i0))];\n                auto tmp1 = in_ptr1[static_cast<long>(i1)];\n                // Corresponding FX code line: neg = torch.ops.aten.neg.default(arg0_1);  arg0_1 = None\n                auto tmp2 = decltype(tmp1)(-tmp1);\n                // Corresponding FX code line: maximum = torch.ops.aten.maximum.default(arg1_1, neg);  arg1_1 = neg = None\n                auto tmp3 = max_propagate_nan(tmp0, tmp2);\n                // Corresponding FX code line: clone = torch.ops.aten.clone.default(maximum);  maximum = None\n                out_ptr0[static_cast<long>(i1 + (8L*i0))] = tmp3;\n            }\n        }\n    }\n}''')",
      "torch.neg (Python) -> torch.ops.aten.neg.default (within FX graph) -> ops.neg (within IR node) -> tmp2 = -tmp1 (within C++ kernel)",
      "def neg2(x):\n    return f\"-{x}\"",
      "torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n CppCompileError: C++ compile error\n /tmp/torchinductor_root/xg/cxga5tk3b4lkwoxyigrtocjp5s7vc5cg2ikuscf6bk6pjqip2bhx.cpp: In function \u2018void kernel(const unsigned char*, const unsigned char*, unsigned char*)\u2019:\n /tmp/torchinductor_root/xg/cxga5tk3b4lkwoxyigrtocjp5s7vc5cg2ikuscf6bk6pjqip2bhx.cpp:17:57: error: no matching function for call to \u2018max_propagate_nan(unsigned char&, int&)\u2019\n   17 |                 auto tmp3 = max_propagate_nan(tmp0, tmp2);\n        |                                                         ^\n In file included from /tmp/torchinductor_root/xg/cxga5tk3b4lkwoxyigrtocjp5s7vc5cg2ikuscf6bk6pjqip2bhx.cpp:2:\n /tmp/torchinductor_root/gv/cgv6n5aotqjo5w4vknjibhengeycuattfto532hkxpozszcgxr3x.h:27:17: note: candidate: \u2018template<class scalar_t> scalar_t max_propagate_nan(scalar_t, scalar_t)\u2019\n 27 | inline scalar_t max_propagate_nan(scalar_t a, scalar_t b) {\n      |                 ^~~~~~~~~~~~~~~~~\n /tmp/torchinductor_root/gv/cgv6n5aotqjo5w4vknjibhengeycuattfto532hkxpozszcgxr3x.h:27:17: note:   template argument deduction/substitution failed:\n/tmp/torchinductor_root/xg/cxga5tk3b4lkwoxyigrtocjp5s7vc5cg2ikuscf6bk6pjqip2bhx.cpp:17:57: note:   deduced conflicting types for parameter \u2018scalar_t\u2019 (\u2018unsigned char\u2019 and \u2018int\u2019)\n 17 |                 auto tmp3 = max_propagate_nan(tmp0, tmp2);\n      |                                                         ^",
      "buf0: SchedulerNode(ComputedBuffer)\nbuf0.writes = [MemoryDep('buf0', c0, {c0: 67120})]\nbuf0.unmet_dependencies = []\nbuf0.met_dependencies =\n    [   MemoryDep('arg0_1', c1, {c0: 8390, c1: 8}),\n        MemoryDep('arg1_1', c0, {c0: 67120})]\nbuf0.users = [NodeUser(node=OUTPUT, can_inplace=False)]\nbuf0.group.device = cpu\nbuf0.group.iteration = ((8390, 8), ())\nbuf0.sizes = ([8390, 8], [])\nclass buf0_loop_body:\n    var_ranges = {z0: 8390, z1: 8}\n    index0 = 8*z0 + z1\n    index1 = z1\n    def body(self, ops):\n        get_index = self.get_index('index0')\n        load = ops.load('arg1_1', get_index)\n        get_index_1 = self.get_index('index1')\n        load_1 = ops.load('arg0_1', get_index_1)\n        neg = ops.neg(load_1)\n        maximum = ops.maximum(load, neg)\n        get_index_2 = self.get_index('index0')\n        store = ops.store('buf0', get_index_2, maximum, None)\n        return store",
      "from torch._dynamo.utils import same\n\ndef foo2(x1, x2):\n    a = torch.neg(x1)\n    b = torch.maximum(x2, a)\n    y = torch.cat([b], dim=0)\n    return y\n\nx1 = torch.randn((1, 8), dtype=torch.float32)\nx2 = torch.randn((8390, 8), dtype=torch.float32)\n\nexpected_result = foo2(x1, x2)\n\ncompiled_foo2 = torch.compile(foo2)\nactual_result = compiled_foo2(x1, x2)\n\nassert same(expected_result, actual_result) == True",
      "def neg3(x):\n    return f\"decltype({x})(2 * {x})\"",
      "torch._dynamo.utils: [ERROR] Accuracy failed: allclose not within tol=0.0001\nTraceback (most recent call last):\n  File \"test_script.py\", line 18, in <module>\n    assert same(expected_result, actual_result) == True\nAssertionError",
      "def forward2(self, arg0_1):\n    neg = torch.ops.aten.neg.default(arg0_1);  arg0_1 = None\n    return (neg,)",
      "# bench.py\nfrom transformers import MobileBertForQuestionAnswering\n# Initialize an eager model\nmodel = MobileBertForQuestionAnswering.from_pretrained(\"csarron/mobilebert-uncased-squad-v2\")\nseq_length = 128\nbs = 128\nvocab_size = model.config.vocab_size\ninput = torch.randint(0, vocab_size, (bs, seq_length), dtype=torch.int64)\ninput_dict = {\"input_ids\": input}\n\n# Initialize the inductor model\ncompiled_model = torch.compile(model)\nwith torch.no_grad():\n    compiled_model(**input_dict)\n\nNUM_ITERS=50\nimport timeit\nwith torch.no_grad():\n    # warmup\n    for _ in range(10):\n        model(**input_dict)\n    eager_t = timeit.timeit(\"model(**input_dict)\", number=NUM_ITERS, globals=globals())\n\nwith torch.no_grad():\n    # warmup\n    for _ in range(10):\n        compiled_model(**input_dict)\n    inductor_t = timeit.timeit(\"compiled_model(**input_dict)\", number=NUM_ITERS, globals=globals())\n# print(f\"eager use: {eager_t * 1000 / NUM_ITERS} ms/iter\")\n# print(f\"inductor use: {inductor_t * 1000 / NUM_ITERS} ms/iter\")\n# print(f\"speed up ratio: {eager_t / inductor_t}\")",
      "from torch._inductor import config\nconfig.cpp.enable_kernel_profile = True",
      "# bench.py\nfrom torch.profiler import profile, schedule, ProfilerActivity\nRESULT_DIR = \"./prof_trace\"\nmy_schedule = schedule(\n    skip_first=10,\n    wait=5,\n    warmup=5,\n    active=1,\n    repeat=5)\n\ndef trace_handler(p):\n    output = p.key_averages().table(sort_by=\"self_cpu_time_total\", row_limit=20)\n    # print(output)\n    p.export_chrome_trace(f\"{RESULT_DIR}/{p.step_num}.json\")\n\nfor _ in range(10):\n    model(**input_dict)  # compiled_model(**input_dict) to get inductor model profiling\n\ntotal = 0\nwith profile(\n    activities=[ProfilerActivity.CPU],\n    schedule=my_schedule,\n    on_trace_ready=trace_handler\n) as p:\n    for _ in range(50):\n        model(**input_dict)  # compiled_model(**input_dict) to get inductor model profiling\n        p.step()",
      "# bench.py\ndef func(arg_0, arg_1, arg_2, arg_3, arg_4):\n    add_0 = arg_0 + arg_1\n    add_1 = add_0 + arg_2\n    mul_1 = add_1 * arg_3\n    add_2 = mul_1 + arg_4\n    arg_2 = add_2\n    return arg_2\n\narg_0 = torch.rand(16384, 512)\narg_1 = torch.rand(1, 512)\narg_2 = torch.zeros(16384, 512)\narg_3 = torch.rand(1, 512)\narg_4 = torch.rand(1, 512)\n\ninput = (arg_0, arg_1, arg_2, arg_3, arg_4)\ninductor_func = torch.compile(func)\nwith torch.no_grad():\n    inductor_func(*input)\n\nimport timeit\nNUM_ITERS=100\nwith torch.no_grad():\n    # warmup\n    for _ in range(10):\n        func(*input)\n    eager_t = timeit.timeit(\"func(*input)\", number=NUM_ITERS, globals=globals())\n\nwith torch.no_grad():\n    # warmup\n    for _ in range(10):\n        inductor_func(*input)\n    inductor_t = timeit.timeit(\"inductor_func(*input)\", number=NUM_ITERS, globals=globals())\n# print(f\"eager use: {eager_t * 1000 / NUM_ITERS} ms/iter\")\n# print(f\"inductor use: {inductor_t * 1000 / NUM_ITERS} ms/iter\")\n# print(f\"speed up ratio: {eager_t / inductor_t}\")"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/intermediate/custom_function_conv_bn_tutorial.html",
    "title": "Fusing Convolution and Batch Norm using Custom Function\u00b6",
    "code_snippets": [
      "import torch\nfrom torch.autograd.function import once_differentiable\nimport torch.nn.functional as F\n\ndef convolution_backward(grad_out, X, weight):\n    grad_input = F.conv2d(X.transpose(0, 1), grad_out.transpose(0, 1)).transpose(0, 1)\n    grad_X = F.conv_transpose2d(grad_out, weight)\n    return grad_X, grad_input\n\nclass Conv2D(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, X, weight):\n        ctx.save_for_backward(X, weight)\n        return F.conv2d(X, weight)\n\n    # Use @once_differentiable by default unless we intend to double backward\n    @staticmethod\n    @once_differentiable\n    def backward(ctx, grad_out):\n        X, weight = ctx.saved_tensors\n        return convolution_backward(grad_out, X, weight)",
      "weight = torch.rand(5, 3, 3, 3, requires_grad=True, dtype=torch.double)\nX = torch.rand(10, 3, 7, 7, requires_grad=True, dtype=torch.double)\ntorch.autograd.gradcheck(Conv2D.apply, (X, weight))",
      "def unsqueeze_all(t):\n    # Helper function to ``unsqueeze`` all the dimensions that we reduce over\n    return t[None, :, None, None]\n\ndef batch_norm_backward(grad_out, X, sum, sqrt_var, N, eps):\n    # We use the formula: ``out = (X - mean(X)) / (sqrt(var(X)) + eps)``\n    # in batch norm 2D forward. To simplify our derivation, we follow the\n    # chain rule and compute the gradients as follows before accumulating\n    # them all into a final grad_input.\n    #  1) ``grad of out wrt var(X)`` * ``grad of var(X) wrt X``\n    #  2) ``grad of out wrt mean(X)`` * ``grad of mean(X) wrt X``\n    #  3) ``grad of out wrt X in the numerator`` * ``grad of X wrt X``\n    # We then rewrite the formulas to use as few extra buffers as possible\n    tmp = ((X - unsqueeze_all(sum) / N) * grad_out).sum(dim=(0, 2, 3))\n    tmp *= -1\n    d_denom = tmp / (sqrt_var + eps)**2  # ``d_denom = -num / denom**2``\n    # It is useful to delete tensors when you no longer need them with ``del``\n    # For example, we could've done ``del tmp`` here because we won't use it later\n    # In this case, it's not a big difference because ``tmp`` only has size of (C,)\n    # The important thing is avoid allocating NCHW-sized tensors unnecessarily\n    d_var = d_denom / (2 * sqrt_var)  # ``denom = torch.sqrt(var) + eps``\n    # Compute ``d_mean_dx`` before allocating the final NCHW-sized grad_input buffer\n    d_mean_dx = grad_out / unsqueeze_all(sqrt_var + eps)\n    d_mean_dx = unsqueeze_all(-d_mean_dx.sum(dim=(0, 2, 3)) / N)\n    # ``d_mean_dx`` has already been reassigned to a C-sized buffer so no need to worry\n\n    # ``(1) unbiased_var(x) = ((X - unsqueeze_all(mean))**2).sum(dim=(0, 2, 3)) / (N - 1)``\n    grad_input = X * unsqueeze_all(d_var * N)\n    grad_input += unsqueeze_all(-d_var * sum)\n    grad_input *= 2 / ((N - 1) * N)\n    # (2) mean (see above)\n    grad_input += d_mean_dx\n    # (3) Add 'grad_out / <factor>' without allocating an extra buffer\n    grad_input *= unsqueeze_all(sqrt_var + eps)\n    grad_input += grad_out\n    grad_input /= unsqueeze_all(sqrt_var + eps)  # ``sqrt_var + eps > 0!``\n    return grad_input\n\nclass BatchNorm(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, X, eps=1e-3):\n        # Don't save ``keepdim`` values for backward\n        sum = X.sum(dim=(0, 2, 3))\n        var = X.var(unbiased=True, dim=(0, 2, 3))\n        N = X.numel() / X.size(1)\n        sqrt_var = torch.sqrt(var)\n        ctx.save_for_backward(X)\n        ctx.eps = eps\n        ctx.sum = sum\n        ctx.N = N\n        ctx.sqrt_var = sqrt_var\n        mean = sum / N\n        denom = sqrt_var + eps\n        out = X - unsqueeze_all(mean)\n        out /= unsqueeze_all(denom)\n        return out\n\n    @staticmethod\n    @once_differentiable\n    def backward(ctx, grad_out):\n        X, = ctx.saved_tensors\n        return batch_norm_backward(grad_out, X, ctx.sum, ctx.sqrt_var, ctx.N, ctx.eps)",
      "a = torch.rand(1, 2, 3, 4, requires_grad=True, dtype=torch.double)\ntorch.autograd.gradcheck(BatchNorm.apply, (a,), fast_mode=False)",
      "class FusedConvBN2DFunction(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, X, conv_weight, eps=1e-3):\n        assert X.ndim == 4  # N, C, H, W\n        # (1) Only need to save this single buffer for backward!\n        ctx.save_for_backward(X, conv_weight)\n\n        # (2) Exact same Conv2D forward from example above\n        X = F.conv2d(X, conv_weight)\n        # (3) Exact same BatchNorm2D forward from example above\n        sum = X.sum(dim=(0, 2, 3))\n        var = X.var(unbiased=True, dim=(0, 2, 3))\n        N = X.numel() / X.size(1)\n        sqrt_var = torch.sqrt(var)\n        ctx.eps = eps\n        ctx.sum = sum\n        ctx.N = N\n        ctx.sqrt_var = sqrt_var\n        mean = sum / N\n        denom = sqrt_var + eps\n        # Try to do as many things in-place as possible\n        # Instead of `out = (X - a) / b`, doing `out = X - a; out /= b`\n        # avoids allocating one extra NCHW-sized buffer here\n        out = X - unsqueeze_all(mean)\n        out /= unsqueeze_all(denom)\n        return out\n\n    @staticmethod\n    def backward(ctx, grad_out):\n        X, conv_weight, = ctx.saved_tensors\n        # (4) Batch norm backward\n        # (5) We need to recompute conv\n        X_conv_out = F.conv2d(X, conv_weight)\n        grad_out = batch_norm_backward(grad_out, X_conv_out, ctx.sum, ctx.sqrt_var,\n                                       ctx.N, ctx.eps)\n        # (6) Conv2d backward\n        grad_X, grad_input = convolution_backward(grad_out, X, conv_weight)\n        return grad_X, grad_input, None, None, None, None, None",
      "import torch.nn as nn\nimport math\n\nclass FusedConvBN(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, exp_avg_factor=0.1,\n                 eps=1e-3, device=None, dtype=None):\n        super(FusedConvBN, self).__init__()\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        # Conv parameters\n        weight_shape = (out_channels, in_channels, kernel_size, kernel_size)\n        self.conv_weight = nn.Parameter(torch.empty(*weight_shape, **factory_kwargs))\n        # Batch norm parameters\n        num_features = out_channels\n        self.num_features = num_features\n        self.eps = eps\n        # Initialize\n        self.reset_parameters()\n\n    def forward(self, X):\n        return FusedConvBN2DFunction.apply(X, self.conv_weight, self.eps)\n\n    def reset_parameters(self) -> None:\n        nn.init.kaiming_uniform_(self.conv_weight, a=math.sqrt(5))",
      "weight = torch.rand(5, 3, 3, 3, requires_grad=True, dtype=torch.double)\nX = torch.rand(2, 3, 4, 4, requires_grad=True, dtype=torch.double)\ntorch.autograd.gradcheck(FusedConvBN2DFunction.apply, (X, weight))",
      "import torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.optim.lr_scheduler import StepLR\n\n# Record memory allocated at the end of the forward pass\nmemory_allocated = [[],[]]\n\nclass Net(nn.Module):\n    def __init__(self, fused=True):\n        super(Net, self).__init__()\n        self.fused = fused\n        if fused:\n            self.convbn1 = FusedConvBN(1, 32, 3)\n            self.convbn2 = FusedConvBN(32, 64, 3)\n        else:\n            self.conv1 = nn.Conv2d(1, 32, 3, 1, bias=False)\n            self.bn1 = nn.BatchNorm2d(32, affine=False, track_running_stats=False)\n            self.conv2 = nn.Conv2d(32, 64, 3, 1, bias=False)\n            self.bn2 = nn.BatchNorm2d(64, affine=False, track_running_stats=False)\n        self.fc1 = nn.Linear(9216, 128)\n        self.dropout = nn.Dropout(0.5)\n        self.fc2 = nn.Linear(128, 10)\n\n    def forward(self, x):\n        if self.fused:\n            x = self.convbn1(x)\n        else:\n            x = self.conv1(x)\n            x = self.bn1(x)\n        F.relu_(x)\n        if self.fused:\n            x = self.convbn2(x)\n        else:\n            x = self.conv2(x)\n            x = self.bn2(x)\n        F.relu_(x)\n        x = F.max_pool2d(x, 2)\n        F.relu_(x)\n        x = x.flatten(1)\n        x = self.fc1(x)\n        x = self.dropout(x)\n        F.relu_(x)\n        x = self.fc2(x)\n        output = F.log_softmax(x, dim=1)\n        if fused:\n            memory_allocated[0].append(torch.cuda.memory_allocated())\n        else:\n            memory_allocated[1].append(torch.cuda.memory_allocated())\n        return output\n\ndef train(model, device, train_loader, optimizer, epoch):\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(device), target.to(device)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = F.nll_loss(output, target)\n        loss.backward()\n        optimizer.step()\n        if batch_idx % 2 == 0:\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                epoch, batch_idx * len(data), len(train_loader.dataset),\n                100. * batch_idx / len(train_loader), loss.item()))\n\ndef test(model, device, test_loader):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    # Use inference mode instead of no_grad, for free improved test-time performance\n    with torch.inference_mode():\n        for data, target in test_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            # sum up batch loss\n            test_loss += F.nll_loss(output, target, reduction='sum').item()\n            # get the index of the max log-probability\n            pred = output.argmax(dim=1, keepdim=True)\n            correct += pred.eq(target.view_as(pred)).sum().item()\n\n    test_loss /= len(test_loader.dataset)\n\n    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n        test_loss, correct, len(test_loader.dataset),\n        100. * correct / len(test_loader.dataset)))\n\nuse_cuda = torch.cuda.is_available()\ndevice = torch.device(\"cuda\" if use_cuda else \"cpu\")\ntrain_kwargs = {'batch_size': 2048}\ntest_kwargs = {'batch_size': 2048}\n\nif use_cuda:\n    cuda_kwargs = {'num_workers': 1,\n                   'pin_memory': True,\n                   'shuffle': True}\n    train_kwargs.update(cuda_kwargs)\n    test_kwargs.update(cuda_kwargs)\n\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.1307,), (0.3081,))\n])\ndataset1 = datasets.MNIST('../data', train=True, download=True,\n                          transform=transform)\ndataset2 = datasets.MNIST('../data', train=False,\n                          transform=transform)\ntrain_loader = torch.utils.data.DataLoader(dataset1, **train_kwargs)\ntest_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)",
      "from statistics import mean\n\ntorch.backends.cudnn.enabled = True\n\nif use_cuda:\n    peak_memory_allocated = []\n\n    for fused in (True, False):\n        torch.manual_seed(123456)\n\n        model = Net(fused=fused).to(device)\n        optimizer = optim.Adadelta(model.parameters(), lr=1.0)\n        scheduler = StepLR(optimizer, step_size=1, gamma=0.7)\n\n        for epoch in range(1):\n            train(model, device, train_loader, optimizer, epoch)\n            test(model, device, test_loader)\n            scheduler.step()\n        peak_memory_allocated.append(torch.cuda.max_memory_allocated())\n        torch.cuda.reset_peak_memory_stats()\n    print(\"cuDNN version:\", torch.backends.cudnn.version())\n    print()\n    print(\"Peak memory allocated:\")\n    print(f\"fused: {peak_memory_allocated[0]/1024**3:.2f}GB, unfused: {peak_memory_allocated[1]/1024**3:.2f}GB\")\n    print(\"Memory allocated at end of forward pass:\")\n    print(f\"fused: {mean(memory_allocated[0])/1024**3:.2f}GB, unfused: {mean(memory_allocated[1])/1024**3:.2f}GB\")"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html",
    "title": "Neural Networks\u00b6",
    "code_snippets": [
      "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Net(nn.Module):\n\n    def __init__(self):\n        super(Net, self).__init__()\n        # 1 input image channel, 6 output channels, 5x5 square convolution\n        # kernel\n        self.conv1 = nn.Conv2d(1, 6, 5)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        # an affine operation: y = Wx + b\n        self.fc1 = nn.Linear(16 * 5 * 5, 120)  # 5*5 from image dimension\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n\n    def forward(self, input):\n        # Convolution layer C1: 1 input image channel, 6 output channels,\n        # 5x5 square convolution, it uses RELU activation function, and\n        # outputs a Tensor with size (N, 6, 28, 28), where N is the size of the batch\n        c1 = F.relu(self.conv1(input))\n        # Subsampling layer S2: 2x2 grid, purely functional,\n        # this layer does not have any parameter, and outputs a (N, 6, 14, 14) Tensor\n        s2 = F.max_pool2d(c1, (2, 2))\n        # Convolution layer C3: 6 input channels, 16 output channels,\n        # 5x5 square convolution, it uses RELU activation function, and\n        # outputs a (N, 16, 10, 10) Tensor\n        c3 = F.relu(self.conv2(s2))\n        # Subsampling layer S4: 2x2 grid, purely functional,\n        # this layer does not have any parameter, and outputs a (N, 16, 5, 5) Tensor\n        s4 = F.max_pool2d(c3, 2)\n        # Flatten operation: purely functional, outputs a (N, 400) Tensor\n        s4 = torch.flatten(s4, 1)\n        # Fully connected layer F5: (N, 400) Tensor input,\n        # and outputs a (N, 120) Tensor, it uses RELU activation function\n        f5 = F.relu(self.fc1(s4))\n        # Fully connected layer F6: (N, 120) Tensor input,\n        # and outputs a (N, 84) Tensor, it uses RELU activation function\n        f6 = F.relu(self.fc2(f5))\n        # Gaussian layer OUTPUT: (N, 84) Tensor input, and\n        # outputs a (N, 10) Tensor\n        output = self.fc3(f6)\n        return output\n\n\nnet = Net()\nprint(net)",
      "10\ntorch.Size([6, 1, 5, 5])",
      "input = torch.randn(1, 1, 32, 32)\nout = net(input)\nprint(out)",
      "net.zero_grad()\nout.backward(torch.randn(1, 10))",
      "output = net(input)\ntarget = torch.randn(10)  # a dummy target, for example\ntarget = target.view(1, -1)  # make it the same shape as output\ncriterion = nn.MSELoss()\n\nloss = criterion(output, target)\nprint(loss)",
      "import torch.optim as optim\n\n# create your optimizer\noptimizer = optim.SGD(net.parameters(), lr=0.01)\n\n# in your training loop:\noptimizer.zero_grad()   # zero the gradient buffers\noutput = net(input)\nloss = criterion(output, target)\nloss.backward()\noptimizer.step()    # Does the update"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/intermediate/pipelining_tutorial.html",
    "title": "Introduction to Distributed Pipeline Parallelism\u00b6",
    "code_snippets": [
      "import torch\nimport torch.nn as nn\nfrom dataclasses import dataclass\n\n@dataclass\nclass ModelArgs:\n   dim: int = 512\n   n_layers: int = 8\n   n_heads: int = 8\n   vocab_size: int = 10000\n\nclass Transformer(nn.Module):\n   def __init__(self, model_args: ModelArgs):\n      super().__init__()\n\n      self.tok_embeddings = nn.Embedding(model_args.vocab_size, model_args.dim)\n\n      # Using a ModuleDict lets us delete layers witout affecting names,\n      # ensuring checkpoints will correctly save and load.\n      self.layers = torch.nn.ModuleDict()\n      for layer_id in range(model_args.n_layers):\n            self.layers[str(layer_id)] = nn.TransformerDecoderLayer(model_args.dim, model_args.n_heads)\n\n      self.norm = nn.LayerNorm(model_args.dim)\n      self.output = nn.Linear(model_args.dim, model_args.vocab_size)\n\n   def forward(self, tokens: torch.Tensor):\n      # Handling layers being 'None' at runtime enables easy pipeline splitting\n      h = self.tok_embeddings(tokens) if self.tok_embeddings else tokens\n\n      for layer in self.layers.values():\n            h = layer(h, h)\n\n      h = self.norm(h) if self.norm else h\n      output = self.output(h).clone() if self.output else h\n      return output",
      "import os\nimport torch.distributed as dist\nfrom torch.distributed.pipelining import pipeline, SplitPoint, PipelineStage, ScheduleGPipe\n\nglobal rank, device, pp_group, stage_index, num_stages\ndef init_distributed():\n   global rank, device, pp_group, stage_index, num_stages\n   rank = int(os.environ[\"LOCAL_RANK\"])\n   world_size = int(os.environ[\"WORLD_SIZE\"])\n   device = torch.device(f\"cuda:{rank}\") if torch.cuda.is_available() else torch.device(\"cpu\")\n   dist.init_process_group()\n\n   # This group can be a sub-group in the N-D parallel case\n   pp_group = dist.new_group()\n   stage_index = rank\n   num_stages = world_size",
      "def manual_model_split(model) -> PipelineStage:\n   if stage_index == 0:\n      # prepare the first stage model\n      for i in range(4, 8):\n            del model.layers[str(i)]\n      model.norm = None\n      model.output = None\n\n   elif stage_index == 1:\n      # prepare the second stage model\n      for i in range(4):\n            del model.layers[str(i)]\n      model.tok_embeddings = None\n\n   stage = PipelineStage(\n      model,\n      stage_index,\n      num_stages,\n      device,\n   )\n   return stage",
      "if __name__ == \"__main__\":\n   init_distributed()\n   num_microbatches = 4\n   model_args = ModelArgs()\n   model = Transformer(model_args)\n\n   # Dummy data\n   x = torch.ones(32, 500, dtype=torch.long)\n   y = torch.randint(0, model_args.vocab_size, (32, 500), dtype=torch.long)\n   example_input_microbatch = x.chunk(num_microbatches)[0]\n\n   # Option 1: Manual model splitting\n   stage = manual_model_split(model)\n\n   # Option 2: Tracer model splitting\n   # stage = tracer_model_split(model, example_input_microbatch)\n\n   model.to(device)\n   x = x.to(device)\n   y = y.to(device)\n\n   def tokenwise_loss_fn(outputs, targets):\n      loss_fn = nn.CrossEntropyLoss()\n      outputs = outputs.reshape(-1, model_args.vocab_size)\n      targets = targets.reshape(-1)\n      return loss_fn(outputs, targets)\n\n   schedule = ScheduleGPipe(stage, n_microbatches=num_microbatches, loss_fn=tokenwise_loss_fn)\n\n   if rank == 0:\n      schedule.step(x)\n   elif rank == 1:\n      losses = []\n      output = schedule.step(target=y, losses=losses)\n      print(f\"losses: {losses}\")\n   dist.destroy_process_group()"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html",
    "title": "Quickstart\u00b6",
    "code_snippets": [
      "import torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets\nfrom torchvision.transforms import ToTensor",
      "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\nShape of y: torch.Size([64]) torch.int64",
      "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\nprint(f\"Using {device} device\")\n\n# Define model\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.flatten = nn.Flatten()\n        self.linear_relu_stack = nn.Sequential(\n            nn.Linear(28*28, 512),\n            nn.ReLU(),\n            nn.Linear(512, 512),\n            nn.ReLU(),\n            nn.Linear(512, 10)\n        )\n\n    def forward(self, x):\n        x = self.flatten(x)\n        logits = self.linear_relu_stack(x)\n        return logits\n\nmodel = NeuralNetwork().to(device)\nprint(model)",
      "loss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=1e-3)",
      "def train(dataloader, model, loss_fn, optimizer):\n    size = len(dataloader.dataset)\n    model.train()\n    for batch, (X, y) in enumerate(dataloader):\n        X, y = X.to(device), y.to(device)\n\n        # Compute prediction error\n        pred = model(X)\n        loss = loss_fn(pred, y)\n\n        # Backpropagation\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n        if batch % 100 == 0:\n            loss, current = loss.item(), (batch + 1) * len(X)\n            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")",
      "def test(dataloader, model, loss_fn):\n    size = len(dataloader.dataset)\n    num_batches = len(dataloader)\n    model.eval()\n    test_loss, correct = 0, 0\n    with torch.no_grad():\n        for X, y in dataloader:\n            X, y = X.to(device), y.to(device)\n            pred = model(X)\n            test_loss += loss_fn(pred, y).item()\n            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n    test_loss /= num_batches\n    correct /= size\n    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")",
      "torch.save(model.state_dict(), \"model.pth\")\nprint(\"Saved PyTorch Model State to model.pth\")",
      "model = NeuralNetwork().to(device)\nmodel.load_state_dict(torch.load(\"model.pth\", weights_only=True))",
      "classes = [\n    \"T-shirt/top\",\n    \"Trouser\",\n    \"Pullover\",\n    \"Dress\",\n    \"Coat\",\n    \"Sandal\",\n    \"Shirt\",\n    \"Sneaker\",\n    \"Bag\",\n    \"Ankle boot\",\n]\n\nmodel.eval()\nx, y = test_data[0][0], test_data[0][1]\nwith torch.no_grad():\n    x = x.to(device)\n    pred = model(x)\n    predicted, actual = classes[pred[0].argmax(0)], classes[y]\n    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/intermediate/pruning_tutorial.html",
    "title": "Pruning Tutorial\u00b6",
    "code_snippets": [
      "import torch\nfrom torch import nn\nimport torch.nn.utils.prune as prune\nimport torch.nn.functional as F",
      "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nclass LeNet(nn.Module):\n    def __init__(self):\n        super(LeNet, self).__init__()\n        # 1 input image channel, 6 output channels, 5x5 square conv kernel\n        self.conv1 = nn.Conv2d(1, 6, 5)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1 = nn.Linear(16 * 5 * 5, 120)  # 5x5 image dimension\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n\n    def forward(self, x):\n        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n        x = x.view(-1, int(x.nelement() / x.shape[0]))\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\nmodel = LeNet().to(device=device)",
      "OrderedDict([(0, <torch.nn.utils.prune.RandomUnstructured object at 0x7f85df039450>)])",
      "OrderedDict([(0, <torch.nn.utils.prune.RandomUnstructured object at 0x7f85df039450>), (1, <torch.nn.utils.prune.L1Unstructured object at 0x7f85df03a470>)])",
      "[<torch.nn.utils.prune.RandomUnstructured object at 0x7f85df039450>, <torch.nn.utils.prune.LnStructured object at 0x7f85df03ae00>]",
      "new_model = LeNet()\nfor name, module in new_model.named_modules():\n    # prune 20% of connections in all 2D-conv layers\n    if isinstance(module, torch.nn.Conv2d):\n        prune.l1_unstructured(module, name='weight', amount=0.2)\n    # prune 40% of connections in all linear layers\n    elif isinstance(module, torch.nn.Linear):\n        prune.l1_unstructured(module, name='weight', amount=0.4)\n\nprint(dict(new_model.named_buffers()).keys())  # to verify that all masks exist",
      "print(\n    \"Sparsity in conv1.weight: {:.2f}%\".format(\n        100. * float(torch.sum(model.conv1.weight == 0))\n        / float(model.conv1.weight.nelement())\n    )\n)\nprint(\n    \"Sparsity in conv2.weight: {:.2f}%\".format(\n        100. * float(torch.sum(model.conv2.weight == 0))\n        / float(model.conv2.weight.nelement())\n    )\n)\nprint(\n    \"Sparsity in fc1.weight: {:.2f}%\".format(\n        100. * float(torch.sum(model.fc1.weight == 0))\n        / float(model.fc1.weight.nelement())\n    )\n)\nprint(\n    \"Sparsity in fc2.weight: {:.2f}%\".format(\n        100. * float(torch.sum(model.fc2.weight == 0))\n        / float(model.fc2.weight.nelement())\n    )\n)\nprint(\n    \"Sparsity in fc3.weight: {:.2f}%\".format(\n        100. * float(torch.sum(model.fc3.weight == 0))\n        / float(model.fc3.weight.nelement())\n    )\n)\nprint(\n    \"Global sparsity: {:.2f}%\".format(\n        100. * float(\n            torch.sum(model.conv1.weight == 0)\n            + torch.sum(model.conv2.weight == 0)\n            + torch.sum(model.fc1.weight == 0)\n            + torch.sum(model.fc2.weight == 0)\n            + torch.sum(model.fc3.weight == 0)\n        )\n        / float(\n            model.conv1.weight.nelement()\n            + model.conv2.weight.nelement()\n            + model.fc1.weight.nelement()\n            + model.fc2.weight.nelement()\n            + model.fc3.weight.nelement()\n        )\n    )\n)",
      "class FooBarPruningMethod(prune.BasePruningMethod):\n    \"\"\"Prune every other entry in a tensor\n    \"\"\"\n    PRUNING_TYPE = 'unstructured'\n\n    def compute_mask(self, t, default_mask):\n        mask = default_mask.clone()\n        mask.view(-1)[::2] = 0\n        return mask",
      "def foobar_unstructured(module, name):\n    \"\"\"Prunes tensor corresponding to parameter called `name` in `module`\n    by removing every other entry in the tensors.\n    Modifies module in place (and also return the modified module)\n    by:\n    1) adding a named buffer called `name+'_mask'` corresponding to the\n    binary mask applied to the parameter `name` by the pruning method.\n    The parameter `name` is replaced by its pruned version, while the\n    original (unpruned) parameter is stored in a new parameter named\n    `name+'_orig'`.\n\n    Args:\n        module (nn.Module): module containing the tensor to prune\n        name (string): parameter name within `module` on which pruning\n                will act.\n\n    Returns:\n        module (nn.Module): modified (i.e. pruned) version of the input\n            module\n\n    Examples:\n        >>> m = nn.Linear(3, 4)\n        >>> foobar_unstructured(m, name='bias')\n    \"\"\"\n    FooBarPruningMethod.apply(module, name)\n    return module"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/recipes/recipes/timer_quick_start.html",
    "title": "Timer quick start\u00b6",
    "code_snippets": [
      "from torch.utils.benchmark import Timer\n\ntimer = Timer(\n    # The computation which will be run in a loop and timed.\n    stmt=\"x * y\",\n\n    # `setup` will be run before calling the measurement loop, and is used to\n    # populate any state which is needed by `stmt`\n    setup=\"\"\"\n        x = torch.ones((128,))\n        y = torch.ones((128,))\n    \"\"\",\n\n    # Alternatively, ``globals`` can be used to pass variables from the outer scope.\n    #\n    #    globals={\n    #        \"x\": torch.ones((128,)),\n    #        \"y\": torch.ones((128,)),\n    #    },\n\n    # Control the number of threads that PyTorch uses. (Default: 1)\n    num_threads=1,\n)",
      "# Measurement objects store the results of multiple repeats, and provide\n# various utility features.\nfrom torch.utils.benchmark import Measurement\n\nm: Measurement = timer.blocked_autorange(min_run_time=1)\nprint(m)",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7f1929a38ed0>\n     x * y\n     setup:\n       x = torch.ones((128,))\n       y = torch.ones((128,))\n\n       Median: 2.34 us\n       IQR:    0.07 us (2.31 to 2.38)\n       424 measurements, 1000 runs per measurement, 1 thread",
      "from torch.utils.benchmark import Language\n\ncpp_timer = Timer(\n    \"x * y;\",\n    \"\"\"\n        auto x = torch::ones({128});\n        auto y = torch::ones({128});\n    \"\"\",\n    language=Language.CPP,\n)\n\nprint(cpp_timer.blocked_autorange(min_run_time=1))",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7f192b019ed0>\n     x * y;\n     setup:\n       auto x = torch::ones({128});\n       auto y = torch::ones({128});\n\n       Median: 1.21 us\n       IQR:    0.03 us (1.20 to 1.23)\n       83 measurements, 10000 runs per measurement, 1 thread",
      "from torch.utils.benchmark import CallgrindStats, FunctionCounts\n\nstats: CallgrindStats = cpp_timer.collect_callgrind()\nprint(stats)",
      "<torch.utils.benchmark.utils.valgrind_wrapper.timer_interface.CallgrindStats object at 0x7f1929a35850>\n     x * y;\n     setup:\n       auto x = torch::ones({128});\n       auto y = torch::ones({128});\n\n                             All          Noisy symbols removed\n         Instructions:       563600                     563600\n         Baseline:                0                          0\n     100 runs per measurement, 1 thread",
      "torch.utils.benchmark.utils.valgrind_wrapper.timer_interface.FunctionCounts object at 0x7f192a6dfd90>\n       47264  ???:_int_free\n       25963  ???:_int_malloc\n       19900  build/../aten/src/ATen/TensorIter ... (at::TensorIteratorConfig const&)\n       18000  ???:__tls_get_addr\n       13500  ???:malloc\n       11300  build/../c10/util/SmallVector.h:a ... (at::TensorIteratorConfig const&)\n       10345  ???:_int_memalign\n       10000  build/../aten/src/ATen/TensorIter ... (at::TensorIteratorConfig const&)\n        9200  ???:free\n        8000  build/../c10/util/SmallVector.h:a ... IteratorBase::get_strides() const\n\n     Total: 173472",
      "import os\nimport re\n\ndef group_by_file(fn_name: str):\n    if fn_name.startswith(\"???\"):\n        fn_dir, fn_file = fn_name.split(\":\")[:2]\n    else:\n        fn_dir, fn_file = os.path.split(fn_name.split(\":\")[0])\n        fn_dir = re.sub(\"^.*build/../\", \"\", fn_dir)\n        fn_dir = re.sub(\"^.*torch/\", \"torch/\", fn_dir)\n\n    return f\"{fn_dir:<15} {fn_file}\"\n\nprint(inclusive_stats.transform(group_by_file)[:10])",
      "<torch.utils.benchmark.utils.valgrind_wrapper.timer_interface.FunctionCounts object at 0x7f192995d750>\n       118200  aten/src/ATen   TensorIterator.cpp\n        65000  c10/util        SmallVector.h\n        47264  ???             _int_free\n        25963  ???             _int_malloc\n        20900  c10/util        intrusive_ptr.h\n        18000  ???             __tls_get_addr\n        15900  c10/core        TensorImpl.h\n        15100  c10/core        CPUAllocator.cpp\n        13500  ???             malloc\n        12500  c10/core        TensorImpl.cpp\n\n     Total: 352327",
      "import pickle\n\n# Let's round trip `broadcasting_stats` just to show that we can.\nbroadcasting_stats = pickle.loads(pickle.dumps(broadcasting_stats))\n\n\n# And now to diff the two tasks:\ndelta = broadcasting_stats - inclusive_stats\n\ndef extract_fn_name(fn: str):\n    \"\"\"Trim everything except the function name.\"\"\"\n    fn = \":\".join(fn.split(\":\")[1:])\n    return re.sub(r\"\\(.+\\)\", \"(...)\", fn)\n\n# We use `.transform` to make the diff readable:\nprint(delta.transform(extract_fn_name))",
      "<torch.utils.benchmark.utils.valgrind_wrapper.timer_interface.FunctionCounts object at 0x7f192995d750>\n         17600  at::TensorIteratorBase::compute_strides(...)\n         12700  at::TensorIteratorBase::allocate_or_resize_outputs()\n         10200  c10::SmallVectorImpl<long>::operator=(...)\n          7400  at::infer_size(...)\n          6200  at::TensorIteratorBase::invert_perm(...) const\n          6064  _int_free\n          5100  at::TensorIteratorBase::reorder_dimensions()\n          4300  malloc\n          4300  at::TensorIteratorBase::compatible_stride(...) const\n           ...\n           -28  _int_memalign\n          -100  c10::impl::check_tensor_options_and_extract_memory_format(...)\n          -300  __memcmp_avx2_movbe\n          -400  at::detail::empty_cpu(...)\n         -1100  at::TensorIteratorBase::numel() const\n         -1300  void at::native::(...)\n         -2400  c10::TensorImpl::is_contiguous(...) const\n         -6100  at::TensorIteratorBase::compute_fast_setup_type(...)\n        -22600  at::TensorIteratorBase::fast_set_up(...)\n\n     Total: 58091",
      "<torch.utils.benchmark.utils.valgrind_wrapper.timer_interface.FunctionCounts object at 0x7f19299544d0>\n         17600  at::TensorIteratorBase::compute_strides(...)\n         12700  at::TensorIteratorBase::allocate_or_resize_outputs()\n          6200  at::TensorIteratorBase::invert_perm(...) const\n          5100  at::TensorIteratorBase::reorder_dimensions()\n          4300  at::TensorIteratorBase::compatible_stride(...) const\n          4000  at::TensorIteratorBase::compute_shape(...)\n          2300  at::TensorIteratorBase::coalesce_dimensions()\n          1600  at::TensorIteratorBase::build(...)\n         -1100  at::TensorIteratorBase::numel() const\n         -6100  at::TensorIteratorBase::compute_fast_setup_type(...)\n        -22600  at::TensorIteratorBase::fast_set_up(...)\n\n     Total: 24000"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/recipes/fuse.html",
    "title": "Fuse Modules Recipe\u00b6",
    "code_snippets": [
      "import torch\nfrom torch.utils.mobile_optimizer import optimize_for_mobile\n\nclass AnnotatedConvBnReLUModel(torch.nn.Module):\n    def __init__(self):\n        super(AnnotatedConvBnReLUModel, self).__init__()\n        self.conv = torch.nn.Conv2d(3, 5, 3, bias=False).to(dtype=torch.float)\n        self.bn = torch.nn.BatchNorm2d(5).to(dtype=torch.float)\n        self.relu = torch.nn.ReLU(inplace=True)\n        self.quant = torch.quantization.QuantStub()\n        self.dequant = torch.quantization.DeQuantStub()\n\n    def forward(self, x):\n        x = x.contiguous(memory_format=torch.channels_last)\n        x = self.quant(x)\n        x = self.conv(x)\n        x = self.bn(x)\n        x = self.relu(x)\n        x = self.dequant(x)\n        return x",
      "model = AnnotatedConvBnReLUModel()\nprint(model)\n\ndef prepare_save(model, fused):\n    model.qconfig = torch.quantization.get_default_qconfig('qnnpack')\n    torch.quantization.prepare(model, inplace=True)\n    torch.quantization.convert(model, inplace=True)\n    torchscript_model = torch.jit.script(model)\n    torchscript_model_optimized = optimize_for_mobile(torchscript_model)\n    torch.jit.save(torchscript_model_optimized, \"model.pt\" if not fused else \"model_fused.pt\")\n\nprepare_save(model, False)\n\nmodel = AnnotatedConvBnReLUModel()\nmodel_fused = torch.quantization.fuse_modules(model, [['bn', 'relu']], inplace=False)\nprint(model_fused)\n\nprepare_save(model_fused, True)"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html",
    "title": "Transfer Learning for Computer Vision Tutorial\u00b6",
    "code_snippets": [
      "# License: BSD\n# Author: Sasank Chilamkurthy\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nimport torch.backends.cudnn as cudnn\nimport numpy as np\nimport torchvision\nfrom torchvision import datasets, models, transforms\nimport matplotlib.pyplot as plt\nimport time\nimport os\nfrom PIL import Image\nfrom tempfile import TemporaryDirectory\n\ncudnn.benchmark = True\nplt.ion()   # interactive mode",
      "# Data augmentation and normalization for training\n# Just normalization for validation\ndata_transforms = {\n    'train': transforms.Compose([\n        transforms.RandomResizedCrop(224),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n    'val': transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n}\n\ndata_dir = 'data/hymenoptera_data'\nimage_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n                                          data_transforms[x])\n                  for x in ['train', 'val']}\ndataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n                                             shuffle=True, num_workers=4)\n              for x in ['train', 'val']}\ndataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\nclass_names = image_datasets['train'].classes\n\n# We want to be able to train our model on an `accelerator <https://pytorch.org/docs/stable/torch.html#accelerators>`__\n# such as CUDA, MPS, MTIA, or XPU. If the current accelerator is available, we will use it. Otherwise, we use the CPU.\n\ndevice = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\nprint(f\"Using {device} device\")",
      "def imshow(inp, title=None):\n    \"\"\"Display image for Tensor.\"\"\"\n    inp = inp.numpy().transpose((1, 2, 0))\n    mean = np.array([0.485, 0.456, 0.406])\n    std = np.array([0.229, 0.224, 0.225])\n    inp = std * inp + mean\n    inp = np.clip(inp, 0, 1)\n    plt.imshow(inp)\n    if title is not None:\n        plt.title(title)\n    plt.pause(0.001)  # pause a bit so that plots are updated\n\n\n# Get a batch of training data\ninputs, classes = next(iter(dataloaders['train']))\n\n# Make a grid from batch\nout = torchvision.utils.make_grid(inputs)\n\nimshow(out, title=[class_names[x] for x in classes])",
      "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n    since = time.time()\n\n    # Create a temporary directory to save training checkpoints\n    with TemporaryDirectory() as tempdir:\n        best_model_params_path = os.path.join(tempdir, 'best_model_params.pt')\n\n        torch.save(model.state_dict(), best_model_params_path)\n        best_acc = 0.0\n\n        for epoch in range(num_epochs):\n            print(f'Epoch {epoch}/{num_epochs - 1}')\n            print('-' * 10)\n\n            # Each epoch has a training and validation phase\n            for phase in ['train', 'val']:\n                if phase == 'train':\n                    model.train()  # Set model to training mode\n                else:\n                    model.eval()   # Set model to evaluate mode\n\n                running_loss = 0.0\n                running_corrects = 0\n\n                # Iterate over data.\n                for inputs, labels in dataloaders[phase]:\n                    inputs = inputs.to(device)\n                    labels = labels.to(device)\n\n                    # zero the parameter gradients\n                    optimizer.zero_grad()\n\n                    # forward\n                    # track history if only in train\n                    with torch.set_grad_enabled(phase == 'train'):\n                        outputs = model(inputs)\n                        _, preds = torch.max(outputs, 1)\n                        loss = criterion(outputs, labels)\n\n                        # backward + optimize only if in training phase\n                        if phase == 'train':\n                            loss.backward()\n                            optimizer.step()\n\n                    # statistics\n                    running_loss += loss.item() * inputs.size(0)\n                    running_corrects += torch.sum(preds == labels.data)\n                if phase == 'train':\n                    scheduler.step()\n\n                epoch_loss = running_loss / dataset_sizes[phase]\n                epoch_acc = running_corrects.double() / dataset_sizes[phase]\n\n                print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n\n                # deep copy the model\n                if phase == 'val' and epoch_acc > best_acc:\n                    best_acc = epoch_acc\n                    torch.save(model.state_dict(), best_model_params_path)\n\n            print()\n\n        time_elapsed = time.time() - since\n        print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n        print(f'Best val Acc: {best_acc:4f}')\n\n        # load best model weights\n        model.load_state_dict(torch.load(best_model_params_path, weights_only=True))\n    return model",
      "def visualize_model(model, num_images=6):\n    was_training = model.training\n    model.eval()\n    images_so_far = 0\n    fig = plt.figure()\n\n    with torch.no_grad():\n        for i, (inputs, labels) in enumerate(dataloaders['val']):\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n\n            outputs = model(inputs)\n            _, preds = torch.max(outputs, 1)\n\n            for j in range(inputs.size()[0]):\n                images_so_far += 1\n                ax = plt.subplot(num_images//2, 2, images_so_far)\n                ax.axis('off')\n                ax.set_title(f'predicted: {class_names[preds[j]]}')\n                imshow(inputs.cpu().data[j])\n\n                if images_so_far == num_images:\n                    model.train(mode=was_training)\n                    return\n        model.train(mode=was_training)",
      "model_ft = models.resnet18(weights='IMAGENET1K_V1')\nnum_ftrs = model_ft.fc.in_features\n# Here the size of each output sample is set to 2.\n# Alternatively, it can be generalized to ``nn.Linear(num_ftrs, len(class_names))``.\nmodel_ft.fc = nn.Linear(num_ftrs, 2)\n\nmodel_ft = model_ft.to(device)\n\ncriterion = nn.CrossEntropyLoss()\n\n# Observe that all parameters are being optimized\noptimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n\n# Decay LR by a factor of 0.1 every 7 epochs\nexp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)",
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /var/lib/ci-user/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n\n  0%|          | 0.00/44.7M [00:00<?, ?B/s]\n 94%|#########4| 42.1M/44.7M [00:00<00:00, 441MB/s]\n100%|##########| 44.7M/44.7M [00:00<00:00, 440MB/s]",
      "model_conv = torchvision.models.resnet18(weights='IMAGENET1K_V1')\nfor param in model_conv.parameters():\n    param.requires_grad = False\n\n# Parameters of newly constructed modules have requires_grad=True by default\nnum_ftrs = model_conv.fc.in_features\nmodel_conv.fc = nn.Linear(num_ftrs, 2)\n\nmodel_conv = model_conv.to(device)\n\ncriterion = nn.CrossEntropyLoss()\n\n# Observe that only parameters of final layer are being optimized as\n# opposed to before.\noptimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.001, momentum=0.9)\n\n# Decay LR by a factor of 0.1 every 7 epochs\nexp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1)",
      "def visualize_model_predictions(model,img_path):\n    was_training = model.training\n    model.eval()\n\n    img = Image.open(img_path)\n    img = data_transforms['val'](img)\n    img = img.unsqueeze(0)\n    img = img.to(device)\n\n    with torch.no_grad():\n        outputs = model(img)\n        _, preds = torch.max(outputs, 1)\n\n        ax = plt.subplot(2,2,1)\n        ax.axis('off')\n        ax.set_title(f'Predicted: {class_names[preds[0]]}')\n        imshow(img.cpu().data[0])\n\n        model.train(mode=was_training)"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/recipes/recipes/warmstarting_model_using_parameters_from_a_different_model.html",
    "title": "Warmstarting model using parameters from a different model in PyTorch\u00b6",
    "code_snippets": [
      "import torch\nimport torch.nn as nn\nimport torch.optim as optim",
      "class NetA(nn.Module):\n    def __init__(self):\n        super(NetA, self).__init__()\n        self.conv1 = nn.Conv2d(3, 6, 5)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = x.view(-1, 16 * 5 * 5)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\nnetA = NetA()\n\nclass NetB(nn.Module):\n    def __init__(self):\n        super(NetB, self).__init__()\n        self.conv1 = nn.Conv2d(3, 6, 5)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = x.view(-1, 16 * 5 * 5)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\nnetB = NetB()",
      "# Specify a path to save to\nPATH = \"model.pt\"\n\ntorch.save(netA.state_dict(), PATH)",
      "netB.load_state_dict(torch.load(PATH, weights_only=True), strict=False)"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/intermediate/rpc_param_server_tutorial.html",
    "title": "Implementing a Parameter Server Using Distributed RPC Framework\u00b6",
    "code_snippets": [
      "import argparse\nimport os\nimport time\nfrom threading import Lock\n\nimport torch\nimport torch.distributed.autograd as dist_autograd\nimport torch.distributed.rpc as rpc\nimport torch.multiprocessing as mp\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch import optim\nfrom torch.distributed.optim import DistributedOptimizer\nfrom torchvision import datasets, transforms\n\n# --------- MNIST Network to train, from pytorch/examples -----\n\nclass Net(nn.Module):\n    def __init__(self, num_gpus=0):\n        super(Net, self).__init__()\n        print(f\"Using {num_gpus} GPUs to train\")\n        self.num_gpus = num_gpus\n        device = torch.device(\n            \"cuda:0\" if torch.cuda.is_available() and self.num_gpus > 0 else \"cpu\")\n        print(f\"Putting first 2 convs on {str(device)}\")\n        # Put conv layers on the first cuda device, or CPU if no cuda device\n        self.conv1 = nn.Conv2d(1, 32, 3, 1).to(device)\n        self.conv2 = nn.Conv2d(32, 64, 3, 1).to(device)\n        # Put rest of the network on the 2nd cuda device, if there is one\n        if \"cuda\" in str(device) and num_gpus > 1:\n            device = torch.device(\"cuda:1\")\n\n        print(f\"Putting rest of layers on {str(device)}\")\n        self.dropout1 = nn.Dropout2d(0.25).to(device)\n        self.dropout2 = nn.Dropout2d(0.5).to(device)\n        self.fc1 = nn.Linear(9216, 128).to(device)\n        self.fc2 = nn.Linear(128, 10).to(device)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = F.relu(x)\n        x = self.conv2(x)\n        x = F.max_pool2d(x, 2)\n\n        x = self.dropout1(x)\n        x = torch.flatten(x, 1)\n        # Move tensor to next device if necessary\n        next_device = next(self.fc1.parameters()).device\n        x = x.to(next_device)\n\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.dropout2(x)\n        x = self.fc2(x)\n        output = F.log_softmax(x, dim=1)\n        return output",
      "# --------- Helper Methods --------------------\n\n# On the local node, call a method with first arg as the value held by the\n# RRef. Other args are passed in as arguments to the function called.\n# Useful for calling instance methods. method could be any matching function, including\n# class methods.\ndef call_method(method, rref, *args, **kwargs):\n    return method(rref.local_value(), *args, **kwargs)\n\n# Given an RRef, return the result of calling the passed in method on the value\n# held by the RRef. This call is done on the remote node that owns\n# the RRef and passes along the given argument.\n# Example: If the value held by the RRef is of type Foo, then\n# remote_method(Foo.bar, rref, arg1, arg2) is equivalent to calling\n# <foo_instance>.bar(arg1, arg2) on the remote node and getting the result\n# back.\n\ndef remote_method(method, rref, *args, **kwargs):\n    args = [method, rref] + list(args)\n    return rpc.rpc_sync(rref.owner(), call_method, args=args, kwargs=kwargs)",
      "# --------- Parameter Server --------------------\nclass ParameterServer(nn.Module):\n    def __init__(self, num_gpus=0):\n        super().__init__()\n        model = Net(num_gpus=num_gpus)\n        self.model = model\n        self.input_device = torch.device(\n            \"cuda:0\" if torch.cuda.is_available() and num_gpus > 0 else \"cpu\")",
      "class ParameterServer(nn.Module):\n...\n    def forward(self, inp):\n        inp = inp.to(self.input_device)\n        out = self.model(inp)\n        # This output is forwarded over RPC, which as of 1.5.0 only accepts CPU tensors.\n        # Tensors must be moved in and out of GPU memory due to this.\n        out = out.to(\"cpu\")\n        return out",
      "# Use dist autograd to retrieve gradients accumulated for this model.\n# Primarily used for verification.\ndef get_dist_gradients(self, cid):\n    grads = dist_autograd.get_gradients(cid)\n    # This output is forwarded over RPC, which as of 1.5.0 only accepts CPU tensors.\n    # Tensors must be moved in and out of GPU memory due to this.\n    cpu_grads = {}\n    for k, v in grads.items():\n        k_cpu, v_cpu = k.to(\"cpu\"), v.to(\"cpu\")\n        cpu_grads[k_cpu] = v_cpu\n    return cpu_grads\n\n# Wrap local parameters in a RRef. Needed for building the\n# DistributedOptimizer which optimizes paramters remotely.\ndef get_param_rrefs(self):\n    param_rrefs = [rpc.RRef(param) for param in self.model.parameters()]\n    return param_rrefs",
      "# The global parameter server instance.\nparam_server = None\n# A lock to ensure we only have one parameter server.\nglobal_lock = Lock()\n\n\ndef get_parameter_server(num_gpus=0):\n    \"\"\"\n    Returns a singleton parameter server to all trainer processes\n    \"\"\"\n    global param_server\n    # Ensure that we get only one handle to the ParameterServer.\n    with global_lock:\n        if not param_server:\n            # construct it once\n            param_server = ParameterServer(num_gpus=num_gpus)\n        return param_server\n\ndef run_parameter_server(rank, world_size):\n    # The parameter server just acts as a host for the model and responds to\n    # requests from trainers.\n    # rpc.shutdown() will wait for all workers to complete by default, which\n    # in this case means that the parameter server will wait for all trainers\n    # to complete, and then exit.\n    print(\"PS master initializing RPC\")\n    rpc.init_rpc(name=\"parameter_server\", rank=rank, world_size=world_size)\n    print(\"RPC initialized! Running parameter server...\")\n    rpc.shutdown()\n    print(\"RPC shutdown on parameter server.\")",
      "# --------- Trainers --------------------\n\n# nn.Module corresponding to the network trained by this trainer. The\n# forward() method simply invokes the network on the given parameter\n# server.\nclass TrainerNet(nn.Module):\n    def __init__(self, num_gpus=0):\n        super().__init__()\n        self.num_gpus = num_gpus\n        self.param_server_rref = rpc.remote(\n            \"parameter_server\", get_parameter_server, args=(num_gpus,))",
      "class TrainerNet(nn.Module):\n...\n    def get_global_param_rrefs(self):\n        remote_params = remote_method(\n            ParameterServer.get_param_rrefs,\n            self.param_server_rref)\n        return remote_params",
      "class TrainerNet(nn.Module):\n...\n    def forward(self, x):\n        model_output = remote_method(\n            ParameterServer.forward, self.param_server_rref, x)\n        return model_output",
      "def run_training_loop(rank, num_gpus, train_loader, test_loader):\n    # Runs the typical nueral network forward + backward + optimizer step, but\n    # in a distributed fashion.\n    net = TrainerNet(num_gpus=num_gpus)\n    # Build DistributedOptimizer.\n    param_rrefs = net.get_global_param_rrefs()\n    opt = DistributedOptimizer(optim.SGD, param_rrefs, lr=0.03)",
      "def run_training_loop(rank, num_gpus, train_loader, test_loader):\n...\n    for i, (data, target) in enumerate(train_loader):\n        with dist_autograd.context() as cid:\n            model_output = net(data)\n            target = target.to(model_output.device)\n            loss = F.nll_loss(model_output, target)\n            if i % 5 == 0:\n                print(f\"Rank {rank} training batch {i} loss {loss.item()}\")\n            dist_autograd.backward(cid, [loss])\n            # Ensure that dist autograd ran successfully and gradients were\n            # returned.\n            assert remote_method(\n                ParameterServer.get_dist_gradients,\n                net.param_server_rref,\n                cid) != {}\n            opt.step(cid)\n\n     print(\"Training complete!\")\n     print(\"Getting accuracy....\")\n     get_accuracy(test_loader, net)",
      "def get_accuracy(test_loader, model):\n    model.eval()\n    correct_sum = 0\n    # Use GPU to evaluate if possible\n    device = torch.device(\"cuda:0\" if model.num_gpus > 0\n        and torch.cuda.is_available() else \"cpu\")\n    with torch.no_grad():\n        for i, (data, target) in enumerate(test_loader):\n            out = model(data, -1)\n            pred = out.argmax(dim=1, keepdim=True)\n            pred, target = pred.to(device), target.to(device)\n            correct = pred.eq(target.view_as(pred)).sum().item()\n            correct_sum += correct\n\n    print(f\"Accuracy {correct_sum / len(test_loader.dataset)}\")",
      "# Main loop for trainers.\ndef run_worker(rank, world_size, num_gpus, train_loader, test_loader):\n    print(f\"Worker rank {rank} initializing RPC\")\n    rpc.init_rpc(\n        name=f\"trainer_{rank}\",\n        rank=rank,\n        world_size=world_size)\n\n    print(f\"Worker {rank} done initializing RPC\")\n\n    run_training_loop(rank, num_gpus, train_loader, test_loader)\n    rpc.shutdown()",
      "if __name__ == '__main__':\n    parser = argparse.ArgumentParser(\n        description=\"Parameter-Server RPC based training\")\n    parser.add_argument(\n        \"--world_size\",\n        type=int,\n        default=4,\n        help=\"\"\"Total number of participating processes. Should be the sum of\n        master node and all training nodes.\"\"\")\n    parser.add_argument(\n        \"--rank\",\n        type=int,\n        default=None,\n        help=\"Global rank of this process. Pass in 0 for master.\")\n    parser.add_argument(\n        \"--num_gpus\",\n        type=int,\n        default=0,\n        help=\"\"\"Number of GPUs to use for training, Currently supports between 0\n         and 2 GPUs. Note that this argument will be passed to the parameter servers.\"\"\")\n    parser.add_argument(\n        \"--master_addr\",\n        type=str,\n        default=\"localhost\",\n        help=\"\"\"Address of master, will default to localhost if not provided.\n        Master must be able to accept network traffic on the address + port.\"\"\")\n    parser.add_argument(\n        \"--master_port\",\n        type=str,\n        default=\"29500\",\n        help=\"\"\"Port that master is listening on, will default to 29500 if not\n        provided. Master must be able to accept network traffic on the host and port.\"\"\")\n\n    args = parser.parse_args()\n    assert args.rank is not None, \"must provide rank argument.\"\n    assert args.num_gpus <= 3, f\"Only 0-2 GPUs currently supported (got {args.num_gpus}).\"\n    os.environ['MASTER_ADDR'] = args.master_addr\n    os.environ[\"MASTER_PORT\"] = args.master_port",
      "processes = []\nworld_size = args.world_size\nif args.rank == 0:\n    p = mp.Process(target=run_parameter_server, args=(0, world_size))\n    p.start()\n    processes.append(p)\nelse:\n    # Get data to train on\n    train_loader = torch.utils.data.DataLoader(\n        datasets.MNIST('../data', train=True, download=True,\n                       transform=transforms.Compose([\n                           transforms.ToTensor(),\n                           transforms.Normalize((0.1307,), (0.3081,))\n                       ])),\n        batch_size=32, shuffle=True,)\n    test_loader = torch.utils.data.DataLoader(\n        datasets.MNIST(\n            '../data',\n            train=False,\n            transform=transforms.Compose([\n                    transforms.ToTensor(),\n                    transforms.Normalize((0.1307,), (0.3081,))\n                        ])),\n        batch_size=32,\n        shuffle=True,\n    )\n    # start training worker on this node\n    p = mp.Process(\n        target=run_worker,\n        args=(\n            args.rank,\n            world_size, args.num_gpus,\n            train_loader,\n            test_loader))\n    p.start()\n    processes.append(p)\n\nfor p in processes:\n    p.join()"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html",
    "title": "Training a Classifier\u00b6",
    "code_snippets": [
      "import torch\nimport torchvision\nimport torchvision.transforms as transforms",
      "transform = transforms.Compose(\n    [transforms.ToTensor(),\n     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n\nbatch_size = 4\n\ntrainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n                                        download=True, transform=transform)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n                                          shuffle=True, num_workers=2)\n\ntestset = torchvision.datasets.CIFAR10(root='./data', train=False,\n                                       download=True, transform=transform)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n                                         shuffle=False, num_workers=2)\n\nclasses = ('plane', 'car', 'bird', 'cat',\n           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')",
      "import matplotlib.pyplot as plt\nimport numpy as np\n\n# functions to show an image\n\n\ndef imshow(img):\n    img = img / 2 + 0.5     # unnormalize\n    npimg = img.numpy()\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n    plt.show()\n\n\n# get some random training images\ndataiter = iter(trainloader)\nimages, labels = next(dataiter)\n\n# show images\nimshow(torchvision.utils.make_grid(images))\n# print labels\nprint(' '.join(f'{classes[labels[j]]:5s}' for j in range(batch_size)))",
      "import torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Net(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 6, 5)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = torch.flatten(x, 1) # flatten all dimensions except batch\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\n\nnet = Net()",
      "import torch.optim as optim\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)",
      "PATH = './cifar_net.pth'\ntorch.save(net.state_dict(), PATH)",
      "dataiter = iter(testloader)\nimages, labels = next(dataiter)\n\n# print images\nimshow(torchvision.utils.make_grid(images))\nprint('GroundTruth: ', ' '.join(f'{classes[labels[j]]:5s}' for j in range(4)))",
      "net = Net()\nnet.load_state_dict(torch.load(PATH, weights_only=True))",
      "_, predicted = torch.max(outputs, 1)\n\nprint('Predicted: ', ' '.join(f'{classes[predicted[j]]:5s}'\n                              for j in range(4)))",
      "correct = 0\ntotal = 0\n# since we're not training, we don't need to calculate the gradients for our outputs\nwith torch.no_grad():\n    for data in testloader:\n        images, labels = data\n        # calculate outputs by running images through the network\n        outputs = net(images)\n        # the class with the highest energy is what we choose as prediction\n        _, predicted = torch.max(outputs, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\nprint(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')",
      "# prepare to count predictions for each class\ncorrect_pred = {classname: 0 for classname in classes}\ntotal_pred = {classname: 0 for classname in classes}\n\n# again no gradients needed\nwith torch.no_grad():\n    for data in testloader:\n        images, labels = data\n        outputs = net(images)\n        _, predictions = torch.max(outputs, 1)\n        # collect the correct predictions for each class\n        for label, prediction in zip(labels, predictions):\n            if label == prediction:\n                correct_pred[classes[label]] += 1\n            total_pred[classes[label]] += 1\n\n\n# print accuracy for each class\nfor classname, correct_count in correct_pred.items():\n    accuracy = 100 * float(correct_count) / total_pred[classname]\n    print(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %')",
      "Accuracy for class: plane is 40.0 %\nAccuracy for class: car   is 71.8 %\nAccuracy for class: bird  is 45.9 %\nAccuracy for class: cat   is 28.1 %\nAccuracy for class: deer  is 18.3 %\nAccuracy for class: dog   is 50.4 %\nAccuracy for class: frog  is 78.0 %\nAccuracy for class: horse is 55.5 %\nAccuracy for class: ship  is 85.5 %\nAccuracy for class: truck is 39.3 %",
      "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n\n# Assuming that we are on a CUDA machine, this should print a CUDA device:\n\nprint(device)"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/intermediate/jacobians_hessians.html",
    "title": "Jacobians, Hessians, hvp, vhp, and more: composing function transforms\u00b6",
    "code_snippets": [
      "import torch\nimport torch.nn.functional as F\nfrom functools import partial\n_ = torch.manual_seed(0)",
      "def predict(weight, bias, x):\n    return F.linear(x, weight, bias).tanh()",
      "D = 16\nweight = torch.randn(D, D)\nbias = torch.randn(D)\nx = torch.randn(D)  # feature vector",
      "def compute_jac(xp):\n    jacobian_rows = [torch.autograd.grad(predict(weight, bias, xp), xp, vec)[0]\n                     for vec in unit_vectors]\n    return torch.stack(jacobian_rows)\n\nxp = x.clone().requires_grad_()\nunit_vectors = torch.eye(D)\n\njacobian = compute_jac(xp)\n\nprint(jacobian.shape)\nprint(jacobian[0])  # show first row",
      "torch.Size([16, 16])\ntensor([-0.5956, -0.6096, -0.1326, -0.2295,  0.4490,  0.3661, -0.1672, -1.1190,\n         0.1705, -0.6683,  0.1851,  0.1630,  0.0634,  0.6547,  0.5908, -0.1308])",
      "from torch.func import vmap, vjp\n\n_, vjp_fn = vjp(partial(predict, weight, bias), x)\n\nft_jacobian, = vmap(vjp_fn)(unit_vectors)\n\n# let's confirm both methods compute the same result\nassert torch.allclose(ft_jacobian, jacobian)",
      "from torch.func import jacrev\n\nft_jacobian = jacrev(predict, argnums=2)(weight, bias, x)\n\n# Confirm by running the following:\nassert torch.allclose(ft_jacobian, jacobian)",
      "def get_perf(first, first_descriptor, second, second_descriptor):\n    \"\"\"takes torch.benchmark objects and compares delta of second vs first.\"\"\"\n    faster = second.times[0]\n    slower = first.times[0]\n    gain = (slower-faster)/slower\n    if gain < 0: gain *=-1\n    final_gain = gain*100\n    print(f\" Performance delta: {final_gain:.4f} percent improvement with {second_descriptor} \")",
      "from torch.utils.benchmark import Timer\n\nwithout_vmap = Timer(stmt=\"compute_jac(xp)\", globals=globals())\nwith_vmap = Timer(stmt=\"jacrev(predict, argnums=2)(weight, bias, x)\", globals=globals())\n\nno_vmap_timer = without_vmap.timeit(500)\nwith_vmap_timer = with_vmap.timeit(500)\n\nprint(no_vmap_timer)\nprint(with_vmap_timer)",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7f81032af8e0>\ncompute_jac(xp)\n  1.44 ms\n  1 measurement, 500 runs , 1 thread\n<torch.utils.benchmark.utils.common.Measurement object at 0x7f813acf6e30>\njacrev(predict, argnums=2)(weight, bias, x)\n  399.29 us\n  1 measurement, 500 runs , 1 thread",
      "from torch.func import jacrev, jacfwd",
      "Din = 32\nDout = 2048\nweight = torch.randn(Dout, Din)\n\nbias = torch.randn(Dout)\nx = torch.randn(Din)\n\n# remember the general rule about taller vs wider... here we have a taller matrix:\nprint(weight.shape)\n\nusing_fwd = Timer(stmt=\"jacfwd(predict, argnums=2)(weight, bias, x)\", globals=globals())\nusing_bwd = Timer(stmt=\"jacrev(predict, argnums=2)(weight, bias, x)\", globals=globals())\n\njacfwd_timing = using_fwd.timeit(500)\njacrev_timing = using_bwd.timeit(500)\n\nprint(f'jacfwd time: {jacfwd_timing}')\nprint(f'jacrev time: {jacrev_timing}')",
      "torch.Size([2048, 32])\njacfwd time: <torch.utils.benchmark.utils.common.Measurement object at 0x7f81034777f0>\njacfwd(predict, argnums=2)(weight, bias, x)\n  715.71 us\n  1 measurement, 500 runs , 1 thread\njacrev time: <torch.utils.benchmark.utils.common.Measurement object at 0x7f81360cf4f0>\njacrev(predict, argnums=2)(weight, bias, x)\n  8.77 ms\n  1 measurement, 500 runs , 1 thread",
      "Din = 2048\nDout = 32\nweight = torch.randn(Dout, Din)\nbias = torch.randn(Dout)\nx = torch.randn(Din)\n\nusing_fwd = Timer(stmt=\"jacfwd(predict, argnums=2)(weight, bias, x)\", globals=globals())\nusing_bwd = Timer(stmt=\"jacrev(predict, argnums=2)(weight, bias, x)\", globals=globals())\n\njacfwd_timing = using_fwd.timeit(500)\njacrev_timing = using_bwd.timeit(500)\n\nprint(f'jacfwd time: {jacfwd_timing}')\nprint(f'jacrev time: {jacrev_timing}')",
      "jacfwd time: <torch.utils.benchmark.utils.common.Measurement object at 0x7f81032af640>\njacfwd(predict, argnums=2)(weight, bias, x)\n  7.15 ms\n  1 measurement, 500 runs , 1 thread\njacrev time: <torch.utils.benchmark.utils.common.Measurement object at 0x7f813602f520>\njacrev(predict, argnums=2)(weight, bias, x)\n  471.28 us\n  1 measurement, 500 runs , 1 thread",
      "from torch.func import hessian\n\n# lets reduce the size in order not to overwhelm Colab. Hessians require\n# significant memory:\nDin = 512\nDout = 32\nweight = torch.randn(Dout, Din)\nbias = torch.randn(Dout)\nx = torch.randn(Din)\n\nhess_api = hessian(predict, argnums=2)(weight, bias, x)\nhess_fwdfwd = jacfwd(jacfwd(predict, argnums=2), argnums=2)(weight, bias, x)\nhess_revrev = jacrev(jacrev(predict, argnums=2), argnums=2)(weight, bias, x)",
      "torch.allclose(hess_api, hess_fwdfwd)",
      "batch_size = 64\nDin = 31\nDout = 33\n\nweight = torch.randn(Dout, Din)\nprint(f\"weight shape = {weight.shape}\")\n\nbias = torch.randn(Dout)\n\nx = torch.randn(batch_size, Din)\n\ncompute_batch_jacobian = vmap(jacrev(predict, argnums=2), in_dims=(None, None, 0))\nbatch_jacobian0 = compute_batch_jacobian(weight, bias, x)",
      "weight shape = torch.Size([33, 31])",
      "def predict_with_output_summed(weight, bias, x):\n    return predict(weight, bias, x).sum(0)\n\nbatch_jacobian1 = jacrev(predict_with_output_summed, argnums=2)(weight, bias, x).movedim(1, 0)\nassert torch.allclose(batch_jacobian0, batch_jacobian1)",
      "torch.Size([64, 33, 31, 31])",
      "from torch.func import jvp, grad, vjp\n\ndef hvp(f, primals, tangents):\n  return jvp(grad(f), primals, tangents)[1]",
      "def f(x):\n  return x.sin().sum()\n\nx = torch.randn(2048)\ntangent = torch.randn(2048)\n\nresult = hvp(f, (x,), (tangent,))",
      "def hvp_revrev(f, primals, tangents):\n  _, vjp_fn = vjp(grad(f), *primals)\n  return vjp_fn(*tangents)\n\nresult_hvp_revrev = hvp_revrev(f, (x,), (tangent,))\nassert torch.allclose(result, result_hvp_revrev[0])"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/recipes/torch_compile_caching_tutorial.html",
    "title": "Compile Time Caching in torch.compile\u00b6",
    "code_snippets": [
      "@torch.compile\ndef fn(x, y):\n    return x.sin() @ y\n\na = torch.rand(100, 100, dtype=dtype, device=device)\nb = torch.rand(100, 100, dtype=dtype, device=device)\n\nresult = fn(a, b)\n\nartifacts = torch.compiler.save_cache_artifacts()\n\nassert artifacts is not None\nartifact_bytes, cache_info = artifacts\n\n# Now, potentially store artifact_bytes in a database\n# You can use cache_info for logging",
      "# Potentially download/fetch the artifacts from the database\ntorch.compiler.load_cache_artifacts(artifact_bytes)"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html",
    "title": "Introduction to torch.compile\u00b6",
    "code_snippets": [
      "import torch\nimport warnings\n\ngpu_ok = False\nif torch.cuda.is_available():\n    device_cap = torch.cuda.get_device_capability()\n    if device_cap in ((7, 0), (8, 0), (9, 0)):\n        gpu_ok = True\n\nif not gpu_ok:\n    warnings.warn(\n        \"GPU is not NVIDIA V100, A100, or H100. Speedup numbers may be lower \"\n        \"than expected.\"\n    )",
      "def foo(x, y):\n    a = torch.sin(x)\n    b = torch.cos(y)\n    return a + b\nopt_foo1 = torch.compile(foo)\nprint(opt_foo1(torch.randn(10, 10), torch.randn(10, 10)))",
      "t1 = torch.randn(10, 10)\nt2 = torch.randn(10, 10)\n\n@torch.compile\ndef opt_foo2(x, y):\n    a = torch.sin(x)\n    b = torch.cos(y)\n    return a + b\nprint(opt_foo2(t1, t2))",
      "t = torch.randn(10, 100)\n\nclass MyModule(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.lin = torch.nn.Linear(100, 10)\n\n    def forward(self, x):\n        return torch.nn.functional.relu(self.lin(x))\n\nmod = MyModule()\nmod.compile()\nprint(mod(t))\n## or:\n# opt_mod = torch.compile(mod)\n# print(opt_mod(t))",
      "def nested_function(x):\n    return torch.sin(x)\n\n@torch.compile\ndef outer_function(x, y):\n    a = nested_function(x)\n    b = torch.cos(y)\n    return a + b\n\nprint(outer_function(t1, t2))",
      "class OuterModule(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.inner_module = MyModule()\n        self.outer_lin = torch.nn.Linear(10, 2)\n\n    def forward(self, x):\n        x = self.inner_module(x)\n        return torch.nn.functional.relu(self.outer_lin(x))\n\nouter_mod = OuterModule()\nouter_mod.compile()\nprint(outer_mod(t))",
      "def complex_conjugate(z):\n    return torch.conj(z)\n\n@torch.compiler.disable(recursive=False)\ndef complex_function(real, imag):\n    # Assuming this function cause problems in the compilation\n    z = torch.complex(real, imag)\n    return complex_conjugate(z)\n\ndef outer_function():\n    real = torch.tensor([2, 3], dtype=torch.float32)\n    imag = torch.tensor([4, 5], dtype=torch.float32)\n    z = complex_function(real, imag)\n    return torch.abs(z)\n\n# Try to compile the outer_function\ntry:\n    opt_outer_function = torch.compile(outer_function)\n    print(opt_outer_function())\nexcept Exception as e:\n    print(\"Compilation of outer_function failed:\", e)",
      "# Returns the result of running `fn()` and the time it took for `fn()` to run,\n# in seconds. We use CUDA events and synchronization for the most accurate\n# measurements.\ndef timed(fn):\n    start = torch.cuda.Event(enable_timing=True)\n    end = torch.cuda.Event(enable_timing=True)\n    start.record()\n    result = fn()\n    end.record()\n    torch.cuda.synchronize()\n    return result, start.elapsed_time(end) / 1000\n\n# Generates random input and targets data for the model, where `b` is\n# batch size.\ndef generate_data(b):\n    return (\n        torch.randn(b, 3, 128, 128).to(torch.float32).cuda(),\n        torch.randint(1000, (b,)).cuda(),\n    )\n\nN_ITERS = 10\n\nfrom torchvision.models import densenet121\ndef init_model():\n    return densenet121().to(torch.float32).cuda()",
      "model = init_model()\n\n# Reset since we are using a different mode.\nimport torch._dynamo\ntorch._dynamo.reset()\n\nmodel_opt = torch.compile(model, mode=\"reduce-overhead\")\n\ninp = generate_data(16)[0]\nwith torch.no_grad():\n    print(\"eager:\", timed(lambda: model(inp))[1])\n    print(\"compile:\", timed(lambda: model_opt(inp))[1])",
      "eager: 0.35041281127929685\n/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py:236: UserWarning:\n\nTensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.\n\ncompile: 67.0737421875",
      "eager_times = []\nfor i in range(N_ITERS):\n    inp = generate_data(16)[0]\n    with torch.no_grad():\n        _, eager_time = timed(lambda: model(inp))\n    eager_times.append(eager_time)\n    print(f\"eager eval time {i}: {eager_time}\")\n\nprint(\"~\" * 10)\n\ncompile_times = []\nfor i in range(N_ITERS):\n    inp = generate_data(16)[0]\n    with torch.no_grad():\n        _, compile_time = timed(lambda: model_opt(inp))\n    compile_times.append(compile_time)\n    print(f\"compile eval time {i}: {compile_time}\")\nprint(\"~\" * 10)\n\nimport numpy as np\neager_med = np.median(eager_times)\ncompile_med = np.median(compile_times)\nspeedup = eager_med / compile_med\nassert(speedup > 1)\nprint(f\"(eval) eager median: {eager_med}, compile median: {compile_med}, speedup: {speedup}x\")\nprint(\"~\" * 10)",
      "model = init_model()\nopt = torch.optim.Adam(model.parameters())\n\ndef train(mod, data):\n    opt.zero_grad(True)\n    pred = mod(data[0])\n    loss = torch.nn.CrossEntropyLoss()(pred, data[1])\n    loss.backward()\n    opt.step()\n\neager_times = []\nfor i in range(N_ITERS):\n    inp = generate_data(16)\n    _, eager_time = timed(lambda: train(model, inp))\n    eager_times.append(eager_time)\n    print(f\"eager train time {i}: {eager_time}\")\nprint(\"~\" * 10)\n\nmodel = init_model()\nopt = torch.optim.Adam(model.parameters())\ntrain_opt = torch.compile(train, mode=\"reduce-overhead\")\n\ncompile_times = []\nfor i in range(N_ITERS):\n    inp = generate_data(16)\n    _, compile_time = timed(lambda: train_opt(model, inp))\n    compile_times.append(compile_time)\n    print(f\"compile train time {i}: {compile_time}\")\nprint(\"~\" * 10)\n\neager_med = np.median(eager_times)\ncompile_med = np.median(compile_times)\nspeedup = eager_med / compile_med\nassert(speedup > 1)\nprint(f\"(train) eager median: {eager_med}, compile median: {compile_med}, speedup: {speedup}x\")\nprint(\"~\" * 10)",
      "def f1(x, y):\n    if x.sum() < 0:\n        return -y\n    return y\n\n# Test that `fn1` and `fn2` return the same result, given\n# the same arguments `args`. Typically, `fn1` will be an eager function\n# while `fn2` will be a compiled function (torch.compile, TorchScript, or FX graph).\ndef test_fns(fn1, fn2, args):\n    out1 = fn1(*args)\n    out2 = fn2(*args)\n    return torch.allclose(out1, out2)\n\ninp1 = torch.randn(5, 5)\ninp2 = torch.randn(5, 5)",
      "traced_f1 = torch.jit.trace(f1, (inp1, inp2))\nprint(\"traced 1, 1:\", test_fns(f1, traced_f1, (inp1, inp2)))\nprint(\"traced 1, 2:\", test_fns(f1, traced_f1, (-inp1, inp2)))",
      "import traceback as tb\ntry:\n    torch.fx.symbolic_trace(f1)\nexcept:\n    tb.print_exc()",
      "Traceback (most recent call last):\n  File \"/var/lib/workspace/intermediate_source/torch_compile_tutorial.py\", line 415, in <module>\n    torch.fx.symbolic_trace(f1)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/fx/_symbolic_trace.py\", line 1314, in symbolic_trace\n    graph = tracer.trace(root, concrete_args)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py\", line 838, in _fn\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/fx/_symbolic_trace.py\", line 838, in trace\n    (self.create_arg(fn(*args)),),\n  File \"/var/lib/workspace/intermediate_source/torch_compile_tutorial.py\", line 385, in f1\n    if x.sum() < 0:\n  File \"/usr/local/lib/python3.10/dist-packages/torch/fx/proxy.py\", line 555, in __bool__\n    return self.tracer.to_bool(self)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/fx/proxy.py\", line 366, in to_bool\n    raise TraceError(\ntorch.fx.proxy.TraceError: symbolically traced variables cannot be used as inputs to control flow",
      "fx_f1 = torch.fx.symbolic_trace(f1, concrete_args={\"x\": inp1})\nprint(\"fx 1, 1:\", test_fns(f1, fx_f1, (inp1, inp2)))\nprint(\"fx 1, 2:\", test_fns(f1, fx_f1, (-inp1, inp2)))",
      "# Reset since we are using a different mode.\ntorch._dynamo.reset()\n\ncompile_f1 = torch.compile(f1)\nprint(\"compile 1, 1:\", test_fns(f1, compile_f1, (inp1, inp2)))\nprint(\"compile 1, 2:\", test_fns(f1, compile_f1, (-inp1, inp2)))\nprint(\"~\" * 10)",
      "def f2(x, y):\n    return x + y\n\ninp1 = torch.randn(5, 5)\ninp2 = 3\n\nscript_f2 = torch.jit.script(f2)\ntry:\n    script_f2(inp1, inp2)\nexcept:\n    tb.print_exc()",
      "compile_f2 = torch.compile(f2)\nprint(\"compile 2:\", test_fns(f2, compile_f2, (inp1, inp2)))\nprint(\"~\" * 10)",
      "import scipy\ndef f3(x):\n    x = x * 2\n    x = scipy.fft.dct(x.numpy())\n    x = torch.from_numpy(x)\n    x = x * 2\n    return x",
      "inp1 = torch.randn(5, 5)\ninp2 = torch.randn(5, 5)\ntraced_f3 = torch.jit.trace(f3, (inp1,))\nprint(\"traced 3:\", test_fns(f3, traced_f3, (inp2,)))",
      "/var/lib/workspace/intermediate_source/torch_compile_tutorial.py:476: TracerWarning:\n\nConverting a tensor to a NumPy array might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n\n/var/lib/workspace/intermediate_source/torch_compile_tutorial.py:477: TracerWarning:\n\ntorch.from_numpy results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n\ntraced 3: False",
      "try:\n    torch.jit.script(f3)\nexcept:\n    tb.print_exc()\n\ntry:\n    torch.fx.symbolic_trace(f3)\nexcept:\n    tb.print_exc()",
      "Traceback (most recent call last):\n  File \"/var/lib/workspace/intermediate_source/torch_compile_tutorial.py\", line 494, in <module>\n    torch.jit.script(f3)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/jit/_script.py\", line 1443, in script\n    ret = _script_impl(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/jit/_script.py\", line 1214, in _script_impl\n    fn = torch._C._jit_script_compile(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/_jit_internal.py\", line 1233, in _try_get_dispatched_fn\n    return boolean_dispatched.get(fn)\n  File \"/usr/lib/python3.10/weakref.py\", line 453, in get\n    return self.data.get(ref(key),default)\nTypeError: cannot create weak reference to 'uarray._Function' object\nTraceback (most recent call last):\n  File \"/var/lib/workspace/intermediate_source/torch_compile_tutorial.py\", line 499, in <module>\n    torch.fx.symbolic_trace(f3)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/fx/_symbolic_trace.py\", line 1314, in symbolic_trace\n    graph = tracer.trace(root, concrete_args)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py\", line 838, in _fn\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/fx/_symbolic_trace.py\", line 838, in trace\n    (self.create_arg(fn(*args)),),\n  File \"/var/lib/workspace/intermediate_source/torch_compile_tutorial.py\", line 476, in f3\n    x = scipy.fft.dct(x.numpy())\n  File \"/usr/local/lib/python3.10/dist-packages/scipy/fft/_backend.py\", line 25, in __ua_function__\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/scipy/fft/_pocketfft/realtransforms.py\", line 19, in _r2r\n    tmp = _asfarray(x)\n  File \"/usr/local/lib/python3.10/dist-packages/scipy/fft/_pocketfft/helper.py\", line 89, in _asfarray\n    if x.dtype == np.float16:\n  File \"/usr/local/lib/python3.10/dist-packages/torch/fx/proxy.py\", line 717, in impl\n    return tracer.create_proxy(\"call_function\", target, args, kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/fx/proxy.py\", line 236, in create_proxy\n    args_ = self.create_arg(args)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/fx/_symbolic_trace.py\", line 428, in create_arg\n    return super().create_arg(a)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/fx/proxy.py\", line 311, in create_arg\n    return handler(self, a)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/fx/proxy.py\", line 773, in <lambda>\n    _create_arg_bypass[tuple] = lambda self, a: tuple([self.create_arg(elem) for elem in a])\n  File \"/usr/local/lib/python3.10/dist-packages/torch/fx/proxy.py\", line 773, in <listcomp>\n    _create_arg_bypass[tuple] = lambda self, a: tuple([self.create_arg(elem) for elem in a])\n  File \"/usr/local/lib/python3.10/dist-packages/torch/fx/_symbolic_trace.py\", line 428, in create_arg\n    return super().create_arg(a)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/fx/proxy.py\", line 357, in create_arg\n    raise NotImplementedError(f\"argument of type: {type(a)}\")\nNotImplementedError: argument of type: <class 'type'>",
      "compile_f3 = torch.compile(f3)\nprint(\"compile 3:\", test_fns(f3, compile_f3, (inp2,)))",
      "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/variables/functions.py:1262: UserWarning:\n\nDynamo does not know how to trace the builtin `scipy.fft._pocketfft.pypocketfft.PyCapsule.dct.` This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind).\nIf it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround.\nIf it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use `torch.compiler.allow_in_graph`.\n\ncompile 3: True",
      "from typing import List\ndef custom_backend(gm: torch.fx.GraphModule, example_inputs: List[torch.Tensor]):\n    print(\"custom backend called with FX graph:\")\n    gm.graph.print_tabular()\n    return gm.forward\n\n# Reset since we are using a different backend.\ntorch._dynamo.reset()\n\nopt_model = torch.compile(init_model(), backend=custom_backend)\nopt_model(generate_data(16)[0])",
      "custom backend called with FX graph:\nopcode         name                                                                                                         target                                                                                                       args                                                                                                                                                                                                                                                                                                                                                                                                                                                     kwargs\n-------------  -----------------------------------------------------------------------------------------------------------  -----------------------------------------------------------------------------------------------------------  -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------  ---------------------------------------------\nplaceholder    l_self_modules_features_modules_conv0_parameters_weight_                                                     L_self_modules_features_modules_conv0_parameters_weight_                                                     ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_x_                                                                                                         L_x_                                                                                                         ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_norm0_buffers_num_batches_tracked_                                           L_self_modules_features_modules_norm0_buffers_num_batches_tracked_                                           ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_norm0_buffers_running_mean_                                                  L_self_modules_features_modules_norm0_buffers_running_mean_                                                  ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_norm0_buffers_running_var_                                                   L_self_modules_features_modules_norm0_buffers_running_var_                                                   ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_norm0_parameters_weight_                                                     L_self_modules_features_modules_norm0_parameters_weight_                                                     ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_norm0_parameters_bias_                                                       L_self_modules_features_modules_norm0_parameters_bias_                                                       ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock1_modules_denselayer1_modules_norm1_buffers_num_batches_tracked_   L_self_modules_features_modules_denseblock1_modules_denselayer1_modules_norm1_buffers_num_batches_tracked_   ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock1_modules_denselayer1_modules_norm1_buffers_running_mean_          L_self_modules_features_modules_denseblock1_modules_denselayer1_modules_norm1_buffers_running_mean_          ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock1_modules_denselayer1_modules_norm1_buffers_running_var_           L_self_modules_features_modules_denseblock1_modules_denselayer1_modules_norm1_buffers_running_var_           ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock1_modules_denselayer1_modules_norm1_parameters_weight_             L_self_modules_features_modules_denseblock1_modules_denselayer1_modules_norm1_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock1_modules_denselayer1_modules_norm1_parameters_bias_               L_self_modules_features_modules_denseblock1_modules_denselayer1_modules_norm1_parameters_bias_               ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock1_modules_denselayer1_modules_conv1_parameters_weight_             L_self_modules_features_modules_denseblock1_modules_denselayer1_modules_conv1_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock1_modules_denselayer1_modules_norm2_buffers_num_batches_tracked_   L_self_modules_features_modules_denseblock1_modules_denselayer1_modules_norm2_buffers_num_batches_tracked_   ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock1_modules_denselayer1_modules_norm2_buffers_running_mean_          L_self_modules_features_modules_denseblock1_modules_denselayer1_modules_norm2_buffers_running_mean_          ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock1_modules_denselayer1_modules_norm2_buffers_running_var_           L_self_modules_features_modules_denseblock1_modules_denselayer1_modules_norm2_buffers_running_var_           ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock1_modules_denselayer1_modules_norm2_parameters_weight_             L_self_modules_features_modules_denseblock1_modules_denselayer1_modules_norm2_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock1_modules_denselayer1_modules_norm2_parameters_bias_               L_self_modules_features_modules_denseblock1_modules_denselayer1_modules_norm2_parameters_bias_               ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock1_modules_denselayer1_modules_conv2_parameters_weight_             L_self_modules_features_modules_denseblock1_modules_denselayer1_modules_conv2_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock1_modules_denselayer2_modules_norm1_buffers_num_batches_tracked_   L_self_modules_features_modules_denseblock1_modules_denselayer2_modules_norm1_buffers_num_batches_tracked_   ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock1_modules_denselayer2_modules_norm1_buffers_running_mean_          L_self_modules_features_modules_denseblock1_modules_denselayer2_modules_norm1_buffers_running_mean_          ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock1_modules_denselayer2_modules_norm1_buffers_running_var_           L_self_modules_features_modules_denseblock1_modules_denselayer2_modules_norm1_buffers_running_var_           ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock1_modules_denselayer2_modules_norm1_parameters_weight_             L_self_modules_features_modules_denseblock1_modules_denselayer2_modules_norm1_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock1_modules_denselayer2_modules_norm1_parameters_bias_               L_self_modules_features_modules_denseblock1_modules_denselayer2_modules_norm1_parameters_bias_               ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock1_modules_denselayer2_modules_conv1_parameters_weight_             L_self_modules_features_modules_denseblock1_modules_denselayer2_modules_conv1_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock1_modules_denselayer2_modules_norm2_buffers_num_batches_tracked_   L_self_modules_features_modules_denseblock1_modules_denselayer2_modules_norm2_buffers_num_batches_tracked_   ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock1_modules_denselayer2_modules_norm2_buffers_running_mean_          L_self_modules_features_modules_denseblock1_modules_denselayer2_modules_norm2_buffers_running_mean_          ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock1_modules_denselayer2_modules_norm2_buffers_running_var_           L_self_modules_features_modules_denseblock1_modules_denselayer2_modules_norm2_buffers_running_var_           ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock1_modules_denselayer2_modules_norm2_parameters_weight_             L_self_modules_features_modules_denseblock1_modules_denselayer2_modules_norm2_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock1_modules_denselayer2_modules_norm2_parameters_bias_               L_self_modules_features_modules_denseblock1_modules_denselayer2_modules_norm2_parameters_bias_               ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock1_modules_denselayer2_modules_conv2_parameters_weight_             L_self_modules_features_modules_denseblock1_modules_denselayer2_modules_conv2_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock1_modules_denselayer3_modules_norm1_buffers_num_batches_tracked_   L_self_modules_features_modules_denseblock1_modules_denselayer3_modules_norm1_buffers_num_batches_tracked_   ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock1_modules_denselayer3_modules_norm1_buffers_running_mean_          L_self_modules_features_modules_denseblock1_modules_denselayer3_modules_norm1_buffers_running_mean_          ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock1_modules_denselayer3_modules_norm1_buffers_running_var_           L_self_modules_features_modules_denseblock1_modules_denselayer3_modules_norm1_buffers_running_var_           ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock1_modules_denselayer3_modules_norm1_parameters_weight_             L_self_modules_features_modules_denseblock1_modules_denselayer3_modules_norm1_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock1_modules_denselayer3_modules_norm1_parameters_bias_               L_self_modules_features_modules_denseblock1_modules_denselayer3_modules_norm1_parameters_bias_               ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock1_modules_denselayer3_modules_conv1_parameters_weight_             L_self_modules_features_modules_denseblock1_modules_denselayer3_modules_conv1_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock1_modules_denselayer3_modules_norm2_buffers_num_batches_tracked_   L_self_modules_features_modules_denseblock1_modules_denselayer3_modules_norm2_buffers_num_batches_tracked_   ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock1_modules_denselayer3_modules_norm2_buffers_running_mean_          L_self_modules_features_modules_denseblock1_modules_denselayer3_modules_norm2_buffers_running_mean_          ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock1_modules_denselayer3_modules_norm2_buffers_running_var_           L_self_modules_features_modules_denseblock1_modules_denselayer3_modules_norm2_buffers_running_var_           ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock1_modules_denselayer3_modules_norm2_parameters_weight_             L_self_modules_features_modules_denseblock1_modules_denselayer3_modules_norm2_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock1_modules_denselayer3_modules_norm2_parameters_bias_               L_self_modules_features_modules_denseblock1_modules_denselayer3_modules_norm2_parameters_bias_               ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock1_modules_denselayer3_modules_conv2_parameters_weight_             L_self_modules_features_modules_denseblock1_modules_denselayer3_modules_conv2_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock1_modules_denselayer4_modules_norm1_buffers_num_batches_tracked_   L_self_modules_features_modules_denseblock1_modules_denselayer4_modules_norm1_buffers_num_batches_tracked_   ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock1_modules_denselayer4_modules_norm1_buffers_running_mean_          L_self_modules_features_modules_denseblock1_modules_denselayer4_modules_norm1_buffers_running_mean_          ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock1_modules_denselayer4_modules_norm1_buffers_running_var_           L_self_modules_features_modules_denseblock1_modules_denselayer4_modules_norm1_buffers_running_var_           ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock1_modules_denselayer4_modules_norm1_parameters_weight_             L_self_modules_features_modules_denseblock1_modules_denselayer4_modules_norm1_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock1_modules_denselayer4_modules_norm1_parameters_bias_               L_self_modules_features_modules_denseblock1_modules_denselayer4_modules_norm1_parameters_bias_               ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock1_modules_denselayer4_modules_conv1_parameters_weight_             L_self_modules_features_modules_denseblock1_modules_denselayer4_modules_conv1_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock1_modules_denselayer4_modules_norm2_buffers_num_batches_tracked_   L_self_modules_features_modules_denseblock1_modules_denselayer4_modules_norm2_buffers_num_batches_tracked_   ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock1_modules_denselayer4_modules_norm2_buffers_running_mean_          L_self_modules_features_modules_denseblock1_modules_denselayer4_modules_norm2_buffers_running_mean_          ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock1_modules_denselayer4_modules_norm2_buffers_running_var_           L_self_modules_features_modules_denseblock1_modules_denselayer4_modules_norm2_buffers_running_var_           ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock1_modules_denselayer4_modules_norm2_parameters_weight_             L_self_modules_features_modules_denseblock1_modules_denselayer4_modules_norm2_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock1_modules_denselayer4_modules_norm2_parameters_bias_               L_self_modules_features_modules_denseblock1_modules_denselayer4_modules_norm2_parameters_bias_               ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock1_modules_denselayer4_modules_conv2_parameters_weight_             L_self_modules_features_modules_denseblock1_modules_denselayer4_modules_conv2_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock1_modules_denselayer5_modules_norm1_buffers_num_batches_tracked_   L_self_modules_features_modules_denseblock1_modules_denselayer5_modules_norm1_buffers_num_batches_tracked_   ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock1_modules_denselayer5_modules_norm1_buffers_running_mean_          L_self_modules_features_modules_denseblock1_modules_denselayer5_modules_norm1_buffers_running_mean_          ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock1_modules_denselayer5_modules_norm1_buffers_running_var_           L_self_modules_features_modules_denseblock1_modules_denselayer5_modules_norm1_buffers_running_var_           ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock1_modules_denselayer5_modules_norm1_parameters_weight_             L_self_modules_features_modules_denseblock1_modules_denselayer5_modules_norm1_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock1_modules_denselayer5_modules_norm1_parameters_bias_               L_self_modules_features_modules_denseblock1_modules_denselayer5_modules_norm1_parameters_bias_               ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock1_modules_denselayer5_modules_conv1_parameters_weight_             L_self_modules_features_modules_denseblock1_modules_denselayer5_modules_conv1_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock1_modules_denselayer5_modules_norm2_buffers_num_batches_tracked_   L_self_modules_features_modules_denseblock1_modules_denselayer5_modules_norm2_buffers_num_batches_tracked_   ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock1_modules_denselayer5_modules_norm2_buffers_running_mean_          L_self_modules_features_modules_denseblock1_modules_denselayer5_modules_norm2_buffers_running_mean_          ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock1_modules_denselayer5_modules_norm2_buffers_running_var_           L_self_modules_features_modules_denseblock1_modules_denselayer5_modules_norm2_buffers_running_var_           ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock1_modules_denselayer5_modules_norm2_parameters_weight_             L_self_modules_features_modules_denseblock1_modules_denselayer5_modules_norm2_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock1_modules_denselayer5_modules_norm2_parameters_bias_               L_self_modules_features_modules_denseblock1_modules_denselayer5_modules_norm2_parameters_bias_               ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock1_modules_denselayer5_modules_conv2_parameters_weight_             L_self_modules_features_modules_denseblock1_modules_denselayer5_modules_conv2_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock1_modules_denselayer6_modules_norm1_buffers_num_batches_tracked_   L_self_modules_features_modules_denseblock1_modules_denselayer6_modules_norm1_buffers_num_batches_tracked_   ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock1_modules_denselayer6_modules_norm1_buffers_running_mean_          L_self_modules_features_modules_denseblock1_modules_denselayer6_modules_norm1_buffers_running_mean_          ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock1_modules_denselayer6_modules_norm1_buffers_running_var_           L_self_modules_features_modules_denseblock1_modules_denselayer6_modules_norm1_buffers_running_var_           ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock1_modules_denselayer6_modules_norm1_parameters_weight_             L_self_modules_features_modules_denseblock1_modules_denselayer6_modules_norm1_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock1_modules_denselayer6_modules_norm1_parameters_bias_               L_self_modules_features_modules_denseblock1_modules_denselayer6_modules_norm1_parameters_bias_               ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock1_modules_denselayer6_modules_conv1_parameters_weight_             L_self_modules_features_modules_denseblock1_modules_denselayer6_modules_conv1_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock1_modules_denselayer6_modules_norm2_buffers_num_batches_tracked_   L_self_modules_features_modules_denseblock1_modules_denselayer6_modules_norm2_buffers_num_batches_tracked_   ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock1_modules_denselayer6_modules_norm2_buffers_running_mean_          L_self_modules_features_modules_denseblock1_modules_denselayer6_modules_norm2_buffers_running_mean_          ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock1_modules_denselayer6_modules_norm2_buffers_running_var_           L_self_modules_features_modules_denseblock1_modules_denselayer6_modules_norm2_buffers_running_var_           ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock1_modules_denselayer6_modules_norm2_parameters_weight_             L_self_modules_features_modules_denseblock1_modules_denselayer6_modules_norm2_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock1_modules_denselayer6_modules_norm2_parameters_bias_               L_self_modules_features_modules_denseblock1_modules_denselayer6_modules_norm2_parameters_bias_               ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock1_modules_denselayer6_modules_conv2_parameters_weight_             L_self_modules_features_modules_denseblock1_modules_denselayer6_modules_conv2_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_transition1_modules_norm_buffers_num_batches_tracked_                        L_self_modules_features_modules_transition1_modules_norm_buffers_num_batches_tracked_                        ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_transition1_modules_norm_buffers_running_mean_                               L_self_modules_features_modules_transition1_modules_norm_buffers_running_mean_                               ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_transition1_modules_norm_buffers_running_var_                                L_self_modules_features_modules_transition1_modules_norm_buffers_running_var_                                ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_transition1_modules_norm_parameters_weight_                                  L_self_modules_features_modules_transition1_modules_norm_parameters_weight_                                  ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_transition1_modules_norm_parameters_bias_                                    L_self_modules_features_modules_transition1_modules_norm_parameters_bias_                                    ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_transition1_modules_conv_parameters_weight_                                  L_self_modules_features_modules_transition1_modules_conv_parameters_weight_                                  ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer1_modules_norm1_buffers_num_batches_tracked_   L_self_modules_features_modules_denseblock2_modules_denselayer1_modules_norm1_buffers_num_batches_tracked_   ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer1_modules_norm1_buffers_running_mean_          L_self_modules_features_modules_denseblock2_modules_denselayer1_modules_norm1_buffers_running_mean_          ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer1_modules_norm1_buffers_running_var_           L_self_modules_features_modules_denseblock2_modules_denselayer1_modules_norm1_buffers_running_var_           ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer1_modules_norm1_parameters_weight_             L_self_modules_features_modules_denseblock2_modules_denselayer1_modules_norm1_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer1_modules_norm1_parameters_bias_               L_self_modules_features_modules_denseblock2_modules_denselayer1_modules_norm1_parameters_bias_               ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer1_modules_conv1_parameters_weight_             L_self_modules_features_modules_denseblock2_modules_denselayer1_modules_conv1_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer1_modules_norm2_buffers_num_batches_tracked_   L_self_modules_features_modules_denseblock2_modules_denselayer1_modules_norm2_buffers_num_batches_tracked_   ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer1_modules_norm2_buffers_running_mean_          L_self_modules_features_modules_denseblock2_modules_denselayer1_modules_norm2_buffers_running_mean_          ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer1_modules_norm2_buffers_running_var_           L_self_modules_features_modules_denseblock2_modules_denselayer1_modules_norm2_buffers_running_var_           ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer1_modules_norm2_parameters_weight_             L_self_modules_features_modules_denseblock2_modules_denselayer1_modules_norm2_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer1_modules_norm2_parameters_bias_               L_self_modules_features_modules_denseblock2_modules_denselayer1_modules_norm2_parameters_bias_               ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer1_modules_conv2_parameters_weight_             L_self_modules_features_modules_denseblock2_modules_denselayer1_modules_conv2_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer2_modules_norm1_buffers_num_batches_tracked_   L_self_modules_features_modules_denseblock2_modules_denselayer2_modules_norm1_buffers_num_batches_tracked_   ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer2_modules_norm1_buffers_running_mean_          L_self_modules_features_modules_denseblock2_modules_denselayer2_modules_norm1_buffers_running_mean_          ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer2_modules_norm1_buffers_running_var_           L_self_modules_features_modules_denseblock2_modules_denselayer2_modules_norm1_buffers_running_var_           ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer2_modules_norm1_parameters_weight_             L_self_modules_features_modules_denseblock2_modules_denselayer2_modules_norm1_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer2_modules_norm1_parameters_bias_               L_self_modules_features_modules_denseblock2_modules_denselayer2_modules_norm1_parameters_bias_               ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer2_modules_conv1_parameters_weight_             L_self_modules_features_modules_denseblock2_modules_denselayer2_modules_conv1_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer2_modules_norm2_buffers_num_batches_tracked_   L_self_modules_features_modules_denseblock2_modules_denselayer2_modules_norm2_buffers_num_batches_tracked_   ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer2_modules_norm2_buffers_running_mean_          L_self_modules_features_modules_denseblock2_modules_denselayer2_modules_norm2_buffers_running_mean_          ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer2_modules_norm2_buffers_running_var_           L_self_modules_features_modules_denseblock2_modules_denselayer2_modules_norm2_buffers_running_var_           ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer2_modules_norm2_parameters_weight_             L_self_modules_features_modules_denseblock2_modules_denselayer2_modules_norm2_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer2_modules_norm2_parameters_bias_               L_self_modules_features_modules_denseblock2_modules_denselayer2_modules_norm2_parameters_bias_               ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer2_modules_conv2_parameters_weight_             L_self_modules_features_modules_denseblock2_modules_denselayer2_modules_conv2_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer3_modules_norm1_buffers_num_batches_tracked_   L_self_modules_features_modules_denseblock2_modules_denselayer3_modules_norm1_buffers_num_batches_tracked_   ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer3_modules_norm1_buffers_running_mean_          L_self_modules_features_modules_denseblock2_modules_denselayer3_modules_norm1_buffers_running_mean_          ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer3_modules_norm1_buffers_running_var_           L_self_modules_features_modules_denseblock2_modules_denselayer3_modules_norm1_buffers_running_var_           ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer3_modules_norm1_parameters_weight_             L_self_modules_features_modules_denseblock2_modules_denselayer3_modules_norm1_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer3_modules_norm1_parameters_bias_               L_self_modules_features_modules_denseblock2_modules_denselayer3_modules_norm1_parameters_bias_               ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer3_modules_conv1_parameters_weight_             L_self_modules_features_modules_denseblock2_modules_denselayer3_modules_conv1_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer3_modules_norm2_buffers_num_batches_tracked_   L_self_modules_features_modules_denseblock2_modules_denselayer3_modules_norm2_buffers_num_batches_tracked_   ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer3_modules_norm2_buffers_running_mean_          L_self_modules_features_modules_denseblock2_modules_denselayer3_modules_norm2_buffers_running_mean_          ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer3_modules_norm2_buffers_running_var_           L_self_modules_features_modules_denseblock2_modules_denselayer3_modules_norm2_buffers_running_var_           ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer3_modules_norm2_parameters_weight_             L_self_modules_features_modules_denseblock2_modules_denselayer3_modules_norm2_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer3_modules_norm2_parameters_bias_               L_self_modules_features_modules_denseblock2_modules_denselayer3_modules_norm2_parameters_bias_               ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer3_modules_conv2_parameters_weight_             L_self_modules_features_modules_denseblock2_modules_denselayer3_modules_conv2_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer4_modules_norm1_buffers_num_batches_tracked_   L_self_modules_features_modules_denseblock2_modules_denselayer4_modules_norm1_buffers_num_batches_tracked_   ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer4_modules_norm1_buffers_running_mean_          L_self_modules_features_modules_denseblock2_modules_denselayer4_modules_norm1_buffers_running_mean_          ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer4_modules_norm1_buffers_running_var_           L_self_modules_features_modules_denseblock2_modules_denselayer4_modules_norm1_buffers_running_var_           ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer4_modules_norm1_parameters_weight_             L_self_modules_features_modules_denseblock2_modules_denselayer4_modules_norm1_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer4_modules_norm1_parameters_bias_               L_self_modules_features_modules_denseblock2_modules_denselayer4_modules_norm1_parameters_bias_               ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer4_modules_conv1_parameters_weight_             L_self_modules_features_modules_denseblock2_modules_denselayer4_modules_conv1_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer4_modules_norm2_buffers_num_batches_tracked_   L_self_modules_features_modules_denseblock2_modules_denselayer4_modules_norm2_buffers_num_batches_tracked_   ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer4_modules_norm2_buffers_running_mean_          L_self_modules_features_modules_denseblock2_modules_denselayer4_modules_norm2_buffers_running_mean_          ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer4_modules_norm2_buffers_running_var_           L_self_modules_features_modules_denseblock2_modules_denselayer4_modules_norm2_buffers_running_var_           ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer4_modules_norm2_parameters_weight_             L_self_modules_features_modules_denseblock2_modules_denselayer4_modules_norm2_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer4_modules_norm2_parameters_bias_               L_self_modules_features_modules_denseblock2_modules_denselayer4_modules_norm2_parameters_bias_               ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer4_modules_conv2_parameters_weight_             L_self_modules_features_modules_denseblock2_modules_denselayer4_modules_conv2_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer5_modules_norm1_buffers_num_batches_tracked_   L_self_modules_features_modules_denseblock2_modules_denselayer5_modules_norm1_buffers_num_batches_tracked_   ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer5_modules_norm1_buffers_running_mean_          L_self_modules_features_modules_denseblock2_modules_denselayer5_modules_norm1_buffers_running_mean_          ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer5_modules_norm1_buffers_running_var_           L_self_modules_features_modules_denseblock2_modules_denselayer5_modules_norm1_buffers_running_var_           ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer5_modules_norm1_parameters_weight_             L_self_modules_features_modules_denseblock2_modules_denselayer5_modules_norm1_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer5_modules_norm1_parameters_bias_               L_self_modules_features_modules_denseblock2_modules_denselayer5_modules_norm1_parameters_bias_               ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer5_modules_conv1_parameters_weight_             L_self_modules_features_modules_denseblock2_modules_denselayer5_modules_conv1_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer5_modules_norm2_buffers_num_batches_tracked_   L_self_modules_features_modules_denseblock2_modules_denselayer5_modules_norm2_buffers_num_batches_tracked_   ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer5_modules_norm2_buffers_running_mean_          L_self_modules_features_modules_denseblock2_modules_denselayer5_modules_norm2_buffers_running_mean_          ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer5_modules_norm2_buffers_running_var_           L_self_modules_features_modules_denseblock2_modules_denselayer5_modules_norm2_buffers_running_var_           ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer5_modules_norm2_parameters_weight_             L_self_modules_features_modules_denseblock2_modules_denselayer5_modules_norm2_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer5_modules_norm2_parameters_bias_               L_self_modules_features_modules_denseblock2_modules_denselayer5_modules_norm2_parameters_bias_               ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer5_modules_conv2_parameters_weight_             L_self_modules_features_modules_denseblock2_modules_denselayer5_modules_conv2_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer6_modules_norm1_buffers_num_batches_tracked_   L_self_modules_features_modules_denseblock2_modules_denselayer6_modules_norm1_buffers_num_batches_tracked_   ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer6_modules_norm1_buffers_running_mean_          L_self_modules_features_modules_denseblock2_modules_denselayer6_modules_norm1_buffers_running_mean_          ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer6_modules_norm1_buffers_running_var_           L_self_modules_features_modules_denseblock2_modules_denselayer6_modules_norm1_buffers_running_var_           ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer6_modules_norm1_parameters_weight_             L_self_modules_features_modules_denseblock2_modules_denselayer6_modules_norm1_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer6_modules_norm1_parameters_bias_               L_self_modules_features_modules_denseblock2_modules_denselayer6_modules_norm1_parameters_bias_               ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer6_modules_conv1_parameters_weight_             L_self_modules_features_modules_denseblock2_modules_denselayer6_modules_conv1_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer6_modules_norm2_buffers_num_batches_tracked_   L_self_modules_features_modules_denseblock2_modules_denselayer6_modules_norm2_buffers_num_batches_tracked_   ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer6_modules_norm2_buffers_running_mean_          L_self_modules_features_modules_denseblock2_modules_denselayer6_modules_norm2_buffers_running_mean_          ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer6_modules_norm2_buffers_running_var_           L_self_modules_features_modules_denseblock2_modules_denselayer6_modules_norm2_buffers_running_var_           ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer6_modules_norm2_parameters_weight_             L_self_modules_features_modules_denseblock2_modules_denselayer6_modules_norm2_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer6_modules_norm2_parameters_bias_               L_self_modules_features_modules_denseblock2_modules_denselayer6_modules_norm2_parameters_bias_               ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer6_modules_conv2_parameters_weight_             L_self_modules_features_modules_denseblock2_modules_denselayer6_modules_conv2_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer7_modules_norm1_buffers_num_batches_tracked_   L_self_modules_features_modules_denseblock2_modules_denselayer7_modules_norm1_buffers_num_batches_tracked_   ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer7_modules_norm1_buffers_running_mean_          L_self_modules_features_modules_denseblock2_modules_denselayer7_modules_norm1_buffers_running_mean_          ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer7_modules_norm1_buffers_running_var_           L_self_modules_features_modules_denseblock2_modules_denselayer7_modules_norm1_buffers_running_var_           ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer7_modules_norm1_parameters_weight_             L_self_modules_features_modules_denseblock2_modules_denselayer7_modules_norm1_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer7_modules_norm1_parameters_bias_               L_self_modules_features_modules_denseblock2_modules_denselayer7_modules_norm1_parameters_bias_               ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer7_modules_conv1_parameters_weight_             L_self_modules_features_modules_denseblock2_modules_denselayer7_modules_conv1_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer7_modules_norm2_buffers_num_batches_tracked_   L_self_modules_features_modules_denseblock2_modules_denselayer7_modules_norm2_buffers_num_batches_tracked_   ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer7_modules_norm2_buffers_running_mean_          L_self_modules_features_modules_denseblock2_modules_denselayer7_modules_norm2_buffers_running_mean_          ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer7_modules_norm2_buffers_running_var_           L_self_modules_features_modules_denseblock2_modules_denselayer7_modules_norm2_buffers_running_var_           ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer7_modules_norm2_parameters_weight_             L_self_modules_features_modules_denseblock2_modules_denselayer7_modules_norm2_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer7_modules_norm2_parameters_bias_               L_self_modules_features_modules_denseblock2_modules_denselayer7_modules_norm2_parameters_bias_               ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer7_modules_conv2_parameters_weight_             L_self_modules_features_modules_denseblock2_modules_denselayer7_modules_conv2_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer8_modules_norm1_buffers_num_batches_tracked_   L_self_modules_features_modules_denseblock2_modules_denselayer8_modules_norm1_buffers_num_batches_tracked_   ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer8_modules_norm1_buffers_running_mean_          L_self_modules_features_modules_denseblock2_modules_denselayer8_modules_norm1_buffers_running_mean_          ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer8_modules_norm1_buffers_running_var_           L_self_modules_features_modules_denseblock2_modules_denselayer8_modules_norm1_buffers_running_var_           ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer8_modules_norm1_parameters_weight_             L_self_modules_features_modules_denseblock2_modules_denselayer8_modules_norm1_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer8_modules_norm1_parameters_bias_               L_self_modules_features_modules_denseblock2_modules_denselayer8_modules_norm1_parameters_bias_               ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer8_modules_conv1_parameters_weight_             L_self_modules_features_modules_denseblock2_modules_denselayer8_modules_conv1_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer8_modules_norm2_buffers_num_batches_tracked_   L_self_modules_features_modules_denseblock2_modules_denselayer8_modules_norm2_buffers_num_batches_tracked_   ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer8_modules_norm2_buffers_running_mean_          L_self_modules_features_modules_denseblock2_modules_denselayer8_modules_norm2_buffers_running_mean_          ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer8_modules_norm2_buffers_running_var_           L_self_modules_features_modules_denseblock2_modules_denselayer8_modules_norm2_buffers_running_var_           ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer8_modules_norm2_parameters_weight_             L_self_modules_features_modules_denseblock2_modules_denselayer8_modules_norm2_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer8_modules_norm2_parameters_bias_               L_self_modules_features_modules_denseblock2_modules_denselayer8_modules_norm2_parameters_bias_               ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer8_modules_conv2_parameters_weight_             L_self_modules_features_modules_denseblock2_modules_denselayer8_modules_conv2_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer9_modules_norm1_buffers_num_batches_tracked_   L_self_modules_features_modules_denseblock2_modules_denselayer9_modules_norm1_buffers_num_batches_tracked_   ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer9_modules_norm1_buffers_running_mean_          L_self_modules_features_modules_denseblock2_modules_denselayer9_modules_norm1_buffers_running_mean_          ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer9_modules_norm1_buffers_running_var_           L_self_modules_features_modules_denseblock2_modules_denselayer9_modules_norm1_buffers_running_var_           ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer9_modules_norm1_parameters_weight_             L_self_modules_features_modules_denseblock2_modules_denselayer9_modules_norm1_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer9_modules_norm1_parameters_bias_               L_self_modules_features_modules_denseblock2_modules_denselayer9_modules_norm1_parameters_bias_               ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer9_modules_conv1_parameters_weight_             L_self_modules_features_modules_denseblock2_modules_denselayer9_modules_conv1_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer9_modules_norm2_buffers_num_batches_tracked_   L_self_modules_features_modules_denseblock2_modules_denselayer9_modules_norm2_buffers_num_batches_tracked_   ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer9_modules_norm2_buffers_running_mean_          L_self_modules_features_modules_denseblock2_modules_denselayer9_modules_norm2_buffers_running_mean_          ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer9_modules_norm2_buffers_running_var_           L_self_modules_features_modules_denseblock2_modules_denselayer9_modules_norm2_buffers_running_var_           ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer9_modules_norm2_parameters_weight_             L_self_modules_features_modules_denseblock2_modules_denselayer9_modules_norm2_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer9_modules_norm2_parameters_bias_               L_self_modules_features_modules_denseblock2_modules_denselayer9_modules_norm2_parameters_bias_               ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer9_modules_conv2_parameters_weight_             L_self_modules_features_modules_denseblock2_modules_denselayer9_modules_conv2_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer10_modules_norm1_buffers_num_batches_tracked_  L_self_modules_features_modules_denseblock2_modules_denselayer10_modules_norm1_buffers_num_batches_tracked_  ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer10_modules_norm1_buffers_running_mean_         L_self_modules_features_modules_denseblock2_modules_denselayer10_modules_norm1_buffers_running_mean_         ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer10_modules_norm1_buffers_running_var_          L_self_modules_features_modules_denseblock2_modules_denselayer10_modules_norm1_buffers_running_var_          ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer10_modules_norm1_parameters_weight_            L_self_modules_features_modules_denseblock2_modules_denselayer10_modules_norm1_parameters_weight_            ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer10_modules_norm1_parameters_bias_              L_self_modules_features_modules_denseblock2_modules_denselayer10_modules_norm1_parameters_bias_              ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer10_modules_conv1_parameters_weight_            L_self_modules_features_modules_denseblock2_modules_denselayer10_modules_conv1_parameters_weight_            ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer10_modules_norm2_buffers_num_batches_tracked_  L_self_modules_features_modules_denseblock2_modules_denselayer10_modules_norm2_buffers_num_batches_tracked_  ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer10_modules_norm2_buffers_running_mean_         L_self_modules_features_modules_denseblock2_modules_denselayer10_modules_norm2_buffers_running_mean_         ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer10_modules_norm2_buffers_running_var_          L_self_modules_features_modules_denseblock2_modules_denselayer10_modules_norm2_buffers_running_var_          ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer10_modules_norm2_parameters_weight_            L_self_modules_features_modules_denseblock2_modules_denselayer10_modules_norm2_parameters_weight_            ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer10_modules_norm2_parameters_bias_              L_self_modules_features_modules_denseblock2_modules_denselayer10_modules_norm2_parameters_bias_              ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer10_modules_conv2_parameters_weight_            L_self_modules_features_modules_denseblock2_modules_denselayer10_modules_conv2_parameters_weight_            ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer11_modules_norm1_buffers_num_batches_tracked_  L_self_modules_features_modules_denseblock2_modules_denselayer11_modules_norm1_buffers_num_batches_tracked_  ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer11_modules_norm1_buffers_running_mean_         L_self_modules_features_modules_denseblock2_modules_denselayer11_modules_norm1_buffers_running_mean_         ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer11_modules_norm1_buffers_running_var_          L_self_modules_features_modules_denseblock2_modules_denselayer11_modules_norm1_buffers_running_var_          ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer11_modules_norm1_parameters_weight_            L_self_modules_features_modules_denseblock2_modules_denselayer11_modules_norm1_parameters_weight_            ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer11_modules_norm1_parameters_bias_              L_self_modules_features_modules_denseblock2_modules_denselayer11_modules_norm1_parameters_bias_              ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer11_modules_conv1_parameters_weight_            L_self_modules_features_modules_denseblock2_modules_denselayer11_modules_conv1_parameters_weight_            ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer11_modules_norm2_buffers_num_batches_tracked_  L_self_modules_features_modules_denseblock2_modules_denselayer11_modules_norm2_buffers_num_batches_tracked_  ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer11_modules_norm2_buffers_running_mean_         L_self_modules_features_modules_denseblock2_modules_denselayer11_modules_norm2_buffers_running_mean_         ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer11_modules_norm2_buffers_running_var_          L_self_modules_features_modules_denseblock2_modules_denselayer11_modules_norm2_buffers_running_var_          ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer11_modules_norm2_parameters_weight_            L_self_modules_features_modules_denseblock2_modules_denselayer11_modules_norm2_parameters_weight_            ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer11_modules_norm2_parameters_bias_              L_self_modules_features_modules_denseblock2_modules_denselayer11_modules_norm2_parameters_bias_              ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer11_modules_conv2_parameters_weight_            L_self_modules_features_modules_denseblock2_modules_denselayer11_modules_conv2_parameters_weight_            ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer12_modules_norm1_buffers_num_batches_tracked_  L_self_modules_features_modules_denseblock2_modules_denselayer12_modules_norm1_buffers_num_batches_tracked_  ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer12_modules_norm1_buffers_running_mean_         L_self_modules_features_modules_denseblock2_modules_denselayer12_modules_norm1_buffers_running_mean_         ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer12_modules_norm1_buffers_running_var_          L_self_modules_features_modules_denseblock2_modules_denselayer12_modules_norm1_buffers_running_var_          ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer12_modules_norm1_parameters_weight_            L_self_modules_features_modules_denseblock2_modules_denselayer12_modules_norm1_parameters_weight_            ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer12_modules_norm1_parameters_bias_              L_self_modules_features_modules_denseblock2_modules_denselayer12_modules_norm1_parameters_bias_              ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer12_modules_conv1_parameters_weight_            L_self_modules_features_modules_denseblock2_modules_denselayer12_modules_conv1_parameters_weight_            ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer12_modules_norm2_buffers_num_batches_tracked_  L_self_modules_features_modules_denseblock2_modules_denselayer12_modules_norm2_buffers_num_batches_tracked_  ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer12_modules_norm2_buffers_running_mean_         L_self_modules_features_modules_denseblock2_modules_denselayer12_modules_norm2_buffers_running_mean_         ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer12_modules_norm2_buffers_running_var_          L_self_modules_features_modules_denseblock2_modules_denselayer12_modules_norm2_buffers_running_var_          ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer12_modules_norm2_parameters_weight_            L_self_modules_features_modules_denseblock2_modules_denselayer12_modules_norm2_parameters_weight_            ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer12_modules_norm2_parameters_bias_              L_self_modules_features_modules_denseblock2_modules_denselayer12_modules_norm2_parameters_bias_              ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock2_modules_denselayer12_modules_conv2_parameters_weight_            L_self_modules_features_modules_denseblock2_modules_denselayer12_modules_conv2_parameters_weight_            ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_transition2_modules_norm_buffers_num_batches_tracked_                        L_self_modules_features_modules_transition2_modules_norm_buffers_num_batches_tracked_                        ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_transition2_modules_norm_buffers_running_mean_                               L_self_modules_features_modules_transition2_modules_norm_buffers_running_mean_                               ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_transition2_modules_norm_buffers_running_var_                                L_self_modules_features_modules_transition2_modules_norm_buffers_running_var_                                ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_transition2_modules_norm_parameters_weight_                                  L_self_modules_features_modules_transition2_modules_norm_parameters_weight_                                  ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_transition2_modules_norm_parameters_bias_                                    L_self_modules_features_modules_transition2_modules_norm_parameters_bias_                                    ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_transition2_modules_conv_parameters_weight_                                  L_self_modules_features_modules_transition2_modules_conv_parameters_weight_                                  ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer1_modules_norm1_buffers_num_batches_tracked_   L_self_modules_features_modules_denseblock3_modules_denselayer1_modules_norm1_buffers_num_batches_tracked_   ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer1_modules_norm1_buffers_running_mean_          L_self_modules_features_modules_denseblock3_modules_denselayer1_modules_norm1_buffers_running_mean_          ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer1_modules_norm1_buffers_running_var_           L_self_modules_features_modules_denseblock3_modules_denselayer1_modules_norm1_buffers_running_var_           ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer1_modules_norm1_parameters_weight_             L_self_modules_features_modules_denseblock3_modules_denselayer1_modules_norm1_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer1_modules_norm1_parameters_bias_               L_self_modules_features_modules_denseblock3_modules_denselayer1_modules_norm1_parameters_bias_               ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer1_modules_conv1_parameters_weight_             L_self_modules_features_modules_denseblock3_modules_denselayer1_modules_conv1_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer1_modules_norm2_buffers_num_batches_tracked_   L_self_modules_features_modules_denseblock3_modules_denselayer1_modules_norm2_buffers_num_batches_tracked_   ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer1_modules_norm2_buffers_running_mean_          L_self_modules_features_modules_denseblock3_modules_denselayer1_modules_norm2_buffers_running_mean_          ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer1_modules_norm2_buffers_running_var_           L_self_modules_features_modules_denseblock3_modules_denselayer1_modules_norm2_buffers_running_var_           ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer1_modules_norm2_parameters_weight_             L_self_modules_features_modules_denseblock3_modules_denselayer1_modules_norm2_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer1_modules_norm2_parameters_bias_               L_self_modules_features_modules_denseblock3_modules_denselayer1_modules_norm2_parameters_bias_               ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer1_modules_conv2_parameters_weight_             L_self_modules_features_modules_denseblock3_modules_denselayer1_modules_conv2_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer2_modules_norm1_buffers_num_batches_tracked_   L_self_modules_features_modules_denseblock3_modules_denselayer2_modules_norm1_buffers_num_batches_tracked_   ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer2_modules_norm1_buffers_running_mean_          L_self_modules_features_modules_denseblock3_modules_denselayer2_modules_norm1_buffers_running_mean_          ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer2_modules_norm1_buffers_running_var_           L_self_modules_features_modules_denseblock3_modules_denselayer2_modules_norm1_buffers_running_var_           ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer2_modules_norm1_parameters_weight_             L_self_modules_features_modules_denseblock3_modules_denselayer2_modules_norm1_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer2_modules_norm1_parameters_bias_               L_self_modules_features_modules_denseblock3_modules_denselayer2_modules_norm1_parameters_bias_               ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer2_modules_conv1_parameters_weight_             L_self_modules_features_modules_denseblock3_modules_denselayer2_modules_conv1_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer2_modules_norm2_buffers_num_batches_tracked_   L_self_modules_features_modules_denseblock3_modules_denselayer2_modules_norm2_buffers_num_batches_tracked_   ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer2_modules_norm2_buffers_running_mean_          L_self_modules_features_modules_denseblock3_modules_denselayer2_modules_norm2_buffers_running_mean_          ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer2_modules_norm2_buffers_running_var_           L_self_modules_features_modules_denseblock3_modules_denselayer2_modules_norm2_buffers_running_var_           ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer2_modules_norm2_parameters_weight_             L_self_modules_features_modules_denseblock3_modules_denselayer2_modules_norm2_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer2_modules_norm2_parameters_bias_               L_self_modules_features_modules_denseblock3_modules_denselayer2_modules_norm2_parameters_bias_               ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer2_modules_conv2_parameters_weight_             L_self_modules_features_modules_denseblock3_modules_denselayer2_modules_conv2_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer3_modules_norm1_buffers_num_batches_tracked_   L_self_modules_features_modules_denseblock3_modules_denselayer3_modules_norm1_buffers_num_batches_tracked_   ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer3_modules_norm1_buffers_running_mean_          L_self_modules_features_modules_denseblock3_modules_denselayer3_modules_norm1_buffers_running_mean_          ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer3_modules_norm1_buffers_running_var_           L_self_modules_features_modules_denseblock3_modules_denselayer3_modules_norm1_buffers_running_var_           ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer3_modules_norm1_parameters_weight_             L_self_modules_features_modules_denseblock3_modules_denselayer3_modules_norm1_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer3_modules_norm1_parameters_bias_               L_self_modules_features_modules_denseblock3_modules_denselayer3_modules_norm1_parameters_bias_               ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer3_modules_conv1_parameters_weight_             L_self_modules_features_modules_denseblock3_modules_denselayer3_modules_conv1_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer3_modules_norm2_buffers_num_batches_tracked_   L_self_modules_features_modules_denseblock3_modules_denselayer3_modules_norm2_buffers_num_batches_tracked_   ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer3_modules_norm2_buffers_running_mean_          L_self_modules_features_modules_denseblock3_modules_denselayer3_modules_norm2_buffers_running_mean_          ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer3_modules_norm2_buffers_running_var_           L_self_modules_features_modules_denseblock3_modules_denselayer3_modules_norm2_buffers_running_var_           ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer3_modules_norm2_parameters_weight_             L_self_modules_features_modules_denseblock3_modules_denselayer3_modules_norm2_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer3_modules_norm2_parameters_bias_               L_self_modules_features_modules_denseblock3_modules_denselayer3_modules_norm2_parameters_bias_               ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer3_modules_conv2_parameters_weight_             L_self_modules_features_modules_denseblock3_modules_denselayer3_modules_conv2_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer4_modules_norm1_buffers_num_batches_tracked_   L_self_modules_features_modules_denseblock3_modules_denselayer4_modules_norm1_buffers_num_batches_tracked_   ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer4_modules_norm1_buffers_running_mean_          L_self_modules_features_modules_denseblock3_modules_denselayer4_modules_norm1_buffers_running_mean_          ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer4_modules_norm1_buffers_running_var_           L_self_modules_features_modules_denseblock3_modules_denselayer4_modules_norm1_buffers_running_var_           ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer4_modules_norm1_parameters_weight_             L_self_modules_features_modules_denseblock3_modules_denselayer4_modules_norm1_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer4_modules_norm1_parameters_bias_               L_self_modules_features_modules_denseblock3_modules_denselayer4_modules_norm1_parameters_bias_               ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer4_modules_conv1_parameters_weight_             L_self_modules_features_modules_denseblock3_modules_denselayer4_modules_conv1_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer4_modules_norm2_buffers_num_batches_tracked_   L_self_modules_features_modules_denseblock3_modules_denselayer4_modules_norm2_buffers_num_batches_tracked_   ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer4_modules_norm2_buffers_running_mean_          L_self_modules_features_modules_denseblock3_modules_denselayer4_modules_norm2_buffers_running_mean_          ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer4_modules_norm2_buffers_running_var_           L_self_modules_features_modules_denseblock3_modules_denselayer4_modules_norm2_buffers_running_var_           ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer4_modules_norm2_parameters_weight_             L_self_modules_features_modules_denseblock3_modules_denselayer4_modules_norm2_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer4_modules_norm2_parameters_bias_               L_self_modules_features_modules_denseblock3_modules_denselayer4_modules_norm2_parameters_bias_               ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer4_modules_conv2_parameters_weight_             L_self_modules_features_modules_denseblock3_modules_denselayer4_modules_conv2_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer5_modules_norm1_buffers_num_batches_tracked_   L_self_modules_features_modules_denseblock3_modules_denselayer5_modules_norm1_buffers_num_batches_tracked_   ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer5_modules_norm1_buffers_running_mean_          L_self_modules_features_modules_denseblock3_modules_denselayer5_modules_norm1_buffers_running_mean_          ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer5_modules_norm1_buffers_running_var_           L_self_modules_features_modules_denseblock3_modules_denselayer5_modules_norm1_buffers_running_var_           ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer5_modules_norm1_parameters_weight_             L_self_modules_features_modules_denseblock3_modules_denselayer5_modules_norm1_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer5_modules_norm1_parameters_bias_               L_self_modules_features_modules_denseblock3_modules_denselayer5_modules_norm1_parameters_bias_               ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer5_modules_conv1_parameters_weight_             L_self_modules_features_modules_denseblock3_modules_denselayer5_modules_conv1_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer5_modules_norm2_buffers_num_batches_tracked_   L_self_modules_features_modules_denseblock3_modules_denselayer5_modules_norm2_buffers_num_batches_tracked_   ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer5_modules_norm2_buffers_running_mean_          L_self_modules_features_modules_denseblock3_modules_denselayer5_modules_norm2_buffers_running_mean_          ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer5_modules_norm2_buffers_running_var_           L_self_modules_features_modules_denseblock3_modules_denselayer5_modules_norm2_buffers_running_var_           ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer5_modules_norm2_parameters_weight_             L_self_modules_features_modules_denseblock3_modules_denselayer5_modules_norm2_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer5_modules_norm2_parameters_bias_               L_self_modules_features_modules_denseblock3_modules_denselayer5_modules_norm2_parameters_bias_               ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer5_modules_conv2_parameters_weight_             L_self_modules_features_modules_denseblock3_modules_denselayer5_modules_conv2_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer6_modules_norm1_buffers_num_batches_tracked_   L_self_modules_features_modules_denseblock3_modules_denselayer6_modules_norm1_buffers_num_batches_tracked_   ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer6_modules_norm1_buffers_running_mean_          L_self_modules_features_modules_denseblock3_modules_denselayer6_modules_norm1_buffers_running_mean_          ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer6_modules_norm1_buffers_running_var_           L_self_modules_features_modules_denseblock3_modules_denselayer6_modules_norm1_buffers_running_var_           ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer6_modules_norm1_parameters_weight_             L_self_modules_features_modules_denseblock3_modules_denselayer6_modules_norm1_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer6_modules_norm1_parameters_bias_               L_self_modules_features_modules_denseblock3_modules_denselayer6_modules_norm1_parameters_bias_               ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer6_modules_conv1_parameters_weight_             L_self_modules_features_modules_denseblock3_modules_denselayer6_modules_conv1_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer6_modules_norm2_buffers_num_batches_tracked_   L_self_modules_features_modules_denseblock3_modules_denselayer6_modules_norm2_buffers_num_batches_tracked_   ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer6_modules_norm2_buffers_running_mean_          L_self_modules_features_modules_denseblock3_modules_denselayer6_modules_norm2_buffers_running_mean_          ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer6_modules_norm2_buffers_running_var_           L_self_modules_features_modules_denseblock3_modules_denselayer6_modules_norm2_buffers_running_var_           ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer6_modules_norm2_parameters_weight_             L_self_modules_features_modules_denseblock3_modules_denselayer6_modules_norm2_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer6_modules_norm2_parameters_bias_               L_self_modules_features_modules_denseblock3_modules_denselayer6_modules_norm2_parameters_bias_               ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer6_modules_conv2_parameters_weight_             L_self_modules_features_modules_denseblock3_modules_denselayer6_modules_conv2_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer7_modules_norm1_buffers_num_batches_tracked_   L_self_modules_features_modules_denseblock3_modules_denselayer7_modules_norm1_buffers_num_batches_tracked_   ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer7_modules_norm1_buffers_running_mean_          L_self_modules_features_modules_denseblock3_modules_denselayer7_modules_norm1_buffers_running_mean_          ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer7_modules_norm1_buffers_running_var_           L_self_modules_features_modules_denseblock3_modules_denselayer7_modules_norm1_buffers_running_var_           ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer7_modules_norm1_parameters_weight_             L_self_modules_features_modules_denseblock3_modules_denselayer7_modules_norm1_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer7_modules_norm1_parameters_bias_               L_self_modules_features_modules_denseblock3_modules_denselayer7_modules_norm1_parameters_bias_               ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer7_modules_conv1_parameters_weight_             L_self_modules_features_modules_denseblock3_modules_denselayer7_modules_conv1_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer7_modules_norm2_buffers_num_batches_tracked_   L_self_modules_features_modules_denseblock3_modules_denselayer7_modules_norm2_buffers_num_batches_tracked_   ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer7_modules_norm2_buffers_running_mean_          L_self_modules_features_modules_denseblock3_modules_denselayer7_modules_norm2_buffers_running_mean_          ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer7_modules_norm2_buffers_running_var_           L_self_modules_features_modules_denseblock3_modules_denselayer7_modules_norm2_buffers_running_var_           ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer7_modules_norm2_parameters_weight_             L_self_modules_features_modules_denseblock3_modules_denselayer7_modules_norm2_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer7_modules_norm2_parameters_bias_               L_self_modules_features_modules_denseblock3_modules_denselayer7_modules_norm2_parameters_bias_               ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer7_modules_conv2_parameters_weight_             L_self_modules_features_modules_denseblock3_modules_denselayer7_modules_conv2_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer8_modules_norm1_buffers_num_batches_tracked_   L_self_modules_features_modules_denseblock3_modules_denselayer8_modules_norm1_buffers_num_batches_tracked_   ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer8_modules_norm1_buffers_running_mean_          L_self_modules_features_modules_denseblock3_modules_denselayer8_modules_norm1_buffers_running_mean_          ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer8_modules_norm1_buffers_running_var_           L_self_modules_features_modules_denseblock3_modules_denselayer8_modules_norm1_buffers_running_var_           ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer8_modules_norm1_parameters_weight_             L_self_modules_features_modules_denseblock3_modules_denselayer8_modules_norm1_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer8_modules_norm1_parameters_bias_               L_self_modules_features_modules_denseblock3_modules_denselayer8_modules_norm1_parameters_bias_               ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer8_modules_conv1_parameters_weight_             L_self_modules_features_modules_denseblock3_modules_denselayer8_modules_conv1_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer8_modules_norm2_buffers_num_batches_tracked_   L_self_modules_features_modules_denseblock3_modules_denselayer8_modules_norm2_buffers_num_batches_tracked_   ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer8_modules_norm2_buffers_running_mean_          L_self_modules_features_modules_denseblock3_modules_denselayer8_modules_norm2_buffers_running_mean_          ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer8_modules_norm2_buffers_running_var_           L_self_modules_features_modules_denseblock3_modules_denselayer8_modules_norm2_buffers_running_var_           ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer8_modules_norm2_parameters_weight_             L_self_modules_features_modules_denseblock3_modules_denselayer8_modules_norm2_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer8_modules_norm2_parameters_bias_               L_self_modules_features_modules_denseblock3_modules_denselayer8_modules_norm2_parameters_bias_               ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer8_modules_conv2_parameters_weight_             L_self_modules_features_modules_denseblock3_modules_denselayer8_modules_conv2_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer9_modules_norm1_buffers_num_batches_tracked_   L_self_modules_features_modules_denseblock3_modules_denselayer9_modules_norm1_buffers_num_batches_tracked_   ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer9_modules_norm1_buffers_running_mean_          L_self_modules_features_modules_denseblock3_modules_denselayer9_modules_norm1_buffers_running_mean_          ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer9_modules_norm1_buffers_running_var_           L_self_modules_features_modules_denseblock3_modules_denselayer9_modules_norm1_buffers_running_var_           ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer9_modules_norm1_parameters_weight_             L_self_modules_features_modules_denseblock3_modules_denselayer9_modules_norm1_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer9_modules_norm1_parameters_bias_               L_self_modules_features_modules_denseblock3_modules_denselayer9_modules_norm1_parameters_bias_               ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer9_modules_conv1_parameters_weight_             L_self_modules_features_modules_denseblock3_modules_denselayer9_modules_conv1_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer9_modules_norm2_buffers_num_batches_tracked_   L_self_modules_features_modules_denseblock3_modules_denselayer9_modules_norm2_buffers_num_batches_tracked_   ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer9_modules_norm2_buffers_running_mean_          L_self_modules_features_modules_denseblock3_modules_denselayer9_modules_norm2_buffers_running_mean_          ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer9_modules_norm2_buffers_running_var_           L_self_modules_features_modules_denseblock3_modules_denselayer9_modules_norm2_buffers_running_var_           ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer9_modules_norm2_parameters_weight_             L_self_modules_features_modules_denseblock3_modules_denselayer9_modules_norm2_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer9_modules_norm2_parameters_bias_               L_self_modules_features_modules_denseblock3_modules_denselayer9_modules_norm2_parameters_bias_               ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer9_modules_conv2_parameters_weight_             L_self_modules_features_modules_denseblock3_modules_denselayer9_modules_conv2_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer10_modules_norm1_buffers_num_batches_tracked_  L_self_modules_features_modules_denseblock3_modules_denselayer10_modules_norm1_buffers_num_batches_tracked_  ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer10_modules_norm1_buffers_running_mean_         L_self_modules_features_modules_denseblock3_modules_denselayer10_modules_norm1_buffers_running_mean_         ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer10_modules_norm1_buffers_running_var_          L_self_modules_features_modules_denseblock3_modules_denselayer10_modules_norm1_buffers_running_var_          ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer10_modules_norm1_parameters_weight_            L_self_modules_features_modules_denseblock3_modules_denselayer10_modules_norm1_parameters_weight_            ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer10_modules_norm1_parameters_bias_              L_self_modules_features_modules_denseblock3_modules_denselayer10_modules_norm1_parameters_bias_              ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer10_modules_conv1_parameters_weight_            L_self_modules_features_modules_denseblock3_modules_denselayer10_modules_conv1_parameters_weight_            ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer10_modules_norm2_buffers_num_batches_tracked_  L_self_modules_features_modules_denseblock3_modules_denselayer10_modules_norm2_buffers_num_batches_tracked_  ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer10_modules_norm2_buffers_running_mean_         L_self_modules_features_modules_denseblock3_modules_denselayer10_modules_norm2_buffers_running_mean_         ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer10_modules_norm2_buffers_running_var_          L_self_modules_features_modules_denseblock3_modules_denselayer10_modules_norm2_buffers_running_var_          ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer10_modules_norm2_parameters_weight_            L_self_modules_features_modules_denseblock3_modules_denselayer10_modules_norm2_parameters_weight_            ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer10_modules_norm2_parameters_bias_              L_self_modules_features_modules_denseblock3_modules_denselayer10_modules_norm2_parameters_bias_              ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer10_modules_conv2_parameters_weight_            L_self_modules_features_modules_denseblock3_modules_denselayer10_modules_conv2_parameters_weight_            ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer11_modules_norm1_buffers_num_batches_tracked_  L_self_modules_features_modules_denseblock3_modules_denselayer11_modules_norm1_buffers_num_batches_tracked_  ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer11_modules_norm1_buffers_running_mean_         L_self_modules_features_modules_denseblock3_modules_denselayer11_modules_norm1_buffers_running_mean_         ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer11_modules_norm1_buffers_running_var_          L_self_modules_features_modules_denseblock3_modules_denselayer11_modules_norm1_buffers_running_var_          ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer11_modules_norm1_parameters_weight_            L_self_modules_features_modules_denseblock3_modules_denselayer11_modules_norm1_parameters_weight_            ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer11_modules_norm1_parameters_bias_              L_self_modules_features_modules_denseblock3_modules_denselayer11_modules_norm1_parameters_bias_              ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer11_modules_conv1_parameters_weight_            L_self_modules_features_modules_denseblock3_modules_denselayer11_modules_conv1_parameters_weight_            ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer11_modules_norm2_buffers_num_batches_tracked_  L_self_modules_features_modules_denseblock3_modules_denselayer11_modules_norm2_buffers_num_batches_tracked_  ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer11_modules_norm2_buffers_running_mean_         L_self_modules_features_modules_denseblock3_modules_denselayer11_modules_norm2_buffers_running_mean_         ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer11_modules_norm2_buffers_running_var_          L_self_modules_features_modules_denseblock3_modules_denselayer11_modules_norm2_buffers_running_var_          ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer11_modules_norm2_parameters_weight_            L_self_modules_features_modules_denseblock3_modules_denselayer11_modules_norm2_parameters_weight_            ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer11_modules_norm2_parameters_bias_              L_self_modules_features_modules_denseblock3_modules_denselayer11_modules_norm2_parameters_bias_              ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer11_modules_conv2_parameters_weight_            L_self_modules_features_modules_denseblock3_modules_denselayer11_modules_conv2_parameters_weight_            ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer12_modules_norm1_buffers_num_batches_tracked_  L_self_modules_features_modules_denseblock3_modules_denselayer12_modules_norm1_buffers_num_batches_tracked_  ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer12_modules_norm1_buffers_running_mean_         L_self_modules_features_modules_denseblock3_modules_denselayer12_modules_norm1_buffers_running_mean_         ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer12_modules_norm1_buffers_running_var_          L_self_modules_features_modules_denseblock3_modules_denselayer12_modules_norm1_buffers_running_var_          ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer12_modules_norm1_parameters_weight_            L_self_modules_features_modules_denseblock3_modules_denselayer12_modules_norm1_parameters_weight_            ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer12_modules_norm1_parameters_bias_              L_self_modules_features_modules_denseblock3_modules_denselayer12_modules_norm1_parameters_bias_              ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer12_modules_conv1_parameters_weight_            L_self_modules_features_modules_denseblock3_modules_denselayer12_modules_conv1_parameters_weight_            ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer12_modules_norm2_buffers_num_batches_tracked_  L_self_modules_features_modules_denseblock3_modules_denselayer12_modules_norm2_buffers_num_batches_tracked_  ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer12_modules_norm2_buffers_running_mean_         L_self_modules_features_modules_denseblock3_modules_denselayer12_modules_norm2_buffers_running_mean_         ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer12_modules_norm2_buffers_running_var_          L_self_modules_features_modules_denseblock3_modules_denselayer12_modules_norm2_buffers_running_var_          ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer12_modules_norm2_parameters_weight_            L_self_modules_features_modules_denseblock3_modules_denselayer12_modules_norm2_parameters_weight_            ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer12_modules_norm2_parameters_bias_              L_self_modules_features_modules_denseblock3_modules_denselayer12_modules_norm2_parameters_bias_              ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer12_modules_conv2_parameters_weight_            L_self_modules_features_modules_denseblock3_modules_denselayer12_modules_conv2_parameters_weight_            ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer13_modules_norm1_buffers_num_batches_tracked_  L_self_modules_features_modules_denseblock3_modules_denselayer13_modules_norm1_buffers_num_batches_tracked_  ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer13_modules_norm1_buffers_running_mean_         L_self_modules_features_modules_denseblock3_modules_denselayer13_modules_norm1_buffers_running_mean_         ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer13_modules_norm1_buffers_running_var_          L_self_modules_features_modules_denseblock3_modules_denselayer13_modules_norm1_buffers_running_var_          ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer13_modules_norm1_parameters_weight_            L_self_modules_features_modules_denseblock3_modules_denselayer13_modules_norm1_parameters_weight_            ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer13_modules_norm1_parameters_bias_              L_self_modules_features_modules_denseblock3_modules_denselayer13_modules_norm1_parameters_bias_              ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer13_modules_conv1_parameters_weight_            L_self_modules_features_modules_denseblock3_modules_denselayer13_modules_conv1_parameters_weight_            ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer13_modules_norm2_buffers_num_batches_tracked_  L_self_modules_features_modules_denseblock3_modules_denselayer13_modules_norm2_buffers_num_batches_tracked_  ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer13_modules_norm2_buffers_running_mean_         L_self_modules_features_modules_denseblock3_modules_denselayer13_modules_norm2_buffers_running_mean_         ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer13_modules_norm2_buffers_running_var_          L_self_modules_features_modules_denseblock3_modules_denselayer13_modules_norm2_buffers_running_var_          ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer13_modules_norm2_parameters_weight_            L_self_modules_features_modules_denseblock3_modules_denselayer13_modules_norm2_parameters_weight_            ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer13_modules_norm2_parameters_bias_              L_self_modules_features_modules_denseblock3_modules_denselayer13_modules_norm2_parameters_bias_              ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer13_modules_conv2_parameters_weight_            L_self_modules_features_modules_denseblock3_modules_denselayer13_modules_conv2_parameters_weight_            ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer14_modules_norm1_buffers_num_batches_tracked_  L_self_modules_features_modules_denseblock3_modules_denselayer14_modules_norm1_buffers_num_batches_tracked_  ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer14_modules_norm1_buffers_running_mean_         L_self_modules_features_modules_denseblock3_modules_denselayer14_modules_norm1_buffers_running_mean_         ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer14_modules_norm1_buffers_running_var_          L_self_modules_features_modules_denseblock3_modules_denselayer14_modules_norm1_buffers_running_var_          ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer14_modules_norm1_parameters_weight_            L_self_modules_features_modules_denseblock3_modules_denselayer14_modules_norm1_parameters_weight_            ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer14_modules_norm1_parameters_bias_              L_self_modules_features_modules_denseblock3_modules_denselayer14_modules_norm1_parameters_bias_              ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer14_modules_conv1_parameters_weight_            L_self_modules_features_modules_denseblock3_modules_denselayer14_modules_conv1_parameters_weight_            ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer14_modules_norm2_buffers_num_batches_tracked_  L_self_modules_features_modules_denseblock3_modules_denselayer14_modules_norm2_buffers_num_batches_tracked_  ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer14_modules_norm2_buffers_running_mean_         L_self_modules_features_modules_denseblock3_modules_denselayer14_modules_norm2_buffers_running_mean_         ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer14_modules_norm2_buffers_running_var_          L_self_modules_features_modules_denseblock3_modules_denselayer14_modules_norm2_buffers_running_var_          ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer14_modules_norm2_parameters_weight_            L_self_modules_features_modules_denseblock3_modules_denselayer14_modules_norm2_parameters_weight_            ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer14_modules_norm2_parameters_bias_              L_self_modules_features_modules_denseblock3_modules_denselayer14_modules_norm2_parameters_bias_              ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer14_modules_conv2_parameters_weight_            L_self_modules_features_modules_denseblock3_modules_denselayer14_modules_conv2_parameters_weight_            ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer15_modules_norm1_buffers_num_batches_tracked_  L_self_modules_features_modules_denseblock3_modules_denselayer15_modules_norm1_buffers_num_batches_tracked_  ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer15_modules_norm1_buffers_running_mean_         L_self_modules_features_modules_denseblock3_modules_denselayer15_modules_norm1_buffers_running_mean_         ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer15_modules_norm1_buffers_running_var_          L_self_modules_features_modules_denseblock3_modules_denselayer15_modules_norm1_buffers_running_var_          ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer15_modules_norm1_parameters_weight_            L_self_modules_features_modules_denseblock3_modules_denselayer15_modules_norm1_parameters_weight_            ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer15_modules_norm1_parameters_bias_              L_self_modules_features_modules_denseblock3_modules_denselayer15_modules_norm1_parameters_bias_              ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer15_modules_conv1_parameters_weight_            L_self_modules_features_modules_denseblock3_modules_denselayer15_modules_conv1_parameters_weight_            ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer15_modules_norm2_buffers_num_batches_tracked_  L_self_modules_features_modules_denseblock3_modules_denselayer15_modules_norm2_buffers_num_batches_tracked_  ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer15_modules_norm2_buffers_running_mean_         L_self_modules_features_modules_denseblock3_modules_denselayer15_modules_norm2_buffers_running_mean_         ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer15_modules_norm2_buffers_running_var_          L_self_modules_features_modules_denseblock3_modules_denselayer15_modules_norm2_buffers_running_var_          ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer15_modules_norm2_parameters_weight_            L_self_modules_features_modules_denseblock3_modules_denselayer15_modules_norm2_parameters_weight_            ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer15_modules_norm2_parameters_bias_              L_self_modules_features_modules_denseblock3_modules_denselayer15_modules_norm2_parameters_bias_              ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer15_modules_conv2_parameters_weight_            L_self_modules_features_modules_denseblock3_modules_denselayer15_modules_conv2_parameters_weight_            ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer16_modules_norm1_buffers_num_batches_tracked_  L_self_modules_features_modules_denseblock3_modules_denselayer16_modules_norm1_buffers_num_batches_tracked_  ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer16_modules_norm1_buffers_running_mean_         L_self_modules_features_modules_denseblock3_modules_denselayer16_modules_norm1_buffers_running_mean_         ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer16_modules_norm1_buffers_running_var_          L_self_modules_features_modules_denseblock3_modules_denselayer16_modules_norm1_buffers_running_var_          ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer16_modules_norm1_parameters_weight_            L_self_modules_features_modules_denseblock3_modules_denselayer16_modules_norm1_parameters_weight_            ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer16_modules_norm1_parameters_bias_              L_self_modules_features_modules_denseblock3_modules_denselayer16_modules_norm1_parameters_bias_              ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer16_modules_conv1_parameters_weight_            L_self_modules_features_modules_denseblock3_modules_denselayer16_modules_conv1_parameters_weight_            ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer16_modules_norm2_buffers_num_batches_tracked_  L_self_modules_features_modules_denseblock3_modules_denselayer16_modules_norm2_buffers_num_batches_tracked_  ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer16_modules_norm2_buffers_running_mean_         L_self_modules_features_modules_denseblock3_modules_denselayer16_modules_norm2_buffers_running_mean_         ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer16_modules_norm2_buffers_running_var_          L_self_modules_features_modules_denseblock3_modules_denselayer16_modules_norm2_buffers_running_var_          ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer16_modules_norm2_parameters_weight_            L_self_modules_features_modules_denseblock3_modules_denselayer16_modules_norm2_parameters_weight_            ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer16_modules_norm2_parameters_bias_              L_self_modules_features_modules_denseblock3_modules_denselayer16_modules_norm2_parameters_bias_              ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer16_modules_conv2_parameters_weight_            L_self_modules_features_modules_denseblock3_modules_denselayer16_modules_conv2_parameters_weight_            ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer17_modules_norm1_buffers_num_batches_tracked_  L_self_modules_features_modules_denseblock3_modules_denselayer17_modules_norm1_buffers_num_batches_tracked_  ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer17_modules_norm1_buffers_running_mean_         L_self_modules_features_modules_denseblock3_modules_denselayer17_modules_norm1_buffers_running_mean_         ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer17_modules_norm1_buffers_running_var_          L_self_modules_features_modules_denseblock3_modules_denselayer17_modules_norm1_buffers_running_var_          ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer17_modules_norm1_parameters_weight_            L_self_modules_features_modules_denseblock3_modules_denselayer17_modules_norm1_parameters_weight_            ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer17_modules_norm1_parameters_bias_              L_self_modules_features_modules_denseblock3_modules_denselayer17_modules_norm1_parameters_bias_              ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer17_modules_conv1_parameters_weight_            L_self_modules_features_modules_denseblock3_modules_denselayer17_modules_conv1_parameters_weight_            ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer17_modules_norm2_buffers_num_batches_tracked_  L_self_modules_features_modules_denseblock3_modules_denselayer17_modules_norm2_buffers_num_batches_tracked_  ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer17_modules_norm2_buffers_running_mean_         L_self_modules_features_modules_denseblock3_modules_denselayer17_modules_norm2_buffers_running_mean_         ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer17_modules_norm2_buffers_running_var_          L_self_modules_features_modules_denseblock3_modules_denselayer17_modules_norm2_buffers_running_var_          ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer17_modules_norm2_parameters_weight_            L_self_modules_features_modules_denseblock3_modules_denselayer17_modules_norm2_parameters_weight_            ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer17_modules_norm2_parameters_bias_              L_self_modules_features_modules_denseblock3_modules_denselayer17_modules_norm2_parameters_bias_              ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer17_modules_conv2_parameters_weight_            L_self_modules_features_modules_denseblock3_modules_denselayer17_modules_conv2_parameters_weight_            ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer18_modules_norm1_buffers_num_batches_tracked_  L_self_modules_features_modules_denseblock3_modules_denselayer18_modules_norm1_buffers_num_batches_tracked_  ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer18_modules_norm1_buffers_running_mean_         L_self_modules_features_modules_denseblock3_modules_denselayer18_modules_norm1_buffers_running_mean_         ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer18_modules_norm1_buffers_running_var_          L_self_modules_features_modules_denseblock3_modules_denselayer18_modules_norm1_buffers_running_var_          ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer18_modules_norm1_parameters_weight_            L_self_modules_features_modules_denseblock3_modules_denselayer18_modules_norm1_parameters_weight_            ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer18_modules_norm1_parameters_bias_              L_self_modules_features_modules_denseblock3_modules_denselayer18_modules_norm1_parameters_bias_              ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer18_modules_conv1_parameters_weight_            L_self_modules_features_modules_denseblock3_modules_denselayer18_modules_conv1_parameters_weight_            ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer18_modules_norm2_buffers_num_batches_tracked_  L_self_modules_features_modules_denseblock3_modules_denselayer18_modules_norm2_buffers_num_batches_tracked_  ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer18_modules_norm2_buffers_running_mean_         L_self_modules_features_modules_denseblock3_modules_denselayer18_modules_norm2_buffers_running_mean_         ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer18_modules_norm2_buffers_running_var_          L_self_modules_features_modules_denseblock3_modules_denselayer18_modules_norm2_buffers_running_var_          ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer18_modules_norm2_parameters_weight_            L_self_modules_features_modules_denseblock3_modules_denselayer18_modules_norm2_parameters_weight_            ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer18_modules_norm2_parameters_bias_              L_self_modules_features_modules_denseblock3_modules_denselayer18_modules_norm2_parameters_bias_              ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer18_modules_conv2_parameters_weight_            L_self_modules_features_modules_denseblock3_modules_denselayer18_modules_conv2_parameters_weight_            ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer19_modules_norm1_buffers_num_batches_tracked_  L_self_modules_features_modules_denseblock3_modules_denselayer19_modules_norm1_buffers_num_batches_tracked_  ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer19_modules_norm1_buffers_running_mean_         L_self_modules_features_modules_denseblock3_modules_denselayer19_modules_norm1_buffers_running_mean_         ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer19_modules_norm1_buffers_running_var_          L_self_modules_features_modules_denseblock3_modules_denselayer19_modules_norm1_buffers_running_var_          ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer19_modules_norm1_parameters_weight_            L_self_modules_features_modules_denseblock3_modules_denselayer19_modules_norm1_parameters_weight_            ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer19_modules_norm1_parameters_bias_              L_self_modules_features_modules_denseblock3_modules_denselayer19_modules_norm1_parameters_bias_              ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer19_modules_conv1_parameters_weight_            L_self_modules_features_modules_denseblock3_modules_denselayer19_modules_conv1_parameters_weight_            ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer19_modules_norm2_buffers_num_batches_tracked_  L_self_modules_features_modules_denseblock3_modules_denselayer19_modules_norm2_buffers_num_batches_tracked_  ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer19_modules_norm2_buffers_running_mean_         L_self_modules_features_modules_denseblock3_modules_denselayer19_modules_norm2_buffers_running_mean_         ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer19_modules_norm2_buffers_running_var_          L_self_modules_features_modules_denseblock3_modules_denselayer19_modules_norm2_buffers_running_var_          ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer19_modules_norm2_parameters_weight_            L_self_modules_features_modules_denseblock3_modules_denselayer19_modules_norm2_parameters_weight_            ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer19_modules_norm2_parameters_bias_              L_self_modules_features_modules_denseblock3_modules_denselayer19_modules_norm2_parameters_bias_              ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer19_modules_conv2_parameters_weight_            L_self_modules_features_modules_denseblock3_modules_denselayer19_modules_conv2_parameters_weight_            ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer20_modules_norm1_buffers_num_batches_tracked_  L_self_modules_features_modules_denseblock3_modules_denselayer20_modules_norm1_buffers_num_batches_tracked_  ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer20_modules_norm1_buffers_running_mean_         L_self_modules_features_modules_denseblock3_modules_denselayer20_modules_norm1_buffers_running_mean_         ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer20_modules_norm1_buffers_running_var_          L_self_modules_features_modules_denseblock3_modules_denselayer20_modules_norm1_buffers_running_var_          ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer20_modules_norm1_parameters_weight_            L_self_modules_features_modules_denseblock3_modules_denselayer20_modules_norm1_parameters_weight_            ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer20_modules_norm1_parameters_bias_              L_self_modules_features_modules_denseblock3_modules_denselayer20_modules_norm1_parameters_bias_              ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer20_modules_conv1_parameters_weight_            L_self_modules_features_modules_denseblock3_modules_denselayer20_modules_conv1_parameters_weight_            ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer20_modules_norm2_buffers_num_batches_tracked_  L_self_modules_features_modules_denseblock3_modules_denselayer20_modules_norm2_buffers_num_batches_tracked_  ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer20_modules_norm2_buffers_running_mean_         L_self_modules_features_modules_denseblock3_modules_denselayer20_modules_norm2_buffers_running_mean_         ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer20_modules_norm2_buffers_running_var_          L_self_modules_features_modules_denseblock3_modules_denselayer20_modules_norm2_buffers_running_var_          ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer20_modules_norm2_parameters_weight_            L_self_modules_features_modules_denseblock3_modules_denselayer20_modules_norm2_parameters_weight_            ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer20_modules_norm2_parameters_bias_              L_self_modules_features_modules_denseblock3_modules_denselayer20_modules_norm2_parameters_bias_              ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer20_modules_conv2_parameters_weight_            L_self_modules_features_modules_denseblock3_modules_denselayer20_modules_conv2_parameters_weight_            ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer21_modules_norm1_buffers_num_batches_tracked_  L_self_modules_features_modules_denseblock3_modules_denselayer21_modules_norm1_buffers_num_batches_tracked_  ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer21_modules_norm1_buffers_running_mean_         L_self_modules_features_modules_denseblock3_modules_denselayer21_modules_norm1_buffers_running_mean_         ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer21_modules_norm1_buffers_running_var_          L_self_modules_features_modules_denseblock3_modules_denselayer21_modules_norm1_buffers_running_var_          ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer21_modules_norm1_parameters_weight_            L_self_modules_features_modules_denseblock3_modules_denselayer21_modules_norm1_parameters_weight_            ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer21_modules_norm1_parameters_bias_              L_self_modules_features_modules_denseblock3_modules_denselayer21_modules_norm1_parameters_bias_              ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer21_modules_conv1_parameters_weight_            L_self_modules_features_modules_denseblock3_modules_denselayer21_modules_conv1_parameters_weight_            ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer21_modules_norm2_buffers_num_batches_tracked_  L_self_modules_features_modules_denseblock3_modules_denselayer21_modules_norm2_buffers_num_batches_tracked_  ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer21_modules_norm2_buffers_running_mean_         L_self_modules_features_modules_denseblock3_modules_denselayer21_modules_norm2_buffers_running_mean_         ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer21_modules_norm2_buffers_running_var_          L_self_modules_features_modules_denseblock3_modules_denselayer21_modules_norm2_buffers_running_var_          ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer21_modules_norm2_parameters_weight_            L_self_modules_features_modules_denseblock3_modules_denselayer21_modules_norm2_parameters_weight_            ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer21_modules_norm2_parameters_bias_              L_self_modules_features_modules_denseblock3_modules_denselayer21_modules_norm2_parameters_bias_              ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer21_modules_conv2_parameters_weight_            L_self_modules_features_modules_denseblock3_modules_denselayer21_modules_conv2_parameters_weight_            ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer22_modules_norm1_buffers_num_batches_tracked_  L_self_modules_features_modules_denseblock3_modules_denselayer22_modules_norm1_buffers_num_batches_tracked_  ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer22_modules_norm1_buffers_running_mean_         L_self_modules_features_modules_denseblock3_modules_denselayer22_modules_norm1_buffers_running_mean_         ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer22_modules_norm1_buffers_running_var_          L_self_modules_features_modules_denseblock3_modules_denselayer22_modules_norm1_buffers_running_var_          ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer22_modules_norm1_parameters_weight_            L_self_modules_features_modules_denseblock3_modules_denselayer22_modules_norm1_parameters_weight_            ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer22_modules_norm1_parameters_bias_              L_self_modules_features_modules_denseblock3_modules_denselayer22_modules_norm1_parameters_bias_              ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer22_modules_conv1_parameters_weight_            L_self_modules_features_modules_denseblock3_modules_denselayer22_modules_conv1_parameters_weight_            ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer22_modules_norm2_buffers_num_batches_tracked_  L_self_modules_features_modules_denseblock3_modules_denselayer22_modules_norm2_buffers_num_batches_tracked_  ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer22_modules_norm2_buffers_running_mean_         L_self_modules_features_modules_denseblock3_modules_denselayer22_modules_norm2_buffers_running_mean_         ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer22_modules_norm2_buffers_running_var_          L_self_modules_features_modules_denseblock3_modules_denselayer22_modules_norm2_buffers_running_var_          ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer22_modules_norm2_parameters_weight_            L_self_modules_features_modules_denseblock3_modules_denselayer22_modules_norm2_parameters_weight_            ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer22_modules_norm2_parameters_bias_              L_self_modules_features_modules_denseblock3_modules_denselayer22_modules_norm2_parameters_bias_              ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer22_modules_conv2_parameters_weight_            L_self_modules_features_modules_denseblock3_modules_denselayer22_modules_conv2_parameters_weight_            ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer23_modules_norm1_buffers_num_batches_tracked_  L_self_modules_features_modules_denseblock3_modules_denselayer23_modules_norm1_buffers_num_batches_tracked_  ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer23_modules_norm1_buffers_running_mean_         L_self_modules_features_modules_denseblock3_modules_denselayer23_modules_norm1_buffers_running_mean_         ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer23_modules_norm1_buffers_running_var_          L_self_modules_features_modules_denseblock3_modules_denselayer23_modules_norm1_buffers_running_var_          ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer23_modules_norm1_parameters_weight_            L_self_modules_features_modules_denseblock3_modules_denselayer23_modules_norm1_parameters_weight_            ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer23_modules_norm1_parameters_bias_              L_self_modules_features_modules_denseblock3_modules_denselayer23_modules_norm1_parameters_bias_              ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer23_modules_conv1_parameters_weight_            L_self_modules_features_modules_denseblock3_modules_denselayer23_modules_conv1_parameters_weight_            ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer23_modules_norm2_buffers_num_batches_tracked_  L_self_modules_features_modules_denseblock3_modules_denselayer23_modules_norm2_buffers_num_batches_tracked_  ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer23_modules_norm2_buffers_running_mean_         L_self_modules_features_modules_denseblock3_modules_denselayer23_modules_norm2_buffers_running_mean_         ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer23_modules_norm2_buffers_running_var_          L_self_modules_features_modules_denseblock3_modules_denselayer23_modules_norm2_buffers_running_var_          ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer23_modules_norm2_parameters_weight_            L_self_modules_features_modules_denseblock3_modules_denselayer23_modules_norm2_parameters_weight_            ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer23_modules_norm2_parameters_bias_              L_self_modules_features_modules_denseblock3_modules_denselayer23_modules_norm2_parameters_bias_              ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer23_modules_conv2_parameters_weight_            L_self_modules_features_modules_denseblock3_modules_denselayer23_modules_conv2_parameters_weight_            ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer24_modules_norm1_buffers_num_batches_tracked_  L_self_modules_features_modules_denseblock3_modules_denselayer24_modules_norm1_buffers_num_batches_tracked_  ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer24_modules_norm1_buffers_running_mean_         L_self_modules_features_modules_denseblock3_modules_denselayer24_modules_norm1_buffers_running_mean_         ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer24_modules_norm1_buffers_running_var_          L_self_modules_features_modules_denseblock3_modules_denselayer24_modules_norm1_buffers_running_var_          ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer24_modules_norm1_parameters_weight_            L_self_modules_features_modules_denseblock3_modules_denselayer24_modules_norm1_parameters_weight_            ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer24_modules_norm1_parameters_bias_              L_self_modules_features_modules_denseblock3_modules_denselayer24_modules_norm1_parameters_bias_              ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer24_modules_conv1_parameters_weight_            L_self_modules_features_modules_denseblock3_modules_denselayer24_modules_conv1_parameters_weight_            ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer24_modules_norm2_buffers_num_batches_tracked_  L_self_modules_features_modules_denseblock3_modules_denselayer24_modules_norm2_buffers_num_batches_tracked_  ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer24_modules_norm2_buffers_running_mean_         L_self_modules_features_modules_denseblock3_modules_denselayer24_modules_norm2_buffers_running_mean_         ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer24_modules_norm2_buffers_running_var_          L_self_modules_features_modules_denseblock3_modules_denselayer24_modules_norm2_buffers_running_var_          ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer24_modules_norm2_parameters_weight_            L_self_modules_features_modules_denseblock3_modules_denselayer24_modules_norm2_parameters_weight_            ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer24_modules_norm2_parameters_bias_              L_self_modules_features_modules_denseblock3_modules_denselayer24_modules_norm2_parameters_bias_              ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock3_modules_denselayer24_modules_conv2_parameters_weight_            L_self_modules_features_modules_denseblock3_modules_denselayer24_modules_conv2_parameters_weight_            ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_transition3_modules_norm_buffers_num_batches_tracked_                        L_self_modules_features_modules_transition3_modules_norm_buffers_num_batches_tracked_                        ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_transition3_modules_norm_buffers_running_mean_                               L_self_modules_features_modules_transition3_modules_norm_buffers_running_mean_                               ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_transition3_modules_norm_buffers_running_var_                                L_self_modules_features_modules_transition3_modules_norm_buffers_running_var_                                ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_transition3_modules_norm_parameters_weight_                                  L_self_modules_features_modules_transition3_modules_norm_parameters_weight_                                  ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_transition3_modules_norm_parameters_bias_                                    L_self_modules_features_modules_transition3_modules_norm_parameters_bias_                                    ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_transition3_modules_conv_parameters_weight_                                  L_self_modules_features_modules_transition3_modules_conv_parameters_weight_                                  ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer1_modules_norm1_buffers_num_batches_tracked_   L_self_modules_features_modules_denseblock4_modules_denselayer1_modules_norm1_buffers_num_batches_tracked_   ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer1_modules_norm1_buffers_running_mean_          L_self_modules_features_modules_denseblock4_modules_denselayer1_modules_norm1_buffers_running_mean_          ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer1_modules_norm1_buffers_running_var_           L_self_modules_features_modules_denseblock4_modules_denselayer1_modules_norm1_buffers_running_var_           ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer1_modules_norm1_parameters_weight_             L_self_modules_features_modules_denseblock4_modules_denselayer1_modules_norm1_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer1_modules_norm1_parameters_bias_               L_self_modules_features_modules_denseblock4_modules_denselayer1_modules_norm1_parameters_bias_               ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer1_modules_conv1_parameters_weight_             L_self_modules_features_modules_denseblock4_modules_denselayer1_modules_conv1_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer1_modules_norm2_buffers_num_batches_tracked_   L_self_modules_features_modules_denseblock4_modules_denselayer1_modules_norm2_buffers_num_batches_tracked_   ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer1_modules_norm2_buffers_running_mean_          L_self_modules_features_modules_denseblock4_modules_denselayer1_modules_norm2_buffers_running_mean_          ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer1_modules_norm2_buffers_running_var_           L_self_modules_features_modules_denseblock4_modules_denselayer1_modules_norm2_buffers_running_var_           ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer1_modules_norm2_parameters_weight_             L_self_modules_features_modules_denseblock4_modules_denselayer1_modules_norm2_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer1_modules_norm2_parameters_bias_               L_self_modules_features_modules_denseblock4_modules_denselayer1_modules_norm2_parameters_bias_               ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer1_modules_conv2_parameters_weight_             L_self_modules_features_modules_denseblock4_modules_denselayer1_modules_conv2_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer2_modules_norm1_buffers_num_batches_tracked_   L_self_modules_features_modules_denseblock4_modules_denselayer2_modules_norm1_buffers_num_batches_tracked_   ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer2_modules_norm1_buffers_running_mean_          L_self_modules_features_modules_denseblock4_modules_denselayer2_modules_norm1_buffers_running_mean_          ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer2_modules_norm1_buffers_running_var_           L_self_modules_features_modules_denseblock4_modules_denselayer2_modules_norm1_buffers_running_var_           ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer2_modules_norm1_parameters_weight_             L_self_modules_features_modules_denseblock4_modules_denselayer2_modules_norm1_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer2_modules_norm1_parameters_bias_               L_self_modules_features_modules_denseblock4_modules_denselayer2_modules_norm1_parameters_bias_               ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer2_modules_conv1_parameters_weight_             L_self_modules_features_modules_denseblock4_modules_denselayer2_modules_conv1_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer2_modules_norm2_buffers_num_batches_tracked_   L_self_modules_features_modules_denseblock4_modules_denselayer2_modules_norm2_buffers_num_batches_tracked_   ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer2_modules_norm2_buffers_running_mean_          L_self_modules_features_modules_denseblock4_modules_denselayer2_modules_norm2_buffers_running_mean_          ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer2_modules_norm2_buffers_running_var_           L_self_modules_features_modules_denseblock4_modules_denselayer2_modules_norm2_buffers_running_var_           ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer2_modules_norm2_parameters_weight_             L_self_modules_features_modules_denseblock4_modules_denselayer2_modules_norm2_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer2_modules_norm2_parameters_bias_               L_self_modules_features_modules_denseblock4_modules_denselayer2_modules_norm2_parameters_bias_               ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer2_modules_conv2_parameters_weight_             L_self_modules_features_modules_denseblock4_modules_denselayer2_modules_conv2_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer3_modules_norm1_buffers_num_batches_tracked_   L_self_modules_features_modules_denseblock4_modules_denselayer3_modules_norm1_buffers_num_batches_tracked_   ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer3_modules_norm1_buffers_running_mean_          L_self_modules_features_modules_denseblock4_modules_denselayer3_modules_norm1_buffers_running_mean_          ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer3_modules_norm1_buffers_running_var_           L_self_modules_features_modules_denseblock4_modules_denselayer3_modules_norm1_buffers_running_var_           ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer3_modules_norm1_parameters_weight_             L_self_modules_features_modules_denseblock4_modules_denselayer3_modules_norm1_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer3_modules_norm1_parameters_bias_               L_self_modules_features_modules_denseblock4_modules_denselayer3_modules_norm1_parameters_bias_               ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer3_modules_conv1_parameters_weight_             L_self_modules_features_modules_denseblock4_modules_denselayer3_modules_conv1_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer3_modules_norm2_buffers_num_batches_tracked_   L_self_modules_features_modules_denseblock4_modules_denselayer3_modules_norm2_buffers_num_batches_tracked_   ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer3_modules_norm2_buffers_running_mean_          L_self_modules_features_modules_denseblock4_modules_denselayer3_modules_norm2_buffers_running_mean_          ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer3_modules_norm2_buffers_running_var_           L_self_modules_features_modules_denseblock4_modules_denselayer3_modules_norm2_buffers_running_var_           ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer3_modules_norm2_parameters_weight_             L_self_modules_features_modules_denseblock4_modules_denselayer3_modules_norm2_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer3_modules_norm2_parameters_bias_               L_self_modules_features_modules_denseblock4_modules_denselayer3_modules_norm2_parameters_bias_               ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer3_modules_conv2_parameters_weight_             L_self_modules_features_modules_denseblock4_modules_denselayer3_modules_conv2_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer4_modules_norm1_buffers_num_batches_tracked_   L_self_modules_features_modules_denseblock4_modules_denselayer4_modules_norm1_buffers_num_batches_tracked_   ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer4_modules_norm1_buffers_running_mean_          L_self_modules_features_modules_denseblock4_modules_denselayer4_modules_norm1_buffers_running_mean_          ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer4_modules_norm1_buffers_running_var_           L_self_modules_features_modules_denseblock4_modules_denselayer4_modules_norm1_buffers_running_var_           ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer4_modules_norm1_parameters_weight_             L_self_modules_features_modules_denseblock4_modules_denselayer4_modules_norm1_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer4_modules_norm1_parameters_bias_               L_self_modules_features_modules_denseblock4_modules_denselayer4_modules_norm1_parameters_bias_               ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer4_modules_conv1_parameters_weight_             L_self_modules_features_modules_denseblock4_modules_denselayer4_modules_conv1_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer4_modules_norm2_buffers_num_batches_tracked_   L_self_modules_features_modules_denseblock4_modules_denselayer4_modules_norm2_buffers_num_batches_tracked_   ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer4_modules_norm2_buffers_running_mean_          L_self_modules_features_modules_denseblock4_modules_denselayer4_modules_norm2_buffers_running_mean_          ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer4_modules_norm2_buffers_running_var_           L_self_modules_features_modules_denseblock4_modules_denselayer4_modules_norm2_buffers_running_var_           ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer4_modules_norm2_parameters_weight_             L_self_modules_features_modules_denseblock4_modules_denselayer4_modules_norm2_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer4_modules_norm2_parameters_bias_               L_self_modules_features_modules_denseblock4_modules_denselayer4_modules_norm2_parameters_bias_               ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer4_modules_conv2_parameters_weight_             L_self_modules_features_modules_denseblock4_modules_denselayer4_modules_conv2_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer5_modules_norm1_buffers_num_batches_tracked_   L_self_modules_features_modules_denseblock4_modules_denselayer5_modules_norm1_buffers_num_batches_tracked_   ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer5_modules_norm1_buffers_running_mean_          L_self_modules_features_modules_denseblock4_modules_denselayer5_modules_norm1_buffers_running_mean_          ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer5_modules_norm1_buffers_running_var_           L_self_modules_features_modules_denseblock4_modules_denselayer5_modules_norm1_buffers_running_var_           ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer5_modules_norm1_parameters_weight_             L_self_modules_features_modules_denseblock4_modules_denselayer5_modules_norm1_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer5_modules_norm1_parameters_bias_               L_self_modules_features_modules_denseblock4_modules_denselayer5_modules_norm1_parameters_bias_               ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer5_modules_conv1_parameters_weight_             L_self_modules_features_modules_denseblock4_modules_denselayer5_modules_conv1_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer5_modules_norm2_buffers_num_batches_tracked_   L_self_modules_features_modules_denseblock4_modules_denselayer5_modules_norm2_buffers_num_batches_tracked_   ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer5_modules_norm2_buffers_running_mean_          L_self_modules_features_modules_denseblock4_modules_denselayer5_modules_norm2_buffers_running_mean_          ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer5_modules_norm2_buffers_running_var_           L_self_modules_features_modules_denseblock4_modules_denselayer5_modules_norm2_buffers_running_var_           ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer5_modules_norm2_parameters_weight_             L_self_modules_features_modules_denseblock4_modules_denselayer5_modules_norm2_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer5_modules_norm2_parameters_bias_               L_self_modules_features_modules_denseblock4_modules_denselayer5_modules_norm2_parameters_bias_               ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer5_modules_conv2_parameters_weight_             L_self_modules_features_modules_denseblock4_modules_denselayer5_modules_conv2_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer6_modules_norm1_buffers_num_batches_tracked_   L_self_modules_features_modules_denseblock4_modules_denselayer6_modules_norm1_buffers_num_batches_tracked_   ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer6_modules_norm1_buffers_running_mean_          L_self_modules_features_modules_denseblock4_modules_denselayer6_modules_norm1_buffers_running_mean_          ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer6_modules_norm1_buffers_running_var_           L_self_modules_features_modules_denseblock4_modules_denselayer6_modules_norm1_buffers_running_var_           ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer6_modules_norm1_parameters_weight_             L_self_modules_features_modules_denseblock4_modules_denselayer6_modules_norm1_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer6_modules_norm1_parameters_bias_               L_self_modules_features_modules_denseblock4_modules_denselayer6_modules_norm1_parameters_bias_               ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer6_modules_conv1_parameters_weight_             L_self_modules_features_modules_denseblock4_modules_denselayer6_modules_conv1_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer6_modules_norm2_buffers_num_batches_tracked_   L_self_modules_features_modules_denseblock4_modules_denselayer6_modules_norm2_buffers_num_batches_tracked_   ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer6_modules_norm2_buffers_running_mean_          L_self_modules_features_modules_denseblock4_modules_denselayer6_modules_norm2_buffers_running_mean_          ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer6_modules_norm2_buffers_running_var_           L_self_modules_features_modules_denseblock4_modules_denselayer6_modules_norm2_buffers_running_var_           ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer6_modules_norm2_parameters_weight_             L_self_modules_features_modules_denseblock4_modules_denselayer6_modules_norm2_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer6_modules_norm2_parameters_bias_               L_self_modules_features_modules_denseblock4_modules_denselayer6_modules_norm2_parameters_bias_               ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer6_modules_conv2_parameters_weight_             L_self_modules_features_modules_denseblock4_modules_denselayer6_modules_conv2_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer7_modules_norm1_buffers_num_batches_tracked_   L_self_modules_features_modules_denseblock4_modules_denselayer7_modules_norm1_buffers_num_batches_tracked_   ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer7_modules_norm1_buffers_running_mean_          L_self_modules_features_modules_denseblock4_modules_denselayer7_modules_norm1_buffers_running_mean_          ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer7_modules_norm1_buffers_running_var_           L_self_modules_features_modules_denseblock4_modules_denselayer7_modules_norm1_buffers_running_var_           ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer7_modules_norm1_parameters_weight_             L_self_modules_features_modules_denseblock4_modules_denselayer7_modules_norm1_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer7_modules_norm1_parameters_bias_               L_self_modules_features_modules_denseblock4_modules_denselayer7_modules_norm1_parameters_bias_               ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer7_modules_conv1_parameters_weight_             L_self_modules_features_modules_denseblock4_modules_denselayer7_modules_conv1_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer7_modules_norm2_buffers_num_batches_tracked_   L_self_modules_features_modules_denseblock4_modules_denselayer7_modules_norm2_buffers_num_batches_tracked_   ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer7_modules_norm2_buffers_running_mean_          L_self_modules_features_modules_denseblock4_modules_denselayer7_modules_norm2_buffers_running_mean_          ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer7_modules_norm2_buffers_running_var_           L_self_modules_features_modules_denseblock4_modules_denselayer7_modules_norm2_buffers_running_var_           ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer7_modules_norm2_parameters_weight_             L_self_modules_features_modules_denseblock4_modules_denselayer7_modules_norm2_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer7_modules_norm2_parameters_bias_               L_self_modules_features_modules_denseblock4_modules_denselayer7_modules_norm2_parameters_bias_               ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer7_modules_conv2_parameters_weight_             L_self_modules_features_modules_denseblock4_modules_denselayer7_modules_conv2_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer8_modules_norm1_buffers_num_batches_tracked_   L_self_modules_features_modules_denseblock4_modules_denselayer8_modules_norm1_buffers_num_batches_tracked_   ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer8_modules_norm1_buffers_running_mean_          L_self_modules_features_modules_denseblock4_modules_denselayer8_modules_norm1_buffers_running_mean_          ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer8_modules_norm1_buffers_running_var_           L_self_modules_features_modules_denseblock4_modules_denselayer8_modules_norm1_buffers_running_var_           ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer8_modules_norm1_parameters_weight_             L_self_modules_features_modules_denseblock4_modules_denselayer8_modules_norm1_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer8_modules_norm1_parameters_bias_               L_self_modules_features_modules_denseblock4_modules_denselayer8_modules_norm1_parameters_bias_               ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer8_modules_conv1_parameters_weight_             L_self_modules_features_modules_denseblock4_modules_denselayer8_modules_conv1_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer8_modules_norm2_buffers_num_batches_tracked_   L_self_modules_features_modules_denseblock4_modules_denselayer8_modules_norm2_buffers_num_batches_tracked_   ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer8_modules_norm2_buffers_running_mean_          L_self_modules_features_modules_denseblock4_modules_denselayer8_modules_norm2_buffers_running_mean_          ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer8_modules_norm2_buffers_running_var_           L_self_modules_features_modules_denseblock4_modules_denselayer8_modules_norm2_buffers_running_var_           ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer8_modules_norm2_parameters_weight_             L_self_modules_features_modules_denseblock4_modules_denselayer8_modules_norm2_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer8_modules_norm2_parameters_bias_               L_self_modules_features_modules_denseblock4_modules_denselayer8_modules_norm2_parameters_bias_               ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer8_modules_conv2_parameters_weight_             L_self_modules_features_modules_denseblock4_modules_denselayer8_modules_conv2_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer9_modules_norm1_buffers_num_batches_tracked_   L_self_modules_features_modules_denseblock4_modules_denselayer9_modules_norm1_buffers_num_batches_tracked_   ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer9_modules_norm1_buffers_running_mean_          L_self_modules_features_modules_denseblock4_modules_denselayer9_modules_norm1_buffers_running_mean_          ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer9_modules_norm1_buffers_running_var_           L_self_modules_features_modules_denseblock4_modules_denselayer9_modules_norm1_buffers_running_var_           ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer9_modules_norm1_parameters_weight_             L_self_modules_features_modules_denseblock4_modules_denselayer9_modules_norm1_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer9_modules_norm1_parameters_bias_               L_self_modules_features_modules_denseblock4_modules_denselayer9_modules_norm1_parameters_bias_               ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer9_modules_conv1_parameters_weight_             L_self_modules_features_modules_denseblock4_modules_denselayer9_modules_conv1_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer9_modules_norm2_buffers_num_batches_tracked_   L_self_modules_features_modules_denseblock4_modules_denselayer9_modules_norm2_buffers_num_batches_tracked_   ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer9_modules_norm2_buffers_running_mean_          L_self_modules_features_modules_denseblock4_modules_denselayer9_modules_norm2_buffers_running_mean_          ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer9_modules_norm2_buffers_running_var_           L_self_modules_features_modules_denseblock4_modules_denselayer9_modules_norm2_buffers_running_var_           ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer9_modules_norm2_parameters_weight_             L_self_modules_features_modules_denseblock4_modules_denselayer9_modules_norm2_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer9_modules_norm2_parameters_bias_               L_self_modules_features_modules_denseblock4_modules_denselayer9_modules_norm2_parameters_bias_               ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer9_modules_conv2_parameters_weight_             L_self_modules_features_modules_denseblock4_modules_denselayer9_modules_conv2_parameters_weight_             ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer10_modules_norm1_buffers_num_batches_tracked_  L_self_modules_features_modules_denseblock4_modules_denselayer10_modules_norm1_buffers_num_batches_tracked_  ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer10_modules_norm1_buffers_running_mean_         L_self_modules_features_modules_denseblock4_modules_denselayer10_modules_norm1_buffers_running_mean_         ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer10_modules_norm1_buffers_running_var_          L_self_modules_features_modules_denseblock4_modules_denselayer10_modules_norm1_buffers_running_var_          ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer10_modules_norm1_parameters_weight_            L_self_modules_features_modules_denseblock4_modules_denselayer10_modules_norm1_parameters_weight_            ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer10_modules_norm1_parameters_bias_              L_self_modules_features_modules_denseblock4_modules_denselayer10_modules_norm1_parameters_bias_              ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer10_modules_conv1_parameters_weight_            L_self_modules_features_modules_denseblock4_modules_denselayer10_modules_conv1_parameters_weight_            ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer10_modules_norm2_buffers_num_batches_tracked_  L_self_modules_features_modules_denseblock4_modules_denselayer10_modules_norm2_buffers_num_batches_tracked_  ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer10_modules_norm2_buffers_running_mean_         L_self_modules_features_modules_denseblock4_modules_denselayer10_modules_norm2_buffers_running_mean_         ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer10_modules_norm2_buffers_running_var_          L_self_modules_features_modules_denseblock4_modules_denselayer10_modules_norm2_buffers_running_var_          ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer10_modules_norm2_parameters_weight_            L_self_modules_features_modules_denseblock4_modules_denselayer10_modules_norm2_parameters_weight_            ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer10_modules_norm2_parameters_bias_              L_self_modules_features_modules_denseblock4_modules_denselayer10_modules_norm2_parameters_bias_              ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer10_modules_conv2_parameters_weight_            L_self_modules_features_modules_denseblock4_modules_denselayer10_modules_conv2_parameters_weight_            ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer11_modules_norm1_buffers_num_batches_tracked_  L_self_modules_features_modules_denseblock4_modules_denselayer11_modules_norm1_buffers_num_batches_tracked_  ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer11_modules_norm1_buffers_running_mean_         L_self_modules_features_modules_denseblock4_modules_denselayer11_modules_norm1_buffers_running_mean_         ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer11_modules_norm1_buffers_running_var_          L_self_modules_features_modules_denseblock4_modules_denselayer11_modules_norm1_buffers_running_var_          ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer11_modules_norm1_parameters_weight_            L_self_modules_features_modules_denseblock4_modules_denselayer11_modules_norm1_parameters_weight_            ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer11_modules_norm1_parameters_bias_              L_self_modules_features_modules_denseblock4_modules_denselayer11_modules_norm1_parameters_bias_              ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer11_modules_conv1_parameters_weight_            L_self_modules_features_modules_denseblock4_modules_denselayer11_modules_conv1_parameters_weight_            ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer11_modules_norm2_buffers_num_batches_tracked_  L_self_modules_features_modules_denseblock4_modules_denselayer11_modules_norm2_buffers_num_batches_tracked_  ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer11_modules_norm2_buffers_running_mean_         L_self_modules_features_modules_denseblock4_modules_denselayer11_modules_norm2_buffers_running_mean_         ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer11_modules_norm2_buffers_running_var_          L_self_modules_features_modules_denseblock4_modules_denselayer11_modules_norm2_buffers_running_var_          ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer11_modules_norm2_parameters_weight_            L_self_modules_features_modules_denseblock4_modules_denselayer11_modules_norm2_parameters_weight_            ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer11_modules_norm2_parameters_bias_              L_self_modules_features_modules_denseblock4_modules_denselayer11_modules_norm2_parameters_bias_              ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer11_modules_conv2_parameters_weight_            L_self_modules_features_modules_denseblock4_modules_denselayer11_modules_conv2_parameters_weight_            ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer12_modules_norm1_buffers_num_batches_tracked_  L_self_modules_features_modules_denseblock4_modules_denselayer12_modules_norm1_buffers_num_batches_tracked_  ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer12_modules_norm1_buffers_running_mean_         L_self_modules_features_modules_denseblock4_modules_denselayer12_modules_norm1_buffers_running_mean_         ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer12_modules_norm1_buffers_running_var_          L_self_modules_features_modules_denseblock4_modules_denselayer12_modules_norm1_buffers_running_var_          ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer12_modules_norm1_parameters_weight_            L_self_modules_features_modules_denseblock4_modules_denselayer12_modules_norm1_parameters_weight_            ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer12_modules_norm1_parameters_bias_              L_self_modules_features_modules_denseblock4_modules_denselayer12_modules_norm1_parameters_bias_              ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer12_modules_conv1_parameters_weight_            L_self_modules_features_modules_denseblock4_modules_denselayer12_modules_conv1_parameters_weight_            ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer12_modules_norm2_buffers_num_batches_tracked_  L_self_modules_features_modules_denseblock4_modules_denselayer12_modules_norm2_buffers_num_batches_tracked_  ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer12_modules_norm2_buffers_running_mean_         L_self_modules_features_modules_denseblock4_modules_denselayer12_modules_norm2_buffers_running_mean_         ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer12_modules_norm2_buffers_running_var_          L_self_modules_features_modules_denseblock4_modules_denselayer12_modules_norm2_buffers_running_var_          ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer12_modules_norm2_parameters_weight_            L_self_modules_features_modules_denseblock4_modules_denselayer12_modules_norm2_parameters_weight_            ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer12_modules_norm2_parameters_bias_              L_self_modules_features_modules_denseblock4_modules_denselayer12_modules_norm2_parameters_bias_              ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer12_modules_conv2_parameters_weight_            L_self_modules_features_modules_denseblock4_modules_denselayer12_modules_conv2_parameters_weight_            ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer13_modules_norm1_buffers_num_batches_tracked_  L_self_modules_features_modules_denseblock4_modules_denselayer13_modules_norm1_buffers_num_batches_tracked_  ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer13_modules_norm1_buffers_running_mean_         L_self_modules_features_modules_denseblock4_modules_denselayer13_modules_norm1_buffers_running_mean_         ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer13_modules_norm1_buffers_running_var_          L_self_modules_features_modules_denseblock4_modules_denselayer13_modules_norm1_buffers_running_var_          ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer13_modules_norm1_parameters_weight_            L_self_modules_features_modules_denseblock4_modules_denselayer13_modules_norm1_parameters_weight_            ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer13_modules_norm1_parameters_bias_              L_self_modules_features_modules_denseblock4_modules_denselayer13_modules_norm1_parameters_bias_              ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer13_modules_conv1_parameters_weight_            L_self_modules_features_modules_denseblock4_modules_denselayer13_modules_conv1_parameters_weight_            ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer13_modules_norm2_buffers_num_batches_tracked_  L_self_modules_features_modules_denseblock4_modules_denselayer13_modules_norm2_buffers_num_batches_tracked_  ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer13_modules_norm2_buffers_running_mean_         L_self_modules_features_modules_denseblock4_modules_denselayer13_modules_norm2_buffers_running_mean_         ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer13_modules_norm2_buffers_running_var_          L_self_modules_features_modules_denseblock4_modules_denselayer13_modules_norm2_buffers_running_var_          ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer13_modules_norm2_parameters_weight_            L_self_modules_features_modules_denseblock4_modules_denselayer13_modules_norm2_parameters_weight_            ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer13_modules_norm2_parameters_bias_              L_self_modules_features_modules_denseblock4_modules_denselayer13_modules_norm2_parameters_bias_              ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer13_modules_conv2_parameters_weight_            L_self_modules_features_modules_denseblock4_modules_denselayer13_modules_conv2_parameters_weight_            ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer14_modules_norm1_buffers_num_batches_tracked_  L_self_modules_features_modules_denseblock4_modules_denselayer14_modules_norm1_buffers_num_batches_tracked_  ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer14_modules_norm1_buffers_running_mean_         L_self_modules_features_modules_denseblock4_modules_denselayer14_modules_norm1_buffers_running_mean_         ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer14_modules_norm1_buffers_running_var_          L_self_modules_features_modules_denseblock4_modules_denselayer14_modules_norm1_buffers_running_var_          ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer14_modules_norm1_parameters_weight_            L_self_modules_features_modules_denseblock4_modules_denselayer14_modules_norm1_parameters_weight_            ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer14_modules_norm1_parameters_bias_              L_self_modules_features_modules_denseblock4_modules_denselayer14_modules_norm1_parameters_bias_              ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer14_modules_conv1_parameters_weight_            L_self_modules_features_modules_denseblock4_modules_denselayer14_modules_conv1_parameters_weight_            ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer14_modules_norm2_buffers_num_batches_tracked_  L_self_modules_features_modules_denseblock4_modules_denselayer14_modules_norm2_buffers_num_batches_tracked_  ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer14_modules_norm2_buffers_running_mean_         L_self_modules_features_modules_denseblock4_modules_denselayer14_modules_norm2_buffers_running_mean_         ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer14_modules_norm2_buffers_running_var_          L_self_modules_features_modules_denseblock4_modules_denselayer14_modules_norm2_buffers_running_var_          ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer14_modules_norm2_parameters_weight_            L_self_modules_features_modules_denseblock4_modules_denselayer14_modules_norm2_parameters_weight_            ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer14_modules_norm2_parameters_bias_              L_self_modules_features_modules_denseblock4_modules_denselayer14_modules_norm2_parameters_bias_              ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer14_modules_conv2_parameters_weight_            L_self_modules_features_modules_denseblock4_modules_denselayer14_modules_conv2_parameters_weight_            ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer15_modules_norm1_buffers_num_batches_tracked_  L_self_modules_features_modules_denseblock4_modules_denselayer15_modules_norm1_buffers_num_batches_tracked_  ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer15_modules_norm1_buffers_running_mean_         L_self_modules_features_modules_denseblock4_modules_denselayer15_modules_norm1_buffers_running_mean_         ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer15_modules_norm1_buffers_running_var_          L_self_modules_features_modules_denseblock4_modules_denselayer15_modules_norm1_buffers_running_var_          ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer15_modules_norm1_parameters_weight_            L_self_modules_features_modules_denseblock4_modules_denselayer15_modules_norm1_parameters_weight_            ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer15_modules_norm1_parameters_bias_              L_self_modules_features_modules_denseblock4_modules_denselayer15_modules_norm1_parameters_bias_              ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer15_modules_conv1_parameters_weight_            L_self_modules_features_modules_denseblock4_modules_denselayer15_modules_conv1_parameters_weight_            ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer15_modules_norm2_buffers_num_batches_tracked_  L_self_modules_features_modules_denseblock4_modules_denselayer15_modules_norm2_buffers_num_batches_tracked_  ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer15_modules_norm2_buffers_running_mean_         L_self_modules_features_modules_denseblock4_modules_denselayer15_modules_norm2_buffers_running_mean_         ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer15_modules_norm2_buffers_running_var_          L_self_modules_features_modules_denseblock4_modules_denselayer15_modules_norm2_buffers_running_var_          ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer15_modules_norm2_parameters_weight_            L_self_modules_features_modules_denseblock4_modules_denselayer15_modules_norm2_parameters_weight_            ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer15_modules_norm2_parameters_bias_              L_self_modules_features_modules_denseblock4_modules_denselayer15_modules_norm2_parameters_bias_              ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer15_modules_conv2_parameters_weight_            L_self_modules_features_modules_denseblock4_modules_denselayer15_modules_conv2_parameters_weight_            ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer16_modules_norm1_buffers_num_batches_tracked_  L_self_modules_features_modules_denseblock4_modules_denselayer16_modules_norm1_buffers_num_batches_tracked_  ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer16_modules_norm1_buffers_running_mean_         L_self_modules_features_modules_denseblock4_modules_denselayer16_modules_norm1_buffers_running_mean_         ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer16_modules_norm1_buffers_running_var_          L_self_modules_features_modules_denseblock4_modules_denselayer16_modules_norm1_buffers_running_var_          ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer16_modules_norm1_parameters_weight_            L_self_modules_features_modules_denseblock4_modules_denselayer16_modules_norm1_parameters_weight_            ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer16_modules_norm1_parameters_bias_              L_self_modules_features_modules_denseblock4_modules_denselayer16_modules_norm1_parameters_bias_              ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer16_modules_conv1_parameters_weight_            L_self_modules_features_modules_denseblock4_modules_denselayer16_modules_conv1_parameters_weight_            ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer16_modules_norm2_buffers_num_batches_tracked_  L_self_modules_features_modules_denseblock4_modules_denselayer16_modules_norm2_buffers_num_batches_tracked_  ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer16_modules_norm2_buffers_running_mean_         L_self_modules_features_modules_denseblock4_modules_denselayer16_modules_norm2_buffers_running_mean_         ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer16_modules_norm2_buffers_running_var_          L_self_modules_features_modules_denseblock4_modules_denselayer16_modules_norm2_buffers_running_var_          ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer16_modules_norm2_parameters_weight_            L_self_modules_features_modules_denseblock4_modules_denselayer16_modules_norm2_parameters_weight_            ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer16_modules_norm2_parameters_bias_              L_self_modules_features_modules_denseblock4_modules_denselayer16_modules_norm2_parameters_bias_              ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_denseblock4_modules_denselayer16_modules_conv2_parameters_weight_            L_self_modules_features_modules_denseblock4_modules_denselayer16_modules_conv2_parameters_weight_            ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_norm5_buffers_num_batches_tracked_                                           L_self_modules_features_modules_norm5_buffers_num_batches_tracked_                                           ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_norm5_buffers_running_mean_                                                  L_self_modules_features_modules_norm5_buffers_running_mean_                                                  ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_norm5_buffers_running_var_                                                   L_self_modules_features_modules_norm5_buffers_running_var_                                                   ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_norm5_parameters_weight_                                                     L_self_modules_features_modules_norm5_parameters_weight_                                                     ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_features_modules_norm5_parameters_bias_                                                       L_self_modules_features_modules_norm5_parameters_bias_                                                       ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_classifier_parameters_weight_                                                                 L_self_modules_classifier_parameters_weight_                                                                 ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\nplaceholder    l_self_modules_classifier_parameters_bias_                                                                   L_self_modules_classifier_parameters_bias_                                                                   ()                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\ncall_function  input_1                                                                                                      <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (l_x_, l_self_modules_features_modules_conv0_parameters_weight_, None, (2, 2), (3, 3), (1, 1), 1)                                                                                                                                                                                                                                                                                                                                                        {}\ncall_method    add_                                                                                                         add_                                                                                                         (l_self_modules_features_modules_norm0_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                                                                  {}\ncall_function  input_2                                                                                                      <function batch_norm at 0x7f3527413be0>                                                                      (input_1, l_self_modules_features_modules_norm0_buffers_running_mean_, l_self_modules_features_modules_norm0_buffers_running_var_, l_self_modules_features_modules_norm0_parameters_weight_, l_self_modules_features_modules_norm0_parameters_bias_, True, 0.1, 1e-05)                                                                                                                                                                                   {}\ncall_function  input_3                                                                                                      <function relu at 0x7f3527412c20>                                                                            (input_2,)                                                                                                                                                                                                                                                                                                                                                                                                                                               {'inplace': True}\ncall_function  input_4                                                                                                      <function boolean_dispatch.<locals>.fn at 0x7f3527411a20>                                                    (input_3, 3, 2, 1, 1)                                                                                                                                                                                                                                                                                                                                                                                                                                    {'ceil_mode': False, 'return_indices': False}\ncall_function  concated_features                                                                                            <built-in method cat of type object at 0x7f351f2f6f40>                                                       ([input_4], 1)                                                                                                                                                                                                                                                                                                                                                                                                                                           {}\ncall_method    add__1                                                                                                       add_                                                                                                         (l_self_modules_features_modules_denseblock1_modules_denselayer1_modules_norm1_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                          {}\ncall_function  batch_norm_1                                                                                                 <function batch_norm at 0x7f3527413be0>                                                                      (concated_features, l_self_modules_features_modules_denseblock1_modules_denselayer1_modules_norm1_buffers_running_mean_, l_self_modules_features_modules_denseblock1_modules_denselayer1_modules_norm1_buffers_running_var_, l_self_modules_features_modules_denseblock1_modules_denselayer1_modules_norm1_parameters_weight_, l_self_modules_features_modules_denseblock1_modules_denselayer1_modules_norm1_parameters_bias_, True, 0.1, 1e-05)         {}\ncall_function  relu_1                                                                                                       <function relu at 0x7f3527412c20>                                                                            (batch_norm_1,)                                                                                                                                                                                                                                                                                                                                                                                                                                          {'inplace': True}\ncall_function  bottleneck_output                                                                                            <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (relu_1, l_self_modules_features_modules_denseblock1_modules_denselayer1_modules_conv1_parameters_weight_, None, (1, 1), (0, 0), (1, 1), 1)                                                                                                                                                                                                                                                                                                              {}\ncall_method    add__2                                                                                                       add_                                                                                                         (l_self_modules_features_modules_denseblock1_modules_denselayer1_modules_norm2_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                          {}\ncall_function  batch_norm_2                                                                                                 <function batch_norm at 0x7f3527413be0>                                                                      (bottleneck_output, l_self_modules_features_modules_denseblock1_modules_denselayer1_modules_norm2_buffers_running_mean_, l_self_modules_features_modules_denseblock1_modules_denselayer1_modules_norm2_buffers_running_var_, l_self_modules_features_modules_denseblock1_modules_denselayer1_modules_norm2_parameters_weight_, l_self_modules_features_modules_denseblock1_modules_denselayer1_modules_norm2_parameters_bias_, True, 0.1, 1e-05)         {}\ncall_function  relu_2                                                                                                       <function relu at 0x7f3527412c20>                                                                            (batch_norm_2,)                                                                                                                                                                                                                                                                                                                                                                                                                                          {'inplace': True}\ncall_function  new_features                                                                                                 <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (relu_2, l_self_modules_features_modules_denseblock1_modules_denselayer1_modules_conv2_parameters_weight_, None, (1, 1), (1, 1), (1, 1), 1)                                                                                                                                                                                                                                                                                                              {}\ncall_function  concated_features_1                                                                                          <built-in method cat of type object at 0x7f351f2f6f40>                                                       ([input_4, new_features], 1)                                                                                                                                                                                                                                                                                                                                                                                                                             {}\ncall_method    add__3                                                                                                       add_                                                                                                         (l_self_modules_features_modules_denseblock1_modules_denselayer2_modules_norm1_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                          {}\ncall_function  batch_norm_3                                                                                                 <function batch_norm at 0x7f3527413be0>                                                                      (concated_features_1, l_self_modules_features_modules_denseblock1_modules_denselayer2_modules_norm1_buffers_running_mean_, l_self_modules_features_modules_denseblock1_modules_denselayer2_modules_norm1_buffers_running_var_, l_self_modules_features_modules_denseblock1_modules_denselayer2_modules_norm1_parameters_weight_, l_self_modules_features_modules_denseblock1_modules_denselayer2_modules_norm1_parameters_bias_, True, 0.1, 1e-05)       {}\ncall_function  relu_3                                                                                                       <function relu at 0x7f3527412c20>                                                                            (batch_norm_3,)                                                                                                                                                                                                                                                                                                                                                                                                                                          {'inplace': True}\ncall_function  bottleneck_output_1                                                                                          <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (relu_3, l_self_modules_features_modules_denseblock1_modules_denselayer2_modules_conv1_parameters_weight_, None, (1, 1), (0, 0), (1, 1), 1)                                                                                                                                                                                                                                                                                                              {}\ncall_method    add__4                                                                                                       add_                                                                                                         (l_self_modules_features_modules_denseblock1_modules_denselayer2_modules_norm2_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                          {}\ncall_function  batch_norm_4                                                                                                 <function batch_norm at 0x7f3527413be0>                                                                      (bottleneck_output_1, l_self_modules_features_modules_denseblock1_modules_denselayer2_modules_norm2_buffers_running_mean_, l_self_modules_features_modules_denseblock1_modules_denselayer2_modules_norm2_buffers_running_var_, l_self_modules_features_modules_denseblock1_modules_denselayer2_modules_norm2_parameters_weight_, l_self_modules_features_modules_denseblock1_modules_denselayer2_modules_norm2_parameters_bias_, True, 0.1, 1e-05)       {}\ncall_function  relu_4                                                                                                       <function relu at 0x7f3527412c20>                                                                            (batch_norm_4,)                                                                                                                                                                                                                                                                                                                                                                                                                                          {'inplace': True}\ncall_function  new_features_1                                                                                               <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (relu_4, l_self_modules_features_modules_denseblock1_modules_denselayer2_modules_conv2_parameters_weight_, None, (1, 1), (1, 1), (1, 1), 1)                                                                                                                                                                                                                                                                                                              {}\ncall_function  concated_features_2                                                                                          <built-in method cat of type object at 0x7f351f2f6f40>                                                       ([input_4, new_features, new_features_1], 1)                                                                                                                                                                                                                                                                                                                                                                                                             {}\ncall_method    add__5                                                                                                       add_                                                                                                         (l_self_modules_features_modules_denseblock1_modules_denselayer3_modules_norm1_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                          {}\ncall_function  batch_norm_5                                                                                                 <function batch_norm at 0x7f3527413be0>                                                                      (concated_features_2, l_self_modules_features_modules_denseblock1_modules_denselayer3_modules_norm1_buffers_running_mean_, l_self_modules_features_modules_denseblock1_modules_denselayer3_modules_norm1_buffers_running_var_, l_self_modules_features_modules_denseblock1_modules_denselayer3_modules_norm1_parameters_weight_, l_self_modules_features_modules_denseblock1_modules_denselayer3_modules_norm1_parameters_bias_, True, 0.1, 1e-05)       {}\ncall_function  relu_5                                                                                                       <function relu at 0x7f3527412c20>                                                                            (batch_norm_5,)                                                                                                                                                                                                                                                                                                                                                                                                                                          {'inplace': True}\ncall_function  bottleneck_output_2                                                                                          <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (relu_5, l_self_modules_features_modules_denseblock1_modules_denselayer3_modules_conv1_parameters_weight_, None, (1, 1), (0, 0), (1, 1), 1)                                                                                                                                                                                                                                                                                                              {}\ncall_method    add__6                                                                                                       add_                                                                                                         (l_self_modules_features_modules_denseblock1_modules_denselayer3_modules_norm2_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                          {}\ncall_function  batch_norm_6                                                                                                 <function batch_norm at 0x7f3527413be0>                                                                      (bottleneck_output_2, l_self_modules_features_modules_denseblock1_modules_denselayer3_modules_norm2_buffers_running_mean_, l_self_modules_features_modules_denseblock1_modules_denselayer3_modules_norm2_buffers_running_var_, l_self_modules_features_modules_denseblock1_modules_denselayer3_modules_norm2_parameters_weight_, l_self_modules_features_modules_denseblock1_modules_denselayer3_modules_norm2_parameters_bias_, True, 0.1, 1e-05)       {}\ncall_function  relu_6                                                                                                       <function relu at 0x7f3527412c20>                                                                            (batch_norm_6,)                                                                                                                                                                                                                                                                                                                                                                                                                                          {'inplace': True}\ncall_function  new_features_2                                                                                               <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (relu_6, l_self_modules_features_modules_denseblock1_modules_denselayer3_modules_conv2_parameters_weight_, None, (1, 1), (1, 1), (1, 1), 1)                                                                                                                                                                                                                                                                                                              {}\ncall_function  concated_features_3                                                                                          <built-in method cat of type object at 0x7f351f2f6f40>                                                       ([input_4, new_features, new_features_1, new_features_2], 1)                                                                                                                                                                                                                                                                                                                                                                                             {}\ncall_method    add__7                                                                                                       add_                                                                                                         (l_self_modules_features_modules_denseblock1_modules_denselayer4_modules_norm1_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                          {}\ncall_function  batch_norm_7                                                                                                 <function batch_norm at 0x7f3527413be0>                                                                      (concated_features_3, l_self_modules_features_modules_denseblock1_modules_denselayer4_modules_norm1_buffers_running_mean_, l_self_modules_features_modules_denseblock1_modules_denselayer4_modules_norm1_buffers_running_var_, l_self_modules_features_modules_denseblock1_modules_denselayer4_modules_norm1_parameters_weight_, l_self_modules_features_modules_denseblock1_modules_denselayer4_modules_norm1_parameters_bias_, True, 0.1, 1e-05)       {}\ncall_function  relu_7                                                                                                       <function relu at 0x7f3527412c20>                                                                            (batch_norm_7,)                                                                                                                                                                                                                                                                                                                                                                                                                                          {'inplace': True}\ncall_function  bottleneck_output_3                                                                                          <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (relu_7, l_self_modules_features_modules_denseblock1_modules_denselayer4_modules_conv1_parameters_weight_, None, (1, 1), (0, 0), (1, 1), 1)                                                                                                                                                                                                                                                                                                              {}\ncall_method    add__8                                                                                                       add_                                                                                                         (l_self_modules_features_modules_denseblock1_modules_denselayer4_modules_norm2_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                          {}\ncall_function  batch_norm_8                                                                                                 <function batch_norm at 0x7f3527413be0>                                                                      (bottleneck_output_3, l_self_modules_features_modules_denseblock1_modules_denselayer4_modules_norm2_buffers_running_mean_, l_self_modules_features_modules_denseblock1_modules_denselayer4_modules_norm2_buffers_running_var_, l_self_modules_features_modules_denseblock1_modules_denselayer4_modules_norm2_parameters_weight_, l_self_modules_features_modules_denseblock1_modules_denselayer4_modules_norm2_parameters_bias_, True, 0.1, 1e-05)       {}\ncall_function  relu_8                                                                                                       <function relu at 0x7f3527412c20>                                                                            (batch_norm_8,)                                                                                                                                                                                                                                                                                                                                                                                                                                          {'inplace': True}\ncall_function  new_features_3                                                                                               <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (relu_8, l_self_modules_features_modules_denseblock1_modules_denselayer4_modules_conv2_parameters_weight_, None, (1, 1), (1, 1), (1, 1), 1)                                                                                                                                                                                                                                                                                                              {}\ncall_function  concated_features_4                                                                                          <built-in method cat of type object at 0x7f351f2f6f40>                                                       ([input_4, new_features, new_features_1, new_features_2, new_features_3], 1)                                                                                                                                                                                                                                                                                                                                                                             {}\ncall_method    add__9                                                                                                       add_                                                                                                         (l_self_modules_features_modules_denseblock1_modules_denselayer5_modules_norm1_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                          {}\ncall_function  batch_norm_9                                                                                                 <function batch_norm at 0x7f3527413be0>                                                                      (concated_features_4, l_self_modules_features_modules_denseblock1_modules_denselayer5_modules_norm1_buffers_running_mean_, l_self_modules_features_modules_denseblock1_modules_denselayer5_modules_norm1_buffers_running_var_, l_self_modules_features_modules_denseblock1_modules_denselayer5_modules_norm1_parameters_weight_, l_self_modules_features_modules_denseblock1_modules_denselayer5_modules_norm1_parameters_bias_, True, 0.1, 1e-05)       {}\ncall_function  relu_9                                                                                                       <function relu at 0x7f3527412c20>                                                                            (batch_norm_9,)                                                                                                                                                                                                                                                                                                                                                                                                                                          {'inplace': True}\ncall_function  bottleneck_output_4                                                                                          <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (relu_9, l_self_modules_features_modules_denseblock1_modules_denselayer5_modules_conv1_parameters_weight_, None, (1, 1), (0, 0), (1, 1), 1)                                                                                                                                                                                                                                                                                                              {}\ncall_method    add__10                                                                                                      add_                                                                                                         (l_self_modules_features_modules_denseblock1_modules_denselayer5_modules_norm2_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                          {}\ncall_function  batch_norm_10                                                                                                <function batch_norm at 0x7f3527413be0>                                                                      (bottleneck_output_4, l_self_modules_features_modules_denseblock1_modules_denselayer5_modules_norm2_buffers_running_mean_, l_self_modules_features_modules_denseblock1_modules_denselayer5_modules_norm2_buffers_running_var_, l_self_modules_features_modules_denseblock1_modules_denselayer5_modules_norm2_parameters_weight_, l_self_modules_features_modules_denseblock1_modules_denselayer5_modules_norm2_parameters_bias_, True, 0.1, 1e-05)       {}\ncall_function  relu_10                                                                                                      <function relu at 0x7f3527412c20>                                                                            (batch_norm_10,)                                                                                                                                                                                                                                                                                                                                                                                                                                         {'inplace': True}\ncall_function  new_features_4                                                                                               <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (relu_10, l_self_modules_features_modules_denseblock1_modules_denselayer5_modules_conv2_parameters_weight_, None, (1, 1), (1, 1), (1, 1), 1)                                                                                                                                                                                                                                                                                                             {}\ncall_function  concated_features_5                                                                                          <built-in method cat of type object at 0x7f351f2f6f40>                                                       ([input_4, new_features, new_features_1, new_features_2, new_features_3, new_features_4], 1)                                                                                                                                                                                                                                                                                                                                                             {}\ncall_method    add__11                                                                                                      add_                                                                                                         (l_self_modules_features_modules_denseblock1_modules_denselayer6_modules_norm1_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                          {}\ncall_function  batch_norm_11                                                                                                <function batch_norm at 0x7f3527413be0>                                                                      (concated_features_5, l_self_modules_features_modules_denseblock1_modules_denselayer6_modules_norm1_buffers_running_mean_, l_self_modules_features_modules_denseblock1_modules_denselayer6_modules_norm1_buffers_running_var_, l_self_modules_features_modules_denseblock1_modules_denselayer6_modules_norm1_parameters_weight_, l_self_modules_features_modules_denseblock1_modules_denselayer6_modules_norm1_parameters_bias_, True, 0.1, 1e-05)       {}\ncall_function  relu_11                                                                                                      <function relu at 0x7f3527412c20>                                                                            (batch_norm_11,)                                                                                                                                                                                                                                                                                                                                                                                                                                         {'inplace': True}\ncall_function  bottleneck_output_5                                                                                          <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (relu_11, l_self_modules_features_modules_denseblock1_modules_denselayer6_modules_conv1_parameters_weight_, None, (1, 1), (0, 0), (1, 1), 1)                                                                                                                                                                                                                                                                                                             {}\ncall_method    add__12                                                                                                      add_                                                                                                         (l_self_modules_features_modules_denseblock1_modules_denselayer6_modules_norm2_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                          {}\ncall_function  batch_norm_12                                                                                                <function batch_norm at 0x7f3527413be0>                                                                      (bottleneck_output_5, l_self_modules_features_modules_denseblock1_modules_denselayer6_modules_norm2_buffers_running_mean_, l_self_modules_features_modules_denseblock1_modules_denselayer6_modules_norm2_buffers_running_var_, l_self_modules_features_modules_denseblock1_modules_denselayer6_modules_norm2_parameters_weight_, l_self_modules_features_modules_denseblock1_modules_denselayer6_modules_norm2_parameters_bias_, True, 0.1, 1e-05)       {}\ncall_function  relu_12                                                                                                      <function relu at 0x7f3527412c20>                                                                            (batch_norm_12,)                                                                                                                                                                                                                                                                                                                                                                                                                                         {'inplace': True}\ncall_function  new_features_5                                                                                               <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (relu_12, l_self_modules_features_modules_denseblock1_modules_denselayer6_modules_conv2_parameters_weight_, None, (1, 1), (1, 1), (1, 1), 1)                                                                                                                                                                                                                                                                                                             {}\ncall_function  input_5                                                                                                      <built-in method cat of type object at 0x7f351f2f6f40>                                                       ([input_4, new_features, new_features_1, new_features_2, new_features_3, new_features_4, new_features_5], 1)                                                                                                                                                                                                                                                                                                                                             {}\ncall_method    add__13                                                                                                      add_                                                                                                         (l_self_modules_features_modules_transition1_modules_norm_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                                               {}\ncall_function  input_6                                                                                                      <function batch_norm at 0x7f3527413be0>                                                                      (input_5, l_self_modules_features_modules_transition1_modules_norm_buffers_running_mean_, l_self_modules_features_modules_transition1_modules_norm_buffers_running_var_, l_self_modules_features_modules_transition1_modules_norm_parameters_weight_, l_self_modules_features_modules_transition1_modules_norm_parameters_bias_, True, 0.1, 1e-05)                                                                                                       {}\ncall_function  input_7                                                                                                      <function relu at 0x7f3527412c20>                                                                            (input_6,)                                                                                                                                                                                                                                                                                                                                                                                                                                               {'inplace': True}\ncall_function  input_8                                                                                                      <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (input_7, l_self_modules_features_modules_transition1_modules_conv_parameters_weight_, None, (1, 1), (0, 0), (1, 1), 1)                                                                                                                                                                                                                                                                                                                                  {}\ncall_function  input_9                                                                                                      <built-in function avg_pool2d>                                                                               (input_8, 2, 2, 0, False, True, None)                                                                                                                                                                                                                                                                                                                                                                                                                    {}\ncall_function  concated_features_6                                                                                          <built-in method cat of type object at 0x7f351f2f6f40>                                                       ([input_9], 1)                                                                                                                                                                                                                                                                                                                                                                                                                                           {}\ncall_method    add__14                                                                                                      add_                                                                                                         (l_self_modules_features_modules_denseblock2_modules_denselayer1_modules_norm1_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                          {}\ncall_function  batch_norm_14                                                                                                <function batch_norm at 0x7f3527413be0>                                                                      (concated_features_6, l_self_modules_features_modules_denseblock2_modules_denselayer1_modules_norm1_buffers_running_mean_, l_self_modules_features_modules_denseblock2_modules_denselayer1_modules_norm1_buffers_running_var_, l_self_modules_features_modules_denseblock2_modules_denselayer1_modules_norm1_parameters_weight_, l_self_modules_features_modules_denseblock2_modules_denselayer1_modules_norm1_parameters_bias_, True, 0.1, 1e-05)       {}\ncall_function  relu_14                                                                                                      <function relu at 0x7f3527412c20>                                                                            (batch_norm_14,)                                                                                                                                                                                                                                                                                                                                                                                                                                         {'inplace': True}\ncall_function  bottleneck_output_6                                                                                          <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (relu_14, l_self_modules_features_modules_denseblock2_modules_denselayer1_modules_conv1_parameters_weight_, None, (1, 1), (0, 0), (1, 1), 1)                                                                                                                                                                                                                                                                                                             {}\ncall_method    add__15                                                                                                      add_                                                                                                         (l_self_modules_features_modules_denseblock2_modules_denselayer1_modules_norm2_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                          {}\ncall_function  batch_norm_15                                                                                                <function batch_norm at 0x7f3527413be0>                                                                      (bottleneck_output_6, l_self_modules_features_modules_denseblock2_modules_denselayer1_modules_norm2_buffers_running_mean_, l_self_modules_features_modules_denseblock2_modules_denselayer1_modules_norm2_buffers_running_var_, l_self_modules_features_modules_denseblock2_modules_denselayer1_modules_norm2_parameters_weight_, l_self_modules_features_modules_denseblock2_modules_denselayer1_modules_norm2_parameters_bias_, True, 0.1, 1e-05)       {}\ncall_function  relu_15                                                                                                      <function relu at 0x7f3527412c20>                                                                            (batch_norm_15,)                                                                                                                                                                                                                                                                                                                                                                                                                                         {'inplace': True}\ncall_function  new_features_6                                                                                               <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (relu_15, l_self_modules_features_modules_denseblock2_modules_denselayer1_modules_conv2_parameters_weight_, None, (1, 1), (1, 1), (1, 1), 1)                                                                                                                                                                                                                                                                                                             {}\ncall_function  concated_features_7                                                                                          <built-in method cat of type object at 0x7f351f2f6f40>                                                       ([input_9, new_features_6], 1)                                                                                                                                                                                                                                                                                                                                                                                                                           {}\ncall_method    add__16                                                                                                      add_                                                                                                         (l_self_modules_features_modules_denseblock2_modules_denselayer2_modules_norm1_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                          {}\ncall_function  batch_norm_16                                                                                                <function batch_norm at 0x7f3527413be0>                                                                      (concated_features_7, l_self_modules_features_modules_denseblock2_modules_denselayer2_modules_norm1_buffers_running_mean_, l_self_modules_features_modules_denseblock2_modules_denselayer2_modules_norm1_buffers_running_var_, l_self_modules_features_modules_denseblock2_modules_denselayer2_modules_norm1_parameters_weight_, l_self_modules_features_modules_denseblock2_modules_denselayer2_modules_norm1_parameters_bias_, True, 0.1, 1e-05)       {}\ncall_function  relu_16                                                                                                      <function relu at 0x7f3527412c20>                                                                            (batch_norm_16,)                                                                                                                                                                                                                                                                                                                                                                                                                                         {'inplace': True}\ncall_function  bottleneck_output_7                                                                                          <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (relu_16, l_self_modules_features_modules_denseblock2_modules_denselayer2_modules_conv1_parameters_weight_, None, (1, 1), (0, 0), (1, 1), 1)                                                                                                                                                                                                                                                                                                             {}\ncall_method    add__17                                                                                                      add_                                                                                                         (l_self_modules_features_modules_denseblock2_modules_denselayer2_modules_norm2_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                          {}\ncall_function  batch_norm_17                                                                                                <function batch_norm at 0x7f3527413be0>                                                                      (bottleneck_output_7, l_self_modules_features_modules_denseblock2_modules_denselayer2_modules_norm2_buffers_running_mean_, l_self_modules_features_modules_denseblock2_modules_denselayer2_modules_norm2_buffers_running_var_, l_self_modules_features_modules_denseblock2_modules_denselayer2_modules_norm2_parameters_weight_, l_self_modules_features_modules_denseblock2_modules_denselayer2_modules_norm2_parameters_bias_, True, 0.1, 1e-05)       {}\ncall_function  relu_17                                                                                                      <function relu at 0x7f3527412c20>                                                                            (batch_norm_17,)                                                                                                                                                                                                                                                                                                                                                                                                                                         {'inplace': True}\ncall_function  new_features_7                                                                                               <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (relu_17, l_self_modules_features_modules_denseblock2_modules_denselayer2_modules_conv2_parameters_weight_, None, (1, 1), (1, 1), (1, 1), 1)                                                                                                                                                                                                                                                                                                             {}\ncall_function  concated_features_8                                                                                          <built-in method cat of type object at 0x7f351f2f6f40>                                                       ([input_9, new_features_6, new_features_7], 1)                                                                                                                                                                                                                                                                                                                                                                                                           {}\ncall_method    add__18                                                                                                      add_                                                                                                         (l_self_modules_features_modules_denseblock2_modules_denselayer3_modules_norm1_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                          {}\ncall_function  batch_norm_18                                                                                                <function batch_norm at 0x7f3527413be0>                                                                      (concated_features_8, l_self_modules_features_modules_denseblock2_modules_denselayer3_modules_norm1_buffers_running_mean_, l_self_modules_features_modules_denseblock2_modules_denselayer3_modules_norm1_buffers_running_var_, l_self_modules_features_modules_denseblock2_modules_denselayer3_modules_norm1_parameters_weight_, l_self_modules_features_modules_denseblock2_modules_denselayer3_modules_norm1_parameters_bias_, True, 0.1, 1e-05)       {}\ncall_function  relu_18                                                                                                      <function relu at 0x7f3527412c20>                                                                            (batch_norm_18,)                                                                                                                                                                                                                                                                                                                                                                                                                                         {'inplace': True}\ncall_function  bottleneck_output_8                                                                                          <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (relu_18, l_self_modules_features_modules_denseblock2_modules_denselayer3_modules_conv1_parameters_weight_, None, (1, 1), (0, 0), (1, 1), 1)                                                                                                                                                                                                                                                                                                             {}\ncall_method    add__19                                                                                                      add_                                                                                                         (l_self_modules_features_modules_denseblock2_modules_denselayer3_modules_norm2_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                          {}\ncall_function  batch_norm_19                                                                                                <function batch_norm at 0x7f3527413be0>                                                                      (bottleneck_output_8, l_self_modules_features_modules_denseblock2_modules_denselayer3_modules_norm2_buffers_running_mean_, l_self_modules_features_modules_denseblock2_modules_denselayer3_modules_norm2_buffers_running_var_, l_self_modules_features_modules_denseblock2_modules_denselayer3_modules_norm2_parameters_weight_, l_self_modules_features_modules_denseblock2_modules_denselayer3_modules_norm2_parameters_bias_, True, 0.1, 1e-05)       {}\ncall_function  relu_19                                                                                                      <function relu at 0x7f3527412c20>                                                                            (batch_norm_19,)                                                                                                                                                                                                                                                                                                                                                                                                                                         {'inplace': True}\ncall_function  new_features_8                                                                                               <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (relu_19, l_self_modules_features_modules_denseblock2_modules_denselayer3_modules_conv2_parameters_weight_, None, (1, 1), (1, 1), (1, 1), 1)                                                                                                                                                                                                                                                                                                             {}\ncall_function  concated_features_9                                                                                          <built-in method cat of type object at 0x7f351f2f6f40>                                                       ([input_9, new_features_6, new_features_7, new_features_8], 1)                                                                                                                                                                                                                                                                                                                                                                                           {}\ncall_method    add__20                                                                                                      add_                                                                                                         (l_self_modules_features_modules_denseblock2_modules_denselayer4_modules_norm1_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                          {}\ncall_function  batch_norm_20                                                                                                <function batch_norm at 0x7f3527413be0>                                                                      (concated_features_9, l_self_modules_features_modules_denseblock2_modules_denselayer4_modules_norm1_buffers_running_mean_, l_self_modules_features_modules_denseblock2_modules_denselayer4_modules_norm1_buffers_running_var_, l_self_modules_features_modules_denseblock2_modules_denselayer4_modules_norm1_parameters_weight_, l_self_modules_features_modules_denseblock2_modules_denselayer4_modules_norm1_parameters_bias_, True, 0.1, 1e-05)       {}\ncall_function  relu_20                                                                                                      <function relu at 0x7f3527412c20>                                                                            (batch_norm_20,)                                                                                                                                                                                                                                                                                                                                                                                                                                         {'inplace': True}\ncall_function  bottleneck_output_9                                                                                          <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (relu_20, l_self_modules_features_modules_denseblock2_modules_denselayer4_modules_conv1_parameters_weight_, None, (1, 1), (0, 0), (1, 1), 1)                                                                                                                                                                                                                                                                                                             {}\ncall_method    add__21                                                                                                      add_                                                                                                         (l_self_modules_features_modules_denseblock2_modules_denselayer4_modules_norm2_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                          {}\ncall_function  batch_norm_21                                                                                                <function batch_norm at 0x7f3527413be0>                                                                      (bottleneck_output_9, l_self_modules_features_modules_denseblock2_modules_denselayer4_modules_norm2_buffers_running_mean_, l_self_modules_features_modules_denseblock2_modules_denselayer4_modules_norm2_buffers_running_var_, l_self_modules_features_modules_denseblock2_modules_denselayer4_modules_norm2_parameters_weight_, l_self_modules_features_modules_denseblock2_modules_denselayer4_modules_norm2_parameters_bias_, True, 0.1, 1e-05)       {}\ncall_function  relu_21                                                                                                      <function relu at 0x7f3527412c20>                                                                            (batch_norm_21,)                                                                                                                                                                                                                                                                                                                                                                                                                                         {'inplace': True}\ncall_function  new_features_9                                                                                               <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (relu_21, l_self_modules_features_modules_denseblock2_modules_denselayer4_modules_conv2_parameters_weight_, None, (1, 1), (1, 1), (1, 1), 1)                                                                                                                                                                                                                                                                                                             {}\ncall_function  concated_features_10                                                                                         <built-in method cat of type object at 0x7f351f2f6f40>                                                       ([input_9, new_features_6, new_features_7, new_features_8, new_features_9], 1)                                                                                                                                                                                                                                                                                                                                                                           {}\ncall_method    add__22                                                                                                      add_                                                                                                         (l_self_modules_features_modules_denseblock2_modules_denselayer5_modules_norm1_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                          {}\ncall_function  batch_norm_22                                                                                                <function batch_norm at 0x7f3527413be0>                                                                      (concated_features_10, l_self_modules_features_modules_denseblock2_modules_denselayer5_modules_norm1_buffers_running_mean_, l_self_modules_features_modules_denseblock2_modules_denselayer5_modules_norm1_buffers_running_var_, l_self_modules_features_modules_denseblock2_modules_denselayer5_modules_norm1_parameters_weight_, l_self_modules_features_modules_denseblock2_modules_denselayer5_modules_norm1_parameters_bias_, True, 0.1, 1e-05)      {}\ncall_function  relu_22                                                                                                      <function relu at 0x7f3527412c20>                                                                            (batch_norm_22,)                                                                                                                                                                                                                                                                                                                                                                                                                                         {'inplace': True}\ncall_function  bottleneck_output_10                                                                                         <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (relu_22, l_self_modules_features_modules_denseblock2_modules_denselayer5_modules_conv1_parameters_weight_, None, (1, 1), (0, 0), (1, 1), 1)                                                                                                                                                                                                                                                                                                             {}\ncall_method    add__23                                                                                                      add_                                                                                                         (l_self_modules_features_modules_denseblock2_modules_denselayer5_modules_norm2_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                          {}\ncall_function  batch_norm_23                                                                                                <function batch_norm at 0x7f3527413be0>                                                                      (bottleneck_output_10, l_self_modules_features_modules_denseblock2_modules_denselayer5_modules_norm2_buffers_running_mean_, l_self_modules_features_modules_denseblock2_modules_denselayer5_modules_norm2_buffers_running_var_, l_self_modules_features_modules_denseblock2_modules_denselayer5_modules_norm2_parameters_weight_, l_self_modules_features_modules_denseblock2_modules_denselayer5_modules_norm2_parameters_bias_, True, 0.1, 1e-05)      {}\ncall_function  relu_23                                                                                                      <function relu at 0x7f3527412c20>                                                                            (batch_norm_23,)                                                                                                                                                                                                                                                                                                                                                                                                                                         {'inplace': True}\ncall_function  new_features_10                                                                                              <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (relu_23, l_self_modules_features_modules_denseblock2_modules_denselayer5_modules_conv2_parameters_weight_, None, (1, 1), (1, 1), (1, 1), 1)                                                                                                                                                                                                                                                                                                             {}\ncall_function  concated_features_11                                                                                         <built-in method cat of type object at 0x7f351f2f6f40>                                                       ([input_9, new_features_6, new_features_7, new_features_8, new_features_9, new_features_10], 1)                                                                                                                                                                                                                                                                                                                                                          {}\ncall_method    add__24                                                                                                      add_                                                                                                         (l_self_modules_features_modules_denseblock2_modules_denselayer6_modules_norm1_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                          {}\ncall_function  batch_norm_24                                                                                                <function batch_norm at 0x7f3527413be0>                                                                      (concated_features_11, l_self_modules_features_modules_denseblock2_modules_denselayer6_modules_norm1_buffers_running_mean_, l_self_modules_features_modules_denseblock2_modules_denselayer6_modules_norm1_buffers_running_var_, l_self_modules_features_modules_denseblock2_modules_denselayer6_modules_norm1_parameters_weight_, l_self_modules_features_modules_denseblock2_modules_denselayer6_modules_norm1_parameters_bias_, True, 0.1, 1e-05)      {}\ncall_function  relu_24                                                                                                      <function relu at 0x7f3527412c20>                                                                            (batch_norm_24,)                                                                                                                                                                                                                                                                                                                                                                                                                                         {'inplace': True}\ncall_function  bottleneck_output_11                                                                                         <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (relu_24, l_self_modules_features_modules_denseblock2_modules_denselayer6_modules_conv1_parameters_weight_, None, (1, 1), (0, 0), (1, 1), 1)                                                                                                                                                                                                                                                                                                             {}\ncall_method    add__25                                                                                                      add_                                                                                                         (l_self_modules_features_modules_denseblock2_modules_denselayer6_modules_norm2_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                          {}\ncall_function  batch_norm_25                                                                                                <function batch_norm at 0x7f3527413be0>                                                                      (bottleneck_output_11, l_self_modules_features_modules_denseblock2_modules_denselayer6_modules_norm2_buffers_running_mean_, l_self_modules_features_modules_denseblock2_modules_denselayer6_modules_norm2_buffers_running_var_, l_self_modules_features_modules_denseblock2_modules_denselayer6_modules_norm2_parameters_weight_, l_self_modules_features_modules_denseblock2_modules_denselayer6_modules_norm2_parameters_bias_, True, 0.1, 1e-05)      {}\ncall_function  relu_25                                                                                                      <function relu at 0x7f3527412c20>                                                                            (batch_norm_25,)                                                                                                                                                                                                                                                                                                                                                                                                                                         {'inplace': True}\ncall_function  new_features_11                                                                                              <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (relu_25, l_self_modules_features_modules_denseblock2_modules_denselayer6_modules_conv2_parameters_weight_, None, (1, 1), (1, 1), (1, 1), 1)                                                                                                                                                                                                                                                                                                             {}\ncall_function  concated_features_12                                                                                         <built-in method cat of type object at 0x7f351f2f6f40>                                                       ([input_9, new_features_6, new_features_7, new_features_8, new_features_9, new_features_10, new_features_11], 1)                                                                                                                                                                                                                                                                                                                                         {}\ncall_method    add__26                                                                                                      add_                                                                                                         (l_self_modules_features_modules_denseblock2_modules_denselayer7_modules_norm1_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                          {}\ncall_function  batch_norm_26                                                                                                <function batch_norm at 0x7f3527413be0>                                                                      (concated_features_12, l_self_modules_features_modules_denseblock2_modules_denselayer7_modules_norm1_buffers_running_mean_, l_self_modules_features_modules_denseblock2_modules_denselayer7_modules_norm1_buffers_running_var_, l_self_modules_features_modules_denseblock2_modules_denselayer7_modules_norm1_parameters_weight_, l_self_modules_features_modules_denseblock2_modules_denselayer7_modules_norm1_parameters_bias_, True, 0.1, 1e-05)      {}\ncall_function  relu_26                                                                                                      <function relu at 0x7f3527412c20>                                                                            (batch_norm_26,)                                                                                                                                                                                                                                                                                                                                                                                                                                         {'inplace': True}\ncall_function  bottleneck_output_12                                                                                         <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (relu_26, l_self_modules_features_modules_denseblock2_modules_denselayer7_modules_conv1_parameters_weight_, None, (1, 1), (0, 0), (1, 1), 1)                                                                                                                                                                                                                                                                                                             {}\ncall_method    add__27                                                                                                      add_                                                                                                         (l_self_modules_features_modules_denseblock2_modules_denselayer7_modules_norm2_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                          {}\ncall_function  batch_norm_27                                                                                                <function batch_norm at 0x7f3527413be0>                                                                      (bottleneck_output_12, l_self_modules_features_modules_denseblock2_modules_denselayer7_modules_norm2_buffers_running_mean_, l_self_modules_features_modules_denseblock2_modules_denselayer7_modules_norm2_buffers_running_var_, l_self_modules_features_modules_denseblock2_modules_denselayer7_modules_norm2_parameters_weight_, l_self_modules_features_modules_denseblock2_modules_denselayer7_modules_norm2_parameters_bias_, True, 0.1, 1e-05)      {}\ncall_function  relu_27                                                                                                      <function relu at 0x7f3527412c20>                                                                            (batch_norm_27,)                                                                                                                                                                                                                                                                                                                                                                                                                                         {'inplace': True}\ncall_function  new_features_12                                                                                              <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (relu_27, l_self_modules_features_modules_denseblock2_modules_denselayer7_modules_conv2_parameters_weight_, None, (1, 1), (1, 1), (1, 1), 1)                                                                                                                                                                                                                                                                                                             {}\ncall_function  concated_features_13                                                                                         <built-in method cat of type object at 0x7f351f2f6f40>                                                       ([input_9, new_features_6, new_features_7, new_features_8, new_features_9, new_features_10, new_features_11, new_features_12], 1)                                                                                                                                                                                                                                                                                                                        {}\ncall_method    add__28                                                                                                      add_                                                                                                         (l_self_modules_features_modules_denseblock2_modules_denselayer8_modules_norm1_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                          {}\ncall_function  batch_norm_28                                                                                                <function batch_norm at 0x7f3527413be0>                                                                      (concated_features_13, l_self_modules_features_modules_denseblock2_modules_denselayer8_modules_norm1_buffers_running_mean_, l_self_modules_features_modules_denseblock2_modules_denselayer8_modules_norm1_buffers_running_var_, l_self_modules_features_modules_denseblock2_modules_denselayer8_modules_norm1_parameters_weight_, l_self_modules_features_modules_denseblock2_modules_denselayer8_modules_norm1_parameters_bias_, True, 0.1, 1e-05)      {}\ncall_function  relu_28                                                                                                      <function relu at 0x7f3527412c20>                                                                            (batch_norm_28,)                                                                                                                                                                                                                                                                                                                                                                                                                                         {'inplace': True}\ncall_function  bottleneck_output_13                                                                                         <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (relu_28, l_self_modules_features_modules_denseblock2_modules_denselayer8_modules_conv1_parameters_weight_, None, (1, 1), (0, 0), (1, 1), 1)                                                                                                                                                                                                                                                                                                             {}\ncall_method    add__29                                                                                                      add_                                                                                                         (l_self_modules_features_modules_denseblock2_modules_denselayer8_modules_norm2_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                          {}\ncall_function  batch_norm_29                                                                                                <function batch_norm at 0x7f3527413be0>                                                                      (bottleneck_output_13, l_self_modules_features_modules_denseblock2_modules_denselayer8_modules_norm2_buffers_running_mean_, l_self_modules_features_modules_denseblock2_modules_denselayer8_modules_norm2_buffers_running_var_, l_self_modules_features_modules_denseblock2_modules_denselayer8_modules_norm2_parameters_weight_, l_self_modules_features_modules_denseblock2_modules_denselayer8_modules_norm2_parameters_bias_, True, 0.1, 1e-05)      {}\ncall_function  relu_29                                                                                                      <function relu at 0x7f3527412c20>                                                                            (batch_norm_29,)                                                                                                                                                                                                                                                                                                                                                                                                                                         {'inplace': True}\ncall_function  new_features_13                                                                                              <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (relu_29, l_self_modules_features_modules_denseblock2_modules_denselayer8_modules_conv2_parameters_weight_, None, (1, 1), (1, 1), (1, 1), 1)                                                                                                                                                                                                                                                                                                             {}\ncall_function  concated_features_14                                                                                         <built-in method cat of type object at 0x7f351f2f6f40>                                                       ([input_9, new_features_6, new_features_7, new_features_8, new_features_9, new_features_10, new_features_11, new_features_12, new_features_13], 1)                                                                                                                                                                                                                                                                                                       {}\ncall_method    add__30                                                                                                      add_                                                                                                         (l_self_modules_features_modules_denseblock2_modules_denselayer9_modules_norm1_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                          {}\ncall_function  batch_norm_30                                                                                                <function batch_norm at 0x7f3527413be0>                                                                      (concated_features_14, l_self_modules_features_modules_denseblock2_modules_denselayer9_modules_norm1_buffers_running_mean_, l_self_modules_features_modules_denseblock2_modules_denselayer9_modules_norm1_buffers_running_var_, l_self_modules_features_modules_denseblock2_modules_denselayer9_modules_norm1_parameters_weight_, l_self_modules_features_modules_denseblock2_modules_denselayer9_modules_norm1_parameters_bias_, True, 0.1, 1e-05)      {}\ncall_function  relu_30                                                                                                      <function relu at 0x7f3527412c20>                                                                            (batch_norm_30,)                                                                                                                                                                                                                                                                                                                                                                                                                                         {'inplace': True}\ncall_function  bottleneck_output_14                                                                                         <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (relu_30, l_self_modules_features_modules_denseblock2_modules_denselayer9_modules_conv1_parameters_weight_, None, (1, 1), (0, 0), (1, 1), 1)                                                                                                                                                                                                                                                                                                             {}\ncall_method    add__31                                                                                                      add_                                                                                                         (l_self_modules_features_modules_denseblock2_modules_denselayer9_modules_norm2_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                          {}\ncall_function  batch_norm_31                                                                                                <function batch_norm at 0x7f3527413be0>                                                                      (bottleneck_output_14, l_self_modules_features_modules_denseblock2_modules_denselayer9_modules_norm2_buffers_running_mean_, l_self_modules_features_modules_denseblock2_modules_denselayer9_modules_norm2_buffers_running_var_, l_self_modules_features_modules_denseblock2_modules_denselayer9_modules_norm2_parameters_weight_, l_self_modules_features_modules_denseblock2_modules_denselayer9_modules_norm2_parameters_bias_, True, 0.1, 1e-05)      {}\ncall_function  relu_31                                                                                                      <function relu at 0x7f3527412c20>                                                                            (batch_norm_31,)                                                                                                                                                                                                                                                                                                                                                                                                                                         {'inplace': True}\ncall_function  new_features_14                                                                                              <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (relu_31, l_self_modules_features_modules_denseblock2_modules_denselayer9_modules_conv2_parameters_weight_, None, (1, 1), (1, 1), (1, 1), 1)                                                                                                                                                                                                                                                                                                             {}\ncall_function  concated_features_15                                                                                         <built-in method cat of type object at 0x7f351f2f6f40>                                                       ([input_9, new_features_6, new_features_7, new_features_8, new_features_9, new_features_10, new_features_11, new_features_12, new_features_13, new_features_14], 1)                                                                                                                                                                                                                                                                                      {}\ncall_method    add__32                                                                                                      add_                                                                                                         (l_self_modules_features_modules_denseblock2_modules_denselayer10_modules_norm1_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                         {}\ncall_function  batch_norm_32                                                                                                <function batch_norm at 0x7f3527413be0>                                                                      (concated_features_15, l_self_modules_features_modules_denseblock2_modules_denselayer10_modules_norm1_buffers_running_mean_, l_self_modules_features_modules_denseblock2_modules_denselayer10_modules_norm1_buffers_running_var_, l_self_modules_features_modules_denseblock2_modules_denselayer10_modules_norm1_parameters_weight_, l_self_modules_features_modules_denseblock2_modules_denselayer10_modules_norm1_parameters_bias_, True, 0.1, 1e-05)  {}\ncall_function  relu_32                                                                                                      <function relu at 0x7f3527412c20>                                                                            (batch_norm_32,)                                                                                                                                                                                                                                                                                                                                                                                                                                         {'inplace': True}\ncall_function  bottleneck_output_15                                                                                         <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (relu_32, l_self_modules_features_modules_denseblock2_modules_denselayer10_modules_conv1_parameters_weight_, None, (1, 1), (0, 0), (1, 1), 1)                                                                                                                                                                                                                                                                                                            {}\ncall_method    add__33                                                                                                      add_                                                                                                         (l_self_modules_features_modules_denseblock2_modules_denselayer10_modules_norm2_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                         {}\ncall_function  batch_norm_33                                                                                                <function batch_norm at 0x7f3527413be0>                                                                      (bottleneck_output_15, l_self_modules_features_modules_denseblock2_modules_denselayer10_modules_norm2_buffers_running_mean_, l_self_modules_features_modules_denseblock2_modules_denselayer10_modules_norm2_buffers_running_var_, l_self_modules_features_modules_denseblock2_modules_denselayer10_modules_norm2_parameters_weight_, l_self_modules_features_modules_denseblock2_modules_denselayer10_modules_norm2_parameters_bias_, True, 0.1, 1e-05)  {}\ncall_function  relu_33                                                                                                      <function relu at 0x7f3527412c20>                                                                            (batch_norm_33,)                                                                                                                                                                                                                                                                                                                                                                                                                                         {'inplace': True}\ncall_function  new_features_15                                                                                              <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (relu_33, l_self_modules_features_modules_denseblock2_modules_denselayer10_modules_conv2_parameters_weight_, None, (1, 1), (1, 1), (1, 1), 1)                                                                                                                                                                                                                                                                                                            {}\ncall_function  concated_features_16                                                                                         <built-in method cat of type object at 0x7f351f2f6f40>                                                       ([input_9, new_features_6, new_features_7, new_features_8, new_features_9, new_features_10, new_features_11, new_features_12, new_features_13, new_features_14, new_features_15], 1)                                                                                                                                                                                                                                                                     {}\ncall_method    add__34                                                                                                      add_                                                                                                         (l_self_modules_features_modules_denseblock2_modules_denselayer11_modules_norm1_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                         {}\ncall_function  batch_norm_34                                                                                                <function batch_norm at 0x7f3527413be0>                                                                      (concated_features_16, l_self_modules_features_modules_denseblock2_modules_denselayer11_modules_norm1_buffers_running_mean_, l_self_modules_features_modules_denseblock2_modules_denselayer11_modules_norm1_buffers_running_var_, l_self_modules_features_modules_denseblock2_modules_denselayer11_modules_norm1_parameters_weight_, l_self_modules_features_modules_denseblock2_modules_denselayer11_modules_norm1_parameters_bias_, True, 0.1, 1e-05)  {}\ncall_function  relu_34                                                                                                      <function relu at 0x7f3527412c20>                                                                            (batch_norm_34,)                                                                                                                                                                                                                                                                                                                                                                                                                                         {'inplace': True}\ncall_function  bottleneck_output_16                                                                                         <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (relu_34, l_self_modules_features_modules_denseblock2_modules_denselayer11_modules_conv1_parameters_weight_, None, (1, 1), (0, 0), (1, 1), 1)                                                                                                                                                                                                                                                                                                            {}\ncall_method    add__35                                                                                                      add_                                                                                                         (l_self_modules_features_modules_denseblock2_modules_denselayer11_modules_norm2_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                         {}\ncall_function  batch_norm_35                                                                                                <function batch_norm at 0x7f3527413be0>                                                                      (bottleneck_output_16, l_self_modules_features_modules_denseblock2_modules_denselayer11_modules_norm2_buffers_running_mean_, l_self_modules_features_modules_denseblock2_modules_denselayer11_modules_norm2_buffers_running_var_, l_self_modules_features_modules_denseblock2_modules_denselayer11_modules_norm2_parameters_weight_, l_self_modules_features_modules_denseblock2_modules_denselayer11_modules_norm2_parameters_bias_, True, 0.1, 1e-05)  {}\ncall_function  relu_35                                                                                                      <function relu at 0x7f3527412c20>                                                                            (batch_norm_35,)                                                                                                                                                                                                                                                                                                                                                                                                                                         {'inplace': True}\ncall_function  new_features_16                                                                                              <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (relu_35, l_self_modules_features_modules_denseblock2_modules_denselayer11_modules_conv2_parameters_weight_, None, (1, 1), (1, 1), (1, 1), 1)                                                                                                                                                                                                                                                                                                            {}\ncall_function  concated_features_17                                                                                         <built-in method cat of type object at 0x7f351f2f6f40>                                                       ([input_9, new_features_6, new_features_7, new_features_8, new_features_9, new_features_10, new_features_11, new_features_12, new_features_13, new_features_14, new_features_15, new_features_16], 1)                                                                                                                                                                                                                                                    {}\ncall_method    add__36                                                                                                      add_                                                                                                         (l_self_modules_features_modules_denseblock2_modules_denselayer12_modules_norm1_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                         {}\ncall_function  batch_norm_36                                                                                                <function batch_norm at 0x7f3527413be0>                                                                      (concated_features_17, l_self_modules_features_modules_denseblock2_modules_denselayer12_modules_norm1_buffers_running_mean_, l_self_modules_features_modules_denseblock2_modules_denselayer12_modules_norm1_buffers_running_var_, l_self_modules_features_modules_denseblock2_modules_denselayer12_modules_norm1_parameters_weight_, l_self_modules_features_modules_denseblock2_modules_denselayer12_modules_norm1_parameters_bias_, True, 0.1, 1e-05)  {}\ncall_function  relu_36                                                                                                      <function relu at 0x7f3527412c20>                                                                            (batch_norm_36,)                                                                                                                                                                                                                                                                                                                                                                                                                                         {'inplace': True}\ncall_function  bottleneck_output_17                                                                                         <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (relu_36, l_self_modules_features_modules_denseblock2_modules_denselayer12_modules_conv1_parameters_weight_, None, (1, 1), (0, 0), (1, 1), 1)                                                                                                                                                                                                                                                                                                            {}\ncall_method    add__37                                                                                                      add_                                                                                                         (l_self_modules_features_modules_denseblock2_modules_denselayer12_modules_norm2_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                         {}\ncall_function  batch_norm_37                                                                                                <function batch_norm at 0x7f3527413be0>                                                                      (bottleneck_output_17, l_self_modules_features_modules_denseblock2_modules_denselayer12_modules_norm2_buffers_running_mean_, l_self_modules_features_modules_denseblock2_modules_denselayer12_modules_norm2_buffers_running_var_, l_self_modules_features_modules_denseblock2_modules_denselayer12_modules_norm2_parameters_weight_, l_self_modules_features_modules_denseblock2_modules_denselayer12_modules_norm2_parameters_bias_, True, 0.1, 1e-05)  {}\ncall_function  relu_37                                                                                                      <function relu at 0x7f3527412c20>                                                                            (batch_norm_37,)                                                                                                                                                                                                                                                                                                                                                                                                                                         {'inplace': True}\ncall_function  new_features_17                                                                                              <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (relu_37, l_self_modules_features_modules_denseblock2_modules_denselayer12_modules_conv2_parameters_weight_, None, (1, 1), (1, 1), (1, 1), 1)                                                                                                                                                                                                                                                                                                            {}\ncall_function  input_10                                                                                                     <built-in method cat of type object at 0x7f351f2f6f40>                                                       ([input_9, new_features_6, new_features_7, new_features_8, new_features_9, new_features_10, new_features_11, new_features_12, new_features_13, new_features_14, new_features_15, new_features_16, new_features_17], 1)                                                                                                                                                                                                                                   {}\ncall_method    add__38                                                                                                      add_                                                                                                         (l_self_modules_features_modules_transition2_modules_norm_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                                               {}\ncall_function  input_11                                                                                                     <function batch_norm at 0x7f3527413be0>                                                                      (input_10, l_self_modules_features_modules_transition2_modules_norm_buffers_running_mean_, l_self_modules_features_modules_transition2_modules_norm_buffers_running_var_, l_self_modules_features_modules_transition2_modules_norm_parameters_weight_, l_self_modules_features_modules_transition2_modules_norm_parameters_bias_, True, 0.1, 1e-05)                                                                                                      {}\ncall_function  input_12                                                                                                     <function relu at 0x7f3527412c20>                                                                            (input_11,)                                                                                                                                                                                                                                                                                                                                                                                                                                              {'inplace': True}\ncall_function  input_13                                                                                                     <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (input_12, l_self_modules_features_modules_transition2_modules_conv_parameters_weight_, None, (1, 1), (0, 0), (1, 1), 1)                                                                                                                                                                                                                                                                                                                                 {}\ncall_function  input_14                                                                                                     <built-in function avg_pool2d>                                                                               (input_13, 2, 2, 0, False, True, None)                                                                                                                                                                                                                                                                                                                                                                                                                   {}\ncall_function  concated_features_18                                                                                         <built-in method cat of type object at 0x7f351f2f6f40>                                                       ([input_14], 1)                                                                                                                                                                                                                                                                                                                                                                                                                                          {}\ncall_method    add__39                                                                                                      add_                                                                                                         (l_self_modules_features_modules_denseblock3_modules_denselayer1_modules_norm1_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                          {}\ncall_function  batch_norm_39                                                                                                <function batch_norm at 0x7f3527413be0>                                                                      (concated_features_18, l_self_modules_features_modules_denseblock3_modules_denselayer1_modules_norm1_buffers_running_mean_, l_self_modules_features_modules_denseblock3_modules_denselayer1_modules_norm1_buffers_running_var_, l_self_modules_features_modules_denseblock3_modules_denselayer1_modules_norm1_parameters_weight_, l_self_modules_features_modules_denseblock3_modules_denselayer1_modules_norm1_parameters_bias_, True, 0.1, 1e-05)      {}\ncall_function  relu_39                                                                                                      <function relu at 0x7f3527412c20>                                                                            (batch_norm_39,)                                                                                                                                                                                                                                                                                                                                                                                                                                         {'inplace': True}\ncall_function  bottleneck_output_18                                                                                         <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (relu_39, l_self_modules_features_modules_denseblock3_modules_denselayer1_modules_conv1_parameters_weight_, None, (1, 1), (0, 0), (1, 1), 1)                                                                                                                                                                                                                                                                                                             {}\ncall_method    add__40                                                                                                      add_                                                                                                         (l_self_modules_features_modules_denseblock3_modules_denselayer1_modules_norm2_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                          {}\ncall_function  batch_norm_40                                                                                                <function batch_norm at 0x7f3527413be0>                                                                      (bottleneck_output_18, l_self_modules_features_modules_denseblock3_modules_denselayer1_modules_norm2_buffers_running_mean_, l_self_modules_features_modules_denseblock3_modules_denselayer1_modules_norm2_buffers_running_var_, l_self_modules_features_modules_denseblock3_modules_denselayer1_modules_norm2_parameters_weight_, l_self_modules_features_modules_denseblock3_modules_denselayer1_modules_norm2_parameters_bias_, True, 0.1, 1e-05)      {}\ncall_function  relu_40                                                                                                      <function relu at 0x7f3527412c20>                                                                            (batch_norm_40,)                                                                                                                                                                                                                                                                                                                                                                                                                                         {'inplace': True}\ncall_function  new_features_18                                                                                              <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (relu_40, l_self_modules_features_modules_denseblock3_modules_denselayer1_modules_conv2_parameters_weight_, None, (1, 1), (1, 1), (1, 1), 1)                                                                                                                                                                                                                                                                                                             {}\ncall_function  concated_features_19                                                                                         <built-in method cat of type object at 0x7f351f2f6f40>                                                       ([input_14, new_features_18], 1)                                                                                                                                                                                                                                                                                                                                                                                                                         {}\ncall_method    add__41                                                                                                      add_                                                                                                         (l_self_modules_features_modules_denseblock3_modules_denselayer2_modules_norm1_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                          {}\ncall_function  batch_norm_41                                                                                                <function batch_norm at 0x7f3527413be0>                                                                      (concated_features_19, l_self_modules_features_modules_denseblock3_modules_denselayer2_modules_norm1_buffers_running_mean_, l_self_modules_features_modules_denseblock3_modules_denselayer2_modules_norm1_buffers_running_var_, l_self_modules_features_modules_denseblock3_modules_denselayer2_modules_norm1_parameters_weight_, l_self_modules_features_modules_denseblock3_modules_denselayer2_modules_norm1_parameters_bias_, True, 0.1, 1e-05)      {}\ncall_function  relu_41                                                                                                      <function relu at 0x7f3527412c20>                                                                            (batch_norm_41,)                                                                                                                                                                                                                                                                                                                                                                                                                                         {'inplace': True}\ncall_function  bottleneck_output_19                                                                                         <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (relu_41, l_self_modules_features_modules_denseblock3_modules_denselayer2_modules_conv1_parameters_weight_, None, (1, 1), (0, 0), (1, 1), 1)                                                                                                                                                                                                                                                                                                             {}\ncall_method    add__42                                                                                                      add_                                                                                                         (l_self_modules_features_modules_denseblock3_modules_denselayer2_modules_norm2_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                          {}\ncall_function  batch_norm_42                                                                                                <function batch_norm at 0x7f3527413be0>                                                                      (bottleneck_output_19, l_self_modules_features_modules_denseblock3_modules_denselayer2_modules_norm2_buffers_running_mean_, l_self_modules_features_modules_denseblock3_modules_denselayer2_modules_norm2_buffers_running_var_, l_self_modules_features_modules_denseblock3_modules_denselayer2_modules_norm2_parameters_weight_, l_self_modules_features_modules_denseblock3_modules_denselayer2_modules_norm2_parameters_bias_, True, 0.1, 1e-05)      {}\ncall_function  relu_42                                                                                                      <function relu at 0x7f3527412c20>                                                                            (batch_norm_42,)                                                                                                                                                                                                                                                                                                                                                                                                                                         {'inplace': True}\ncall_function  new_features_19                                                                                              <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (relu_42, l_self_modules_features_modules_denseblock3_modules_denselayer2_modules_conv2_parameters_weight_, None, (1, 1), (1, 1), (1, 1), 1)                                                                                                                                                                                                                                                                                                             {}\ncall_function  concated_features_20                                                                                         <built-in method cat of type object at 0x7f351f2f6f40>                                                       ([input_14, new_features_18, new_features_19], 1)                                                                                                                                                                                                                                                                                                                                                                                                        {}\ncall_method    add__43                                                                                                      add_                                                                                                         (l_self_modules_features_modules_denseblock3_modules_denselayer3_modules_norm1_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                          {}\ncall_function  batch_norm_43                                                                                                <function batch_norm at 0x7f3527413be0>                                                                      (concated_features_20, l_self_modules_features_modules_denseblock3_modules_denselayer3_modules_norm1_buffers_running_mean_, l_self_modules_features_modules_denseblock3_modules_denselayer3_modules_norm1_buffers_running_var_, l_self_modules_features_modules_denseblock3_modules_denselayer3_modules_norm1_parameters_weight_, l_self_modules_features_modules_denseblock3_modules_denselayer3_modules_norm1_parameters_bias_, True, 0.1, 1e-05)      {}\ncall_function  relu_43                                                                                                      <function relu at 0x7f3527412c20>                                                                            (batch_norm_43,)                                                                                                                                                                                                                                                                                                                                                                                                                                         {'inplace': True}\ncall_function  bottleneck_output_20                                                                                         <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (relu_43, l_self_modules_features_modules_denseblock3_modules_denselayer3_modules_conv1_parameters_weight_, None, (1, 1), (0, 0), (1, 1), 1)                                                                                                                                                                                                                                                                                                             {}\ncall_method    add__44                                                                                                      add_                                                                                                         (l_self_modules_features_modules_denseblock3_modules_denselayer3_modules_norm2_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                          {}\ncall_function  batch_norm_44                                                                                                <function batch_norm at 0x7f3527413be0>                                                                      (bottleneck_output_20, l_self_modules_features_modules_denseblock3_modules_denselayer3_modules_norm2_buffers_running_mean_, l_self_modules_features_modules_denseblock3_modules_denselayer3_modules_norm2_buffers_running_var_, l_self_modules_features_modules_denseblock3_modules_denselayer3_modules_norm2_parameters_weight_, l_self_modules_features_modules_denseblock3_modules_denselayer3_modules_norm2_parameters_bias_, True, 0.1, 1e-05)      {}\ncall_function  relu_44                                                                                                      <function relu at 0x7f3527412c20>                                                                            (batch_norm_44,)                                                                                                                                                                                                                                                                                                                                                                                                                                         {'inplace': True}\ncall_function  new_features_20                                                                                              <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (relu_44, l_self_modules_features_modules_denseblock3_modules_denselayer3_modules_conv2_parameters_weight_, None, (1, 1), (1, 1), (1, 1), 1)                                                                                                                                                                                                                                                                                                             {}\ncall_function  concated_features_21                                                                                         <built-in method cat of type object at 0x7f351f2f6f40>                                                       ([input_14, new_features_18, new_features_19, new_features_20], 1)                                                                                                                                                                                                                                                                                                                                                                                       {}\ncall_method    add__45                                                                                                      add_                                                                                                         (l_self_modules_features_modules_denseblock3_modules_denselayer4_modules_norm1_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                          {}\ncall_function  batch_norm_45                                                                                                <function batch_norm at 0x7f3527413be0>                                                                      (concated_features_21, l_self_modules_features_modules_denseblock3_modules_denselayer4_modules_norm1_buffers_running_mean_, l_self_modules_features_modules_denseblock3_modules_denselayer4_modules_norm1_buffers_running_var_, l_self_modules_features_modules_denseblock3_modules_denselayer4_modules_norm1_parameters_weight_, l_self_modules_features_modules_denseblock3_modules_denselayer4_modules_norm1_parameters_bias_, True, 0.1, 1e-05)      {}\ncall_function  relu_45                                                                                                      <function relu at 0x7f3527412c20>                                                                            (batch_norm_45,)                                                                                                                                                                                                                                                                                                                                                                                                                                         {'inplace': True}\ncall_function  bottleneck_output_21                                                                                         <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (relu_45, l_self_modules_features_modules_denseblock3_modules_denselayer4_modules_conv1_parameters_weight_, None, (1, 1), (0, 0), (1, 1), 1)                                                                                                                                                                                                                                                                                                             {}\ncall_method    add__46                                                                                                      add_                                                                                                         (l_self_modules_features_modules_denseblock3_modules_denselayer4_modules_norm2_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                          {}\ncall_function  batch_norm_46                                                                                                <function batch_norm at 0x7f3527413be0>                                                                      (bottleneck_output_21, l_self_modules_features_modules_denseblock3_modules_denselayer4_modules_norm2_buffers_running_mean_, l_self_modules_features_modules_denseblock3_modules_denselayer4_modules_norm2_buffers_running_var_, l_self_modules_features_modules_denseblock3_modules_denselayer4_modules_norm2_parameters_weight_, l_self_modules_features_modules_denseblock3_modules_denselayer4_modules_norm2_parameters_bias_, True, 0.1, 1e-05)      {}\ncall_function  relu_46                                                                                                      <function relu at 0x7f3527412c20>                                                                            (batch_norm_46,)                                                                                                                                                                                                                                                                                                                                                                                                                                         {'inplace': True}\ncall_function  new_features_21                                                                                              <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (relu_46, l_self_modules_features_modules_denseblock3_modules_denselayer4_modules_conv2_parameters_weight_, None, (1, 1), (1, 1), (1, 1), 1)                                                                                                                                                                                                                                                                                                             {}\ncall_function  concated_features_22                                                                                         <built-in method cat of type object at 0x7f351f2f6f40>                                                       ([input_14, new_features_18, new_features_19, new_features_20, new_features_21], 1)                                                                                                                                                                                                                                                                                                                                                                      {}\ncall_method    add__47                                                                                                      add_                                                                                                         (l_self_modules_features_modules_denseblock3_modules_denselayer5_modules_norm1_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                          {}\ncall_function  batch_norm_47                                                                                                <function batch_norm at 0x7f3527413be0>                                                                      (concated_features_22, l_self_modules_features_modules_denseblock3_modules_denselayer5_modules_norm1_buffers_running_mean_, l_self_modules_features_modules_denseblock3_modules_denselayer5_modules_norm1_buffers_running_var_, l_self_modules_features_modules_denseblock3_modules_denselayer5_modules_norm1_parameters_weight_, l_self_modules_features_modules_denseblock3_modules_denselayer5_modules_norm1_parameters_bias_, True, 0.1, 1e-05)      {}\ncall_function  relu_47                                                                                                      <function relu at 0x7f3527412c20>                                                                            (batch_norm_47,)                                                                                                                                                                                                                                                                                                                                                                                                                                         {'inplace': True}\ncall_function  bottleneck_output_22                                                                                         <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (relu_47, l_self_modules_features_modules_denseblock3_modules_denselayer5_modules_conv1_parameters_weight_, None, (1, 1), (0, 0), (1, 1), 1)                                                                                                                                                                                                                                                                                                             {}\ncall_method    add__48                                                                                                      add_                                                                                                         (l_self_modules_features_modules_denseblock3_modules_denselayer5_modules_norm2_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                          {}\ncall_function  batch_norm_48                                                                                                <function batch_norm at 0x7f3527413be0>                                                                      (bottleneck_output_22, l_self_modules_features_modules_denseblock3_modules_denselayer5_modules_norm2_buffers_running_mean_, l_self_modules_features_modules_denseblock3_modules_denselayer5_modules_norm2_buffers_running_var_, l_self_modules_features_modules_denseblock3_modules_denselayer5_modules_norm2_parameters_weight_, l_self_modules_features_modules_denseblock3_modules_denselayer5_modules_norm2_parameters_bias_, True, 0.1, 1e-05)      {}\ncall_function  relu_48                                                                                                      <function relu at 0x7f3527412c20>                                                                            (batch_norm_48,)                                                                                                                                                                                                                                                                                                                                                                                                                                         {'inplace': True}\ncall_function  new_features_22                                                                                              <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (relu_48, l_self_modules_features_modules_denseblock3_modules_denselayer5_modules_conv2_parameters_weight_, None, (1, 1), (1, 1), (1, 1), 1)                                                                                                                                                                                                                                                                                                             {}\ncall_function  concated_features_23                                                                                         <built-in method cat of type object at 0x7f351f2f6f40>                                                       ([input_14, new_features_18, new_features_19, new_features_20, new_features_21, new_features_22], 1)                                                                                                                                                                                                                                                                                                                                                     {}\ncall_method    add__49                                                                                                      add_                                                                                                         (l_self_modules_features_modules_denseblock3_modules_denselayer6_modules_norm1_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                          {}\ncall_function  batch_norm_49                                                                                                <function batch_norm at 0x7f3527413be0>                                                                      (concated_features_23, l_self_modules_features_modules_denseblock3_modules_denselayer6_modules_norm1_buffers_running_mean_, l_self_modules_features_modules_denseblock3_modules_denselayer6_modules_norm1_buffers_running_var_, l_self_modules_features_modules_denseblock3_modules_denselayer6_modules_norm1_parameters_weight_, l_self_modules_features_modules_denseblock3_modules_denselayer6_modules_norm1_parameters_bias_, True, 0.1, 1e-05)      {}\ncall_function  relu_49                                                                                                      <function relu at 0x7f3527412c20>                                                                            (batch_norm_49,)                                                                                                                                                                                                                                                                                                                                                                                                                                         {'inplace': True}\ncall_function  bottleneck_output_23                                                                                         <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (relu_49, l_self_modules_features_modules_denseblock3_modules_denselayer6_modules_conv1_parameters_weight_, None, (1, 1), (0, 0), (1, 1), 1)                                                                                                                                                                                                                                                                                                             {}\ncall_method    add__50                                                                                                      add_                                                                                                         (l_self_modules_features_modules_denseblock3_modules_denselayer6_modules_norm2_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                          {}\ncall_function  batch_norm_50                                                                                                <function batch_norm at 0x7f3527413be0>                                                                      (bottleneck_output_23, l_self_modules_features_modules_denseblock3_modules_denselayer6_modules_norm2_buffers_running_mean_, l_self_modules_features_modules_denseblock3_modules_denselayer6_modules_norm2_buffers_running_var_, l_self_modules_features_modules_denseblock3_modules_denselayer6_modules_norm2_parameters_weight_, l_self_modules_features_modules_denseblock3_modules_denselayer6_modules_norm2_parameters_bias_, True, 0.1, 1e-05)      {}\ncall_function  relu_50                                                                                                      <function relu at 0x7f3527412c20>                                                                            (batch_norm_50,)                                                                                                                                                                                                                                                                                                                                                                                                                                         {'inplace': True}\ncall_function  new_features_23                                                                                              <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (relu_50, l_self_modules_features_modules_denseblock3_modules_denselayer6_modules_conv2_parameters_weight_, None, (1, 1), (1, 1), (1, 1), 1)                                                                                                                                                                                                                                                                                                             {}\ncall_function  concated_features_24                                                                                         <built-in method cat of type object at 0x7f351f2f6f40>                                                       ([input_14, new_features_18, new_features_19, new_features_20, new_features_21, new_features_22, new_features_23], 1)                                                                                                                                                                                                                                                                                                                                    {}\ncall_method    add__51                                                                                                      add_                                                                                                         (l_self_modules_features_modules_denseblock3_modules_denselayer7_modules_norm1_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                          {}\ncall_function  batch_norm_51                                                                                                <function batch_norm at 0x7f3527413be0>                                                                      (concated_features_24, l_self_modules_features_modules_denseblock3_modules_denselayer7_modules_norm1_buffers_running_mean_, l_self_modules_features_modules_denseblock3_modules_denselayer7_modules_norm1_buffers_running_var_, l_self_modules_features_modules_denseblock3_modules_denselayer7_modules_norm1_parameters_weight_, l_self_modules_features_modules_denseblock3_modules_denselayer7_modules_norm1_parameters_bias_, True, 0.1, 1e-05)      {}\ncall_function  relu_51                                                                                                      <function relu at 0x7f3527412c20>                                                                            (batch_norm_51,)                                                                                                                                                                                                                                                                                                                                                                                                                                         {'inplace': True}\ncall_function  bottleneck_output_24                                                                                         <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (relu_51, l_self_modules_features_modules_denseblock3_modules_denselayer7_modules_conv1_parameters_weight_, None, (1, 1), (0, 0), (1, 1), 1)                                                                                                                                                                                                                                                                                                             {}\ncall_method    add__52                                                                                                      add_                                                                                                         (l_self_modules_features_modules_denseblock3_modules_denselayer7_modules_norm2_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                          {}\ncall_function  batch_norm_52                                                                                                <function batch_norm at 0x7f3527413be0>                                                                      (bottleneck_output_24, l_self_modules_features_modules_denseblock3_modules_denselayer7_modules_norm2_buffers_running_mean_, l_self_modules_features_modules_denseblock3_modules_denselayer7_modules_norm2_buffers_running_var_, l_self_modules_features_modules_denseblock3_modules_denselayer7_modules_norm2_parameters_weight_, l_self_modules_features_modules_denseblock3_modules_denselayer7_modules_norm2_parameters_bias_, True, 0.1, 1e-05)      {}\ncall_function  relu_52                                                                                                      <function relu at 0x7f3527412c20>                                                                            (batch_norm_52,)                                                                                                                                                                                                                                                                                                                                                                                                                                         {'inplace': True}\ncall_function  new_features_24                                                                                              <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (relu_52, l_self_modules_features_modules_denseblock3_modules_denselayer7_modules_conv2_parameters_weight_, None, (1, 1), (1, 1), (1, 1), 1)                                                                                                                                                                                                                                                                                                             {}\ncall_function  concated_features_25                                                                                         <built-in method cat of type object at 0x7f351f2f6f40>                                                       ([input_14, new_features_18, new_features_19, new_features_20, new_features_21, new_features_22, new_features_23, new_features_24], 1)                                                                                                                                                                                                                                                                                                                   {}\ncall_method    add__53                                                                                                      add_                                                                                                         (l_self_modules_features_modules_denseblock3_modules_denselayer8_modules_norm1_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                          {}\ncall_function  batch_norm_53                                                                                                <function batch_norm at 0x7f3527413be0>                                                                      (concated_features_25, l_self_modules_features_modules_denseblock3_modules_denselayer8_modules_norm1_buffers_running_mean_, l_self_modules_features_modules_denseblock3_modules_denselayer8_modules_norm1_buffers_running_var_, l_self_modules_features_modules_denseblock3_modules_denselayer8_modules_norm1_parameters_weight_, l_self_modules_features_modules_denseblock3_modules_denselayer8_modules_norm1_parameters_bias_, True, 0.1, 1e-05)      {}\ncall_function  relu_53                                                                                                      <function relu at 0x7f3527412c20>                                                                            (batch_norm_53,)                                                                                                                                                                                                                                                                                                                                                                                                                                         {'inplace': True}\ncall_function  bottleneck_output_25                                                                                         <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (relu_53, l_self_modules_features_modules_denseblock3_modules_denselayer8_modules_conv1_parameters_weight_, None, (1, 1), (0, 0), (1, 1), 1)                                                                                                                                                                                                                                                                                                             {}\ncall_method    add__54                                                                                                      add_                                                                                                         (l_self_modules_features_modules_denseblock3_modules_denselayer8_modules_norm2_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                          {}\ncall_function  batch_norm_54                                                                                                <function batch_norm at 0x7f3527413be0>                                                                      (bottleneck_output_25, l_self_modules_features_modules_denseblock3_modules_denselayer8_modules_norm2_buffers_running_mean_, l_self_modules_features_modules_denseblock3_modules_denselayer8_modules_norm2_buffers_running_var_, l_self_modules_features_modules_denseblock3_modules_denselayer8_modules_norm2_parameters_weight_, l_self_modules_features_modules_denseblock3_modules_denselayer8_modules_norm2_parameters_bias_, True, 0.1, 1e-05)      {}\ncall_function  relu_54                                                                                                      <function relu at 0x7f3527412c20>                                                                            (batch_norm_54,)                                                                                                                                                                                                                                                                                                                                                                                                                                         {'inplace': True}\ncall_function  new_features_25                                                                                              <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (relu_54, l_self_modules_features_modules_denseblock3_modules_denselayer8_modules_conv2_parameters_weight_, None, (1, 1), (1, 1), (1, 1), 1)                                                                                                                                                                                                                                                                                                             {}\ncall_function  concated_features_26                                                                                         <built-in method cat of type object at 0x7f351f2f6f40>                                                       ([input_14, new_features_18, new_features_19, new_features_20, new_features_21, new_features_22, new_features_23, new_features_24, new_features_25], 1)                                                                                                                                                                                                                                                                                                  {}\ncall_method    add__55                                                                                                      add_                                                                                                         (l_self_modules_features_modules_denseblock3_modules_denselayer9_modules_norm1_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                          {}\ncall_function  batch_norm_55                                                                                                <function batch_norm at 0x7f3527413be0>                                                                      (concated_features_26, l_self_modules_features_modules_denseblock3_modules_denselayer9_modules_norm1_buffers_running_mean_, l_self_modules_features_modules_denseblock3_modules_denselayer9_modules_norm1_buffers_running_var_, l_self_modules_features_modules_denseblock3_modules_denselayer9_modules_norm1_parameters_weight_, l_self_modules_features_modules_denseblock3_modules_denselayer9_modules_norm1_parameters_bias_, True, 0.1, 1e-05)      {}\ncall_function  relu_55                                                                                                      <function relu at 0x7f3527412c20>                                                                            (batch_norm_55,)                                                                                                                                                                                                                                                                                                                                                                                                                                         {'inplace': True}\ncall_function  bottleneck_output_26                                                                                         <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (relu_55, l_self_modules_features_modules_denseblock3_modules_denselayer9_modules_conv1_parameters_weight_, None, (1, 1), (0, 0), (1, 1), 1)                                                                                                                                                                                                                                                                                                             {}\ncall_method    add__56                                                                                                      add_                                                                                                         (l_self_modules_features_modules_denseblock3_modules_denselayer9_modules_norm2_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                          {}\ncall_function  batch_norm_56                                                                                                <function batch_norm at 0x7f3527413be0>                                                                      (bottleneck_output_26, l_self_modules_features_modules_denseblock3_modules_denselayer9_modules_norm2_buffers_running_mean_, l_self_modules_features_modules_denseblock3_modules_denselayer9_modules_norm2_buffers_running_var_, l_self_modules_features_modules_denseblock3_modules_denselayer9_modules_norm2_parameters_weight_, l_self_modules_features_modules_denseblock3_modules_denselayer9_modules_norm2_parameters_bias_, True, 0.1, 1e-05)      {}\ncall_function  relu_56                                                                                                      <function relu at 0x7f3527412c20>                                                                            (batch_norm_56,)                                                                                                                                                                                                                                                                                                                                                                                                                                         {'inplace': True}\ncall_function  new_features_26                                                                                              <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (relu_56, l_self_modules_features_modules_denseblock3_modules_denselayer9_modules_conv2_parameters_weight_, None, (1, 1), (1, 1), (1, 1), 1)                                                                                                                                                                                                                                                                                                             {}\ncall_function  concated_features_27                                                                                         <built-in method cat of type object at 0x7f351f2f6f40>                                                       ([input_14, new_features_18, new_features_19, new_features_20, new_features_21, new_features_22, new_features_23, new_features_24, new_features_25, new_features_26], 1)                                                                                                                                                                                                                                                                                 {}\ncall_method    add__57                                                                                                      add_                                                                                                         (l_self_modules_features_modules_denseblock3_modules_denselayer10_modules_norm1_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                         {}\ncall_function  batch_norm_57                                                                                                <function batch_norm at 0x7f3527413be0>                                                                      (concated_features_27, l_self_modules_features_modules_denseblock3_modules_denselayer10_modules_norm1_buffers_running_mean_, l_self_modules_features_modules_denseblock3_modules_denselayer10_modules_norm1_buffers_running_var_, l_self_modules_features_modules_denseblock3_modules_denselayer10_modules_norm1_parameters_weight_, l_self_modules_features_modules_denseblock3_modules_denselayer10_modules_norm1_parameters_bias_, True, 0.1, 1e-05)  {}\ncall_function  relu_57                                                                                                      <function relu at 0x7f3527412c20>                                                                            (batch_norm_57,)                                                                                                                                                                                                                                                                                                                                                                                                                                         {'inplace': True}\ncall_function  bottleneck_output_27                                                                                         <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (relu_57, l_self_modules_features_modules_denseblock3_modules_denselayer10_modules_conv1_parameters_weight_, None, (1, 1), (0, 0), (1, 1), 1)                                                                                                                                                                                                                                                                                                            {}\ncall_method    add__58                                                                                                      add_                                                                                                         (l_self_modules_features_modules_denseblock3_modules_denselayer10_modules_norm2_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                         {}\ncall_function  batch_norm_58                                                                                                <function batch_norm at 0x7f3527413be0>                                                                      (bottleneck_output_27, l_self_modules_features_modules_denseblock3_modules_denselayer10_modules_norm2_buffers_running_mean_, l_self_modules_features_modules_denseblock3_modules_denselayer10_modules_norm2_buffers_running_var_, l_self_modules_features_modules_denseblock3_modules_denselayer10_modules_norm2_parameters_weight_, l_self_modules_features_modules_denseblock3_modules_denselayer10_modules_norm2_parameters_bias_, True, 0.1, 1e-05)  {}\ncall_function  relu_58                                                                                                      <function relu at 0x7f3527412c20>                                                                            (batch_norm_58,)                                                                                                                                                                                                                                                                                                                                                                                                                                         {'inplace': True}\ncall_function  new_features_27                                                                                              <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (relu_58, l_self_modules_features_modules_denseblock3_modules_denselayer10_modules_conv2_parameters_weight_, None, (1, 1), (1, 1), (1, 1), 1)                                                                                                                                                                                                                                                                                                            {}\ncall_function  concated_features_28                                                                                         <built-in method cat of type object at 0x7f351f2f6f40>                                                       ([input_14, new_features_18, new_features_19, new_features_20, new_features_21, new_features_22, new_features_23, new_features_24, new_features_25, new_features_26, new_features_27], 1)                                                                                                                                                                                                                                                                {}\ncall_method    add__59                                                                                                      add_                                                                                                         (l_self_modules_features_modules_denseblock3_modules_denselayer11_modules_norm1_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                         {}\ncall_function  batch_norm_59                                                                                                <function batch_norm at 0x7f3527413be0>                                                                      (concated_features_28, l_self_modules_features_modules_denseblock3_modules_denselayer11_modules_norm1_buffers_running_mean_, l_self_modules_features_modules_denseblock3_modules_denselayer11_modules_norm1_buffers_running_var_, l_self_modules_features_modules_denseblock3_modules_denselayer11_modules_norm1_parameters_weight_, l_self_modules_features_modules_denseblock3_modules_denselayer11_modules_norm1_parameters_bias_, True, 0.1, 1e-05)  {}\ncall_function  relu_59                                                                                                      <function relu at 0x7f3527412c20>                                                                            (batch_norm_59,)                                                                                                                                                                                                                                                                                                                                                                                                                                         {'inplace': True}\ncall_function  bottleneck_output_28                                                                                         <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (relu_59, l_self_modules_features_modules_denseblock3_modules_denselayer11_modules_conv1_parameters_weight_, None, (1, 1), (0, 0), (1, 1), 1)                                                                                                                                                                                                                                                                                                            {}\ncall_method    add__60                                                                                                      add_                                                                                                         (l_self_modules_features_modules_denseblock3_modules_denselayer11_modules_norm2_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                         {}\ncall_function  batch_norm_60                                                                                                <function batch_norm at 0x7f3527413be0>                                                                      (bottleneck_output_28, l_self_modules_features_modules_denseblock3_modules_denselayer11_modules_norm2_buffers_running_mean_, l_self_modules_features_modules_denseblock3_modules_denselayer11_modules_norm2_buffers_running_var_, l_self_modules_features_modules_denseblock3_modules_denselayer11_modules_norm2_parameters_weight_, l_self_modules_features_modules_denseblock3_modules_denselayer11_modules_norm2_parameters_bias_, True, 0.1, 1e-05)  {}\ncall_function  relu_60                                                                                                      <function relu at 0x7f3527412c20>                                                                            (batch_norm_60,)                                                                                                                                                                                                                                                                                                                                                                                                                                         {'inplace': True}\ncall_function  new_features_28                                                                                              <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (relu_60, l_self_modules_features_modules_denseblock3_modules_denselayer11_modules_conv2_parameters_weight_, None, (1, 1), (1, 1), (1, 1), 1)                                                                                                                                                                                                                                                                                                            {}\ncall_function  concated_features_29                                                                                         <built-in method cat of type object at 0x7f351f2f6f40>                                                       ([input_14, new_features_18, new_features_19, new_features_20, new_features_21, new_features_22, new_features_23, new_features_24, new_features_25, new_features_26, new_features_27, new_features_28], 1)                                                                                                                                                                                                                                               {}\ncall_method    add__61                                                                                                      add_                                                                                                         (l_self_modules_features_modules_denseblock3_modules_denselayer12_modules_norm1_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                         {}\ncall_function  batch_norm_61                                                                                                <function batch_norm at 0x7f3527413be0>                                                                      (concated_features_29, l_self_modules_features_modules_denseblock3_modules_denselayer12_modules_norm1_buffers_running_mean_, l_self_modules_features_modules_denseblock3_modules_denselayer12_modules_norm1_buffers_running_var_, l_self_modules_features_modules_denseblock3_modules_denselayer12_modules_norm1_parameters_weight_, l_self_modules_features_modules_denseblock3_modules_denselayer12_modules_norm1_parameters_bias_, True, 0.1, 1e-05)  {}\ncall_function  relu_61                                                                                                      <function relu at 0x7f3527412c20>                                                                            (batch_norm_61,)                                                                                                                                                                                                                                                                                                                                                                                                                                         {'inplace': True}\ncall_function  bottleneck_output_29                                                                                         <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (relu_61, l_self_modules_features_modules_denseblock3_modules_denselayer12_modules_conv1_parameters_weight_, None, (1, 1), (0, 0), (1, 1), 1)                                                                                                                                                                                                                                                                                                            {}\ncall_method    add__62                                                                                                      add_                                                                                                         (l_self_modules_features_modules_denseblock3_modules_denselayer12_modules_norm2_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                         {}\ncall_function  batch_norm_62                                                                                                <function batch_norm at 0x7f3527413be0>                                                                      (bottleneck_output_29, l_self_modules_features_modules_denseblock3_modules_denselayer12_modules_norm2_buffers_running_mean_, l_self_modules_features_modules_denseblock3_modules_denselayer12_modules_norm2_buffers_running_var_, l_self_modules_features_modules_denseblock3_modules_denselayer12_modules_norm2_parameters_weight_, l_self_modules_features_modules_denseblock3_modules_denselayer12_modules_norm2_parameters_bias_, True, 0.1, 1e-05)  {}\ncall_function  relu_62                                                                                                      <function relu at 0x7f3527412c20>                                                                            (batch_norm_62,)                                                                                                                                                                                                                                                                                                                                                                                                                                         {'inplace': True}\ncall_function  new_features_29                                                                                              <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (relu_62, l_self_modules_features_modules_denseblock3_modules_denselayer12_modules_conv2_parameters_weight_, None, (1, 1), (1, 1), (1, 1), 1)                                                                                                                                                                                                                                                                                                            {}\ncall_function  concated_features_30                                                                                         <built-in method cat of type object at 0x7f351f2f6f40>                                                       ([input_14, new_features_18, new_features_19, new_features_20, new_features_21, new_features_22, new_features_23, new_features_24, new_features_25, new_features_26, new_features_27, new_features_28, new_features_29], 1)                                                                                                                                                                                                                              {}\ncall_method    add__63                                                                                                      add_                                                                                                         (l_self_modules_features_modules_denseblock3_modules_denselayer13_modules_norm1_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                         {}\ncall_function  batch_norm_63                                                                                                <function batch_norm at 0x7f3527413be0>                                                                      (concated_features_30, l_self_modules_features_modules_denseblock3_modules_denselayer13_modules_norm1_buffers_running_mean_, l_self_modules_features_modules_denseblock3_modules_denselayer13_modules_norm1_buffers_running_var_, l_self_modules_features_modules_denseblock3_modules_denselayer13_modules_norm1_parameters_weight_, l_self_modules_features_modules_denseblock3_modules_denselayer13_modules_norm1_parameters_bias_, True, 0.1, 1e-05)  {}\ncall_function  relu_63                                                                                                      <function relu at 0x7f3527412c20>                                                                            (batch_norm_63,)                                                                                                                                                                                                                                                                                                                                                                                                                                         {'inplace': True}\ncall_function  bottleneck_output_30                                                                                         <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (relu_63, l_self_modules_features_modules_denseblock3_modules_denselayer13_modules_conv1_parameters_weight_, None, (1, 1), (0, 0), (1, 1), 1)                                                                                                                                                                                                                                                                                                            {}\ncall_method    add__64                                                                                                      add_                                                                                                         (l_self_modules_features_modules_denseblock3_modules_denselayer13_modules_norm2_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                         {}\ncall_function  batch_norm_64                                                                                                <function batch_norm at 0x7f3527413be0>                                                                      (bottleneck_output_30, l_self_modules_features_modules_denseblock3_modules_denselayer13_modules_norm2_buffers_running_mean_, l_self_modules_features_modules_denseblock3_modules_denselayer13_modules_norm2_buffers_running_var_, l_self_modules_features_modules_denseblock3_modules_denselayer13_modules_norm2_parameters_weight_, l_self_modules_features_modules_denseblock3_modules_denselayer13_modules_norm2_parameters_bias_, True, 0.1, 1e-05)  {}\ncall_function  relu_64                                                                                                      <function relu at 0x7f3527412c20>                                                                            (batch_norm_64,)                                                                                                                                                                                                                                                                                                                                                                                                                                         {'inplace': True}\ncall_function  new_features_30                                                                                              <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (relu_64, l_self_modules_features_modules_denseblock3_modules_denselayer13_modules_conv2_parameters_weight_, None, (1, 1), (1, 1), (1, 1), 1)                                                                                                                                                                                                                                                                                                            {}\ncall_function  concated_features_31                                                                                         <built-in method cat of type object at 0x7f351f2f6f40>                                                       ([input_14, new_features_18, new_features_19, new_features_20, new_features_21, new_features_22, new_features_23, new_features_24, new_features_25, new_features_26, new_features_27, new_features_28, new_features_29, new_features_30], 1)                                                                                                                                                                                                             {}\ncall_method    add__65                                                                                                      add_                                                                                                         (l_self_modules_features_modules_denseblock3_modules_denselayer14_modules_norm1_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                         {}\ncall_function  batch_norm_65                                                                                                <function batch_norm at 0x7f3527413be0>                                                                      (concated_features_31, l_self_modules_features_modules_denseblock3_modules_denselayer14_modules_norm1_buffers_running_mean_, l_self_modules_features_modules_denseblock3_modules_denselayer14_modules_norm1_buffers_running_var_, l_self_modules_features_modules_denseblock3_modules_denselayer14_modules_norm1_parameters_weight_, l_self_modules_features_modules_denseblock3_modules_denselayer14_modules_norm1_parameters_bias_, True, 0.1, 1e-05)  {}\ncall_function  relu_65                                                                                                      <function relu at 0x7f3527412c20>                                                                            (batch_norm_65,)                                                                                                                                                                                                                                                                                                                                                                                                                                         {'inplace': True}\ncall_function  bottleneck_output_31                                                                                         <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (relu_65, l_self_modules_features_modules_denseblock3_modules_denselayer14_modules_conv1_parameters_weight_, None, (1, 1), (0, 0), (1, 1), 1)                                                                                                                                                                                                                                                                                                            {}\ncall_method    add__66                                                                                                      add_                                                                                                         (l_self_modules_features_modules_denseblock3_modules_denselayer14_modules_norm2_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                         {}\ncall_function  batch_norm_66                                                                                                <function batch_norm at 0x7f3527413be0>                                                                      (bottleneck_output_31, l_self_modules_features_modules_denseblock3_modules_denselayer14_modules_norm2_buffers_running_mean_, l_self_modules_features_modules_denseblock3_modules_denselayer14_modules_norm2_buffers_running_var_, l_self_modules_features_modules_denseblock3_modules_denselayer14_modules_norm2_parameters_weight_, l_self_modules_features_modules_denseblock3_modules_denselayer14_modules_norm2_parameters_bias_, True, 0.1, 1e-05)  {}\ncall_function  relu_66                                                                                                      <function relu at 0x7f3527412c20>                                                                            (batch_norm_66,)                                                                                                                                                                                                                                                                                                                                                                                                                                         {'inplace': True}\ncall_function  new_features_31                                                                                              <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (relu_66, l_self_modules_features_modules_denseblock3_modules_denselayer14_modules_conv2_parameters_weight_, None, (1, 1), (1, 1), (1, 1), 1)                                                                                                                                                                                                                                                                                                            {}\ncall_function  concated_features_32                                                                                         <built-in method cat of type object at 0x7f351f2f6f40>                                                       ([input_14, new_features_18, new_features_19, new_features_20, new_features_21, new_features_22, new_features_23, new_features_24, new_features_25, new_features_26, new_features_27, new_features_28, new_features_29, new_features_30, new_features_31], 1)                                                                                                                                                                                            {}\ncall_method    add__67                                                                                                      add_                                                                                                         (l_self_modules_features_modules_denseblock3_modules_denselayer15_modules_norm1_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                         {}\ncall_function  batch_norm_67                                                                                                <function batch_norm at 0x7f3527413be0>                                                                      (concated_features_32, l_self_modules_features_modules_denseblock3_modules_denselayer15_modules_norm1_buffers_running_mean_, l_self_modules_features_modules_denseblock3_modules_denselayer15_modules_norm1_buffers_running_var_, l_self_modules_features_modules_denseblock3_modules_denselayer15_modules_norm1_parameters_weight_, l_self_modules_features_modules_denseblock3_modules_denselayer15_modules_norm1_parameters_bias_, True, 0.1, 1e-05)  {}\ncall_function  relu_67                                                                                                      <function relu at 0x7f3527412c20>                                                                            (batch_norm_67,)                                                                                                                                                                                                                                                                                                                                                                                                                                         {'inplace': True}\ncall_function  bottleneck_output_32                                                                                         <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (relu_67, l_self_modules_features_modules_denseblock3_modules_denselayer15_modules_conv1_parameters_weight_, None, (1, 1), (0, 0), (1, 1), 1)                                                                                                                                                                                                                                                                                                            {}\ncall_method    add__68                                                                                                      add_                                                                                                         (l_self_modules_features_modules_denseblock3_modules_denselayer15_modules_norm2_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                         {}\ncall_function  batch_norm_68                                                                                                <function batch_norm at 0x7f3527413be0>                                                                      (bottleneck_output_32, l_self_modules_features_modules_denseblock3_modules_denselayer15_modules_norm2_buffers_running_mean_, l_self_modules_features_modules_denseblock3_modules_denselayer15_modules_norm2_buffers_running_var_, l_self_modules_features_modules_denseblock3_modules_denselayer15_modules_norm2_parameters_weight_, l_self_modules_features_modules_denseblock3_modules_denselayer15_modules_norm2_parameters_bias_, True, 0.1, 1e-05)  {}\ncall_function  relu_68                                                                                                      <function relu at 0x7f3527412c20>                                                                            (batch_norm_68,)                                                                                                                                                                                                                                                                                                                                                                                                                                         {'inplace': True}\ncall_function  new_features_32                                                                                              <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (relu_68, l_self_modules_features_modules_denseblock3_modules_denselayer15_modules_conv2_parameters_weight_, None, (1, 1), (1, 1), (1, 1), 1)                                                                                                                                                                                                                                                                                                            {}\ncall_function  concated_features_33                                                                                         <built-in method cat of type object at 0x7f351f2f6f40>                                                       ([input_14, new_features_18, new_features_19, new_features_20, new_features_21, new_features_22, new_features_23, new_features_24, new_features_25, new_features_26, new_features_27, new_features_28, new_features_29, new_features_30, new_features_31, new_features_32], 1)                                                                                                                                                                           {}\ncall_method    add__69                                                                                                      add_                                                                                                         (l_self_modules_features_modules_denseblock3_modules_denselayer16_modules_norm1_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                         {}\ncall_function  batch_norm_69                                                                                                <function batch_norm at 0x7f3527413be0>                                                                      (concated_features_33, l_self_modules_features_modules_denseblock3_modules_denselayer16_modules_norm1_buffers_running_mean_, l_self_modules_features_modules_denseblock3_modules_denselayer16_modules_norm1_buffers_running_var_, l_self_modules_features_modules_denseblock3_modules_denselayer16_modules_norm1_parameters_weight_, l_self_modules_features_modules_denseblock3_modules_denselayer16_modules_norm1_parameters_bias_, True, 0.1, 1e-05)  {}\ncall_function  relu_69                                                                                                      <function relu at 0x7f3527412c20>                                                                            (batch_norm_69,)                                                                                                                                                                                                                                                                                                                                                                                                                                         {'inplace': True}\ncall_function  bottleneck_output_33                                                                                         <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (relu_69, l_self_modules_features_modules_denseblock3_modules_denselayer16_modules_conv1_parameters_weight_, None, (1, 1), (0, 0), (1, 1), 1)                                                                                                                                                                                                                                                                                                            {}\ncall_method    add__70                                                                                                      add_                                                                                                         (l_self_modules_features_modules_denseblock3_modules_denselayer16_modules_norm2_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                         {}\ncall_function  batch_norm_70                                                                                                <function batch_norm at 0x7f3527413be0>                                                                      (bottleneck_output_33, l_self_modules_features_modules_denseblock3_modules_denselayer16_modules_norm2_buffers_running_mean_, l_self_modules_features_modules_denseblock3_modules_denselayer16_modules_norm2_buffers_running_var_, l_self_modules_features_modules_denseblock3_modules_denselayer16_modules_norm2_parameters_weight_, l_self_modules_features_modules_denseblock3_modules_denselayer16_modules_norm2_parameters_bias_, True, 0.1, 1e-05)  {}\ncall_function  relu_70                                                                                                      <function relu at 0x7f3527412c20>                                                                            (batch_norm_70,)                                                                                                                                                                                                                                                                                                                                                                                                                                         {'inplace': True}\ncall_function  new_features_33                                                                                              <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (relu_70, l_self_modules_features_modules_denseblock3_modules_denselayer16_modules_conv2_parameters_weight_, None, (1, 1), (1, 1), (1, 1), 1)                                                                                                                                                                                                                                                                                                            {}\ncall_function  concated_features_34                                                                                         <built-in method cat of type object at 0x7f351f2f6f40>                                                       ([input_14, new_features_18, new_features_19, new_features_20, new_features_21, new_features_22, new_features_23, new_features_24, new_features_25, new_features_26, new_features_27, new_features_28, new_features_29, new_features_30, new_features_31, new_features_32, new_features_33], 1)                                                                                                                                                          {}\ncall_method    add__71                                                                                                      add_                                                                                                         (l_self_modules_features_modules_denseblock3_modules_denselayer17_modules_norm1_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                         {}\ncall_function  batch_norm_71                                                                                                <function batch_norm at 0x7f3527413be0>                                                                      (concated_features_34, l_self_modules_features_modules_denseblock3_modules_denselayer17_modules_norm1_buffers_running_mean_, l_self_modules_features_modules_denseblock3_modules_denselayer17_modules_norm1_buffers_running_var_, l_self_modules_features_modules_denseblock3_modules_denselayer17_modules_norm1_parameters_weight_, l_self_modules_features_modules_denseblock3_modules_denselayer17_modules_norm1_parameters_bias_, True, 0.1, 1e-05)  {}\ncall_function  relu_71                                                                                                      <function relu at 0x7f3527412c20>                                                                            (batch_norm_71,)                                                                                                                                                                                                                                                                                                                                                                                                                                         {'inplace': True}\ncall_function  bottleneck_output_34                                                                                         <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (relu_71, l_self_modules_features_modules_denseblock3_modules_denselayer17_modules_conv1_parameters_weight_, None, (1, 1), (0, 0), (1, 1), 1)                                                                                                                                                                                                                                                                                                            {}\ncall_method    add__72                                                                                                      add_                                                                                                         (l_self_modules_features_modules_denseblock3_modules_denselayer17_modules_norm2_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                         {}\ncall_function  batch_norm_72                                                                                                <function batch_norm at 0x7f3527413be0>                                                                      (bottleneck_output_34, l_self_modules_features_modules_denseblock3_modules_denselayer17_modules_norm2_buffers_running_mean_, l_self_modules_features_modules_denseblock3_modules_denselayer17_modules_norm2_buffers_running_var_, l_self_modules_features_modules_denseblock3_modules_denselayer17_modules_norm2_parameters_weight_, l_self_modules_features_modules_denseblock3_modules_denselayer17_modules_norm2_parameters_bias_, True, 0.1, 1e-05)  {}\ncall_function  relu_72                                                                                                      <function relu at 0x7f3527412c20>                                                                            (batch_norm_72,)                                                                                                                                                                                                                                                                                                                                                                                                                                         {'inplace': True}\ncall_function  new_features_34                                                                                              <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (relu_72, l_self_modules_features_modules_denseblock3_modules_denselayer17_modules_conv2_parameters_weight_, None, (1, 1), (1, 1), (1, 1), 1)                                                                                                                                                                                                                                                                                                            {}\ncall_function  concated_features_35                                                                                         <built-in method cat of type object at 0x7f351f2f6f40>                                                       ([input_14, new_features_18, new_features_19, new_features_20, new_features_21, new_features_22, new_features_23, new_features_24, new_features_25, new_features_26, new_features_27, new_features_28, new_features_29, new_features_30, new_features_31, new_features_32, new_features_33, new_features_34], 1)                                                                                                                                         {}\ncall_method    add__73                                                                                                      add_                                                                                                         (l_self_modules_features_modules_denseblock3_modules_denselayer18_modules_norm1_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                         {}\ncall_function  batch_norm_73                                                                                                <function batch_norm at 0x7f3527413be0>                                                                      (concated_features_35, l_self_modules_features_modules_denseblock3_modules_denselayer18_modules_norm1_buffers_running_mean_, l_self_modules_features_modules_denseblock3_modules_denselayer18_modules_norm1_buffers_running_var_, l_self_modules_features_modules_denseblock3_modules_denselayer18_modules_norm1_parameters_weight_, l_self_modules_features_modules_denseblock3_modules_denselayer18_modules_norm1_parameters_bias_, True, 0.1, 1e-05)  {}\ncall_function  relu_73                                                                                                      <function relu at 0x7f3527412c20>                                                                            (batch_norm_73,)                                                                                                                                                                                                                                                                                                                                                                                                                                         {'inplace': True}\ncall_function  bottleneck_output_35                                                                                         <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (relu_73, l_self_modules_features_modules_denseblock3_modules_denselayer18_modules_conv1_parameters_weight_, None, (1, 1), (0, 0), (1, 1), 1)                                                                                                                                                                                                                                                                                                            {}\ncall_method    add__74                                                                                                      add_                                                                                                         (l_self_modules_features_modules_denseblock3_modules_denselayer18_modules_norm2_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                         {}\ncall_function  batch_norm_74                                                                                                <function batch_norm at 0x7f3527413be0>                                                                      (bottleneck_output_35, l_self_modules_features_modules_denseblock3_modules_denselayer18_modules_norm2_buffers_running_mean_, l_self_modules_features_modules_denseblock3_modules_denselayer18_modules_norm2_buffers_running_var_, l_self_modules_features_modules_denseblock3_modules_denselayer18_modules_norm2_parameters_weight_, l_self_modules_features_modules_denseblock3_modules_denselayer18_modules_norm2_parameters_bias_, True, 0.1, 1e-05)  {}\ncall_function  relu_74                                                                                                      <function relu at 0x7f3527412c20>                                                                            (batch_norm_74,)                                                                                                                                                                                                                                                                                                                                                                                                                                         {'inplace': True}\ncall_function  new_features_35                                                                                              <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (relu_74, l_self_modules_features_modules_denseblock3_modules_denselayer18_modules_conv2_parameters_weight_, None, (1, 1), (1, 1), (1, 1), 1)                                                                                                                                                                                                                                                                                                            {}\ncall_function  concated_features_36                                                                                         <built-in method cat of type object at 0x7f351f2f6f40>                                                       ([input_14, new_features_18, new_features_19, new_features_20, new_features_21, new_features_22, new_features_23, new_features_24, new_features_25, new_features_26, new_features_27, new_features_28, new_features_29, new_features_30, new_features_31, new_features_32, new_features_33, new_features_34, new_features_35], 1)                                                                                                                        {}\ncall_method    add__75                                                                                                      add_                                                                                                         (l_self_modules_features_modules_denseblock3_modules_denselayer19_modules_norm1_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                         {}\ncall_function  batch_norm_75                                                                                                <function batch_norm at 0x7f3527413be0>                                                                      (concated_features_36, l_self_modules_features_modules_denseblock3_modules_denselayer19_modules_norm1_buffers_running_mean_, l_self_modules_features_modules_denseblock3_modules_denselayer19_modules_norm1_buffers_running_var_, l_self_modules_features_modules_denseblock3_modules_denselayer19_modules_norm1_parameters_weight_, l_self_modules_features_modules_denseblock3_modules_denselayer19_modules_norm1_parameters_bias_, True, 0.1, 1e-05)  {}\ncall_function  relu_75                                                                                                      <function relu at 0x7f3527412c20>                                                                            (batch_norm_75,)                                                                                                                                                                                                                                                                                                                                                                                                                                         {'inplace': True}\ncall_function  bottleneck_output_36                                                                                         <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (relu_75, l_self_modules_features_modules_denseblock3_modules_denselayer19_modules_conv1_parameters_weight_, None, (1, 1), (0, 0), (1, 1), 1)                                                                                                                                                                                                                                                                                                            {}\ncall_method    add__76                                                                                                      add_                                                                                                         (l_self_modules_features_modules_denseblock3_modules_denselayer19_modules_norm2_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                         {}\ncall_function  batch_norm_76                                                                                                <function batch_norm at 0x7f3527413be0>                                                                      (bottleneck_output_36, l_self_modules_features_modules_denseblock3_modules_denselayer19_modules_norm2_buffers_running_mean_, l_self_modules_features_modules_denseblock3_modules_denselayer19_modules_norm2_buffers_running_var_, l_self_modules_features_modules_denseblock3_modules_denselayer19_modules_norm2_parameters_weight_, l_self_modules_features_modules_denseblock3_modules_denselayer19_modules_norm2_parameters_bias_, True, 0.1, 1e-05)  {}\ncall_function  relu_76                                                                                                      <function relu at 0x7f3527412c20>                                                                            (batch_norm_76,)                                                                                                                                                                                                                                                                                                                                                                                                                                         {'inplace': True}\ncall_function  new_features_36                                                                                              <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (relu_76, l_self_modules_features_modules_denseblock3_modules_denselayer19_modules_conv2_parameters_weight_, None, (1, 1), (1, 1), (1, 1), 1)                                                                                                                                                                                                                                                                                                            {}\ncall_function  concated_features_37                                                                                         <built-in method cat of type object at 0x7f351f2f6f40>                                                       ([input_14, new_features_18, new_features_19, new_features_20, new_features_21, new_features_22, new_features_23, new_features_24, new_features_25, new_features_26, new_features_27, new_features_28, new_features_29, new_features_30, new_features_31, new_features_32, new_features_33, new_features_34, new_features_35, new_features_36], 1)                                                                                                       {}\ncall_method    add__77                                                                                                      add_                                                                                                         (l_self_modules_features_modules_denseblock3_modules_denselayer20_modules_norm1_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                         {}\ncall_function  batch_norm_77                                                                                                <function batch_norm at 0x7f3527413be0>                                                                      (concated_features_37, l_self_modules_features_modules_denseblock3_modules_denselayer20_modules_norm1_buffers_running_mean_, l_self_modules_features_modules_denseblock3_modules_denselayer20_modules_norm1_buffers_running_var_, l_self_modules_features_modules_denseblock3_modules_denselayer20_modules_norm1_parameters_weight_, l_self_modules_features_modules_denseblock3_modules_denselayer20_modules_norm1_parameters_bias_, True, 0.1, 1e-05)  {}\ncall_function  relu_77                                                                                                      <function relu at 0x7f3527412c20>                                                                            (batch_norm_77,)                                                                                                                                                                                                                                                                                                                                                                                                                                         {'inplace': True}\ncall_function  bottleneck_output_37                                                                                         <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (relu_77, l_self_modules_features_modules_denseblock3_modules_denselayer20_modules_conv1_parameters_weight_, None, (1, 1), (0, 0), (1, 1), 1)                                                                                                                                                                                                                                                                                                            {}\ncall_method    add__78                                                                                                      add_                                                                                                         (l_self_modules_features_modules_denseblock3_modules_denselayer20_modules_norm2_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                         {}\ncall_function  batch_norm_78                                                                                                <function batch_norm at 0x7f3527413be0>                                                                      (bottleneck_output_37, l_self_modules_features_modules_denseblock3_modules_denselayer20_modules_norm2_buffers_running_mean_, l_self_modules_features_modules_denseblock3_modules_denselayer20_modules_norm2_buffers_running_var_, l_self_modules_features_modules_denseblock3_modules_denselayer20_modules_norm2_parameters_weight_, l_self_modules_features_modules_denseblock3_modules_denselayer20_modules_norm2_parameters_bias_, True, 0.1, 1e-05)  {}\ncall_function  relu_78                                                                                                      <function relu at 0x7f3527412c20>                                                                            (batch_norm_78,)                                                                                                                                                                                                                                                                                                                                                                                                                                         {'inplace': True}\ncall_function  new_features_37                                                                                              <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (relu_78, l_self_modules_features_modules_denseblock3_modules_denselayer20_modules_conv2_parameters_weight_, None, (1, 1), (1, 1), (1, 1), 1)                                                                                                                                                                                                                                                                                                            {}\ncall_function  concated_features_38                                                                                         <built-in method cat of type object at 0x7f351f2f6f40>                                                       ([input_14, new_features_18, new_features_19, new_features_20, new_features_21, new_features_22, new_features_23, new_features_24, new_features_25, new_features_26, new_features_27, new_features_28, new_features_29, new_features_30, new_features_31, new_features_32, new_features_33, new_features_34, new_features_35, new_features_36, new_features_37], 1)                                                                                      {}\ncall_method    add__79                                                                                                      add_                                                                                                         (l_self_modules_features_modules_denseblock3_modules_denselayer21_modules_norm1_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                         {}\ncall_function  batch_norm_79                                                                                                <function batch_norm at 0x7f3527413be0>                                                                      (concated_features_38, l_self_modules_features_modules_denseblock3_modules_denselayer21_modules_norm1_buffers_running_mean_, l_self_modules_features_modules_denseblock3_modules_denselayer21_modules_norm1_buffers_running_var_, l_self_modules_features_modules_denseblock3_modules_denselayer21_modules_norm1_parameters_weight_, l_self_modules_features_modules_denseblock3_modules_denselayer21_modules_norm1_parameters_bias_, True, 0.1, 1e-05)  {}\ncall_function  relu_79                                                                                                      <function relu at 0x7f3527412c20>                                                                            (batch_norm_79,)                                                                                                                                                                                                                                                                                                                                                                                                                                         {'inplace': True}\ncall_function  bottleneck_output_38                                                                                         <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (relu_79, l_self_modules_features_modules_denseblock3_modules_denselayer21_modules_conv1_parameters_weight_, None, (1, 1), (0, 0), (1, 1), 1)                                                                                                                                                                                                                                                                                                            {}\ncall_method    add__80                                                                                                      add_                                                                                                         (l_self_modules_features_modules_denseblock3_modules_denselayer21_modules_norm2_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                         {}\ncall_function  batch_norm_80                                                                                                <function batch_norm at 0x7f3527413be0>                                                                      (bottleneck_output_38, l_self_modules_features_modules_denseblock3_modules_denselayer21_modules_norm2_buffers_running_mean_, l_self_modules_features_modules_denseblock3_modules_denselayer21_modules_norm2_buffers_running_var_, l_self_modules_features_modules_denseblock3_modules_denselayer21_modules_norm2_parameters_weight_, l_self_modules_features_modules_denseblock3_modules_denselayer21_modules_norm2_parameters_bias_, True, 0.1, 1e-05)  {}\ncall_function  relu_80                                                                                                      <function relu at 0x7f3527412c20>                                                                            (batch_norm_80,)                                                                                                                                                                                                                                                                                                                                                                                                                                         {'inplace': True}\ncall_function  new_features_38                                                                                              <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (relu_80, l_self_modules_features_modules_denseblock3_modules_denselayer21_modules_conv2_parameters_weight_, None, (1, 1), (1, 1), (1, 1), 1)                                                                                                                                                                                                                                                                                                            {}\ncall_function  concated_features_39                                                                                         <built-in method cat of type object at 0x7f351f2f6f40>                                                       ([input_14, new_features_18, new_features_19, new_features_20, new_features_21, new_features_22, new_features_23, new_features_24, new_features_25, new_features_26, new_features_27, new_features_28, new_features_29, new_features_30, new_features_31, new_features_32, new_features_33, new_features_34, new_features_35, new_features_36, new_features_37, new_features_38], 1)                                                                     {}\ncall_method    add__81                                                                                                      add_                                                                                                         (l_self_modules_features_modules_denseblock3_modules_denselayer22_modules_norm1_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                         {}\ncall_function  batch_norm_81                                                                                                <function batch_norm at 0x7f3527413be0>                                                                      (concated_features_39, l_self_modules_features_modules_denseblock3_modules_denselayer22_modules_norm1_buffers_running_mean_, l_self_modules_features_modules_denseblock3_modules_denselayer22_modules_norm1_buffers_running_var_, l_self_modules_features_modules_denseblock3_modules_denselayer22_modules_norm1_parameters_weight_, l_self_modules_features_modules_denseblock3_modules_denselayer22_modules_norm1_parameters_bias_, True, 0.1, 1e-05)  {}\ncall_function  relu_81                                                                                                      <function relu at 0x7f3527412c20>                                                                            (batch_norm_81,)                                                                                                                                                                                                                                                                                                                                                                                                                                         {'inplace': True}\ncall_function  bottleneck_output_39                                                                                         <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (relu_81, l_self_modules_features_modules_denseblock3_modules_denselayer22_modules_conv1_parameters_weight_, None, (1, 1), (0, 0), (1, 1), 1)                                                                                                                                                                                                                                                                                                            {}\ncall_method    add__82                                                                                                      add_                                                                                                         (l_self_modules_features_modules_denseblock3_modules_denselayer22_modules_norm2_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                         {}\ncall_function  batch_norm_82                                                                                                <function batch_norm at 0x7f3527413be0>                                                                      (bottleneck_output_39, l_self_modules_features_modules_denseblock3_modules_denselayer22_modules_norm2_buffers_running_mean_, l_self_modules_features_modules_denseblock3_modules_denselayer22_modules_norm2_buffers_running_var_, l_self_modules_features_modules_denseblock3_modules_denselayer22_modules_norm2_parameters_weight_, l_self_modules_features_modules_denseblock3_modules_denselayer22_modules_norm2_parameters_bias_, True, 0.1, 1e-05)  {}\ncall_function  relu_82                                                                                                      <function relu at 0x7f3527412c20>                                                                            (batch_norm_82,)                                                                                                                                                                                                                                                                                                                                                                                                                                         {'inplace': True}\ncall_function  new_features_39                                                                                              <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (relu_82, l_self_modules_features_modules_denseblock3_modules_denselayer22_modules_conv2_parameters_weight_, None, (1, 1), (1, 1), (1, 1), 1)                                                                                                                                                                                                                                                                                                            {}\ncall_function  concated_features_40                                                                                         <built-in method cat of type object at 0x7f351f2f6f40>                                                       ([input_14, new_features_18, new_features_19, new_features_20, new_features_21, new_features_22, new_features_23, new_features_24, new_features_25, new_features_26, new_features_27, new_features_28, new_features_29, new_features_30, new_features_31, new_features_32, new_features_33, new_features_34, new_features_35, new_features_36, new_features_37, new_features_38, new_features_39], 1)                                                    {}\ncall_method    add__83                                                                                                      add_                                                                                                         (l_self_modules_features_modules_denseblock3_modules_denselayer23_modules_norm1_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                         {}\ncall_function  batch_norm_83                                                                                                <function batch_norm at 0x7f3527413be0>                                                                      (concated_features_40, l_self_modules_features_modules_denseblock3_modules_denselayer23_modules_norm1_buffers_running_mean_, l_self_modules_features_modules_denseblock3_modules_denselayer23_modules_norm1_buffers_running_var_, l_self_modules_features_modules_denseblock3_modules_denselayer23_modules_norm1_parameters_weight_, l_self_modules_features_modules_denseblock3_modules_denselayer23_modules_norm1_parameters_bias_, True, 0.1, 1e-05)  {}\ncall_function  relu_83                                                                                                      <function relu at 0x7f3527412c20>                                                                            (batch_norm_83,)                                                                                                                                                                                                                                                                                                                                                                                                                                         {'inplace': True}\ncall_function  bottleneck_output_40                                                                                         <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (relu_83, l_self_modules_features_modules_denseblock3_modules_denselayer23_modules_conv1_parameters_weight_, None, (1, 1), (0, 0), (1, 1), 1)                                                                                                                                                                                                                                                                                                            {}\ncall_method    add__84                                                                                                      add_                                                                                                         (l_self_modules_features_modules_denseblock3_modules_denselayer23_modules_norm2_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                         {}\ncall_function  batch_norm_84                                                                                                <function batch_norm at 0x7f3527413be0>                                                                      (bottleneck_output_40, l_self_modules_features_modules_denseblock3_modules_denselayer23_modules_norm2_buffers_running_mean_, l_self_modules_features_modules_denseblock3_modules_denselayer23_modules_norm2_buffers_running_var_, l_self_modules_features_modules_denseblock3_modules_denselayer23_modules_norm2_parameters_weight_, l_self_modules_features_modules_denseblock3_modules_denselayer23_modules_norm2_parameters_bias_, True, 0.1, 1e-05)  {}\ncall_function  relu_84                                                                                                      <function relu at 0x7f3527412c20>                                                                            (batch_norm_84,)                                                                                                                                                                                                                                                                                                                                                                                                                                         {'inplace': True}\ncall_function  new_features_40                                                                                              <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (relu_84, l_self_modules_features_modules_denseblock3_modules_denselayer23_modules_conv2_parameters_weight_, None, (1, 1), (1, 1), (1, 1), 1)                                                                                                                                                                                                                                                                                                            {}\ncall_function  concated_features_41                                                                                         <built-in method cat of type object at 0x7f351f2f6f40>                                                       ([input_14, new_features_18, new_features_19, new_features_20, new_features_21, new_features_22, new_features_23, new_features_24, new_features_25, new_features_26, new_features_27, new_features_28, new_features_29, new_features_30, new_features_31, new_features_32, new_features_33, new_features_34, new_features_35, new_features_36, new_features_37, new_features_38, new_features_39, new_features_40], 1)                                   {}\ncall_method    add__85                                                                                                      add_                                                                                                         (l_self_modules_features_modules_denseblock3_modules_denselayer24_modules_norm1_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                         {}\ncall_function  batch_norm_85                                                                                                <function batch_norm at 0x7f3527413be0>                                                                      (concated_features_41, l_self_modules_features_modules_denseblock3_modules_denselayer24_modules_norm1_buffers_running_mean_, l_self_modules_features_modules_denseblock3_modules_denselayer24_modules_norm1_buffers_running_var_, l_self_modules_features_modules_denseblock3_modules_denselayer24_modules_norm1_parameters_weight_, l_self_modules_features_modules_denseblock3_modules_denselayer24_modules_norm1_parameters_bias_, True, 0.1, 1e-05)  {}\ncall_function  relu_85                                                                                                      <function relu at 0x7f3527412c20>                                                                            (batch_norm_85,)                                                                                                                                                                                                                                                                                                                                                                                                                                         {'inplace': True}\ncall_function  bottleneck_output_41                                                                                         <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (relu_85, l_self_modules_features_modules_denseblock3_modules_denselayer24_modules_conv1_parameters_weight_, None, (1, 1), (0, 0), (1, 1), 1)                                                                                                                                                                                                                                                                                                            {}\ncall_method    add__86                                                                                                      add_                                                                                                         (l_self_modules_features_modules_denseblock3_modules_denselayer24_modules_norm2_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                         {}\ncall_function  batch_norm_86                                                                                                <function batch_norm at 0x7f3527413be0>                                                                      (bottleneck_output_41, l_self_modules_features_modules_denseblock3_modules_denselayer24_modules_norm2_buffers_running_mean_, l_self_modules_features_modules_denseblock3_modules_denselayer24_modules_norm2_buffers_running_var_, l_self_modules_features_modules_denseblock3_modules_denselayer24_modules_norm2_parameters_weight_, l_self_modules_features_modules_denseblock3_modules_denselayer24_modules_norm2_parameters_bias_, True, 0.1, 1e-05)  {}\ncall_function  relu_86                                                                                                      <function relu at 0x7f3527412c20>                                                                            (batch_norm_86,)                                                                                                                                                                                                                                                                                                                                                                                                                                         {'inplace': True}\ncall_function  new_features_41                                                                                              <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (relu_86, l_self_modules_features_modules_denseblock3_modules_denselayer24_modules_conv2_parameters_weight_, None, (1, 1), (1, 1), (1, 1), 1)                                                                                                                                                                                                                                                                                                            {}\ncall_function  input_15                                                                                                     <built-in method cat of type object at 0x7f351f2f6f40>                                                       ([input_14, new_features_18, new_features_19, new_features_20, new_features_21, new_features_22, new_features_23, new_features_24, new_features_25, new_features_26, new_features_27, new_features_28, new_features_29, new_features_30, new_features_31, new_features_32, new_features_33, new_features_34, new_features_35, new_features_36, new_features_37, new_features_38, new_features_39, new_features_40, new_features_41], 1)                  {}\ncall_method    add__87                                                                                                      add_                                                                                                         (l_self_modules_features_modules_transition3_modules_norm_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                                               {}\ncall_function  input_16                                                                                                     <function batch_norm at 0x7f3527413be0>                                                                      (input_15, l_self_modules_features_modules_transition3_modules_norm_buffers_running_mean_, l_self_modules_features_modules_transition3_modules_norm_buffers_running_var_, l_self_modules_features_modules_transition3_modules_norm_parameters_weight_, l_self_modules_features_modules_transition3_modules_norm_parameters_bias_, True, 0.1, 1e-05)                                                                                                      {}\ncall_function  input_17                                                                                                     <function relu at 0x7f3527412c20>                                                                            (input_16,)                                                                                                                                                                                                                                                                                                                                                                                                                                              {'inplace': True}\ncall_function  input_18                                                                                                     <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (input_17, l_self_modules_features_modules_transition3_modules_conv_parameters_weight_, None, (1, 1), (0, 0), (1, 1), 1)                                                                                                                                                                                                                                                                                                                                 {}\ncall_function  input_19                                                                                                     <built-in function avg_pool2d>                                                                               (input_18, 2, 2, 0, False, True, None)                                                                                                                                                                                                                                                                                                                                                                                                                   {}\ncall_function  concated_features_42                                                                                         <built-in method cat of type object at 0x7f351f2f6f40>                                                       ([input_19], 1)                                                                                                                                                                                                                                                                                                                                                                                                                                          {}\ncall_method    add__88                                                                                                      add_                                                                                                         (l_self_modules_features_modules_denseblock4_modules_denselayer1_modules_norm1_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                          {}\ncall_function  batch_norm_88                                                                                                <function batch_norm at 0x7f3527413be0>                                                                      (concated_features_42, l_self_modules_features_modules_denseblock4_modules_denselayer1_modules_norm1_buffers_running_mean_, l_self_modules_features_modules_denseblock4_modules_denselayer1_modules_norm1_buffers_running_var_, l_self_modules_features_modules_denseblock4_modules_denselayer1_modules_norm1_parameters_weight_, l_self_modules_features_modules_denseblock4_modules_denselayer1_modules_norm1_parameters_bias_, True, 0.1, 1e-05)      {}\ncall_function  relu_88                                                                                                      <function relu at 0x7f3527412c20>                                                                            (batch_norm_88,)                                                                                                                                                                                                                                                                                                                                                                                                                                         {'inplace': True}\ncall_function  bottleneck_output_42                                                                                         <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (relu_88, l_self_modules_features_modules_denseblock4_modules_denselayer1_modules_conv1_parameters_weight_, None, (1, 1), (0, 0), (1, 1), 1)                                                                                                                                                                                                                                                                                                             {}\ncall_method    add__89                                                                                                      add_                                                                                                         (l_self_modules_features_modules_denseblock4_modules_denselayer1_modules_norm2_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                          {}\ncall_function  batch_norm_89                                                                                                <function batch_norm at 0x7f3527413be0>                                                                      (bottleneck_output_42, l_self_modules_features_modules_denseblock4_modules_denselayer1_modules_norm2_buffers_running_mean_, l_self_modules_features_modules_denseblock4_modules_denselayer1_modules_norm2_buffers_running_var_, l_self_modules_features_modules_denseblock4_modules_denselayer1_modules_norm2_parameters_weight_, l_self_modules_features_modules_denseblock4_modules_denselayer1_modules_norm2_parameters_bias_, True, 0.1, 1e-05)      {}\ncall_function  relu_89                                                                                                      <function relu at 0x7f3527412c20>                                                                            (batch_norm_89,)                                                                                                                                                                                                                                                                                                                                                                                                                                         {'inplace': True}\ncall_function  new_features_42                                                                                              <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (relu_89, l_self_modules_features_modules_denseblock4_modules_denselayer1_modules_conv2_parameters_weight_, None, (1, 1), (1, 1), (1, 1), 1)                                                                                                                                                                                                                                                                                                             {}\ncall_function  concated_features_43                                                                                         <built-in method cat of type object at 0x7f351f2f6f40>                                                       ([input_19, new_features_42], 1)                                                                                                                                                                                                                                                                                                                                                                                                                         {}\ncall_method    add__90                                                                                                      add_                                                                                                         (l_self_modules_features_modules_denseblock4_modules_denselayer2_modules_norm1_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                          {}\ncall_function  batch_norm_90                                                                                                <function batch_norm at 0x7f3527413be0>                                                                      (concated_features_43, l_self_modules_features_modules_denseblock4_modules_denselayer2_modules_norm1_buffers_running_mean_, l_self_modules_features_modules_denseblock4_modules_denselayer2_modules_norm1_buffers_running_var_, l_self_modules_features_modules_denseblock4_modules_denselayer2_modules_norm1_parameters_weight_, l_self_modules_features_modules_denseblock4_modules_denselayer2_modules_norm1_parameters_bias_, True, 0.1, 1e-05)      {}\ncall_function  relu_90                                                                                                      <function relu at 0x7f3527412c20>                                                                            (batch_norm_90,)                                                                                                                                                                                                                                                                                                                                                                                                                                         {'inplace': True}\ncall_function  bottleneck_output_43                                                                                         <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (relu_90, l_self_modules_features_modules_denseblock4_modules_denselayer2_modules_conv1_parameters_weight_, None, (1, 1), (0, 0), (1, 1), 1)                                                                                                                                                                                                                                                                                                             {}\ncall_method    add__91                                                                                                      add_                                                                                                         (l_self_modules_features_modules_denseblock4_modules_denselayer2_modules_norm2_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                          {}\ncall_function  batch_norm_91                                                                                                <function batch_norm at 0x7f3527413be0>                                                                      (bottleneck_output_43, l_self_modules_features_modules_denseblock4_modules_denselayer2_modules_norm2_buffers_running_mean_, l_self_modules_features_modules_denseblock4_modules_denselayer2_modules_norm2_buffers_running_var_, l_self_modules_features_modules_denseblock4_modules_denselayer2_modules_norm2_parameters_weight_, l_self_modules_features_modules_denseblock4_modules_denselayer2_modules_norm2_parameters_bias_, True, 0.1, 1e-05)      {}\ncall_function  relu_91                                                                                                      <function relu at 0x7f3527412c20>                                                                            (batch_norm_91,)                                                                                                                                                                                                                                                                                                                                                                                                                                         {'inplace': True}\ncall_function  new_features_43                                                                                              <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (relu_91, l_self_modules_features_modules_denseblock4_modules_denselayer2_modules_conv2_parameters_weight_, None, (1, 1), (1, 1), (1, 1), 1)                                                                                                                                                                                                                                                                                                             {}\ncall_function  concated_features_44                                                                                         <built-in method cat of type object at 0x7f351f2f6f40>                                                       ([input_19, new_features_42, new_features_43], 1)                                                                                                                                                                                                                                                                                                                                                                                                        {}\ncall_method    add__92                                                                                                      add_                                                                                                         (l_self_modules_features_modules_denseblock4_modules_denselayer3_modules_norm1_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                          {}\ncall_function  batch_norm_92                                                                                                <function batch_norm at 0x7f3527413be0>                                                                      (concated_features_44, l_self_modules_features_modules_denseblock4_modules_denselayer3_modules_norm1_buffers_running_mean_, l_self_modules_features_modules_denseblock4_modules_denselayer3_modules_norm1_buffers_running_var_, l_self_modules_features_modules_denseblock4_modules_denselayer3_modules_norm1_parameters_weight_, l_self_modules_features_modules_denseblock4_modules_denselayer3_modules_norm1_parameters_bias_, True, 0.1, 1e-05)      {}\ncall_function  relu_92                                                                                                      <function relu at 0x7f3527412c20>                                                                            (batch_norm_92,)                                                                                                                                                                                                                                                                                                                                                                                                                                         {'inplace': True}\ncall_function  bottleneck_output_44                                                                                         <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (relu_92, l_self_modules_features_modules_denseblock4_modules_denselayer3_modules_conv1_parameters_weight_, None, (1, 1), (0, 0), (1, 1), 1)                                                                                                                                                                                                                                                                                                             {}\ncall_method    add__93                                                                                                      add_                                                                                                         (l_self_modules_features_modules_denseblock4_modules_denselayer3_modules_norm2_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                          {}\ncall_function  batch_norm_93                                                                                                <function batch_norm at 0x7f3527413be0>                                                                      (bottleneck_output_44, l_self_modules_features_modules_denseblock4_modules_denselayer3_modules_norm2_buffers_running_mean_, l_self_modules_features_modules_denseblock4_modules_denselayer3_modules_norm2_buffers_running_var_, l_self_modules_features_modules_denseblock4_modules_denselayer3_modules_norm2_parameters_weight_, l_self_modules_features_modules_denseblock4_modules_denselayer3_modules_norm2_parameters_bias_, True, 0.1, 1e-05)      {}\ncall_function  relu_93                                                                                                      <function relu at 0x7f3527412c20>                                                                            (batch_norm_93,)                                                                                                                                                                                                                                                                                                                                                                                                                                         {'inplace': True}\ncall_function  new_features_44                                                                                              <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (relu_93, l_self_modules_features_modules_denseblock4_modules_denselayer3_modules_conv2_parameters_weight_, None, (1, 1), (1, 1), (1, 1), 1)                                                                                                                                                                                                                                                                                                             {}\ncall_function  concated_features_45                                                                                         <built-in method cat of type object at 0x7f351f2f6f40>                                                       ([input_19, new_features_42, new_features_43, new_features_44], 1)                                                                                                                                                                                                                                                                                                                                                                                       {}\ncall_method    add__94                                                                                                      add_                                                                                                         (l_self_modules_features_modules_denseblock4_modules_denselayer4_modules_norm1_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                          {}\ncall_function  batch_norm_94                                                                                                <function batch_norm at 0x7f3527413be0>                                                                      (concated_features_45, l_self_modules_features_modules_denseblock4_modules_denselayer4_modules_norm1_buffers_running_mean_, l_self_modules_features_modules_denseblock4_modules_denselayer4_modules_norm1_buffers_running_var_, l_self_modules_features_modules_denseblock4_modules_denselayer4_modules_norm1_parameters_weight_, l_self_modules_features_modules_denseblock4_modules_denselayer4_modules_norm1_parameters_bias_, True, 0.1, 1e-05)      {}\ncall_function  relu_94                                                                                                      <function relu at 0x7f3527412c20>                                                                            (batch_norm_94,)                                                                                                                                                                                                                                                                                                                                                                                                                                         {'inplace': True}\ncall_function  bottleneck_output_45                                                                                         <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (relu_94, l_self_modules_features_modules_denseblock4_modules_denselayer4_modules_conv1_parameters_weight_, None, (1, 1), (0, 0), (1, 1), 1)                                                                                                                                                                                                                                                                                                             {}\ncall_method    add__95                                                                                                      add_                                                                                                         (l_self_modules_features_modules_denseblock4_modules_denselayer4_modules_norm2_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                          {}\ncall_function  batch_norm_95                                                                                                <function batch_norm at 0x7f3527413be0>                                                                      (bottleneck_output_45, l_self_modules_features_modules_denseblock4_modules_denselayer4_modules_norm2_buffers_running_mean_, l_self_modules_features_modules_denseblock4_modules_denselayer4_modules_norm2_buffers_running_var_, l_self_modules_features_modules_denseblock4_modules_denselayer4_modules_norm2_parameters_weight_, l_self_modules_features_modules_denseblock4_modules_denselayer4_modules_norm2_parameters_bias_, True, 0.1, 1e-05)      {}\ncall_function  relu_95                                                                                                      <function relu at 0x7f3527412c20>                                                                            (batch_norm_95,)                                                                                                                                                                                                                                                                                                                                                                                                                                         {'inplace': True}\ncall_function  new_features_45                                                                                              <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (relu_95, l_self_modules_features_modules_denseblock4_modules_denselayer4_modules_conv2_parameters_weight_, None, (1, 1), (1, 1), (1, 1), 1)                                                                                                                                                                                                                                                                                                             {}\ncall_function  concated_features_46                                                                                         <built-in method cat of type object at 0x7f351f2f6f40>                                                       ([input_19, new_features_42, new_features_43, new_features_44, new_features_45], 1)                                                                                                                                                                                                                                                                                                                                                                      {}\ncall_method    add__96                                                                                                      add_                                                                                                         (l_self_modules_features_modules_denseblock4_modules_denselayer5_modules_norm1_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                          {}\ncall_function  batch_norm_96                                                                                                <function batch_norm at 0x7f3527413be0>                                                                      (concated_features_46, l_self_modules_features_modules_denseblock4_modules_denselayer5_modules_norm1_buffers_running_mean_, l_self_modules_features_modules_denseblock4_modules_denselayer5_modules_norm1_buffers_running_var_, l_self_modules_features_modules_denseblock4_modules_denselayer5_modules_norm1_parameters_weight_, l_self_modules_features_modules_denseblock4_modules_denselayer5_modules_norm1_parameters_bias_, True, 0.1, 1e-05)      {}\ncall_function  relu_96                                                                                                      <function relu at 0x7f3527412c20>                                                                            (batch_norm_96,)                                                                                                                                                                                                                                                                                                                                                                                                                                         {'inplace': True}\ncall_function  bottleneck_output_46                                                                                         <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (relu_96, l_self_modules_features_modules_denseblock4_modules_denselayer5_modules_conv1_parameters_weight_, None, (1, 1), (0, 0), (1, 1), 1)                                                                                                                                                                                                                                                                                                             {}\ncall_method    add__97                                                                                                      add_                                                                                                         (l_self_modules_features_modules_denseblock4_modules_denselayer5_modules_norm2_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                          {}\ncall_function  batch_norm_97                                                                                                <function batch_norm at 0x7f3527413be0>                                                                      (bottleneck_output_46, l_self_modules_features_modules_denseblock4_modules_denselayer5_modules_norm2_buffers_running_mean_, l_self_modules_features_modules_denseblock4_modules_denselayer5_modules_norm2_buffers_running_var_, l_self_modules_features_modules_denseblock4_modules_denselayer5_modules_norm2_parameters_weight_, l_self_modules_features_modules_denseblock4_modules_denselayer5_modules_norm2_parameters_bias_, True, 0.1, 1e-05)      {}\ncall_function  relu_97                                                                                                      <function relu at 0x7f3527412c20>                                                                            (batch_norm_97,)                                                                                                                                                                                                                                                                                                                                                                                                                                         {'inplace': True}\ncall_function  new_features_46                                                                                              <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (relu_97, l_self_modules_features_modules_denseblock4_modules_denselayer5_modules_conv2_parameters_weight_, None, (1, 1), (1, 1), (1, 1), 1)                                                                                                                                                                                                                                                                                                             {}\ncall_function  concated_features_47                                                                                         <built-in method cat of type object at 0x7f351f2f6f40>                                                       ([input_19, new_features_42, new_features_43, new_features_44, new_features_45, new_features_46], 1)                                                                                                                                                                                                                                                                                                                                                     {}\ncall_method    add__98                                                                                                      add_                                                                                                         (l_self_modules_features_modules_denseblock4_modules_denselayer6_modules_norm1_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                          {}\ncall_function  batch_norm_98                                                                                                <function batch_norm at 0x7f3527413be0>                                                                      (concated_features_47, l_self_modules_features_modules_denseblock4_modules_denselayer6_modules_norm1_buffers_running_mean_, l_self_modules_features_modules_denseblock4_modules_denselayer6_modules_norm1_buffers_running_var_, l_self_modules_features_modules_denseblock4_modules_denselayer6_modules_norm1_parameters_weight_, l_self_modules_features_modules_denseblock4_modules_denselayer6_modules_norm1_parameters_bias_, True, 0.1, 1e-05)      {}\ncall_function  relu_98                                                                                                      <function relu at 0x7f3527412c20>                                                                            (batch_norm_98,)                                                                                                                                                                                                                                                                                                                                                                                                                                         {'inplace': True}\ncall_function  bottleneck_output_47                                                                                         <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (relu_98, l_self_modules_features_modules_denseblock4_modules_denselayer6_modules_conv1_parameters_weight_, None, (1, 1), (0, 0), (1, 1), 1)                                                                                                                                                                                                                                                                                                             {}\ncall_method    add__99                                                                                                      add_                                                                                                         (l_self_modules_features_modules_denseblock4_modules_denselayer6_modules_norm2_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                          {}\ncall_function  batch_norm_99                                                                                                <function batch_norm at 0x7f3527413be0>                                                                      (bottleneck_output_47, l_self_modules_features_modules_denseblock4_modules_denselayer6_modules_norm2_buffers_running_mean_, l_self_modules_features_modules_denseblock4_modules_denselayer6_modules_norm2_buffers_running_var_, l_self_modules_features_modules_denseblock4_modules_denselayer6_modules_norm2_parameters_weight_, l_self_modules_features_modules_denseblock4_modules_denselayer6_modules_norm2_parameters_bias_, True, 0.1, 1e-05)      {}\ncall_function  relu_99                                                                                                      <function relu at 0x7f3527412c20>                                                                            (batch_norm_99,)                                                                                                                                                                                                                                                                                                                                                                                                                                         {'inplace': True}\ncall_function  new_features_47                                                                                              <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (relu_99, l_self_modules_features_modules_denseblock4_modules_denselayer6_modules_conv2_parameters_weight_, None, (1, 1), (1, 1), (1, 1), 1)                                                                                                                                                                                                                                                                                                             {}\ncall_function  concated_features_48                                                                                         <built-in method cat of type object at 0x7f351f2f6f40>                                                       ([input_19, new_features_42, new_features_43, new_features_44, new_features_45, new_features_46, new_features_47], 1)                                                                                                                                                                                                                                                                                                                                    {}\ncall_method    add__100                                                                                                     add_                                                                                                         (l_self_modules_features_modules_denseblock4_modules_denselayer7_modules_norm1_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                          {}\ncall_function  batch_norm_100                                                                                               <function batch_norm at 0x7f3527413be0>                                                                      (concated_features_48, l_self_modules_features_modules_denseblock4_modules_denselayer7_modules_norm1_buffers_running_mean_, l_self_modules_features_modules_denseblock4_modules_denselayer7_modules_norm1_buffers_running_var_, l_self_modules_features_modules_denseblock4_modules_denselayer7_modules_norm1_parameters_weight_, l_self_modules_features_modules_denseblock4_modules_denselayer7_modules_norm1_parameters_bias_, True, 0.1, 1e-05)      {}\ncall_function  relu_100                                                                                                     <function relu at 0x7f3527412c20>                                                                            (batch_norm_100,)                                                                                                                                                                                                                                                                                                                                                                                                                                        {'inplace': True}\ncall_function  bottleneck_output_48                                                                                         <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (relu_100, l_self_modules_features_modules_denseblock4_modules_denselayer7_modules_conv1_parameters_weight_, None, (1, 1), (0, 0), (1, 1), 1)                                                                                                                                                                                                                                                                                                            {}\ncall_method    add__101                                                                                                     add_                                                                                                         (l_self_modules_features_modules_denseblock4_modules_denselayer7_modules_norm2_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                          {}\ncall_function  batch_norm_101                                                                                               <function batch_norm at 0x7f3527413be0>                                                                      (bottleneck_output_48, l_self_modules_features_modules_denseblock4_modules_denselayer7_modules_norm2_buffers_running_mean_, l_self_modules_features_modules_denseblock4_modules_denselayer7_modules_norm2_buffers_running_var_, l_self_modules_features_modules_denseblock4_modules_denselayer7_modules_norm2_parameters_weight_, l_self_modules_features_modules_denseblock4_modules_denselayer7_modules_norm2_parameters_bias_, True, 0.1, 1e-05)      {}\ncall_function  relu_101                                                                                                     <function relu at 0x7f3527412c20>                                                                            (batch_norm_101,)                                                                                                                                                                                                                                                                                                                                                                                                                                        {'inplace': True}\ncall_function  new_features_48                                                                                              <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (relu_101, l_self_modules_features_modules_denseblock4_modules_denselayer7_modules_conv2_parameters_weight_, None, (1, 1), (1, 1), (1, 1), 1)                                                                                                                                                                                                                                                                                                            {}\ncall_function  concated_features_49                                                                                         <built-in method cat of type object at 0x7f351f2f6f40>                                                       ([input_19, new_features_42, new_features_43, new_features_44, new_features_45, new_features_46, new_features_47, new_features_48], 1)                                                                                                                                                                                                                                                                                                                   {}\ncall_method    add__102                                                                                                     add_                                                                                                         (l_self_modules_features_modules_denseblock4_modules_denselayer8_modules_norm1_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                          {}\ncall_function  batch_norm_102                                                                                               <function batch_norm at 0x7f3527413be0>                                                                      (concated_features_49, l_self_modules_features_modules_denseblock4_modules_denselayer8_modules_norm1_buffers_running_mean_, l_self_modules_features_modules_denseblock4_modules_denselayer8_modules_norm1_buffers_running_var_, l_self_modules_features_modules_denseblock4_modules_denselayer8_modules_norm1_parameters_weight_, l_self_modules_features_modules_denseblock4_modules_denselayer8_modules_norm1_parameters_bias_, True, 0.1, 1e-05)      {}\ncall_function  relu_102                                                                                                     <function relu at 0x7f3527412c20>                                                                            (batch_norm_102,)                                                                                                                                                                                                                                                                                                                                                                                                                                        {'inplace': True}\ncall_function  bottleneck_output_49                                                                                         <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (relu_102, l_self_modules_features_modules_denseblock4_modules_denselayer8_modules_conv1_parameters_weight_, None, (1, 1), (0, 0), (1, 1), 1)                                                                                                                                                                                                                                                                                                            {}\ncall_method    add__103                                                                                                     add_                                                                                                         (l_self_modules_features_modules_denseblock4_modules_denselayer8_modules_norm2_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                          {}\ncall_function  batch_norm_103                                                                                               <function batch_norm at 0x7f3527413be0>                                                                      (bottleneck_output_49, l_self_modules_features_modules_denseblock4_modules_denselayer8_modules_norm2_buffers_running_mean_, l_self_modules_features_modules_denseblock4_modules_denselayer8_modules_norm2_buffers_running_var_, l_self_modules_features_modules_denseblock4_modules_denselayer8_modules_norm2_parameters_weight_, l_self_modules_features_modules_denseblock4_modules_denselayer8_modules_norm2_parameters_bias_, True, 0.1, 1e-05)      {}\ncall_function  relu_103                                                                                                     <function relu at 0x7f3527412c20>                                                                            (batch_norm_103,)                                                                                                                                                                                                                                                                                                                                                                                                                                        {'inplace': True}\ncall_function  new_features_49                                                                                              <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (relu_103, l_self_modules_features_modules_denseblock4_modules_denselayer8_modules_conv2_parameters_weight_, None, (1, 1), (1, 1), (1, 1), 1)                                                                                                                                                                                                                                                                                                            {}\ncall_function  concated_features_50                                                                                         <built-in method cat of type object at 0x7f351f2f6f40>                                                       ([input_19, new_features_42, new_features_43, new_features_44, new_features_45, new_features_46, new_features_47, new_features_48, new_features_49], 1)                                                                                                                                                                                                                                                                                                  {}\ncall_method    add__104                                                                                                     add_                                                                                                         (l_self_modules_features_modules_denseblock4_modules_denselayer9_modules_norm1_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                          {}\ncall_function  batch_norm_104                                                                                               <function batch_norm at 0x7f3527413be0>                                                                      (concated_features_50, l_self_modules_features_modules_denseblock4_modules_denselayer9_modules_norm1_buffers_running_mean_, l_self_modules_features_modules_denseblock4_modules_denselayer9_modules_norm1_buffers_running_var_, l_self_modules_features_modules_denseblock4_modules_denselayer9_modules_norm1_parameters_weight_, l_self_modules_features_modules_denseblock4_modules_denselayer9_modules_norm1_parameters_bias_, True, 0.1, 1e-05)      {}\ncall_function  relu_104                                                                                                     <function relu at 0x7f3527412c20>                                                                            (batch_norm_104,)                                                                                                                                                                                                                                                                                                                                                                                                                                        {'inplace': True}\ncall_function  bottleneck_output_50                                                                                         <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (relu_104, l_self_modules_features_modules_denseblock4_modules_denselayer9_modules_conv1_parameters_weight_, None, (1, 1), (0, 0), (1, 1), 1)                                                                                                                                                                                                                                                                                                            {}\ncall_method    add__105                                                                                                     add_                                                                                                         (l_self_modules_features_modules_denseblock4_modules_denselayer9_modules_norm2_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                          {}\ncall_function  batch_norm_105                                                                                               <function batch_norm at 0x7f3527413be0>                                                                      (bottleneck_output_50, l_self_modules_features_modules_denseblock4_modules_denselayer9_modules_norm2_buffers_running_mean_, l_self_modules_features_modules_denseblock4_modules_denselayer9_modules_norm2_buffers_running_var_, l_self_modules_features_modules_denseblock4_modules_denselayer9_modules_norm2_parameters_weight_, l_self_modules_features_modules_denseblock4_modules_denselayer9_modules_norm2_parameters_bias_, True, 0.1, 1e-05)      {}\ncall_function  relu_105                                                                                                     <function relu at 0x7f3527412c20>                                                                            (batch_norm_105,)                                                                                                                                                                                                                                                                                                                                                                                                                                        {'inplace': True}\ncall_function  new_features_50                                                                                              <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (relu_105, l_self_modules_features_modules_denseblock4_modules_denselayer9_modules_conv2_parameters_weight_, None, (1, 1), (1, 1), (1, 1), 1)                                                                                                                                                                                                                                                                                                            {}\ncall_function  concated_features_51                                                                                         <built-in method cat of type object at 0x7f351f2f6f40>                                                       ([input_19, new_features_42, new_features_43, new_features_44, new_features_45, new_features_46, new_features_47, new_features_48, new_features_49, new_features_50], 1)                                                                                                                                                                                                                                                                                 {}\ncall_method    add__106                                                                                                     add_                                                                                                         (l_self_modules_features_modules_denseblock4_modules_denselayer10_modules_norm1_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                         {}\ncall_function  batch_norm_106                                                                                               <function batch_norm at 0x7f3527413be0>                                                                      (concated_features_51, l_self_modules_features_modules_denseblock4_modules_denselayer10_modules_norm1_buffers_running_mean_, l_self_modules_features_modules_denseblock4_modules_denselayer10_modules_norm1_buffers_running_var_, l_self_modules_features_modules_denseblock4_modules_denselayer10_modules_norm1_parameters_weight_, l_self_modules_features_modules_denseblock4_modules_denselayer10_modules_norm1_parameters_bias_, True, 0.1, 1e-05)  {}\ncall_function  relu_106                                                                                                     <function relu at 0x7f3527412c20>                                                                            (batch_norm_106,)                                                                                                                                                                                                                                                                                                                                                                                                                                        {'inplace': True}\ncall_function  bottleneck_output_51                                                                                         <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (relu_106, l_self_modules_features_modules_denseblock4_modules_denselayer10_modules_conv1_parameters_weight_, None, (1, 1), (0, 0), (1, 1), 1)                                                                                                                                                                                                                                                                                                           {}\ncall_method    add__107                                                                                                     add_                                                                                                         (l_self_modules_features_modules_denseblock4_modules_denselayer10_modules_norm2_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                         {}\ncall_function  batch_norm_107                                                                                               <function batch_norm at 0x7f3527413be0>                                                                      (bottleneck_output_51, l_self_modules_features_modules_denseblock4_modules_denselayer10_modules_norm2_buffers_running_mean_, l_self_modules_features_modules_denseblock4_modules_denselayer10_modules_norm2_buffers_running_var_, l_self_modules_features_modules_denseblock4_modules_denselayer10_modules_norm2_parameters_weight_, l_self_modules_features_modules_denseblock4_modules_denselayer10_modules_norm2_parameters_bias_, True, 0.1, 1e-05)  {}\ncall_function  relu_107                                                                                                     <function relu at 0x7f3527412c20>                                                                            (batch_norm_107,)                                                                                                                                                                                                                                                                                                                                                                                                                                        {'inplace': True}\ncall_function  new_features_51                                                                                              <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (relu_107, l_self_modules_features_modules_denseblock4_modules_denselayer10_modules_conv2_parameters_weight_, None, (1, 1), (1, 1), (1, 1), 1)                                                                                                                                                                                                                                                                                                           {}\ncall_function  concated_features_52                                                                                         <built-in method cat of type object at 0x7f351f2f6f40>                                                       ([input_19, new_features_42, new_features_43, new_features_44, new_features_45, new_features_46, new_features_47, new_features_48, new_features_49, new_features_50, new_features_51], 1)                                                                                                                                                                                                                                                                {}\ncall_method    add__108                                                                                                     add_                                                                                                         (l_self_modules_features_modules_denseblock4_modules_denselayer11_modules_norm1_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                         {}\ncall_function  batch_norm_108                                                                                               <function batch_norm at 0x7f3527413be0>                                                                      (concated_features_52, l_self_modules_features_modules_denseblock4_modules_denselayer11_modules_norm1_buffers_running_mean_, l_self_modules_features_modules_denseblock4_modules_denselayer11_modules_norm1_buffers_running_var_, l_self_modules_features_modules_denseblock4_modules_denselayer11_modules_norm1_parameters_weight_, l_self_modules_features_modules_denseblock4_modules_denselayer11_modules_norm1_parameters_bias_, True, 0.1, 1e-05)  {}\ncall_function  relu_108                                                                                                     <function relu at 0x7f3527412c20>                                                                            (batch_norm_108,)                                                                                                                                                                                                                                                                                                                                                                                                                                        {'inplace': True}\ncall_function  bottleneck_output_52                                                                                         <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (relu_108, l_self_modules_features_modules_denseblock4_modules_denselayer11_modules_conv1_parameters_weight_, None, (1, 1), (0, 0), (1, 1), 1)                                                                                                                                                                                                                                                                                                           {}\ncall_method    add__109                                                                                                     add_                                                                                                         (l_self_modules_features_modules_denseblock4_modules_denselayer11_modules_norm2_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                         {}\ncall_function  batch_norm_109                                                                                               <function batch_norm at 0x7f3527413be0>                                                                      (bottleneck_output_52, l_self_modules_features_modules_denseblock4_modules_denselayer11_modules_norm2_buffers_running_mean_, l_self_modules_features_modules_denseblock4_modules_denselayer11_modules_norm2_buffers_running_var_, l_self_modules_features_modules_denseblock4_modules_denselayer11_modules_norm2_parameters_weight_, l_self_modules_features_modules_denseblock4_modules_denselayer11_modules_norm2_parameters_bias_, True, 0.1, 1e-05)  {}\ncall_function  relu_109                                                                                                     <function relu at 0x7f3527412c20>                                                                            (batch_norm_109,)                                                                                                                                                                                                                                                                                                                                                                                                                                        {'inplace': True}\ncall_function  new_features_52                                                                                              <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (relu_109, l_self_modules_features_modules_denseblock4_modules_denselayer11_modules_conv2_parameters_weight_, None, (1, 1), (1, 1), (1, 1), 1)                                                                                                                                                                                                                                                                                                           {}\ncall_function  concated_features_53                                                                                         <built-in method cat of type object at 0x7f351f2f6f40>                                                       ([input_19, new_features_42, new_features_43, new_features_44, new_features_45, new_features_46, new_features_47, new_features_48, new_features_49, new_features_50, new_features_51, new_features_52], 1)                                                                                                                                                                                                                                               {}\ncall_method    add__110                                                                                                     add_                                                                                                         (l_self_modules_features_modules_denseblock4_modules_denselayer12_modules_norm1_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                         {}\ncall_function  batch_norm_110                                                                                               <function batch_norm at 0x7f3527413be0>                                                                      (concated_features_53, l_self_modules_features_modules_denseblock4_modules_denselayer12_modules_norm1_buffers_running_mean_, l_self_modules_features_modules_denseblock4_modules_denselayer12_modules_norm1_buffers_running_var_, l_self_modules_features_modules_denseblock4_modules_denselayer12_modules_norm1_parameters_weight_, l_self_modules_features_modules_denseblock4_modules_denselayer12_modules_norm1_parameters_bias_, True, 0.1, 1e-05)  {}\ncall_function  relu_110                                                                                                     <function relu at 0x7f3527412c20>                                                                            (batch_norm_110,)                                                                                                                                                                                                                                                                                                                                                                                                                                        {'inplace': True}\ncall_function  bottleneck_output_53                                                                                         <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (relu_110, l_self_modules_features_modules_denseblock4_modules_denselayer12_modules_conv1_parameters_weight_, None, (1, 1), (0, 0), (1, 1), 1)                                                                                                                                                                                                                                                                                                           {}\ncall_method    add__111                                                                                                     add_                                                                                                         (l_self_modules_features_modules_denseblock4_modules_denselayer12_modules_norm2_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                         {}\ncall_function  batch_norm_111                                                                                               <function batch_norm at 0x7f3527413be0>                                                                      (bottleneck_output_53, l_self_modules_features_modules_denseblock4_modules_denselayer12_modules_norm2_buffers_running_mean_, l_self_modules_features_modules_denseblock4_modules_denselayer12_modules_norm2_buffers_running_var_, l_self_modules_features_modules_denseblock4_modules_denselayer12_modules_norm2_parameters_weight_, l_self_modules_features_modules_denseblock4_modules_denselayer12_modules_norm2_parameters_bias_, True, 0.1, 1e-05)  {}\ncall_function  relu_111                                                                                                     <function relu at 0x7f3527412c20>                                                                            (batch_norm_111,)                                                                                                                                                                                                                                                                                                                                                                                                                                        {'inplace': True}\ncall_function  new_features_53                                                                                              <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (relu_111, l_self_modules_features_modules_denseblock4_modules_denselayer12_modules_conv2_parameters_weight_, None, (1, 1), (1, 1), (1, 1), 1)                                                                                                                                                                                                                                                                                                           {}\ncall_function  concated_features_54                                                                                         <built-in method cat of type object at 0x7f351f2f6f40>                                                       ([input_19, new_features_42, new_features_43, new_features_44, new_features_45, new_features_46, new_features_47, new_features_48, new_features_49, new_features_50, new_features_51, new_features_52, new_features_53], 1)                                                                                                                                                                                                                              {}\ncall_method    add__112                                                                                                     add_                                                                                                         (l_self_modules_features_modules_denseblock4_modules_denselayer13_modules_norm1_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                         {}\ncall_function  batch_norm_112                                                                                               <function batch_norm at 0x7f3527413be0>                                                                      (concated_features_54, l_self_modules_features_modules_denseblock4_modules_denselayer13_modules_norm1_buffers_running_mean_, l_self_modules_features_modules_denseblock4_modules_denselayer13_modules_norm1_buffers_running_var_, l_self_modules_features_modules_denseblock4_modules_denselayer13_modules_norm1_parameters_weight_, l_self_modules_features_modules_denseblock4_modules_denselayer13_modules_norm1_parameters_bias_, True, 0.1, 1e-05)  {}\ncall_function  relu_112                                                                                                     <function relu at 0x7f3527412c20>                                                                            (batch_norm_112,)                                                                                                                                                                                                                                                                                                                                                                                                                                        {'inplace': True}\ncall_function  bottleneck_output_54                                                                                         <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (relu_112, l_self_modules_features_modules_denseblock4_modules_denselayer13_modules_conv1_parameters_weight_, None, (1, 1), (0, 0), (1, 1), 1)                                                                                                                                                                                                                                                                                                           {}\ncall_method    add__113                                                                                                     add_                                                                                                         (l_self_modules_features_modules_denseblock4_modules_denselayer13_modules_norm2_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                         {}\ncall_function  batch_norm_113                                                                                               <function batch_norm at 0x7f3527413be0>                                                                      (bottleneck_output_54, l_self_modules_features_modules_denseblock4_modules_denselayer13_modules_norm2_buffers_running_mean_, l_self_modules_features_modules_denseblock4_modules_denselayer13_modules_norm2_buffers_running_var_, l_self_modules_features_modules_denseblock4_modules_denselayer13_modules_norm2_parameters_weight_, l_self_modules_features_modules_denseblock4_modules_denselayer13_modules_norm2_parameters_bias_, True, 0.1, 1e-05)  {}\ncall_function  relu_113                                                                                                     <function relu at 0x7f3527412c20>                                                                            (batch_norm_113,)                                                                                                                                                                                                                                                                                                                                                                                                                                        {'inplace': True}\ncall_function  new_features_54                                                                                              <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (relu_113, l_self_modules_features_modules_denseblock4_modules_denselayer13_modules_conv2_parameters_weight_, None, (1, 1), (1, 1), (1, 1), 1)                                                                                                                                                                                                                                                                                                           {}\ncall_function  concated_features_55                                                                                         <built-in method cat of type object at 0x7f351f2f6f40>                                                       ([input_19, new_features_42, new_features_43, new_features_44, new_features_45, new_features_46, new_features_47, new_features_48, new_features_49, new_features_50, new_features_51, new_features_52, new_features_53, new_features_54], 1)                                                                                                                                                                                                             {}\ncall_method    add__114                                                                                                     add_                                                                                                         (l_self_modules_features_modules_denseblock4_modules_denselayer14_modules_norm1_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                         {}\ncall_function  batch_norm_114                                                                                               <function batch_norm at 0x7f3527413be0>                                                                      (concated_features_55, l_self_modules_features_modules_denseblock4_modules_denselayer14_modules_norm1_buffers_running_mean_, l_self_modules_features_modules_denseblock4_modules_denselayer14_modules_norm1_buffers_running_var_, l_self_modules_features_modules_denseblock4_modules_denselayer14_modules_norm1_parameters_weight_, l_self_modules_features_modules_denseblock4_modules_denselayer14_modules_norm1_parameters_bias_, True, 0.1, 1e-05)  {}\ncall_function  relu_114                                                                                                     <function relu at 0x7f3527412c20>                                                                            (batch_norm_114,)                                                                                                                                                                                                                                                                                                                                                                                                                                        {'inplace': True}\ncall_function  bottleneck_output_55                                                                                         <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (relu_114, l_self_modules_features_modules_denseblock4_modules_denselayer14_modules_conv1_parameters_weight_, None, (1, 1), (0, 0), (1, 1), 1)                                                                                                                                                                                                                                                                                                           {}\ncall_method    add__115                                                                                                     add_                                                                                                         (l_self_modules_features_modules_denseblock4_modules_denselayer14_modules_norm2_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                         {}\ncall_function  batch_norm_115                                                                                               <function batch_norm at 0x7f3527413be0>                                                                      (bottleneck_output_55, l_self_modules_features_modules_denseblock4_modules_denselayer14_modules_norm2_buffers_running_mean_, l_self_modules_features_modules_denseblock4_modules_denselayer14_modules_norm2_buffers_running_var_, l_self_modules_features_modules_denseblock4_modules_denselayer14_modules_norm2_parameters_weight_, l_self_modules_features_modules_denseblock4_modules_denselayer14_modules_norm2_parameters_bias_, True, 0.1, 1e-05)  {}\ncall_function  relu_115                                                                                                     <function relu at 0x7f3527412c20>                                                                            (batch_norm_115,)                                                                                                                                                                                                                                                                                                                                                                                                                                        {'inplace': True}\ncall_function  new_features_55                                                                                              <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (relu_115, l_self_modules_features_modules_denseblock4_modules_denselayer14_modules_conv2_parameters_weight_, None, (1, 1), (1, 1), (1, 1), 1)                                                                                                                                                                                                                                                                                                           {}\ncall_function  concated_features_56                                                                                         <built-in method cat of type object at 0x7f351f2f6f40>                                                       ([input_19, new_features_42, new_features_43, new_features_44, new_features_45, new_features_46, new_features_47, new_features_48, new_features_49, new_features_50, new_features_51, new_features_52, new_features_53, new_features_54, new_features_55], 1)                                                                                                                                                                                            {}\ncall_method    add__116                                                                                                     add_                                                                                                         (l_self_modules_features_modules_denseblock4_modules_denselayer15_modules_norm1_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                         {}\ncall_function  batch_norm_116                                                                                               <function batch_norm at 0x7f3527413be0>                                                                      (concated_features_56, l_self_modules_features_modules_denseblock4_modules_denselayer15_modules_norm1_buffers_running_mean_, l_self_modules_features_modules_denseblock4_modules_denselayer15_modules_norm1_buffers_running_var_, l_self_modules_features_modules_denseblock4_modules_denselayer15_modules_norm1_parameters_weight_, l_self_modules_features_modules_denseblock4_modules_denselayer15_modules_norm1_parameters_bias_, True, 0.1, 1e-05)  {}\ncall_function  relu_116                                                                                                     <function relu at 0x7f3527412c20>                                                                            (batch_norm_116,)                                                                                                                                                                                                                                                                                                                                                                                                                                        {'inplace': True}\ncall_function  bottleneck_output_56                                                                                         <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (relu_116, l_self_modules_features_modules_denseblock4_modules_denselayer15_modules_conv1_parameters_weight_, None, (1, 1), (0, 0), (1, 1), 1)                                                                                                                                                                                                                                                                                                           {}\ncall_method    add__117                                                                                                     add_                                                                                                         (l_self_modules_features_modules_denseblock4_modules_denselayer15_modules_norm2_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                         {}\ncall_function  batch_norm_117                                                                                               <function batch_norm at 0x7f3527413be0>                                                                      (bottleneck_output_56, l_self_modules_features_modules_denseblock4_modules_denselayer15_modules_norm2_buffers_running_mean_, l_self_modules_features_modules_denseblock4_modules_denselayer15_modules_norm2_buffers_running_var_, l_self_modules_features_modules_denseblock4_modules_denselayer15_modules_norm2_parameters_weight_, l_self_modules_features_modules_denseblock4_modules_denselayer15_modules_norm2_parameters_bias_, True, 0.1, 1e-05)  {}\ncall_function  relu_117                                                                                                     <function relu at 0x7f3527412c20>                                                                            (batch_norm_117,)                                                                                                                                                                                                                                                                                                                                                                                                                                        {'inplace': True}\ncall_function  new_features_56                                                                                              <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (relu_117, l_self_modules_features_modules_denseblock4_modules_denselayer15_modules_conv2_parameters_weight_, None, (1, 1), (1, 1), (1, 1), 1)                                                                                                                                                                                                                                                                                                           {}\ncall_function  concated_features_57                                                                                         <built-in method cat of type object at 0x7f351f2f6f40>                                                       ([input_19, new_features_42, new_features_43, new_features_44, new_features_45, new_features_46, new_features_47, new_features_48, new_features_49, new_features_50, new_features_51, new_features_52, new_features_53, new_features_54, new_features_55, new_features_56], 1)                                                                                                                                                                           {}\ncall_method    add__118                                                                                                     add_                                                                                                         (l_self_modules_features_modules_denseblock4_modules_denselayer16_modules_norm1_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                         {}\ncall_function  batch_norm_118                                                                                               <function batch_norm at 0x7f3527413be0>                                                                      (concated_features_57, l_self_modules_features_modules_denseblock4_modules_denselayer16_modules_norm1_buffers_running_mean_, l_self_modules_features_modules_denseblock4_modules_denselayer16_modules_norm1_buffers_running_var_, l_self_modules_features_modules_denseblock4_modules_denselayer16_modules_norm1_parameters_weight_, l_self_modules_features_modules_denseblock4_modules_denselayer16_modules_norm1_parameters_bias_, True, 0.1, 1e-05)  {}\ncall_function  relu_118                                                                                                     <function relu at 0x7f3527412c20>                                                                            (batch_norm_118,)                                                                                                                                                                                                                                                                                                                                                                                                                                        {'inplace': True}\ncall_function  bottleneck_output_57                                                                                         <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (relu_118, l_self_modules_features_modules_denseblock4_modules_denselayer16_modules_conv1_parameters_weight_, None, (1, 1), (0, 0), (1, 1), 1)                                                                                                                                                                                                                                                                                                           {}\ncall_method    add__119                                                                                                     add_                                                                                                         (l_self_modules_features_modules_denseblock4_modules_denselayer16_modules_norm2_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                         {}\ncall_function  batch_norm_119                                                                                               <function batch_norm at 0x7f3527413be0>                                                                      (bottleneck_output_57, l_self_modules_features_modules_denseblock4_modules_denselayer16_modules_norm2_buffers_running_mean_, l_self_modules_features_modules_denseblock4_modules_denselayer16_modules_norm2_buffers_running_var_, l_self_modules_features_modules_denseblock4_modules_denselayer16_modules_norm2_parameters_weight_, l_self_modules_features_modules_denseblock4_modules_denselayer16_modules_norm2_parameters_bias_, True, 0.1, 1e-05)  {}\ncall_function  relu_119                                                                                                     <function relu at 0x7f3527412c20>                                                                            (batch_norm_119,)                                                                                                                                                                                                                                                                                                                                                                                                                                        {'inplace': True}\ncall_function  new_features_57                                                                                              <built-in method conv2d of type object at 0x7f351f2f6f40>                                                    (relu_119, l_self_modules_features_modules_denseblock4_modules_denselayer16_modules_conv2_parameters_weight_, None, (1, 1), (1, 1), (1, 1), 1)                                                                                                                                                                                                                                                                                                           {}\ncall_function  input_20                                                                                                     <built-in method cat of type object at 0x7f351f2f6f40>                                                       ([input_19, new_features_42, new_features_43, new_features_44, new_features_45, new_features_46, new_features_47, new_features_48, new_features_49, new_features_50, new_features_51, new_features_52, new_features_53, new_features_54, new_features_55, new_features_56, new_features_57], 1)                                                                                                                                                          {}\ncall_method    add__120                                                                                                     add_                                                                                                         (l_self_modules_features_modules_norm5_buffers_num_batches_tracked_, 1)                                                                                                                                                                                                                                                                                                                                                                                  {}\ncall_function  input_21                                                                                                     <function batch_norm at 0x7f3527413be0>                                                                      (input_20, l_self_modules_features_modules_norm5_buffers_running_mean_, l_self_modules_features_modules_norm5_buffers_running_var_, l_self_modules_features_modules_norm5_parameters_weight_, l_self_modules_features_modules_norm5_parameters_bias_, True, 0.1, 1e-05)                                                                                                                                                                                  {}\ncall_function  out                                                                                                          <function relu at 0x7f3527412c20>                                                                            (input_21,)                                                                                                                                                                                                                                                                                                                                                                                                                                              {'inplace': True}\ncall_function  out_1                                                                                                        <function adaptive_avg_pool2d at 0x7f3527412710>                                                             (out, (1, 1))                                                                                                                                                                                                                                                                                                                                                                                                                                            {}\ncall_function  out_2                                                                                                        <built-in method flatten of type object at 0x7f351f2f6f40>                                                   (out_1, 1)                                                                                                                                                                                                                                                                                                                                                                                                                                               {}\ncall_function  out_3                                                                                                        <built-in function linear>                                                                                   (out_2, l_self_modules_classifier_parameters_weight_, l_self_modules_classifier_parameters_bias_)                                                                                                                                                                                                                                                                                                                                                        {}\noutput         output                                                                                                       output                                                                                                       ((out_3,),)                                                                                                                                                                                                                                                                                                                                                                                                                                              {}\n\ntensor([[ 0.2379,  0.1570, -0.0721,  ..., -0.1087,  0.3129,  0.1053],\n        [ 0.2520,  0.0299,  0.1547,  ..., -0.2982,  0.3166,  0.1408],\n        [ 0.2852,  0.0762,  0.0871,  ..., -0.2752,  0.3242,  0.0341],\n        ...,\n        [ 0.2102,  0.1828,  0.0535,  ..., -0.1297,  0.3167, -0.0105],\n        [ 0.2877, -0.0611,  0.1003,  ..., -0.1143,  0.2719,  0.0512],\n        [ 0.2041,  0.0058,  0.1551,  ..., -0.0678,  0.4050, -0.0247]],\n       device='cuda:0', grad_fn=<AddmmBackward0>)",
      "def bar(a, b):\n    x = a / (torch.abs(a) + 1)\n    if b.sum() < 0:\n        b = b * -1\n    return x * b\n\nopt_bar = torch.compile(bar, backend=custom_backend)\ninp1 = torch.randn(10)\ninp2 = torch.randn(10)\nopt_bar(inp1, inp2)\nopt_bar(inp1, -inp2)",
      "# Reset since we are using a different backend.\ntorch._dynamo.reset()\nexplain_output = torch._dynamo.explain(bar)(torch.randn(10), torch.randn(10))\nprint(explain_output)",
      "Graph Count: 2\nGraph Break Count: 1\nOp Count: 6\nBreak Reasons:\n  Break Reason 1:\n    Reason: generic_jump TensorVariable()\n    User Stack:\n      <FrameSummary file /var/lib/workspace/intermediate_source/torch_compile_tutorial.py, line 545 in bar>\nOps per Graph:\n  Ops 1:\n    <built-in method abs of type object at 0x7f351f2f6f40>\n    <built-in function add>\n    <built-in function truediv>\n    <built-in function lt>\n  Ops 2:\n    <built-in function mul>\n    <built-in function mul>\nOut Guards:\n  Guard 1:\n    Name: ''\n    Source: global\n    Create Function: DETERMINISTIC_ALGORITHMS\n    Guard Types: None\n    Code List: None\n    Object Weakref: None\n    Guarded Class Weakref: None\n  Guard 2:\n    Name: \"G['torch']\"\n    Source: global\n    Create Function: FUNCTION_MATCH\n    Guard Types: ['ID_MATCH']\n    Code List: [\"___check_obj_id(G['torch'], 139866265043248)\"]\n    Object Weakref: <weakref at 0x7f3394fa2430; to 'module' at 0x7f35270b4130>\n    Guarded Class Weakref: <weakref at 0x7f35288d71a0; to 'type' at 0x55e66bffa0a0 (module)>\n  Guard 3:\n    Name: \"L['a']\"\n    Source: local\n    Create Function: TENSOR_MATCH\n    Guard Types: ['TENSOR_MATCH']\n    Code List: [\"hasattr(L['a'], '_dynamo_dynamic_indices') == False\"]\n    Object Weakref: <weakref at 0x7f3312573380; dead>\n    Guarded Class Weakref: <weakref at 0x7f348c5ac2c0; to 'torch._C._TensorMeta' at 0x55e67b0be7b0 (Tensor)>\n  Guard 4:\n    Name: ''\n    Source: global\n    Create Function: TORCH_FUNCTION_STATE\n    Guard Types: None\n    Code List: None\n    Object Weakref: None\n    Guarded Class Weakref: None\n  Guard 5:\n    Name: \"L['b'].sum\"\n    Source: local\n    Create Function: HASATTR\n    Guard Types: ['HASATTR']\n    Code List: [\"hasattr(L['b'], 'sum')\"]\n    Object Weakref: <weakref at 0x7f32f15d8b80; dead>\n    Guarded Class Weakref: <weakref at 0x7f348c5ac2c0; to 'torch._C._TensorMeta' at 0x55e67b0be7b0 (Tensor)>\n  Guard 6:\n    Name: \"L['b']\"\n    Source: local\n    Create Function: TENSOR_MATCH\n    Guard Types: ['TENSOR_MATCH']\n    Code List: [\"hasattr(L['b'], '_dynamo_dynamic_indices') == False\"]\n    Object Weakref: <weakref at 0x7f32f15d8b80; dead>\n    Guarded Class Weakref: <weakref at 0x7f348c5ac2c0; to 'torch._C._TensorMeta' at 0x55e67b0be7b0 (Tensor)>\n  Guard 7:\n    Name: \"G['torch'].abs\"\n    Source: global\n    Create Function: FUNCTION_MATCH\n    Guard Types: ['ID_MATCH']\n    Code List: [\"___check_obj_id(G['torch'].abs, 139866263969104)\"]\n    Object Weakref: <weakref at 0x7f335445f1f0; to 'builtin_function_or_method' at 0x7f3526fadd50>\n    Guarded Class Weakref: <weakref at 0x7f35288adcb0; to 'type' at 0x55e66bff08a0 (builtin_function_or_method)>\n  Guard 8:\n    Name: ''\n    Source: global\n    Create Function: DEFAULT_DEVICE\n    Guard Types: ['DEFAULT_DEVICE']\n    Code List: ['utils_device.CURRENT_DEVICE == None']\n    Object Weakref: None\n    Guarded Class Weakref: None\n  Guard 9:\n    Name: ''\n    Source: shape_env\n    Create Function: SHAPE_ENV\n    Guard Types: None\n    Code List: None\n    Object Weakref: None\n    Guarded Class Weakref: None\n  Guard 10:\n    Name: ''\n    Source: global\n    Create Function: GRAD_MODE\n    Guard Types: None\n    Code List: None\n    Object Weakref: None\n    Guarded Class Weakref: None\n  Guard 11:\n    Name: ''\n    Source: global\n    Create Function: DETERMINISTIC_ALGORITHMS\n    Guard Types: None\n    Code List: None\n    Object Weakref: None\n    Guarded Class Weakref: None\n  Guard 12:\n    Name: ''\n    Source: global\n    Create Function: TORCH_FUNCTION_STATE\n    Guard Types: None\n    Code List: None\n    Object Weakref: None\n    Guarded Class Weakref: None\n  Guard 13:\n    Name: \"L['b']\"\n    Source: local\n    Create Function: TENSOR_MATCH\n    Guard Types: ['TENSOR_MATCH']\n    Code List: [\"hasattr(L['b'], '_dynamo_dynamic_indices') == False\"]\n    Object Weakref: <weakref at 0x7f32f15d8b80; dead>\n    Guarded Class Weakref: <weakref at 0x7f348c5ac2c0; to 'torch._C._TensorMeta' at 0x55e67b0be7b0 (Tensor)>\n  Guard 14:\n    Name: \"L['x']\"\n    Source: local\n    Create Function: TENSOR_MATCH\n    Guard Types: ['TENSOR_MATCH']\n    Code List: [\"hasattr(L['x'], '_dynamo_dynamic_indices') == False\"]\n    Object Weakref: <weakref at 0x7f32f1fe1170; dead>\n    Guarded Class Weakref: <weakref at 0x7f348c5ac2c0; to 'torch._C._TensorMeta' at 0x55e67b0be7b0 (Tensor)>\n  Guard 15:\n    Name: ''\n    Source: global\n    Create Function: DEFAULT_DEVICE\n    Guard Types: ['DEFAULT_DEVICE']\n    Code List: ['utils_device.CURRENT_DEVICE == None']\n    Object Weakref: None\n    Guarded Class Weakref: None\n  Guard 16:\n    Name: ''\n    Source: shape_env\n    Create Function: SHAPE_ENV\n    Guard Types: None\n    Code List: None\n    Object Weakref: None\n    Guarded Class Weakref: None\n  Guard 17:\n    Name: ''\n    Source: global\n    Create Function: GRAD_MODE\n    Guard Types: None\n    Code List: None\n    Object Weakref: None\n    Guarded Class Weakref: None\nCompile Times: TorchDynamo compilation metrics:\nFunction                        Runtimes (s)\n------------------------------  --------------\n_compile.compile_inner          0.0107, 0.0059\nOutputGraph.call_user_compiler  0.0001, 0.0001\ngc                              0.0003, 0.0002",
      "opt_bar = torch.compile(bar, fullgraph=True)\ntry:\n    opt_bar(torch.randn(10), torch.randn(10))\nexcept:\n    tb.print_exc()",
      "class GraphModule(torch.nn.Module):\n    def forward(self, L_a_: \"f32[10][1]cpu\", L_b_: \"f32[10][1]cpu\"):\n        l_a_ = L_a_\n        l_b_ = L_b_\n\n         # File: /var/lib/workspace/intermediate_source/torch_compile_tutorial.py:544 in bar, code: x = a / (torch.abs(a) + 1)\n        abs_1: \"f32[10][1]cpu\" = torch.abs(l_a_)\n        add: \"f32[10][1]cpu\" = abs_1 + 1;  abs_1 = None\n        x: \"f32[10][1]cpu\" = l_a_ / add;  l_a_ = add = x = None\n\n         # File: /var/lib/workspace/intermediate_source/torch_compile_tutorial.py:545 in bar, code: if b.sum() < 0:\n        sum_1: \"f32[][]cpu\" = l_b_.sum();  l_b_ = None\n        lt: \"b8[][]cpu\" = sum_1 < 0;  sum_1 = lt = None\n\nTraceback (most recent call last):\n  File \"/var/lib/workspace/intermediate_source/torch_compile_tutorial.py\", line 593, in <module>\n    opt_bar(torch.randn(10), torch.randn(10))\n  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py\", line 659, in _fn\n    raise e.with_traceback(None) from None\ntorch._dynamo.exc.Unsupported: Data-dependent branching\n  Explanation: Detected data-dependent branching (e.g. `if my_tensor.sum() > 0:`). Dynamo does not support tracing dynamic control flow.\n  Hint: This graph break is fundamental - it is unlikely that Dynamo will ever be able to trace through your code. Consider finding a workaround.\n  Hint: Use `torch.cond` to express dynamic control flow.\n\n  Developer debug context: attempted to jump with TensorVariable()\n\n\nfrom user code:\n   File \"/var/lib/workspace/intermediate_source/torch_compile_tutorial.py\", line 545, in bar\n    if b.sum() < 0:\n\nSet TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=\"+dynamo\"",
      "opt_model = torch.compile(init_model(), fullgraph=True)\nprint(opt_model(generate_data(16)[0]))"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html",
    "title": "Reinforcement Learning (DQN) Tutorial\u00b6",
    "code_snippets": [
      "%%bash\npip3 install gymnasium[classic_control]",
      "import gymnasium as gym\nimport math\nimport random\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom collections import namedtuple, deque\nfrom itertools import count\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\n\nenv = gym.make(\"CartPole-v1\")\n\n# set up matplotlib\nis_ipython = 'inline' in matplotlib.get_backend()\nif is_ipython:\n    from IPython import display\n\nplt.ion()\n\n# if GPU is to be used\ndevice = torch.device(\n    \"cuda\" if torch.cuda.is_available() else\n    \"mps\" if torch.backends.mps.is_available() else\n    \"cpu\"\n)",
      "Transition = namedtuple('Transition',\n                        ('state', 'action', 'next_state', 'reward'))\n\n\nclass ReplayMemory(object):\n\n    def __init__(self, capacity):\n        self.memory = deque([], maxlen=capacity)\n\n    def push(self, *args):\n        \"\"\"Save a transition\"\"\"\n        self.memory.append(Transition(*args))\n\n    def sample(self, batch_size):\n        return random.sample(self.memory, batch_size)\n\n    def __len__(self):\n        return len(self.memory)",
      "class DQN(nn.Module):\n\n    def __init__(self, n_observations, n_actions):\n        super(DQN, self).__init__()\n        self.layer1 = nn.Linear(n_observations, 128)\n        self.layer2 = nn.Linear(128, 128)\n        self.layer3 = nn.Linear(128, n_actions)\n\n    # Called with either one element to determine next action, or a batch\n    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n    def forward(self, x):\n        x = F.relu(self.layer1(x))\n        x = F.relu(self.layer2(x))\n        return self.layer3(x)",
      "# BATCH_SIZE is the number of transitions sampled from the replay buffer\n# GAMMA is the discount factor as mentioned in the previous section\n# EPS_START is the starting value of epsilon\n# EPS_END is the final value of epsilon\n# EPS_DECAY controls the rate of exponential decay of epsilon, higher means a slower decay\n# TAU is the update rate of the target network\n# LR is the learning rate of the ``AdamW`` optimizer\nBATCH_SIZE = 128\nGAMMA = 0.99\nEPS_START = 0.9\nEPS_END = 0.05\nEPS_DECAY = 1000\nTAU = 0.005\nLR = 1e-4\n\n# Get number of actions from gym action space\nn_actions = env.action_space.n\n# Get the number of state observations\nstate, info = env.reset()\nn_observations = len(state)\n\npolicy_net = DQN(n_observations, n_actions).to(device)\ntarget_net = DQN(n_observations, n_actions).to(device)\ntarget_net.load_state_dict(policy_net.state_dict())\n\noptimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\nmemory = ReplayMemory(10000)\n\n\nsteps_done = 0\n\n\ndef select_action(state):\n    global steps_done\n    sample = random.random()\n    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n        math.exp(-1. * steps_done / EPS_DECAY)\n    steps_done += 1\n    if sample > eps_threshold:\n        with torch.no_grad():\n            # t.max(1) will return the largest column value of each row.\n            # second column on max result is index of where max element was\n            # found, so we pick action with the larger expected reward.\n            return policy_net(state).max(1).indices.view(1, 1)\n    else:\n        return torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.long)\n\n\nepisode_durations = []\n\n\ndef plot_durations(show_result=False):\n    plt.figure(1)\n    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n    if show_result:\n        plt.title('Result')\n    else:\n        plt.clf()\n        plt.title('Training...')\n    plt.xlabel('Episode')\n    plt.ylabel('Duration')\n    plt.plot(durations_t.numpy())\n    # Take 100 episode averages and plot them too\n    if len(durations_t) >= 100:\n        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n        means = torch.cat((torch.zeros(99), means))\n        plt.plot(means.numpy())\n\n    plt.pause(0.001)  # pause a bit so that plots are updated\n    if is_ipython:\n        if not show_result:\n            display.display(plt.gcf())\n            display.clear_output(wait=True)\n        else:\n            display.display(plt.gcf())",
      "def optimize_model():\n    if len(memory) < BATCH_SIZE:\n        return\n    transitions = memory.sample(BATCH_SIZE)\n    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n    # detailed explanation). This converts batch-array of Transitions\n    # to Transition of batch-arrays.\n    batch = Transition(*zip(*transitions))\n\n    # Compute a mask of non-final states and concatenate the batch elements\n    # (a final state would've been the one after which simulation ended)\n    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n                                          batch.next_state)), device=device, dtype=torch.bool)\n    non_final_next_states = torch.cat([s for s in batch.next_state\n                                                if s is not None])\n    state_batch = torch.cat(batch.state)\n    action_batch = torch.cat(batch.action)\n    reward_batch = torch.cat(batch.reward)\n\n    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n    # columns of actions taken. These are the actions which would've been taken\n    # for each batch state according to policy_net\n    state_action_values = policy_net(state_batch).gather(1, action_batch)\n\n    # Compute V(s_{t+1}) for all next states.\n    # Expected values of actions for non_final_next_states are computed based\n    # on the \"older\" target_net; selecting their best reward with max(1).values\n    # This is merged based on the mask, such that we'll have either the expected\n    # state value or 0 in case the state was final.\n    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n    with torch.no_grad():\n        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1).values\n    # Compute the expected Q values\n    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n\n    # Compute Huber loss\n    criterion = nn.SmoothL1Loss()\n    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n\n    # Optimize the model\n    optimizer.zero_grad()\n    loss.backward()\n    # In-place gradient clipping\n    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n    optimizer.step()",
      "if torch.cuda.is_available() or torch.backends.mps.is_available():\n    num_episodes = 600\nelse:\n    num_episodes = 50\n\nfor i_episode in range(num_episodes):\n    # Initialize the environment and get its state\n    state, info = env.reset()\n    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n    for t in count():\n        action = select_action(state)\n        observation, reward, terminated, truncated, _ = env.step(action.item())\n        reward = torch.tensor([reward], device=device)\n        done = terminated or truncated\n\n        if terminated:\n            next_state = None\n        else:\n            next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n\n        # Store the transition in memory\n        memory.push(state, action, next_state, reward)\n\n        # Move to the next state\n        state = next_state\n\n        # Perform one step of the optimization (on the policy network)\n        optimize_model()\n\n        # Soft update of the target network's weights\n        # \u03b8\u2032 \u2190 \u03c4 \u03b8 + (1 \u2212\u03c4 )\u03b8\u2032\n        target_net_state_dict = target_net.state_dict()\n        policy_net_state_dict = policy_net.state_dict()\n        for key in policy_net_state_dict:\n            target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n        target_net.load_state_dict(target_net_state_dict)\n\n        if done:\n            episode_durations.append(t + 1)\n            plot_durations()\n            break\n\nprint('Complete')\nplot_durations(show_result=True)\nplt.ioff()\nplt.show()"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/beginner/introyt/introyt1_tutorial.html",
    "title": "Introduction to PyTorch\u00b6",
    "code_snippets": [
      "import torch",
      "z = torch.zeros(5, 3)\nprint(z)\nprint(z.dtype)",
      "tensor([[0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.]])\ntorch.float32",
      "i = torch.ones((5, 3), dtype=torch.int16)\nprint(i)",
      "tensor([[1, 1, 1],\n        [1, 1, 1],\n        [1, 1, 1],\n        [1, 1, 1],\n        [1, 1, 1]], dtype=torch.int16)",
      "torch.manual_seed(1729)\nr1 = torch.rand(2, 2)\nprint('A random tensor:')\nprint(r1)\n\nr2 = torch.rand(2, 2)\nprint('\\nA different random tensor:')\nprint(r2) # new values\n\ntorch.manual_seed(1729)\nr3 = torch.rand(2, 2)\nprint('\\nShould match r1:')\nprint(r3) # repeats values of r1 because of re-seed",
      "ones = torch.ones(2, 3)\nprint(ones)\n\ntwos = torch.ones(2, 3) * 2 # every element is multiplied by 2\nprint(twos)\n\nthrees = ones + twos       # addition allowed because shapes are similar\nprint(threes)              # tensors are added element-wise\nprint(threes.shape)        # this has the same dimensions as input tensors\n\nr1 = torch.rand(2, 3)\nr2 = torch.rand(3, 2)\n# uncomment this line to get a runtime error\n# r3 = r1 + r2",
      "tensor([[1., 1., 1.],\n        [1., 1., 1.]])\ntensor([[2., 2., 2.],\n        [2., 2., 2.]])\ntensor([[3., 3., 3.],\n        [3., 3., 3.]])\ntorch.Size([2, 3])",
      "r = (torch.rand(2, 2) - 0.5) * 2 # values between -1 and 1\nprint('A random matrix, r:')\nprint(r)\n\n# Common mathematical operations are supported:\nprint('\\nAbsolute value of r:')\nprint(torch.abs(r))\n\n# ...as are trigonometric functions:\nprint('\\nInverse sine of r:')\nprint(torch.asin(r))\n\n# ...and linear algebra operations like determinant and singular value decomposition\nprint('\\nDeterminant of r:')\nprint(torch.det(r))\nprint('\\nSingular value decomposition of r:')\nprint(torch.svd(r))\n\n# ...and statistical and aggregate operations:\nprint('\\nAverage and standard deviation of r:')\nprint(torch.std_mean(r))\nprint('\\nMaximum value of r:')\nprint(torch.max(r))",
      "A random matrix, r:\ntensor([[ 0.9956, -0.2232],\n        [ 0.3858, -0.6593]])\n\nAbsolute value of r:\ntensor([[0.9956, 0.2232],\n        [0.3858, 0.6593]])\n\nInverse sine of r:\ntensor([[ 1.4775, -0.2251],\n        [ 0.3961, -0.7199]])\n\nDeterminant of r:\ntensor(-0.5703)\n\nSingular value decomposition of r:\ntorch.return_types.svd(\nU=tensor([[-0.8353, -0.5497],\n        [-0.5497,  0.8353]]),\nS=tensor([1.1793, 0.4836]),\nV=tensor([[-0.8851, -0.4654],\n        [ 0.4654, -0.8851]]))\n\nAverage and standard deviation of r:\n(tensor(0.7217), tensor(0.1247))\n\nMaximum value of r:\ntensor(0.9956)",
      "import torch                     # for all things PyTorch\nimport torch.nn as nn            # for torch.nn.Module, the parent object for PyTorch models\nimport torch.nn.functional as F  # for the activation function",
      "class LeNet(nn.Module):\n\n    def __init__(self):\n        super(LeNet, self).__init__()\n        # 1 input image channel (black & white), 6 output channels, 5x5 square convolution\n        # kernel\n        self.conv1 = nn.Conv2d(1, 6, 5)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        # an affine operation: y = Wx + b\n        self.fc1 = nn.Linear(16 * 5 * 5, 120)  # 5*5 from image dimension\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n\n    def forward(self, x):\n        # Max pooling over a (2, 2) window\n        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n        # If the size is a square you can only specify a single number\n        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n        x = x.view(-1, self.num_flat_features(x))\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\n    def num_flat_features(self, x):\n        size = x.size()[1:]  # all dimensions except the batch dimension\n        num_features = 1\n        for s in size:\n            num_features *= s\n        return num_features",
      "net = LeNet()\nprint(net)                         # what does the object tell us about itself?\n\ninput = torch.rand(1, 1, 32, 32)   # stand-in for a 32x32 black & white image\nprint('\\nImage batch shape:')\nprint(input.shape)\n\noutput = net(input)                # we don't call forward() directly\nprint('\\nRaw output:')\nprint(output)\nprint(output.shape)",
      "LeNet(\n  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n  (fc1): Linear(in_features=400, out_features=120, bias=True)\n  (fc2): Linear(in_features=120, out_features=84, bias=True)\n  (fc3): Linear(in_features=84, out_features=10, bias=True)\n)\n\nImage batch shape:\ntorch.Size([1, 1, 32, 32])\n\nRaw output:\ntensor([[ 0.0898,  0.0318,  0.1485,  0.0301, -0.0085, -0.1135, -0.0296,  0.0164,\n          0.0039,  0.0616]], grad_fn=<AddmmBackward0>)\ntorch.Size([1, 10])",
      "#%matplotlib inline\n\nimport torch\nimport torchvision\nimport torchvision.transforms as transforms\n\ntransform = transforms.Compose(\n    [transforms.ToTensor(),\n     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))])",
      "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n                                          shuffle=True, num_workers=2)",
      "import matplotlib.pyplot as plt\nimport numpy as np\n\nclasses = ('plane', 'car', 'bird', 'cat',\n           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n\ndef imshow(img):\n    img = img / 2 + 0.5     # unnormalize\n    npimg = img.numpy()\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n\n\n# get some random training images\ndataiter = iter(trainloader)\nimages, labels = next(dataiter)\n\n# show images\nimshow(torchvision.utils.make_grid(images))\n# print labels\nprint(' '.join('%5s' % classes[labels[j]] for j in range(4)))",
      "#%matplotlib inline\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nimport torchvision\nimport torchvision.transforms as transforms\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport numpy as np",
      "transform = transforms.Compose(\n    [transforms.ToTensor(),\n     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n\ntrainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n                                        download=True, transform=transform)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n                                          shuffle=True, num_workers=2)\n\ntestset = torchvision.datasets.CIFAR10(root='./data', train=False,\n                                       download=True, transform=transform)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=4,\n                                         shuffle=False, num_workers=2)\n\nclasses = ('plane', 'car', 'bird', 'cat',\n           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')",
      "import matplotlib.pyplot as plt\nimport numpy as np\n\n# functions to show an image\n\n\ndef imshow(img):\n    img = img / 2 + 0.5     # unnormalize\n    npimg = img.numpy()\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n\n\n# get some random training images\ndataiter = iter(trainloader)\nimages, labels = next(dataiter)\n\n# show images\nimshow(torchvision.utils.make_grid(images))\n# print labels\nprint(' '.join('%5s' % classes[labels[j]] for j in range(4)))",
      "class Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(3, 6, 5)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = x.view(-1, 16 * 5 * 5)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\n\nnet = Net()",
      "correct = 0\ntotal = 0\nwith torch.no_grad():\n    for data in testloader:\n        images, labels = data\n        outputs = net(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\nprint('Accuracy of the network on the 10000 test images: %d %%' % (\n    100 * correct / total))"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/intermediate/forward_ad_usage.html",
    "title": "Forward-mode Automatic Differentiation (Beta)\u00b6",
    "code_snippets": [
      "import torch\nimport torch.autograd.forward_ad as fwAD\n\nprimal = torch.randn(10, 10)\ntangent = torch.randn(10, 10)\n\ndef fn(x, y):\n    return x ** 2 + y ** 2\n\n# All forward AD computation must be performed in the context of\n# a ``dual_level`` context. All dual tensors created in such a context\n# will have their tangents destroyed upon exit. This is to ensure that\n# if the output or intermediate results of this computation are reused\n# in a future forward AD computation, their tangents (which are associated\n# with this computation) won't be confused with tangents from the later\n# computation.\nwith fwAD.dual_level():\n    # To create a dual tensor we associate a tensor, which we call the\n    # primal with another tensor of the same size, which we call the tangent.\n    # If the layout of the tangent is different from that of the primal,\n    # The values of the tangent are copied into a new tensor with the same\n    # metadata as the primal. Otherwise, the tangent itself is used as-is.\n    #\n    # It is also important to note that the dual tensor created by\n    # ``make_dual`` is a view of the primal.\n    dual_input = fwAD.make_dual(primal, tangent)\n    assert fwAD.unpack_dual(dual_input).tangent is tangent\n\n    # To demonstrate the case where the copy of the tangent happens,\n    # we pass in a tangent with a layout different from that of the primal\n    dual_input_alt = fwAD.make_dual(primal, tangent.T)\n    assert fwAD.unpack_dual(dual_input_alt).tangent is not tangent\n\n    # Tensors that do not have an associated tangent are automatically\n    # considered to have a zero-filled tangent of the same shape.\n    plain_tensor = torch.randn(10, 10)\n    dual_output = fn(dual_input, plain_tensor)\n\n    # Unpacking the dual returns a ``namedtuple`` with ``primal`` and ``tangent``\n    # as attributes\n    jvp = fwAD.unpack_dual(dual_output).tangent\n\nassert fwAD.unpack_dual(dual_output).tangent is None",
      "import torch.nn as nn\n\nmodel = nn.Linear(5, 5)\ninput = torch.randn(16, 5)\n\nparams = {name: p for name, p in model.named_parameters()}\ntangents = {name: torch.rand_like(p) for name, p in params.items()}\n\nwith fwAD.dual_level():\n    for name, p in params.items():\n        delattr(model, name)\n        setattr(model, name, fwAD.make_dual(p, tangents[name]))\n\n    out = model(input)\n    jvp = fwAD.unpack_dual(out).tangent",
      "from torch.func import functional_call\n\n# We need a fresh module because the functional call requires the\n# the model to have parameters registered.\nmodel = nn.Linear(5, 5)\n\ndual_params = {}\nwith fwAD.dual_level():\n    for name, p in params.items():\n        # Using the same ``tangents`` from the above section\n        dual_params[name] = fwAD.make_dual(p, tangents[name])\n    out = functional_call(model, dual_params, input)\n    jvp2 = fwAD.unpack_dual(out).tangent\n\n# Check our results\nassert torch.allclose(jvp, jvp2)",
      "class Fn(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, foo):\n        result = torch.exp(foo)\n        # Tensors stored in ``ctx`` can be used in the subsequent forward grad\n        # computation.\n        ctx.result = result\n        return result\n\n    @staticmethod\n    def jvp(ctx, gI):\n        gO = gI * ctx.result\n        # If the tensor stored in`` ctx`` will not also be used in the backward pass,\n        # one can manually free it using ``del``\n        del ctx.result\n        return gO\n\nfn = Fn.apply\n\nprimal = torch.randn(10, 10, dtype=torch.double, requires_grad=True)\ntangent = torch.randn(10, 10)\n\nwith fwAD.dual_level():\n    dual_input = fwAD.make_dual(primal, tangent)\n    dual_output = fn(dual_input)\n    jvp = fwAD.unpack_dual(dual_output).tangent\n\n# It is important to use ``autograd.gradcheck`` to verify that your\n# custom autograd Function computes the gradients correctly. By default,\n# ``gradcheck`` only checks the backward-mode (reverse-mode) AD gradients. Specify\n# ``check_forward_ad=True`` to also check forward grads. If you did not\n# implement the backward formula for your function, you can also tell ``gradcheck``\n# to skip the tests that require backward-mode AD by specifying\n# ``check_backward_ad=False``, ``check_undefined_grad=False``, and\n# ``check_batched_grad=False``.\ntorch.autograd.gradcheck(Fn.apply, (primal,), check_forward_ad=True,\n                         check_backward_ad=False, check_undefined_grad=False,\n                         check_batched_grad=False)",
      "import functorch as ft\n\nprimal0 = torch.randn(10, 10)\ntangent0 = torch.randn(10, 10)\nprimal1 = torch.randn(10, 10)\ntangent1 = torch.randn(10, 10)\n\ndef fn(x, y):\n    return x ** 2 + y ** 2\n\n# Here is a basic example to compute the JVP of the above function.\n# The ``jvp(func, primals, tangents)`` returns ``func(*primals)`` as well as the\n# computed Jacobian-vector product (JVP). Each primal must be associated with a tangent of the same shape.\nprimal_out, tangent_out = ft.jvp(fn, (primal0, primal1), (tangent0, tangent1))\n\n# ``functorch.jvp`` requires every primal to be associated with a tangent.\n# If we only want to associate certain inputs to `fn` with tangents,\n# then we'll need to create a new function that captures inputs without tangents:\nprimal = torch.randn(10, 10)\ntangent = torch.randn(10, 10)\ny = torch.randn(10, 10)\n\nimport functools\nnew_fn = functools.partial(fn, y=y)\nprimal_out, tangent_out = ft.jvp(new_fn, (primal,), (tangent,))",
      "/var/lib/workspace/intermediate_source/forward_ad_usage.py:203: FutureWarning:\n\nWe've integrated functorch into PyTorch. As the final step of the integration, `functorch.jvp` is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use `torch.func.jvp` instead; see the PyTorch 2.0 release notes and/or the `torch.func` migration guide for more details https://pytorch.org/docs/main/func.migrating.html\n\n/var/lib/workspace/intermediate_source/forward_ad_usage.py:214: FutureWarning:\n\nWe've integrated functorch into PyTorch. As the final step of the integration, `functorch.jvp` is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use `torch.func.jvp` instead; see the PyTorch 2.0 release notes and/or the `torch.func` migration guide for more details https://pytorch.org/docs/main/func.migrating.html",
      "model = nn.Linear(5, 5)\ninput = torch.randn(16, 5)\ntangents = tuple([torch.rand_like(p) for p in model.parameters()])\n\n# Given a ``torch.nn.Module``, ``ft.make_functional_with_buffers`` extracts the state\n# (``params`` and buffers) and returns a functional version of the model that\n# can be invoked like a function.\n# That is, the returned ``func`` can be invoked like\n# ``func(params, buffers, input)``.\n# ``ft.make_functional_with_buffers`` is analogous to the ``nn.Modules`` stateless API\n# that you saw previously and we're working on consolidating the two.\nfunc, params, buffers = ft.make_functional_with_buffers(model)\n\n# Because ``jvp`` requires every input to be associated with a tangent, we need to\n# create a new function that, when given the parameters, produces the output\ndef func_params_only(params):\n    return func(params, buffers, input)\n\nmodel_output, jvp_out = ft.jvp(func_params_only, (params,), (tangents,))",
      "/var/lib/workspace/intermediate_source/forward_ad_usage.py:235: FutureWarning:\n\nWe've integrated functorch into PyTorch. As the final step of the integration, `functorch.make_functional_with_buffers` is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use `torch.func.functional_call` instead; see the PyTorch 2.0 release notes and/or the `torch.func` migration guide for more details https://pytorch.org/docs/main/func.migrating.html\n\n/var/lib/workspace/intermediate_source/forward_ad_usage.py:242: FutureWarning:\n\nWe've integrated functorch into PyTorch. As the final step of the integration, `functorch.jvp` is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use `torch.func.jvp` instead; see the PyTorch 2.0 release notes and/or the `torch.func` migration guide for more details https://pytorch.org/docs/main/func.migrating.html"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/advanced/dispatcher.html",
    "title": "Registering a Dispatched Operator in C++\u00b6",
    "code_snippets": [
      "TORCH_LIBRARY(myops, m) {\n  m.def(\"myadd(Tensor self, Tensor other) -> Tensor\");\n}",
      "class MyAddFunction : public torch::autograd::Function<MyAddFunction> {\n public:\n  static Tensor forward(\n      AutogradContext *ctx, torch::Tensor self, torch::Tensor other) {\n    at::AutoNonVariableTypeMode g;\n    return myadd(self, other);\n  }\n\n  static tensor_list backward(AutogradContext *ctx, tensor_list grad_outputs) {\n    auto grad_output = grad_outputs[0];\n    return {grad_output, grad_output};\n  }\n};\n\nTensor myadd_autograd(const Tensor& self, const Tensor& other) {\n  return MyAddFunction::apply(self, other)[0];\n}",
      "class MyAddFunction : ... {\npublic:\n  static Tensor forward(\n    AutogradContext *ctx, torch::Tensor self, torch::Tensor other) {\n\n    if (self.device().type() == DeviceType::CPU) {\n      return add_cpu(self, other);\n    } else if (self.device().type() == DeviceType::CUDA) {\n      return add_cuda(self, other);\n    } else {\n      TORCH_CHECK(0, \"Unsupported device \", self.device().type());\n    }\n  }\n  ...\n}"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/advanced/sharding.html",
    "title": "Exploring TorchRec sharding\u00b6",
    "code_snippets": [
      "import sys\nsys.path = ['', '/env/python', '/usr/local/lib/python37.zip', '/usr/local/lib/python3.7', '/usr/local/lib/python3.7/lib-dynload', '/usr/local/lib/python3.7/site-packages', './.local/lib/python3.7/site-packages']",
      "import os\nimport torch\nimport torchrec\n\nos.environ[\"MASTER_ADDR\"] = \"localhost\"\nos.environ[\"MASTER_PORT\"] = \"29500\"",
      "from torchrec.distributed.planner.types import ParameterConstraints\nfrom torchrec.distributed.embedding_types import EmbeddingComputeKernel\nfrom torchrec.distributed.types import ShardingType\nfrom typing import Dict\n\nlarge_table_cnt = 2\nsmall_table_cnt = 2\nlarge_tables=[\n  torchrec.EmbeddingBagConfig(\n    name=\"large_table_\" + str(i),\n    embedding_dim=64,\n    num_embeddings=4096,\n    feature_names=[\"large_table_feature_\" + str(i)],\n    pooling=torchrec.PoolingType.SUM,\n  ) for i in range(large_table_cnt)\n]\nsmall_tables=[\n  torchrec.EmbeddingBagConfig(\n    name=\"small_table_\" + str(i),\n    embedding_dim=64,\n    num_embeddings=1024,\n    feature_names=[\"small_table_feature_\" + str(i)],\n    pooling=torchrec.PoolingType.SUM,\n  ) for i in range(small_table_cnt)\n]\n\ndef gen_constraints(sharding_type: ShardingType = ShardingType.TABLE_WISE) -> Dict[str, ParameterConstraints]:\n  large_table_constraints = {\n    \"large_table_\" + str(i): ParameterConstraints(\n      sharding_types=[sharding_type.value],\n    ) for i in range(large_table_cnt)\n  }\n  small_table_constraints = {\n    \"small_table_\" + str(i): ParameterConstraints(\n      sharding_types=[sharding_type.value],\n    ) for i in range(small_table_cnt)\n  }\n  constraints = {**large_table_constraints, **small_table_constraints}\n  return constraints",
      "def single_rank_execution(\n    rank: int,\n    world_size: int,\n    constraints: Dict[str, ParameterConstraints],\n    module: torch.nn.Module,\n    backend: str,\n) -> None:\n    import os\n    import torch\n    import torch.distributed as dist\n    from torchrec.distributed.embeddingbag import EmbeddingBagCollectionSharder\n    from torchrec.distributed.model_parallel import DistributedModelParallel\n    from torchrec.distributed.planner import EmbeddingShardingPlanner, Topology\n    from torchrec.distributed.types import ModuleSharder, ShardingEnv\n    from typing import cast\n\n    def init_distributed_single_host(\n        rank: int,\n        world_size: int,\n        backend: str,\n        # pyre-fixme[11]: Annotation `ProcessGroup` is not defined as a type.\n    ) -> dist.ProcessGroup:\n        os.environ[\"RANK\"] = f\"{rank}\"\n        os.environ[\"WORLD_SIZE\"] = f\"{world_size}\"\n        dist.init_process_group(rank=rank, world_size=world_size, backend=backend)\n        return dist.group.WORLD\n\n    if backend == \"nccl\":\n        device = torch.device(f\"cuda:{rank}\")\n        torch.cuda.set_device(device)\n    else:\n        device = torch.device(\"cpu\")\n    topology = Topology(world_size=world_size, compute_device=\"cuda\")\n    pg = init_distributed_single_host(rank, world_size, backend)\n    planner = EmbeddingShardingPlanner(\n        topology=topology,\n        constraints=constraints,\n    )\n    sharders = [cast(ModuleSharder[torch.nn.Module], EmbeddingBagCollectionSharder())]\n    plan: ShardingPlan = planner.collective_plan(module, sharders, pg)\n\n    sharded_model = DistributedModelParallel(\n        module,\n        env=ShardingEnv.from_process_group(pg),\n        plan=plan,\n        sharders=sharders,\n        device=device,\n    )\n    print(f\"rank:{rank},sharding plan: {plan}\")\n    return sharded_model",
      "import multiprocess\n\ndef spmd_sharing_simulation(\n    sharding_type: ShardingType = ShardingType.TABLE_WISE,\n    world_size = 2,\n):\n  ctx = multiprocess.get_context(\"spawn\")\n  processes = []\n  for rank in range(world_size):\n      p = ctx.Process(\n          target=single_rank_execution,\n          args=(\n              rank,\n              world_size,\n              gen_constraints(sharding_type),\n              ebc,\n              \"nccl\"\n          ),\n      )\n      p.start()\n      processes.append(p)\n\n  for p in processes:\n      p.join()\n      assert 0 == p.exitcode"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html",
    "title": "TorchVision Object Detection Finetuning Tutorial\u00b6",
    "code_snippets": [
      "import matplotlib.pyplot as plt\nfrom torchvision.io import read_image\n\n\nimage = read_image(\"data/PennFudanPed/PNGImages/FudanPed00046.png\")\nmask = read_image(\"data/PennFudanPed/PedMasks/FudanPed00046_mask.png\")\n\nplt.figure(figsize=(16, 8))\nplt.subplot(121)\nplt.title(\"Image\")\nplt.imshow(image.permute(1, 2, 0))\nplt.subplot(122)\nplt.title(\"Mask\")\nplt.imshow(mask.permute(1, 2, 0))",
      "import os\nimport torch\n\nfrom torchvision.io import read_image\nfrom torchvision.ops.boxes import masks_to_boxes\nfrom torchvision import tv_tensors\nfrom torchvision.transforms.v2 import functional as F\n\n\nclass PennFudanDataset(torch.utils.data.Dataset):\n    def __init__(self, root, transforms):\n        self.root = root\n        self.transforms = transforms\n        # load all image files, sorting them to\n        # ensure that they are aligned\n        self.imgs = list(sorted(os.listdir(os.path.join(root, \"PNGImages\"))))\n        self.masks = list(sorted(os.listdir(os.path.join(root, \"PedMasks\"))))\n\n    def __getitem__(self, idx):\n        # load images and masks\n        img_path = os.path.join(self.root, \"PNGImages\", self.imgs[idx])\n        mask_path = os.path.join(self.root, \"PedMasks\", self.masks[idx])\n        img = read_image(img_path)\n        mask = read_image(mask_path)\n        # instances are encoded as different colors\n        obj_ids = torch.unique(mask)\n        # first id is the background, so remove it\n        obj_ids = obj_ids[1:]\n        num_objs = len(obj_ids)\n\n        # split the color-encoded mask into a set\n        # of binary masks\n        masks = (mask == obj_ids[:, None, None]).to(dtype=torch.uint8)\n\n        # get bounding box coordinates for each mask\n        boxes = masks_to_boxes(masks)\n\n        # there is only one class\n        labels = torch.ones((num_objs,), dtype=torch.int64)\n\n        image_id = idx\n        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n        # suppose all instances are not crowd\n        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n\n        # Wrap sample and targets into torchvision tv_tensors:\n        img = tv_tensors.Image(img)\n\n        target = {}\n        target[\"boxes\"] = tv_tensors.BoundingBoxes(boxes, format=\"XYXY\", canvas_size=F.get_size(img))\n        target[\"masks\"] = tv_tensors.Mask(masks)\n        target[\"labels\"] = labels\n        target[\"image_id\"] = image_id\n        target[\"area\"] = area\n        target[\"iscrowd\"] = iscrowd\n\n        if self.transforms is not None:\n            img, target = self.transforms(img, target)\n\n        return img, target\n\n    def __len__(self):\n        return len(self.imgs)",
      "import torchvision\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n\n# load a model pre-trained on COCO\nmodel = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n\n# replace the classifier with a new one, that has\n# num_classes which is user-defined\nnum_classes = 2  # 1 class (person) + background\n# get number of input features for the classifier\nin_features = model.roi_heads.box_predictor.cls_score.in_features\n# replace the pre-trained head with a new one\nmodel.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)",
      "Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /var/lib/ci-user/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n\n  0%|          | 0.00/160M [00:00<?, ?B/s]\n  8%|8         | 13.5M/160M [00:00<00:01, 141MB/s]\n 17%|#6        | 27.0M/160M [00:00<00:01, 131MB/s]\n 43%|####2     | 68.0M/160M [00:00<00:00, 261MB/s]\n 62%|######2   | 99.2M/160M [00:00<00:00, 287MB/s]\n 85%|########4 | 136M/160M [00:00<00:00, 319MB/s]\n100%|##########| 160M/160M [00:00<00:00, 293MB/s]",
      "import torchvision\nfrom torchvision.models.detection import FasterRCNN\nfrom torchvision.models.detection.rpn import AnchorGenerator\n\n# load a pre-trained model for classification and return\n# only the features\nbackbone = torchvision.models.mobilenet_v2(weights=\"DEFAULT\").features\n# ``FasterRCNN`` needs to know the number of\n# output channels in a backbone. For mobilenet_v2, it's 1280\n# so we need to add it here\nbackbone.out_channels = 1280\n\n# let's make the RPN generate 5 x 3 anchors per spatial\n# location, with 5 different sizes and 3 different aspect\n# ratios. We have a Tuple[Tuple[int]] because each feature\n# map could potentially have different sizes and\n# aspect ratios\nanchor_generator = AnchorGenerator(\n    sizes=((32, 64, 128, 256, 512),),\n    aspect_ratios=((0.5, 1.0, 2.0),)\n)\n\n# let's define what are the feature maps that we will\n# use to perform the region of interest cropping, as well as\n# the size of the crop after rescaling.\n# if your backbone returns a Tensor, featmap_names is expected to\n# be [0]. More generally, the backbone should return an\n# ``OrderedDict[Tensor]``, and in ``featmap_names`` you can choose which\n# feature maps to use.\nroi_pooler = torchvision.ops.MultiScaleRoIAlign(\n    featmap_names=['0'],\n    output_size=7,\n    sampling_ratio=2\n)\n\n# put the pieces together inside a Faster-RCNN model\nmodel = FasterRCNN(\n    backbone,\n    num_classes=2,\n    rpn_anchor_generator=anchor_generator,\n    box_roi_pool=roi_pooler\n)",
      "Downloading: \"https://download.pytorch.org/models/mobilenet_v2-7ebf99e0.pth\" to /var/lib/ci-user/.cache/torch/hub/checkpoints/mobilenet_v2-7ebf99e0.pth\n\n  0%|          | 0.00/13.6M [00:00<?, ?B/s]\n100%|##########| 13.6M/13.6M [00:00<00:00, 333MB/s]",
      "import torchvision\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n\n\ndef get_model_instance_segmentation(num_classes):\n    # load an instance segmentation model pre-trained on COCO\n    model = torchvision.models.detection.maskrcnn_resnet50_fpn(weights=\"DEFAULT\")\n\n    # get number of input features for the classifier\n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n    # replace the pre-trained head with a new one\n    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n\n    # now get the number of input features for the mask classifier\n    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n    hidden_layer = 256\n    # and replace the mask predictor with a new one\n    model.roi_heads.mask_predictor = MaskRCNNPredictor(\n        in_features_mask,\n        hidden_layer,\n        num_classes\n    )\n\n    return model",
      "from torchvision.transforms import v2 as T\n\n\ndef get_transform(train):\n    transforms = []\n    if train:\n        transforms.append(T.RandomHorizontalFlip(0.5))\n    transforms.append(T.ToDtype(torch.float, scale=True))\n    transforms.append(T.ToPureTensor())\n    return T.Compose(transforms)",
      "import utils\n\nmodel = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\ndataset = PennFudanDataset('data/PennFudanPed', get_transform(train=True))\ndata_loader = torch.utils.data.DataLoader(\n    dataset,\n    batch_size=2,\n    shuffle=True,\n    collate_fn=utils.collate_fn\n)\n\n# For Training\nimages, targets = next(iter(data_loader))\nimages = list(image for image in images)\ntargets = [{k: v for k, v in t.items()} for t in targets]\noutput = model(images, targets)  # Returns losses and detections\nprint(output)\n\n# For inference\nmodel.eval()\nx = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\npredictions = model(x)  # Returns predictions\nprint(predictions[0])",
      "{'loss_classifier': tensor(0.2446, grad_fn=<NllLossBackward0>), 'loss_box_reg': tensor(0.0372, grad_fn=<DivBackward0>), 'loss_objectness': tensor(0.0457, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(0.0025, grad_fn=<DivBackward0>)}\n{'boxes': tensor([], size=(0, 4), grad_fn=<StackBackward0>), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([], grad_fn=<IndexBackward0>)}",
      "from engine import train_one_epoch, evaluate\n\n# train on the GPU or on the CPU, if a GPU is not available\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\n# our dataset has two classes only - background and person\nnum_classes = 2\n# use our dataset and defined transformations\ndataset = PennFudanDataset('data/PennFudanPed', get_transform(train=True))\ndataset_test = PennFudanDataset('data/PennFudanPed', get_transform(train=False))\n\n# split the dataset in train and test set\nindices = torch.randperm(len(dataset)).tolist()\ndataset = torch.utils.data.Subset(dataset, indices[:-50])\ndataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])\n\n# define training and validation data loaders\ndata_loader = torch.utils.data.DataLoader(\n    dataset,\n    batch_size=2,\n    shuffle=True,\n    collate_fn=utils.collate_fn\n)\n\ndata_loader_test = torch.utils.data.DataLoader(\n    dataset_test,\n    batch_size=1,\n    shuffle=False,\n    collate_fn=utils.collate_fn\n)\n\n# get the model using our helper function\nmodel = get_model_instance_segmentation(num_classes)\n\n# move model to the right device\nmodel.to(device)\n\n# construct an optimizer\nparams = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.SGD(\n    params,\n    lr=0.005,\n    momentum=0.9,\n    weight_decay=0.0005\n)\n\n# and a learning rate scheduler\nlr_scheduler = torch.optim.lr_scheduler.StepLR(\n    optimizer,\n    step_size=3,\n    gamma=0.1\n)\n\n# let's train it just for 2 epochs\nnum_epochs = 2\n\nfor epoch in range(num_epochs):\n    # train for one epoch, printing every 10 iterations\n    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n    # update the learning rate\n    lr_scheduler.step()\n    # evaluate on the test dataset\n    evaluate(model, data_loader_test, device=device)\n\nprint(\"That's it!\")",
      "Downloading: \"https://download.pytorch.org/models/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth\" to /var/lib/ci-user/.cache/torch/hub/checkpoints/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth\n\n  0%|          | 0.00/170M [00:00<?, ?B/s]\n 20%|#9        | 33.2M/170M [00:00<00:00, 346MB/s]\n 39%|###9      | 67.0M/170M [00:00<00:00, 350MB/s]\n 65%|######5   | 111M/170M [00:00<00:00, 401MB/s]\n 89%|########8 | 151M/170M [00:00<00:00, 406MB/s]\n100%|##########| 170M/170M [00:00<00:00, 387MB/s]\n/var/lib/workspace/intermediate_source/engine.py:30: FutureWarning:\n\n`torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n\nEpoch: [0]  [ 0/60]  eta: 0:00:45  lr: 0.000090  loss: 3.0666 (3.0666)  loss_classifier: 0.4826 (0.4826)  loss_box_reg: 0.2067 (0.2067)  loss_mask: 2.3689 (2.3689)  loss_objectness: 0.0033 (0.0033)  loss_rpn_box_reg: 0.0050 (0.0050)  time: 0.7524  data: 0.0155  max mem: 1754\nEpoch: [0]  [10/60]  eta: 0:00:13  lr: 0.000936  loss: 1.2854 (1.8949)  loss_classifier: 0.3004 (0.3353)  loss_box_reg: 0.2117 (0.2582)  loss_mask: 0.8299 (1.2883)  loss_objectness: 0.0087 (0.0088)  loss_rpn_box_reg: 0.0031 (0.0044)  time: 0.2652  data: 0.0166  max mem: 2775\nEpoch: [0]  [20/60]  eta: 0:00:09  lr: 0.001783  loss: 0.8578 (1.3559)  loss_classifier: 0.1856 (0.2623)  loss_box_reg: 0.2635 (0.2771)  loss_mask: 0.3214 (0.7965)  loss_objectness: 0.0115 (0.0135)  loss_rpn_box_reg: 0.0059 (0.0066)  time: 0.2181  data: 0.0169  max mem: 2845\nEpoch: [0]  [30/60]  eta: 0:00:06  lr: 0.002629  loss: 0.6497 (1.0673)  loss_classifier: 0.0975 (0.1995)  loss_box_reg: 0.2466 (0.2531)  loss_mask: 0.2138 (0.5968)  loss_objectness: 0.0106 (0.0115)  loss_rpn_box_reg: 0.0070 (0.0064)  time: 0.2114  data: 0.0156  max mem: 2845\nEpoch: [0]  [40/60]  eta: 0:00:04  lr: 0.003476  loss: 0.4160 (0.9138)  loss_classifier: 0.0605 (0.1668)  loss_box_reg: 0.1997 (0.2420)  loss_mask: 0.1541 (0.4889)  loss_objectness: 0.0029 (0.0093)  loss_rpn_box_reg: 0.0062 (0.0068)  time: 0.2059  data: 0.0150  max mem: 2845\nEpoch: [0]  [50/60]  eta: 0:00:02  lr: 0.004323  loss: 0.3854 (0.7979)  loss_classifier: 0.0491 (0.1421)  loss_box_reg: 0.1442 (0.2165)  loss_mask: 0.1446 (0.4247)  loss_objectness: 0.0022 (0.0081)  loss_rpn_box_reg: 0.0055 (0.0065)  time: 0.2069  data: 0.0154  max mem: 2845\nEpoch: [0]  [59/60]  eta: 0:00:00  lr: 0.005000  loss: 0.3484 (0.7311)  loss_classifier: 0.0456 (0.1278)  loss_box_reg: 0.1222 (0.2030)  loss_mask: 0.1481 (0.3865)  loss_objectness: 0.0028 (0.0074)  loss_rpn_box_reg: 0.0052 (0.0064)  time: 0.2078  data: 0.0158  max mem: 2845\nEpoch: [0] Total time: 0:00:13 (0.2192 s / it)\ncreating index...\nindex created!\nTest:  [ 0/50]  eta: 0:00:05  model_time: 0.0822 (0.0822)  evaluator_time: 0.0061 (0.0061)  time: 0.1005  data: 0.0116  max mem: 2845\nTest:  [49/50]  eta: 0:00:00  model_time: 0.0418 (0.0581)  evaluator_time: 0.0043 (0.0063)  time: 0.0649  data: 0.0103  max mem: 2845\nTest: Total time: 0:00:03 (0.0749 s / it)\nAveraged stats: model_time: 0.0418 (0.0581)  evaluator_time: 0.0043 (0.0063)\nAccumulating evaluation results...\nDONE (t=0.01s).\nAccumulating evaluation results...\nDONE (t=0.01s).\nIoU metric: bbox\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.681\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.988\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.874\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.587\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.687\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.284\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.739\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.739\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.727\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.740\nIoU metric: segm\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.757\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.988\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.963\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.575\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.766\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.311\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.789\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.789\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.745\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.793\nEpoch: [1]  [ 0/60]  eta: 0:00:11  lr: 0.005000  loss: 0.2111 (0.2111)  loss_classifier: 0.0262 (0.0262)  loss_box_reg: 0.0602 (0.0602)  loss_mask: 0.1187 (0.1187)  loss_objectness: 0.0032 (0.0032)  loss_rpn_box_reg: 0.0029 (0.0029)  time: 0.1941  data: 0.0161  max mem: 2845\nEpoch: [1]  [10/60]  eta: 0:00:10  lr: 0.005000  loss: 0.2561 (0.2479)  loss_classifier: 0.0275 (0.0324)  loss_box_reg: 0.0865 (0.0871)  loss_mask: 0.1156 (0.1225)  loss_objectness: 0.0005 (0.0022)  loss_rpn_box_reg: 0.0039 (0.0038)  time: 0.2107  data: 0.0159  max mem: 2845\nEpoch: [1]  [20/60]  eta: 0:00:08  lr: 0.005000  loss: 0.2625 (0.2663)  loss_classifier: 0.0335 (0.0347)  loss_box_reg: 0.0901 (0.0886)  loss_mask: 0.1274 (0.1366)  loss_objectness: 0.0005 (0.0022)  loss_rpn_box_reg: 0.0039 (0.0041)  time: 0.2130  data: 0.0159  max mem: 2845\nEpoch: [1]  [30/60]  eta: 0:00:06  lr: 0.005000  loss: 0.2625 (0.2652)  loss_classifier: 0.0348 (0.0365)  loss_box_reg: 0.0875 (0.0840)  loss_mask: 0.1438 (0.1391)  loss_objectness: 0.0004 (0.0016)  loss_rpn_box_reg: 0.0031 (0.0040)  time: 0.2090  data: 0.0148  max mem: 2845\nEpoch: [1]  [40/60]  eta: 0:00:04  lr: 0.005000  loss: 0.2868 (0.2763)  loss_classifier: 0.0391 (0.0382)  loss_box_reg: 0.0875 (0.0873)  loss_mask: 0.1473 (0.1449)  loss_objectness: 0.0004 (0.0019)  loss_rpn_box_reg: 0.0031 (0.0041)  time: 0.2090  data: 0.0153  max mem: 2845\nEpoch: [1]  [50/60]  eta: 0:00:02  lr: 0.005000  loss: 0.3167 (0.2808)  loss_classifier: 0.0407 (0.0391)  loss_box_reg: 0.0864 (0.0874)  loss_mask: 0.1541 (0.1480)  loss_objectness: 0.0013 (0.0020)  loss_rpn_box_reg: 0.0046 (0.0043)  time: 0.2101  data: 0.0160  max mem: 2845\nEpoch: [1]  [59/60]  eta: 0:00:00  lr: 0.005000  loss: 0.2866 (0.2775)  loss_classifier: 0.0343 (0.0383)  loss_box_reg: 0.0855 (0.0846)  loss_mask: 0.1563 (0.1484)  loss_objectness: 0.0009 (0.0018)  loss_rpn_box_reg: 0.0042 (0.0043)  time: 0.2094  data: 0.0160  max mem: 2921\nEpoch: [1] Total time: 0:00:12 (0.2103 s / it)\ncreating index...\nindex created!\nTest:  [ 0/50]  eta: 0:00:03  model_time: 0.0466 (0.0466)  evaluator_time: 0.0034 (0.0034)  time: 0.0616  data: 0.0112  max mem: 2921\nTest:  [49/50]  eta: 0:00:00  model_time: 0.0399 (0.0409)  evaluator_time: 0.0031 (0.0042)  time: 0.0557  data: 0.0103  max mem: 2921\nTest: Total time: 0:00:02 (0.0555 s / it)\nAveraged stats: model_time: 0.0399 (0.0409)  evaluator_time: 0.0031 (0.0042)\nAccumulating evaluation results...\nDONE (t=0.01s).\nAccumulating evaluation results...\nDONE (t=0.01s).\nIoU metric: bbox\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.794\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.989\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.931\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.631\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.800\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.334\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.835\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.835\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.827\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.835\nIoU metric: segm\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.764\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.989\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.943\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.572\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.775\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.321\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.797\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.797\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.727\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.803\nThat's it!",
      "import matplotlib.pyplot as plt\n\nfrom torchvision.utils import draw_bounding_boxes, draw_segmentation_masks\n\n\nimage = read_image(\"data/PennFudanPed/PNGImages/FudanPed00046.png\")\neval_transform = get_transform(train=False)\n\nmodel.eval()\nwith torch.no_grad():\n    x = eval_transform(image)\n    # convert RGBA -> RGB and move to device\n    x = x[:3, ...].to(device)\n    predictions = model([x, ])\n    pred = predictions[0]\n\n\nimage = (255.0 * (image - image.min()) / (image.max() - image.min())).to(torch.uint8)\nimage = image[:3, ...]\npred_labels = [f\"pedestrian: {score:.3f}\" for label, score in zip(pred[\"labels\"], pred[\"scores\"])]\npred_boxes = pred[\"boxes\"].long()\noutput_image = draw_bounding_boxes(image, pred_boxes, pred_labels, colors=\"red\")\n\nmasks = (pred[\"masks\"] > 0.7).squeeze(1)\noutput_image = draw_segmentation_masks(output_image, masks, alpha=0.5, colors=\"blue\")\n\n\nplt.figure(figsize=(12, 12))\nplt.imshow(output_image.permute(1, 2, 0))"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/recipes/torchscript_inference.html",
    "title": "TorchScript for Deployment\u00b6",
    "code_snippets": [
      "import torch\nimport torch.nn.functional as F\nimport torchvision.models as models\n\nr18 = models.resnet18(pretrained=True)       # We now have an instance of the pretrained model\nr18_scripted = torch.jit.script(r18)         # *** This is the TorchScript export\ndummy_input = torch.rand(1, 3, 224, 224)     # We should run a quick test"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/advanced/cpp_frontend.html",
    "title": "Using the PyTorch C++ Frontend\u00b6",
    "code_snippets": [
      "# If you need e.g. CUDA 9.0 support, please replace \"cpu\" with \"cu90\" in the URL below.\nwget https://download.pytorch.org/libtorch/nightly/cpu/libtorch-shared-with-deps-latest.zip\nunzip libtorch-shared-with-deps-latest.zip",
      "#include <torch/torch.h>\n#include <iostream>\n\nint main() {\n  torch::Tensor tensor = torch::eye(3);\n  std::cout << tensor << std::endl;\n}",
      "root@fa350df05ecf:/home# mkdir build\nroot@fa350df05ecf:/home# cd build\nroot@fa350df05ecf:/home/build# cmake -DCMAKE_PREFIX_PATH=/path/to/libtorch ..\n-- The C compiler identification is GNU 5.4.0\n-- The CXX compiler identification is GNU 5.4.0\n-- Check for working C compiler: /usr/bin/cc\n-- Check for working C compiler: /usr/bin/cc -- works\n-- Detecting C compiler ABI info\n-- Detecting C compiler ABI info - done\n-- Detecting C compile features\n-- Detecting C compile features - done\n-- Check for working CXX compiler: /usr/bin/c++\n-- Check for working CXX compiler: /usr/bin/c++ -- works\n-- Detecting CXX compiler ABI info\n-- Detecting CXX compiler ABI info - done\n-- Detecting CXX compile features\n-- Detecting CXX compile features - done\n-- Looking for pthread.h\n-- Looking for pthread.h - found\n-- Looking for pthread_create\n-- Looking for pthread_create - not found\n-- Looking for pthread_create in pthreads\n-- Looking for pthread_create in pthreads - not found\n-- Looking for pthread_create in pthread\n-- Looking for pthread_create in pthread - found\n-- Found Threads: TRUE\n-- Found torch: /path/to/libtorch/lib/libtorch.so\n-- Configuring done\n-- Generating done\n-- Build files have been written to: /home/build\nroot@fa350df05ecf:/home/build# cmake --build . --config Release\nScanning dependencies of target dcgan\n[ 50%] Building CXX object CMakeFiles/dcgan.dir/dcgan.cpp.o\n[100%] Linking CXX executable dcgan\n[100%] Built target dcgan",
      "import torch\n\nclass Net(torch.nn.Module):\n  def __init__(self, N, M):\n    super(Net, self).__init__()\n    self.W = torch.nn.Parameter(torch.randn(N, M))\n    self.b = torch.nn.Parameter(torch.randn(M))\n\n  def forward(self, input):\n    return torch.addmm(self.b, input, self.W)",
      "#include <torch/torch.h>\n\nstruct Net : torch::nn::Module {\n  Net(int64_t N, int64_t M) {\n    W = register_parameter(\"W\", torch::randn({N, M}));\n    b = register_parameter(\"b\", torch::randn(M));\n  }\n  torch::Tensor forward(torch::Tensor input) {\n    return torch::addmm(b, input, W);\n  }\n  torch::Tensor W, b;\n};",
      "class Net(torch.nn.Module):\n  def __init__(self, N, M):\n      super(Net, self).__init__()\n      # Registered as a submodule behind the scenes\n      self.linear = torch.nn.Linear(N, M)\n      self.another_bias = torch.nn.Parameter(torch.rand(M))\n\n  def forward(self, input):\n    return self.linear(input) + self.another_bias",
      "import argparse\n\nimport matplotlib.pyplot as plt\nimport torch\n\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\"-i\", \"--sample-file\", required=True)\nparser.add_argument(\"-o\", \"--out-file\", default=\"out.png\")\nparser.add_argument(\"-d\", \"--dimension\", type=int, default=3)\noptions = parser.parse_args()\n\nmodule = torch.jit.load(options.sample_file)\nimages = list(module.parameters())[0]\n\nfor index in range(options.dimension * options.dimension):\n  image = images[index].detach().cpu().reshape(28, 28).mul(255).to(torch.uint8)\n  array = image.numpy()\n  axis = plt.subplot(options.dimension, options.dimension, 1 + index)\n  plt.imshow(array, cmap=\"gray\")\n  axis.get_xaxis().set_visible(False)\n  axis.get_yaxis().set_visible(False)\n\nplt.savefig(options.out_file)\nprint(\"Saved \", options.out_file)"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/recipes/recipes/what_is_state_dict.html",
    "title": "What is a state_dict in PyTorch\u00b6",
    "code_snippets": [
      "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim",
      "class Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(3, 6, 5)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = x.view(-1, 16 * 5 * 5)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\nnet = Net()\nprint(net)"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html",
    "title": "DCGAN Tutorial\u00b6",
    "code_snippets": [
      "#%matplotlib inline\nimport argparse\nimport os\nimport random\nimport torch\nimport torch.nn as nn\nimport torch.nn.parallel\nimport torch.optim as optim\nimport torch.utils.data\nimport torchvision.datasets as dset\nimport torchvision.transforms as transforms\nimport torchvision.utils as vutils\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom IPython.display import HTML\n\n# Set random seed for reproducibility\nmanualSeed = 999\n#manualSeed = random.randint(1, 10000) # use if you want new results\nprint(\"Random Seed: \", manualSeed)\nrandom.seed(manualSeed)\ntorch.manual_seed(manualSeed)\ntorch.use_deterministic_algorithms(True) # Needed for reproducible results",
      "# We can use an image folder dataset the way we have it setup.\n# Create the dataset\ndataset = dset.ImageFolder(root=dataroot,\n                           transform=transforms.Compose([\n                               transforms.Resize(image_size),\n                               transforms.CenterCrop(image_size),\n                               transforms.ToTensor(),\n                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n                           ]))\n# Create the dataloader\ndataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n                                         shuffle=True, num_workers=workers)\n\n# Decide which device we want to run on\ndevice = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n\n# Plot some training images\nreal_batch = next(iter(dataloader))\nplt.figure(figsize=(8,8))\nplt.axis(\"off\")\nplt.title(\"Training Images\")\nplt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=2, normalize=True).cpu(),(1,2,0)))\nplt.show()",
      "# custom weights initialization called on ``netG`` and ``netD``\ndef weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find('BatchNorm') != -1:\n        nn.init.normal_(m.weight.data, 1.0, 0.02)\n        nn.init.constant_(m.bias.data, 0)",
      "# Generator Code\n\nclass Generator(nn.Module):\n    def __init__(self, ngpu):\n        super(Generator, self).__init__()\n        self.ngpu = ngpu\n        self.main = nn.Sequential(\n            # input is Z, going into a convolution\n            nn.ConvTranspose2d( nz, ngf * 8, 4, 1, 0, bias=False),\n            nn.BatchNorm2d(ngf * 8),\n            nn.ReLU(True),\n            # state size. ``(ngf*8) x 4 x 4``\n            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf * 4),\n            nn.ReLU(True),\n            # state size. ``(ngf*4) x 8 x 8``\n            nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf * 2),\n            nn.ReLU(True),\n            # state size. ``(ngf*2) x 16 x 16``\n            nn.ConvTranspose2d( ngf * 2, ngf, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf),\n            nn.ReLU(True),\n            # state size. ``(ngf) x 32 x 32``\n            nn.ConvTranspose2d( ngf, nc, 4, 2, 1, bias=False),\n            nn.Tanh()\n            # state size. ``(nc) x 64 x 64``\n        )\n\n    def forward(self, input):\n        return self.main(input)",
      "class Discriminator(nn.Module):\n    def __init__(self, ngpu):\n        super(Discriminator, self).__init__()\n        self.ngpu = ngpu\n        self.main = nn.Sequential(\n            # input is ``(nc) x 64 x 64``\n            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. ``(ndf) x 32 x 32``\n            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 2),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. ``(ndf*2) x 16 x 16``\n            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 4),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. ``(ndf*4) x 8 x 8``\n            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 8),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. ``(ndf*8) x 4 x 4``\n            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n            nn.Sigmoid()\n        )\n\n    def forward(self, input):\n        return self.main(input)",
      "# Initialize the ``BCELoss`` function\ncriterion = nn.BCELoss()\n\n# Create batch of latent vectors that we will use to visualize\n#  the progression of the generator\nfixed_noise = torch.randn(64, nz, 1, 1, device=device)\n\n# Establish convention for real and fake labels during training\nreal_label = 1.\nfake_label = 0.\n\n# Setup Adam optimizers for both G and D\noptimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\noptimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))",
      "# Training Loop\n\n# Lists to keep track of progress\nimg_list = []\nG_losses = []\nD_losses = []\niters = 0\n\nprint(\"Starting Training Loop...\")\n# For each epoch\nfor epoch in range(num_epochs):\n    # For each batch in the dataloader\n    for i, data in enumerate(dataloader, 0):\n\n        ############################\n        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n        ###########################\n        ## Train with all-real batch\n        netD.zero_grad()\n        # Format batch\n        real_cpu = data[0].to(device)\n        b_size = real_cpu.size(0)\n        label = torch.full((b_size,), real_label, dtype=torch.float, device=device)\n        # Forward pass real batch through D\n        output = netD(real_cpu).view(-1)\n        # Calculate loss on all-real batch\n        errD_real = criterion(output, label)\n        # Calculate gradients for D in backward pass\n        errD_real.backward()\n        D_x = output.mean().item()\n\n        ## Train with all-fake batch\n        # Generate batch of latent vectors\n        noise = torch.randn(b_size, nz, 1, 1, device=device)\n        # Generate fake image batch with G\n        fake = netG(noise)\n        label.fill_(fake_label)\n        # Classify all fake batch with D\n        output = netD(fake.detach()).view(-1)\n        # Calculate D's loss on the all-fake batch\n        errD_fake = criterion(output, label)\n        # Calculate the gradients for this batch, accumulated (summed) with previous gradients\n        errD_fake.backward()\n        D_G_z1 = output.mean().item()\n        # Compute error of D as sum over the fake and the real batches\n        errD = errD_real + errD_fake\n        # Update D\n        optimizerD.step()\n\n        ############################\n        # (2) Update G network: maximize log(D(G(z)))\n        ###########################\n        netG.zero_grad()\n        label.fill_(real_label)  # fake labels are real for generator cost\n        # Since we just updated D, perform another forward pass of all-fake batch through D\n        output = netD(fake).view(-1)\n        # Calculate G's loss based on this output\n        errG = criterion(output, label)\n        # Calculate gradients for G\n        errG.backward()\n        D_G_z2 = output.mean().item()\n        # Update G\n        optimizerG.step()\n\n        # Output training stats\n        if i % 50 == 0:\n            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n                  % (epoch, num_epochs, i, len(dataloader),\n                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n\n        # Save Losses for plotting later\n        G_losses.append(errG.item())\n        D_losses.append(errD.item())\n\n        # Check how the generator is doing by saving G's output on fixed_noise\n        if (iters % 500 == 0) or ((epoch == num_epochs-1) and (i == len(dataloader)-1)):\n            with torch.no_grad():\n                fake = netG(fixed_noise).detach().cpu()\n            img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n\n        iters += 1"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/beginner/introyt/autogradyt_tutorial.html",
    "title": "The Fundamentals of Autograd\u00b6",
    "code_snippets": [
      "# %matplotlib inline\n\nimport torch\n\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\nimport math",
      "a = torch.linspace(0., 2. * math.pi, steps=25, requires_grad=True)\nprint(a)",
      "b = torch.sin(a)\nplt.plot(a.detach(), b.detach())",
      "a = torch.linspace(0., 2. * math.pi, steps=25, requires_grad=True)\nb = torch.sin(a)\nc = 2 * b\nd = c + 1\nout = d.sum()",
      "BATCH_SIZE = 16\nDIM_IN = 1000\nHIDDEN_SIZE = 100\nDIM_OUT = 10\n\nclass TinyModel(torch.nn.Module):\n\n    def __init__(self):\n        super(TinyModel, self).__init__()\n\n        self.layer1 = torch.nn.Linear(DIM_IN, HIDDEN_SIZE)\n        self.relu = torch.nn.ReLU()\n        self.layer2 = torch.nn.Linear(HIDDEN_SIZE, DIM_OUT)\n\n    def forward(self, x):\n        x = self.layer1(x)\n        x = self.relu(x)\n        x = self.layer2(x)\n        return x\n\nsome_input = torch.randn(BATCH_SIZE, DIM_IN, requires_grad=False)\nideal_output = torch.randn(BATCH_SIZE, DIM_OUT, requires_grad=False)\n\nmodel = TinyModel()",
      "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n\nprediction = model(some_input)\n\nloss = (ideal_output - prediction).pow(2).sum()\nprint(loss)",
      "a = torch.ones(2, 3, requires_grad=True)\nprint(a)\n\nb1 = 2 * a\nprint(b1)\n\na.requires_grad = False\nb2 = 2 * a\nprint(b2)",
      "a = torch.ones(2, 3, requires_grad=True) * 2\nb = torch.ones(2, 3, requires_grad=True) * 3\n\nc1 = a + b\nprint(c1)\n\nwith torch.no_grad():\n    c2 = a + b\n\nprint(c2)\n\nc3 = a * b\nprint(c3)",
      "def add_tensors1(x, y):\n    return x + y\n\n@torch.no_grad()\ndef add_tensors2(x, y):\n    return x + y\n\n\na = torch.ones(2, 3, requires_grad=True) * 2\nb = torch.ones(2, 3, requires_grad=True) * 3\n\nc1 = add_tensors1(a, b)\nprint(c1)\n\nc2 = add_tensors2(a, b)\nprint(c2)",
      "x = torch.rand(5, requires_grad=True)\ny = x.detach()\n\nprint(x)\nprint(y)",
      "a = torch.linspace(0., 2. * math.pi, steps=25, requires_grad=True)\ntorch.sin_(a)",
      "device = torch.device('cpu')\nrun_on_gpu = False\nif torch.cuda.is_available():\n    device = torch.device('cuda')\n    run_on_gpu = True\n\nx = torch.randn(2, 3, requires_grad=True)\ny = torch.rand(2, 3, requires_grad=True)\nz = torch.ones(2, 3, requires_grad=True)\n\nwith torch.autograd.profiler.profile(use_cuda=run_on_gpu) as prf:\n    for _ in range(1000):\n        z = (z / x) * y\n\nprint(prf.key_averages().table(sort_by='self_cpu_time_total'))",
      "x = torch.randn(3, requires_grad=True)\n\ny = x * 2\nwhile y.data.norm() < 1000:\n    y = y * 2\n\nprint(y)",
      "v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float) # stand-in for gradients\ny.backward(v)\n\nprint(x.grad)",
      "def exp_adder(x, y):\n    return 2 * x.exp() + 3 * y\n\ninputs = (torch.rand(1), torch.rand(1)) # arguments for the function\nprint(inputs)\ntorch.autograd.functional.jacobian(exp_adder, inputs)",
      "inputs = (torch.rand(3), torch.rand(3)) # arguments for the function\nprint(inputs)\ntorch.autograd.functional.jacobian(exp_adder, inputs)",
      "def do_some_doubling(x):\n    y = x * 2\n    while y.data.norm() < 1000:\n        y = y * 2\n    return y\n\ninputs = torch.randn(3)\nmy_gradients = torch.tensor([0.1, 1.0, 0.0001])\ntorch.autograd.functional.vjp(do_some_doubling, inputs, v=my_gradients)"
    ]
  },
  {
    "url": "https://pytorch.org/executorch/stable/examples-end-to-end-to-lower-model-to-delegate.html",
    "title": "Lowering a Model as a Delegate\u00b6",
    "code_snippets": [
      "# defined in backend_api.py\ndef to_backend(\n    backend_id: str,\n    edge_program: ExportedProgram,\n    compile_spec: List[CompileSpec],\n) -> LoweredBackendModule:",
      "from executorch.exir.backend.backend_api import to_backend\nimport executorch.exir as exir\nimport torch\nfrom torch.export import export\nfrom executorch.exir import to_edge\n\n# The submodule runs in a specific backend. In this example,  `BackendWithCompilerDemo` backend\nclass LowerableSubModel(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x):\n        return torch.sin(x)\n\n# Convert the lowerable module to Edge IR Representation\nto_be_lowered = LowerableSubModel()\nexample_input = (torch.ones(1), )\nto_be_lowered_exir_submodule = to_edge(export(to_be_lowered, example_input))\n\n# Import the backend implementation\nfrom executorch.exir.backend.test.backend_with_compiler_demo import (\n    BackendWithCompilerDemo,\n)\nlowered_module = to_backend('BackendWithCompilerDemo', to_be_lowered_exir_submodule.exported_program(), [])",
      "# This submodule runs in executor runtime\nclass NonLowerableSubModel(torch.nn.Module):\n    def __init__(self, bias):\n        super().__init__()\n        self.bias = bias\n\n    def forward(self, a, b):\n        return torch.add(torch.add(a, b), self.bias)\n\n\n# The composite module, including lower part and non-lowerpart\nclass CompositeModel(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.non_lowerable = NonLowerableSubModel(torch.ones(1) * 0.3)\n        self.lowerable = lowered_module\n\n    def forward(self, x):\n        a = self.lowerable(x)\n        b = self.lowerable(a)\n        ret = self.non_lowerable(a, b)\n        return a, b, ret\n\ncomposite_model = CompositeModel()\nmodel_inputs = (torch.ones(1), )\nexec_prog = to_edge(export(composite_model, model_inputs)).to_executorch()\n\n# Save the flatbuffer to a local file\nsave_path = \"delegate.pte\"\nwith open(save_path, \"wb\") as f:\n    f.write(exec_prog.buffer)",
      "def to_backend(\n    edge_program: ExportedProgram,\n    partitioner: Partitioner,\n) -> ExportedProgram:",
      "import executorch.exir as exir\nfrom executorch.exir.backend.backend_api import to_backend\nfrom executorch.exir.backend.test.op_partitioner_demo import AddMulPartitionerDemo\nfrom executorch.exir.program import (\n    EdgeProgramManager,\n    to_edge,\n)\nfrom torch.export import export\nimport torch\n\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x, y):\n        x = x + y\n        x = x * y\n        x = x - y\n        x = x / y\n        x = x * y\n        x = x + y\n        return x\n\nmodel = Model()\nmodel_inputs = (torch.randn(1, 3), torch.randn(1, 3))\n\ncore_aten_ep = export(model, model_inputs)\nedge: EdgeProgramManager = to_edge(core_aten_ep)\nedge = edge.to_backend(AddMulPartitionerDemo())\nexec_prog = edge.to_executorch()\n\n# Save the flatbuffer to a local file\nsave_path = \"delegate.pte\"\nwith open(save_path, \"wb\") as f:\n    f.write(exec_prog.buffer)"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/recipes/torchserve_vertexai_tutorial.html",
    "title": "Deploying a PyTorch Stable Diffusion model as a Vertex AI Endpoint\u00b6",
    "code_snippets": [
      "from google.cloud import aiplatform as vertexai\nPYTORCH_PREDICTION_IMAGE_URI = (\n    \"us-docker.pkg.dev/vertex-ai/prediction/pytorch-gpu.1-12:latest\"\n)\nMODEL_DISPLAY_NAME = \"stable_diffusion_1_5-unique\"\nMODEL_DESCRIPTION = \"stable_diffusion_1_5 container\"\n\nvertexai.init(project='your_project', location='us-central1', staging_bucket=BUCKET_NAME)\n\nmodel = aiplatform.Model.upload(\n    display_name=MODEL_DISPLAY_NAME,\n    description=MODEL_DESCRIPTION,\n    serving_container_image_uri=PYTORCH_PREDICTION_IMAGE_URI,\n    artifact_uri=BUCKET_URI,\n)"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/intermediate/parametrizations.html",
    "title": "Parametrizations Tutorial\u00b6",
    "code_snippets": [
      "import torch\nimport torch.nn as nn\nimport torch.nn.utils.parametrize as parametrize\n\ndef symmetric(X):\n    return X.triu() + X.triu(1).transpose(-1, -2)\n\nX = torch.rand(3, 3)\nA = symmetric(X)\nassert torch.allclose(A, A.T)  # A is symmetric\nprint(A)                       # Quick visual check",
      "class LinearSymmetric(nn.Module):\n    def __init__(self, n_features):\n        super().__init__()\n        self.weight = nn.Parameter(torch.rand(n_features, n_features))\n\n    def forward(self, x):\n        A = symmetric(self.weight)\n        return x @ A",
      "layer = LinearSymmetric(3)\nout = layer(torch.rand(8, 3))",
      "class Symmetric(nn.Module):\n    def forward(self, X):\n        return X.triu() + X.triu(1).transpose(-1, -2)",
      "A = layer.weight\nassert torch.allclose(A, A.T)  # A is symmetric\nprint(A)                       # Quick visual check",
      "class Skew(nn.Module):\n    def forward(self, X):\n        A = X.triu(1)\n        return A - A.transpose(-1, -2)\n\n\ncnn = nn.Conv2d(in_channels=5, out_channels=8, kernel_size=3)\nparametrize.register_parametrization(cnn, \"weight\", Skew())\n# Print a few kernels\nprint(cnn.weight[0, 1])\nprint(cnn.weight[2, 2])",
      "symmetric = Symmetric()\nweight_orig = layer.parametrizations.weight.original\nprint(torch.dist(layer.weight, symmetric(weight_orig)))",
      "class NoisyParametrization(nn.Module):\n    def forward(self, X):\n        print(\"Computing the Parametrization\")\n        return X\n\nlayer = nn.Linear(4, 4)\nparametrize.register_parametrization(layer, \"weight\", NoisyParametrization())\nprint(\"Here, layer.weight is recomputed every time we call it\")\nfoo = layer.weight + layer.weight.T\nbar = layer.weight.sum()\nwith parametrize.cached():\n    print(\"Here, it is computed just the first time layer.weight is called\")\n    foo = layer.weight + layer.weight.T\n    bar = layer.weight.sum()",
      "class CayleyMap(nn.Module):\n    def __init__(self, n):\n        super().__init__()\n        self.register_buffer(\"Id\", torch.eye(n))\n\n    def forward(self, X):\n        # (I + X)(I - X)^{-1}\n        return torch.linalg.solve(self.Id - X, self.Id + X)\n\nlayer = nn.Linear(3, 3)\nparametrize.register_parametrization(layer, \"weight\", Skew())\nparametrize.register_parametrization(layer, \"weight\", CayleyMap(3))\nX = layer.weight\nprint(torch.dist(X.T @ X, torch.eye(3)))  # X is orthogonal",
      "class MatrixExponential(nn.Module):\n    def forward(self, X):\n        return torch.matrix_exp(X)\n\nlayer_orthogonal = nn.Linear(3, 3)\nparametrize.register_parametrization(layer_orthogonal, \"weight\", Skew())\nparametrize.register_parametrization(layer_orthogonal, \"weight\", MatrixExponential())\nX = layer_orthogonal.weight\nprint(torch.dist(X.T @ X, torch.eye(3)))         # X is orthogonal\n\nlayer_spd = nn.Linear(3, 3)\nparametrize.register_parametrization(layer_spd, \"weight\", Symmetric())\nparametrize.register_parametrization(layer_spd, \"weight\", MatrixExponential())\nX = layer_spd.weight\nprint(torch.dist(X, X.T))                        # X is symmetric\nprint((torch.linalg.eigvalsh(X) > 0.).all())  # X is positive definite",
      "def right_inverse(self, X: Tensor) -> Tensor",
      "class Skew(nn.Module):\n    def forward(self, X):\n        A = X.triu(1)\n        return A - A.transpose(-1, -2)\n\n    def right_inverse(self, A):\n        # We assume that A is skew-symmetric\n        # We take the upper-triangular elements, as these are those used in the forward\n        return A.triu(1)",
      "layer = nn.Linear(3, 3)\nparametrize.register_parametrization(layer, \"weight\", Skew())\nX = torch.rand(3, 3)\nX = X - X.T                             # X is now skew-symmetric\nlayer.weight = X                        # Initialize layer.weight to be X\nprint(torch.dist(layer.weight, X))      # layer.weight == X",
      "class CayleyMap(nn.Module):\n    def __init__(self, n):\n        super().__init__()\n        self.register_buffer(\"Id\", torch.eye(n))\n\n    def forward(self, X):\n        # Assume X skew-symmetric\n        # (I + X)(I - X)^{-1}\n        return torch.linalg.solve(self.Id - X, self.Id + X)\n\n    def right_inverse(self, A):\n        # Assume A orthogonal\n        # See https://en.wikipedia.org/wiki/Cayley_transform#Matrix_map\n        # (A - I)(A + I)^{-1}\n        return torch.linalg.solve(A + self.Id, self.Id - A)\n\nlayer_orthogonal = nn.Linear(3, 3)\nparametrize.register_parametrization(layer_orthogonal, \"weight\", Skew())\nparametrize.register_parametrization(layer_orthogonal, \"weight\", CayleyMap(3))\n# Sample an orthogonal matrix with positive determinant\nX = torch.empty(3, 3)\nnn.init.orthogonal_(X)\nif X.det() < 0.:\n    X[0].neg_()\nlayer_orthogonal.weight = X\nprint(torch.dist(layer_orthogonal.weight, X))  # layer_orthogonal.weight == X",
      "class PruningParametrization(nn.Module):\n    def __init__(self, X, p_drop=0.2):\n        super().__init__()\n        # sample zeros with probability p_drop\n        mask = torch.full_like(X, 1.0 - p_drop)\n        self.mask = torch.bernoulli(mask)\n\n    def forward(self, X):\n        return X * self.mask\n\n    def right_inverse(self, A):\n        return A",
      "layer = nn.Linear(3, 4)\nX = torch.rand_like(layer.weight)\nprint(f\"Initialization matrix:\\n{X}\")\nparametrize.register_parametrization(layer, \"weight\", PruningParametrization(layer.weight))\nlayer.weight = X\nprint(f\"\\nInitialized weight:\\n{layer.weight}\")"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/recipes/compiling_optimizer.html",
    "title": "(beta) Compiling the optimizer with torch.compile\u00b6",
    "code_snippets": [
      "import torch\n\nmodel = torch.nn.Sequential(\n    *[torch.nn.Linear(1024, 1024, False, device=\"cuda\") for _ in range(10)]\n)\ninput = torch.rand(1024, device=\"cuda\")\noutput = model(input)\noutput.sum().backward()",
      "# exit cleanly if we are on a device that doesn't support torch.compile\nif torch.cuda.get_device_capability() < (7, 0):\n    print(\"Exiting because torch.compile is not supported on this device.\")\n    import sys\n    sys.exit(0)\n\n\nopt = torch.optim.Adam(model.parameters(), lr=0.01)\n\n\n@torch.compile(fullgraph=False)\ndef fn():\n    opt.step()\n\n\n# Let's define a helpful benchmarking function:\nimport torch.utils.benchmark as benchmark\n\n\ndef benchmark_torch_function_in_microseconds(f, *args, **kwargs):\n    t0 = benchmark.Timer(\n        stmt=\"f(*args, **kwargs)\", globals={\"args\": args, \"kwargs\": kwargs, \"f\": f}\n    )\n    return t0.blocked_autorange().mean * 1e6\n\n\n# Warmup runs to compile the function\nfor _ in range(5):\n    fn()\n\neager_runtime = benchmark_torch_function_in_microseconds(opt.step)\ncompiled_runtime = benchmark_torch_function_in_microseconds(fn)\n\nassert eager_runtime > compiled_runtime\n\nprint(f\"eager runtime: {eager_runtime}us\")\nprint(f\"compiled runtime: {compiled_runtime}us\")"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/recipes/intel_extension_for_pytorch.html",
    "title": "Intel\u00ae Extension for PyTorch*\u00b6",
    "code_snippets": [
      "import torch\nimport torchvision\nimport intel_extension_for_pytorch as ipex\n\nLR = 0.001\nDOWNLOAD = True\nDATA = 'datasets/cifar10/'\n\ntransform = torchvision.transforms.Compose([\n    torchvision.transforms.Resize((224, 224)),\n    torchvision.transforms.ToTensor(),\n    torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\ntrain_dataset = torchvision.datasets.CIFAR10(\n        root=DATA,\n        train=True,\n        transform=transform,\n        download=DOWNLOAD,\n)\ntrain_loader = torch.utils.data.DataLoader(\n        dataset=train_dataset,\n        batch_size=128\n)\n\nmodel = torchvision.models.resnet50()\ncriterion = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr = LR, momentum=0.9)\nmodel.train()\nmodel, optimizer = ipex.optimize(model, optimizer=optimizer)\n\nfor batch_idx, (data, target) in enumerate(train_loader):\n    optimizer.zero_grad()\n    output = model(data)\n    loss = criterion(output, target)\n    loss.backward()\n    optimizer.step()\n    print(batch_idx)\ntorch.save({\n     'model_state_dict': model.state_dict(),\n     'optimizer_state_dict': optimizer.state_dict(),\n     }, 'checkpoint.pth')",
      "import torch\nimport torchvision\nimport intel_extension_for_pytorch as ipex\n\nLR = 0.001\nDOWNLOAD = True\nDATA = 'datasets/cifar10/'\n\ntransform = torchvision.transforms.Compose([\n    torchvision.transforms.Resize((224, 224)),\n    torchvision.transforms.ToTensor(),\n    torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\ntrain_dataset = torchvision.datasets.CIFAR10(\n        root=DATA,\n        train=True,\n        transform=transform,\n        download=DOWNLOAD,\n)\ntrain_loader = torch.utils.data.DataLoader(\n        dataset=train_dataset,\n        batch_size=128\n)\n\nmodel = torchvision.models.resnet50()\ncriterion = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr = LR, momentum=0.9)\nmodel.train()\nmodel, optimizer = ipex.optimize(model, optimizer=optimizer, dtype=torch.bfloat16)\n\nfor batch_idx, (data, target) in enumerate(train_loader):\n    optimizer.zero_grad()\n    with torch.cpu.amp.autocast():\n        output = model(data)\n        loss = criterion(output, target)\n        loss.backward()\n    optimizer.step()\n    print(batch_idx)\ntorch.save({\n     'model_state_dict': model.state_dict(),\n     'optimizer_state_dict': optimizer.state_dict(),\n     }, 'checkpoint.pth')",
      "import torch\nimport torchvision.models as models\n\nmodel = models.resnet50(pretrained=True)\nmodel.eval()\ndata = torch.rand(1, 3, 224, 224)\n\n#################### code changes ####################\nimport intel_extension_for_pytorch as ipex\nmodel = ipex.optimize(model)\n######################################################\n\nwith torch.no_grad():\n  model(data)",
      "import torch\nfrom transformers import BertModel\n\nmodel = BertModel.from_pretrained(args.model_name)\nmodel.eval()\n\nvocab_size = model.config.vocab_size\nbatch_size = 1\nseq_length = 512\ndata = torch.randint(vocab_size, size=[batch_size, seq_length])\n\n#################### code changes ####################\nimport intel_extension_for_pytorch as ipex\nmodel = ipex.optimize(model, dtype=torch.bfloat16)\n######################################################\n\nwith torch.no_grad():\n  with torch.cpu.amp.autocast():\n    model(data)",
      "import torch\nimport torchvision.models as models\n\nmodel = models.resnet50(pretrained=True)\nmodel.eval()\ndata = torch.rand(1, 3, 224, 224)\n\n#################### code changes ####################\nimport intel_extension_for_pytorch as ipex\nmodel = ipex.optimize(model)\n######################################################\n\nwith torch.no_grad():\n  d = torch.rand(1, 3, 224, 224)\n  model = torch.jit.trace(model, d)\n  model = torch.jit.freeze(model)\n\n  model(data)",
      "import torch\nfrom transformers import BertModel\n\nmodel = BertModel.from_pretrained(args.model_name)\nmodel.eval()\n\nvocab_size = model.config.vocab_size\nbatch_size = 1\nseq_length = 512\ndata = torch.randint(vocab_size, size=[batch_size, seq_length])\n\n#################### code changes ####################\nimport intel_extension_for_pytorch as ipex\nmodel = ipex.optimize(model, dtype=torch.bfloat16)\n######################################################\n\nwith torch.no_grad():\n  with torch.cpu.amp.autocast():\n    d = torch.randint(vocab_size, size=[batch_size, seq_length])\n    model = torch.jit.trace(model, (d,), check_trace=False, strict=False)\n    model = torch.jit.freeze(model)\n\n    model(data)",
      "import torch\nimport torchvision\n############# code changes ###############\nimport intel_extension_for_pytorch as ipex\n############# code changes ###############\n\nLR = 0.001\nDOWNLOAD = True\nDATA = 'datasets/cifar10/'\n\ntransform = torchvision.transforms.Compose([\n    torchvision.transforms.Resize((224, 224)),\n    torchvision.transforms.ToTensor(),\n    torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\ntrain_dataset = torchvision.datasets.CIFAR10(\n        root=DATA,\n        train=True,\n        transform=transform,\n        download=DOWNLOAD,\n)\ntrain_loader = torch.utils.data.DataLoader(\n        dataset=train_dataset,\n        batch_size=128\n)\n\nmodel = torchvision.models.resnet50()\ncriterion = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr = LR, momentum=0.9)\nmodel.train()\n#################################### code changes ################################\nmodel = model.to(\"xpu\")\nmodel, optimizer = ipex.optimize(model, optimizer=optimizer, dtype=torch.float32)\n#################################### code changes ################################\n\nfor batch_idx, (data, target) in enumerate(train_loader):\n    ########## code changes ##########\n    data = data.to(\"xpu\")\n    target = target.to(\"xpu\")\n    ########## code changes ##########\n    optimizer.zero_grad()\n    output = model(data)\n    loss = criterion(output, target)\n    loss.backward()\n    optimizer.step()\n    print(batch_idx)\ntorch.save({\n     'model_state_dict': model.state_dict(),\n     'optimizer_state_dict': optimizer.state_dict(),\n     }, 'checkpoint.pth')",
      "import torch\nimport torchvision\n############# code changes ###############\nimport intel_extension_for_pytorch as ipex\n############# code changes ###############\n\nLR = 0.001\nDOWNLOAD = True\nDATA = 'datasets/cifar10/'\n\ntransform = torchvision.transforms.Compose([\n    torchvision.transforms.Resize((224, 224)),\n    torchvision.transforms.ToTensor(),\n    torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\ntrain_dataset = torchvision.datasets.CIFAR10(\n        root=DATA,\n        train=True,\n        transform=transform,\n        download=DOWNLOAD,\n)\ntrain_loader = torch.utils.data.DataLoader(\n        dataset=train_dataset,\n        batch_size=128\n)\n\nmodel = torchvision.models.resnet50()\ncriterion = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr = LR, momentum=0.9)\nmodel.train()\n##################################### code changes ################################\nmodel = model.to(\"xpu\")\nmodel, optimizer = ipex.optimize(model, optimizer=optimizer, dtype=torch.bfloat16)\n##################################### code changes ################################\n\nfor batch_idx, (data, target) in enumerate(train_loader):\n    optimizer.zero_grad()\n    ######################### code changes #########################\n    data = data.to(\"xpu\")\n    target = target.to(\"xpu\")\n    with torch.xpu.amp.autocast(enabled=True, dtype=torch.bfloat16):\n    ######################### code changes #########################\n        output = model(data)\n        loss = criterion(output, target)\n    loss.backward()\n    optimizer.step()\n    print(batch_idx)\ntorch.save({\n     'model_state_dict': model.state_dict(),\n     'optimizer_state_dict': optimizer.state_dict(),\n     }, 'checkpoint.pth')",
      "import torch\nimport torchvision.models as models\n############# code changes ###############\nimport intel_extension_for_pytorch as ipex\n############# code changes ###############\n\nmodel = models.resnet50(pretrained=True)\nmodel.eval()\ndata = torch.rand(1, 3, 224, 224)\n\nmodel = model.to(memory_format=torch.channels_last)\ndata = data.to(memory_format=torch.channels_last)\n\n#################### code changes ################\nmodel = model.to(\"xpu\")\ndata = data.to(\"xpu\")\nmodel = ipex.optimize(model, dtype=torch.float32)\n#################### code changes ################\n\nwith torch.no_grad():\n  model(data)",
      "import torch\nimport torchvision.models as models\n############# code changes ###############\nimport intel_extension_for_pytorch as ipex\n############# code changes ###############\n\nmodel = models.resnet50(pretrained=True)\nmodel.eval()\ndata = torch.rand(1, 3, 224, 224)\n\nmodel = model.to(memory_format=torch.channels_last)\ndata = data.to(memory_format=torch.channels_last)\n\n#################### code changes #################\nmodel = model.to(\"xpu\")\ndata = data.to(\"xpu\")\nmodel = ipex.optimize(model, dtype=torch.bfloat16)\n#################### code changes #################\n\nwith torch.no_grad():\n  ################################# code changes ######################################\n  with torch.xpu.amp.autocast(enabled=True, dtype=torch.bfloat16, cache_enabled=False):\n  ################################# code changes ######################################\n    model(data)",
      "import torch\nimport torchvision.models as models\n############# code changes ###############\nimport intel_extension_for_pytorch as ipex\n############# code changes ###############\n\nmodel = models.resnet50(pretrained=True)\nmodel.eval()\ndata = torch.rand(1, 3, 224, 224)\n\nmodel = model.to(memory_format=torch.channels_last)\ndata = data.to(memory_format=torch.channels_last)\n\n#################### code changes ################\nmodel = model.to(\"xpu\")\ndata = data.to(\"xpu\")\nmodel = ipex.optimize(model, dtype=torch.float16)\n#################### code changes ################\n\nwith torch.no_grad():\n  ################################# code changes ######################################\n  with torch.xpu.amp.autocast(enabled=True, dtype=torch.float16, cache_enabled=False):\n  ################################# code changes ######################################\n    model(data)",
      "import torch\nfrom transformers import BertModel\n############# code changes ###############\nimport intel_extension_for_pytorch as ipex\n############# code changes ###############\n\nmodel = BertModel.from_pretrained(args.model_name)\nmodel.eval()\n\nvocab_size = model.config.vocab_size\nbatch_size = 1\nseq_length = 512\ndata = torch.randint(vocab_size, size=[batch_size, seq_length])\n\n#################### code changes ################\nmodel = model.to(\"xpu\")\ndata = data.to(\"xpu\")\nmodel = ipex.optimize(model, dtype=torch.float32)\n#################### code changes ################\n\nwith torch.no_grad():\n  d = torch.randint(vocab_size, size=[batch_size, seq_length])\n  ##### code changes #####\n  d = d.to(\"xpu\")\n  ##### code changes #####\n  model = torch.jit.trace(model, (d,), check_trace=False, strict=False)\n  model = torch.jit.freeze(model)\n\n  model(data)",
      "import torch\nfrom transformers import BertModel\n############# code changes ###############\nimport intel_extension_for_pytorch as ipex\n############# code changes ###############\n\nmodel = BertModel.from_pretrained(args.model_name)\nmodel.eval()\n\nvocab_size = model.config.vocab_size\nbatch_size = 1\nseq_length = 512\ndata = torch.randint(vocab_size, size=[batch_size, seq_length])\n\n#################### code changes #################\nmodel = model.to(\"xpu\")\ndata = data.to(\"xpu\")\nmodel = ipex.optimize(model, dtype=torch.bfloat16)\n#################### code changes #################\n\nwith torch.no_grad():\n  d = torch.randint(vocab_size, size=[batch_size, seq_length])\n  ################################# code changes ######################################\n  d = d.to(\"xpu\")\n  with torch.xpu.amp.autocast(enabled=True, dtype=torch.bfloat16, cache_enabled=False):\n  ################################# code changes ######################################\n    model = torch.jit.trace(model, (d,), check_trace=False, strict=False)\n    model = torch.jit.freeze(model)\n\n    model(data)",
      "import torch\nfrom transformers import BertModel\n############# code changes ###############\nimport intel_extension_for_pytorch as ipex\n############# code changes ###############\n\nmodel = BertModel.from_pretrained(args.model_name)\nmodel.eval()\n\nvocab_size = model.config.vocab_size\nbatch_size = 1\nseq_length = 512\ndata = torch.randint(vocab_size, size=[batch_size, seq_length])\n\n#################### code changes ################\nmodel = model.to(\"xpu\")\ndata = data.to(\"xpu\")\nmodel = ipex.optimize(model, dtype=torch.float16)\n#################### code changes ################\n\nwith torch.no_grad():\n  d = torch.randint(vocab_size, size=[batch_size, seq_length])\n  ################################# code changes ######################################\n  d = d.to(\"xpu\")\n  with torch.xpu.amp.autocast(enabled=True, dtype=torch.float16, cache_enabled=False):\n  ################################# code changes ######################################\n    model = torch.jit.trace(model, (d,), check_trace=False, strict=False)\n    model = torch.jit.freeze(model)\n\n    model(data)",
      "$ cmake -DCMAKE_PREFIX_PATH=/workspace/libtorch ..\n-- The C compiler identification is GNU 9.3.0\n-- The CXX compiler identification is GNU 9.3.0\n-- Check for working C compiler: /usr/bin/cc\n-- Check for working C compiler: /usr/bin/cc -- works\n-- Detecting C compiler ABI info\n-- Detecting C compiler ABI info - done\n-- Detecting C compile features\n-- Detecting C compile features - done\n-- Check for working CXX compiler: /usr/bin/c++\n-- Check for working CXX compiler: /usr/bin/c++ -- works\n-- Detecting CXX compiler ABI info\n-- Detecting CXX compiler ABI info - done\n-- Detecting CXX compile features\n-- Detecting CXX compile features - done\n-- Looking for pthread.h\n-- Looking for pthread.h - found\n-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed\n-- Looking for pthread_create in pthreads\n-- Looking for pthread_create in pthreads - not found\n-- Looking for pthread_create in pthread\n-- Looking for pthread_create in pthread - found\n-- Found Threads: TRUE\n-- Found Torch: /workspace/libtorch/lib/libtorch.so\n-- Found INTEL_EXT_PT_CPU: TRUE\n-- Configuring done\n-- Generating done\n-- Build files have been written to: /workspace/build\n\n$ ldd example-app\n        ...\n        libtorch.so => /workspace/libtorch/lib/libtorch.so (0x00007f3cf98e0000)\n        libc10.so => /workspace/libtorch/lib/libc10.so (0x00007f3cf985a000)\n        libintel-ext-pt-cpu.so => /workspace/libtorch/lib/libintel-ext-pt-cpu.so (0x00007f3cf70fc000)\n        libtorch_cpu.so => /workspace/libtorch/lib/libtorch_cpu.so (0x00007f3ce16ac000)\n        ...\n        libdnnl_graph.so.0 => /workspace/libtorch/lib/libdnnl_graph.so.0 (0x00007f3cde954000)\n        ..."
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/intermediate/mario_rl_tutorial.html",
    "title": "Train a Mario-playing RL Agent\u00b6",
    "code_snippets": [
      "import torch\nfrom torch import nn\nfrom torchvision import transforms as T\nfrom PIL import Image\nimport numpy as np\nfrom pathlib import Path\nfrom collections import deque\nimport random, datetime, os\n\n# Gym is an OpenAI toolkit for RL\nimport gym\nfrom gym.spaces import Box\nfrom gym.wrappers import FrameStack\n\n# NES Emulator for OpenAI Gym\nfrom nes_py.wrappers import JoypadSpace\n\n# Super Mario environment for OpenAI Gym\nimport gym_super_mario_bros\n\nfrom tensordict import TensorDict\nfrom torchrl.data import TensorDictReplayBuffer, LazyMemmapStorage",
      "/usr/local/lib/python3.10/dist-packages/torchrl/__init__.py:43: UserWarning:\n\nfailed to set start method to spawn, and current start method for mp is fork.\n\n/usr/local/lib/python3.10/dist-packages/torchrl/data/replay_buffers/samplers.py:37: UserWarning:\n\nFailed to import torchrl C++ binaries. Some modules (eg, prioritized replay buffers) may not work with your installation. This is likely due to a discrepancy between your package version and the PyTorch version. Make sure both are compatible. Usually, torchrl majors follow the pytorch majors within a few days around the release. For instance, TorchRL 0.5 requires PyTorch 2.4.0, and TorchRL 0.6 requires PyTorch 2.5.0.",
      "class SkipFrame(gym.Wrapper):\n    def __init__(self, env, skip):\n        \"\"\"Return only every `skip`-th frame\"\"\"\n        super().__init__(env)\n        self._skip = skip\n\n    def step(self, action):\n        \"\"\"Repeat action, and sum reward\"\"\"\n        total_reward = 0.0\n        for i in range(self._skip):\n            # Accumulate reward and repeat the same action\n            obs, reward, done, trunk, info = self.env.step(action)\n            total_reward += reward\n            if done:\n                break\n        return obs, total_reward, done, trunk, info\n\n\nclass GrayScaleObservation(gym.ObservationWrapper):\n    def __init__(self, env):\n        super().__init__(env)\n        obs_shape = self.observation_space.shape[:2]\n        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n\n    def permute_orientation(self, observation):\n        # permute [H, W, C] array to [C, H, W] tensor\n        observation = np.transpose(observation, (2, 0, 1))\n        observation = torch.tensor(observation.copy(), dtype=torch.float)\n        return observation\n\n    def observation(self, observation):\n        observation = self.permute_orientation(observation)\n        transform = T.Grayscale()\n        observation = transform(observation)\n        return observation\n\n\nclass ResizeObservation(gym.ObservationWrapper):\n    def __init__(self, env, shape):\n        super().__init__(env)\n        if isinstance(shape, int):\n            self.shape = (shape, shape)\n        else:\n            self.shape = tuple(shape)\n\n        obs_shape = self.shape + self.observation_space.shape[2:]\n        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n\n    def observation(self, observation):\n        transforms = T.Compose(\n            [T.Resize(self.shape, antialias=True), T.Normalize(0, 255)]\n        )\n        observation = transforms(observation).squeeze(0)\n        return observation\n\n\n# Apply Wrappers to environment\nenv = SkipFrame(env, skip=4)\nenv = GrayScaleObservation(env)\nenv = ResizeObservation(env, shape=84)\nif gym.__version__ < '0.26':\n    env = FrameStack(env, num_stack=4, new_step_api=True)\nelse:\n    env = FrameStack(env, num_stack=4)",
      "class Mario:\n    def __init__():\n        pass\n\n    def act(self, state):\n        \"\"\"Given a state, choose an epsilon-greedy action\"\"\"\n        pass\n\n    def cache(self, experience):\n        \"\"\"Add the experience to memory\"\"\"\n        pass\n\n    def recall(self):\n        \"\"\"Sample experiences from memory\"\"\"\n        pass\n\n    def learn(self):\n        \"\"\"Update online action value (Q) function with a batch of experiences\"\"\"\n        pass",
      "class Mario:\n    def __init__(self, state_dim, action_dim, save_dir):\n        self.state_dim = state_dim\n        self.action_dim = action_dim\n        self.save_dir = save_dir\n\n        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n        # Mario's DNN to predict the most optimal action - we implement this in the Learn section\n        self.net = MarioNet(self.state_dim, self.action_dim).float()\n        self.net = self.net.to(device=self.device)\n\n        self.exploration_rate = 1\n        self.exploration_rate_decay = 0.99999975\n        self.exploration_rate_min = 0.1\n        self.curr_step = 0\n\n        self.save_every = 5e5  # no. of experiences between saving Mario Net\n\n    def act(self, state):\n        \"\"\"\n    Given a state, choose an epsilon-greedy action and update value of step.\n\n    Inputs:\n    state(``LazyFrame``): A single observation of the current state, dimension is (state_dim)\n    Outputs:\n    ``action_idx`` (``int``): An integer representing which action Mario will perform\n    \"\"\"\n        # EXPLORE\n        if np.random.rand() < self.exploration_rate:\n            action_idx = np.random.randint(self.action_dim)\n\n        # EXPLOIT\n        else:\n            state = state[0].__array__() if isinstance(state, tuple) else state.__array__()\n            state = torch.tensor(state, device=self.device).unsqueeze(0)\n            action_values = self.net(state, model=\"online\")\n            action_idx = torch.argmax(action_values, axis=1).item()\n\n        # decrease exploration_rate\n        self.exploration_rate *= self.exploration_rate_decay\n        self.exploration_rate = max(self.exploration_rate_min, self.exploration_rate)\n\n        # increment step\n        self.curr_step += 1\n        return action_idx",
      "class Mario(Mario):  # subclassing for continuity\n    def __init__(self, state_dim, action_dim, save_dir):\n        super().__init__(state_dim, action_dim, save_dir)\n        self.memory = TensorDictReplayBuffer(storage=LazyMemmapStorage(100000, device=torch.device(\"cpu\")))\n        self.batch_size = 32\n\n    def cache(self, state, next_state, action, reward, done):\n        \"\"\"\n        Store the experience to self.memory (replay buffer)\n\n        Inputs:\n        state (``LazyFrame``),\n        next_state (``LazyFrame``),\n        action (``int``),\n        reward (``float``),\n        done(``bool``))\n        \"\"\"\n        def first_if_tuple(x):\n            return x[0] if isinstance(x, tuple) else x\n        state = first_if_tuple(state).__array__()\n        next_state = first_if_tuple(next_state).__array__()\n\n        state = torch.tensor(state)\n        next_state = torch.tensor(next_state)\n        action = torch.tensor([action])\n        reward = torch.tensor([reward])\n        done = torch.tensor([done])\n\n        # self.memory.append((state, next_state, action, reward, done,))\n        self.memory.add(TensorDict({\"state\": state, \"next_state\": next_state, \"action\": action, \"reward\": reward, \"done\": done}, batch_size=[]))\n\n    def recall(self):\n        \"\"\"\n        Retrieve a batch of experiences from memory\n        \"\"\"\n        batch = self.memory.sample(self.batch_size).to(self.device)\n        state, next_state, action, reward, done = (batch.get(key) for key in (\"state\", \"next_state\", \"action\", \"reward\", \"done\"))\n        return state, next_state, action.squeeze(), reward.squeeze(), done.squeeze()",
      "class MarioNet(nn.Module):\n    \"\"\"mini CNN structure\n  input -> (conv2d + relu) x 3 -> flatten -> (dense + relu) x 2 -> output\n  \"\"\"\n\n    def __init__(self, input_dim, output_dim):\n        super().__init__()\n        c, h, w = input_dim\n\n        if h != 84:\n            raise ValueError(f\"Expecting input height: 84, got: {h}\")\n        if w != 84:\n            raise ValueError(f\"Expecting input width: 84, got: {w}\")\n\n        self.online = self.__build_cnn(c, output_dim)\n\n        self.target = self.__build_cnn(c, output_dim)\n        self.target.load_state_dict(self.online.state_dict())\n\n        # Q_target parameters are frozen.\n        for p in self.target.parameters():\n            p.requires_grad = False\n\n    def forward(self, input, model):\n        if model == \"online\":\n            return self.online(input)\n        elif model == \"target\":\n            return self.target(input)\n\n    def __build_cnn(self, c, output_dim):\n        return nn.Sequential(\n            nn.Conv2d(in_channels=c, out_channels=32, kernel_size=8, stride=4),\n            nn.ReLU(),\n            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),\n            nn.ReLU(),\n            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n            nn.ReLU(),\n            nn.Flatten(),\n            nn.Linear(3136, 512),\n            nn.ReLU(),\n            nn.Linear(512, output_dim),\n        )",
      "class Mario(Mario):\n    def __init__(self, state_dim, action_dim, save_dir):\n        super().__init__(state_dim, action_dim, save_dir)\n        self.gamma = 0.9\n\n    def td_estimate(self, state, action):\n        current_Q = self.net(state, model=\"online\")[\n            np.arange(0, self.batch_size), action\n        ]  # Q_online(s,a)\n        return current_Q\n\n    @torch.no_grad()\n    def td_target(self, reward, next_state, done):\n        next_state_Q = self.net(next_state, model=\"online\")\n        best_action = torch.argmax(next_state_Q, axis=1)\n        next_Q = self.net(next_state, model=\"target\")[\n            np.arange(0, self.batch_size), best_action\n        ]\n        return (reward + (1 - done.float()) * self.gamma * next_Q).float()",
      "class Mario(Mario):\n    def __init__(self, state_dim, action_dim, save_dir):\n        super().__init__(state_dim, action_dim, save_dir)\n        self.optimizer = torch.optim.Adam(self.net.parameters(), lr=0.00025)\n        self.loss_fn = torch.nn.SmoothL1Loss()\n\n    def update_Q_online(self, td_estimate, td_target):\n        loss = self.loss_fn(td_estimate, td_target)\n        self.optimizer.zero_grad()\n        loss.backward()\n        self.optimizer.step()\n        return loss.item()\n\n    def sync_Q_target(self):\n        self.net.target.load_state_dict(self.net.online.state_dict())",
      "class Mario(Mario):\n    def save(self):\n        save_path = (\n            self.save_dir / f\"mario_net_{int(self.curr_step // self.save_every)}.chkpt\"\n        )\n        torch.save(\n            dict(model=self.net.state_dict(), exploration_rate=self.exploration_rate),\n            save_path,\n        )\n        print(f\"MarioNet saved to {save_path} at step {self.curr_step}\")",
      "class Mario(Mario):\n    def __init__(self, state_dim, action_dim, save_dir):\n        super().__init__(state_dim, action_dim, save_dir)\n        self.burnin = 1e4  # min. experiences before training\n        self.learn_every = 3  # no. of experiences between updates to Q_online\n        self.sync_every = 1e4  # no. of experiences between Q_target & Q_online sync\n\n    def learn(self):\n        if self.curr_step % self.sync_every == 0:\n            self.sync_Q_target()\n\n        if self.curr_step % self.save_every == 0:\n            self.save()\n\n        if self.curr_step < self.burnin:\n            return None, None\n\n        if self.curr_step % self.learn_every != 0:\n            return None, None\n\n        # Sample from memory\n        state, next_state, action, reward, done = self.recall()\n\n        # Get TD Estimate\n        td_est = self.td_estimate(state, action)\n\n        # Get TD Target\n        td_tgt = self.td_target(reward, next_state, done)\n\n        # Backpropagate loss through Q_online\n        loss = self.update_Q_online(td_est, td_tgt)\n\n        return (td_est.mean().item(), loss)",
      "import numpy as np\nimport time, datetime\nimport matplotlib.pyplot as plt\n\n\nclass MetricLogger:\n    def __init__(self, save_dir):\n        self.save_log = save_dir / \"log\"\n        with open(self.save_log, \"w\") as f:\n            f.write(\n                f\"{'Episode':>8}{'Step':>8}{'Epsilon':>10}{'MeanReward':>15}\"\n                f\"{'MeanLength':>15}{'MeanLoss':>15}{'MeanQValue':>15}\"\n                f\"{'TimeDelta':>15}{'Time':>20}\\n\"\n            )\n        self.ep_rewards_plot = save_dir / \"reward_plot.jpg\"\n        self.ep_lengths_plot = save_dir / \"length_plot.jpg\"\n        self.ep_avg_losses_plot = save_dir / \"loss_plot.jpg\"\n        self.ep_avg_qs_plot = save_dir / \"q_plot.jpg\"\n\n        # History metrics\n        self.ep_rewards = []\n        self.ep_lengths = []\n        self.ep_avg_losses = []\n        self.ep_avg_qs = []\n\n        # Moving averages, added for every call to record()\n        self.moving_avg_ep_rewards = []\n        self.moving_avg_ep_lengths = []\n        self.moving_avg_ep_avg_losses = []\n        self.moving_avg_ep_avg_qs = []\n\n        # Current episode metric\n        self.init_episode()\n\n        # Timing\n        self.record_time = time.time()\n\n    def log_step(self, reward, loss, q):\n        self.curr_ep_reward += reward\n        self.curr_ep_length += 1\n        if loss:\n            self.curr_ep_loss += loss\n            self.curr_ep_q += q\n            self.curr_ep_loss_length += 1\n\n    def log_episode(self):\n        \"Mark end of episode\"\n        self.ep_rewards.append(self.curr_ep_reward)\n        self.ep_lengths.append(self.curr_ep_length)\n        if self.curr_ep_loss_length == 0:\n            ep_avg_loss = 0\n            ep_avg_q = 0\n        else:\n            ep_avg_loss = np.round(self.curr_ep_loss / self.curr_ep_loss_length, 5)\n            ep_avg_q = np.round(self.curr_ep_q / self.curr_ep_loss_length, 5)\n        self.ep_avg_losses.append(ep_avg_loss)\n        self.ep_avg_qs.append(ep_avg_q)\n\n        self.init_episode()\n\n    def init_episode(self):\n        self.curr_ep_reward = 0.0\n        self.curr_ep_length = 0\n        self.curr_ep_loss = 0.0\n        self.curr_ep_q = 0.0\n        self.curr_ep_loss_length = 0\n\n    def record(self, episode, epsilon, step):\n        mean_ep_reward = np.round(np.mean(self.ep_rewards[-100:]), 3)\n        mean_ep_length = np.round(np.mean(self.ep_lengths[-100:]), 3)\n        mean_ep_loss = np.round(np.mean(self.ep_avg_losses[-100:]), 3)\n        mean_ep_q = np.round(np.mean(self.ep_avg_qs[-100:]), 3)\n        self.moving_avg_ep_rewards.append(mean_ep_reward)\n        self.moving_avg_ep_lengths.append(mean_ep_length)\n        self.moving_avg_ep_avg_losses.append(mean_ep_loss)\n        self.moving_avg_ep_avg_qs.append(mean_ep_q)\n\n        last_record_time = self.record_time\n        self.record_time = time.time()\n        time_since_last_record = np.round(self.record_time - last_record_time, 3)\n\n        print(\n            f\"Episode {episode} - \"\n            f\"Step {step} - \"\n            f\"Epsilon {epsilon} - \"\n            f\"Mean Reward {mean_ep_reward} - \"\n            f\"Mean Length {mean_ep_length} - \"\n            f\"Mean Loss {mean_ep_loss} - \"\n            f\"Mean Q Value {mean_ep_q} - \"\n            f\"Time Delta {time_since_last_record} - \"\n            f\"Time {datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S')}\"\n        )\n\n        with open(self.save_log, \"a\") as f:\n            f.write(\n                f\"{episode:8d}{step:8d}{epsilon:10.3f}\"\n                f\"{mean_ep_reward:15.3f}{mean_ep_length:15.3f}{mean_ep_loss:15.3f}{mean_ep_q:15.3f}\"\n                f\"{time_since_last_record:15.3f}\"\n                f\"{datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S'):>20}\\n\"\n            )\n\n        for metric in [\"ep_lengths\", \"ep_avg_losses\", \"ep_avg_qs\", \"ep_rewards\"]:\n            plt.clf()\n            plt.plot(getattr(self, f\"moving_avg_{metric}\"), label=f\"moving_avg_{metric}\")\n            plt.legend()\n            plt.savefig(getattr(self, f\"{metric}_plot\"))",
      "use_cuda = torch.cuda.is_available()\nprint(f\"Using CUDA: {use_cuda}\")\nprint()\n\nsave_dir = Path(\"checkpoints\") / datetime.datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S\")\nsave_dir.mkdir(parents=True)\n\nmario = Mario(state_dim=(4, 84, 84), action_dim=env.action_space.n, save_dir=save_dir)\n\nlogger = MetricLogger(save_dir)\n\nepisodes = 40\nfor e in range(episodes):\n\n    state = env.reset()\n\n    # Play the game!\n    while True:\n\n        # Run agent on the state\n        action = mario.act(state)\n\n        # Agent performs action\n        next_state, reward, done, trunc, info = env.step(action)\n\n        # Remember\n        mario.cache(state, next_state, action, reward, done)\n\n        # Learn\n        q, loss = mario.learn()\n\n        # Logging\n        logger.log_step(reward, loss, q)\n\n        # Update state\n        state = next_state\n\n        # Check if end of game\n        if done or info[\"flag_get\"]:\n            break\n\n    logger.log_episode()\n\n    if (e % 20 == 0) or (e == episodes - 1):\n        logger.record(episode=e, epsilon=mario.exploration_rate, step=mario.curr_step)"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/recipes/recipes/benchmark.html",
    "title": "PyTorch Benchmark\u00b6",
    "code_snippets": [
      "import torch\n\n\ndef batched_dot_mul_sum(a, b):\n    '''Computes batched dot by multiplying and summing'''\n    return a.mul(b).sum(-1)\n\n\ndef batched_dot_bmm(a, b):\n    '''Computes batched dot by reducing to ``bmm``'''\n    a = a.reshape(-1, 1, a.shape[-1])\n    b = b.reshape(-1, b.shape[-1], 1)\n    return torch.bmm(a, b).flatten(-3)\n\n\n# Input for benchmarking\nx = torch.randn(10000, 64)\n\n# Ensure that both functions compute the same output\nassert batched_dot_mul_sum(x, x).allclose(batched_dot_bmm(x, x))",
      "import timeit\n\nt0 = timeit.Timer(\n    stmt='batched_dot_mul_sum(x, x)',\n    setup='from __main__ import batched_dot_mul_sum',\n    globals={'x': x})\n\nt1 = timeit.Timer(\n    stmt='batched_dot_bmm(x, x)',\n    setup='from __main__ import batched_dot_bmm',\n    globals={'x': x})\n\nprint(f'mul_sum(x, x):  {t0.timeit(100) / 100 * 1e6:>5.1f} us')\nprint(f'bmm(x, x):      {t1.timeit(100) / 100 * 1e6:>5.1f} us')",
      "import torch.utils.benchmark as benchmark\n\nt0 = benchmark.Timer(\n    stmt='batched_dot_mul_sum(x, x)',\n    setup='from __main__ import batched_dot_mul_sum',\n    globals={'x': x})\n\nt1 = benchmark.Timer(\n    stmt='batched_dot_bmm(x, x)',\n    setup='from __main__ import batched_dot_bmm',\n    globals={'x': x})\n\nprint(t0.timeit(100))\nprint(t1.timeit(100))",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fb10400d0f0>\n batched_dot_mul_sum(x, x)\n setup: from __main__ import batched_dot_mul_sum\n   379.29 us\n   1 measurement, 100 runs , 1 thread\n <torch.utils.benchmark.utils.common.Measurement object at 0x7fb103d67048>\n batched_dot_bmm(x, x)\n setup: from __main__ import batched_dot_bmm\n   716.42 us\n   1 measurement, 100 runs , 1 thread",
      "num_threads = torch.get_num_threads()\nprint(f'Benchmarking on {num_threads} threads')\n\nt0 = benchmark.Timer(\n    stmt='batched_dot_mul_sum(x, x)',\n    setup='from __main__ import batched_dot_mul_sum',\n    globals={'x': x},\n    num_threads=num_threads,\n    label='Multithreaded batch dot',\n    sub_label='Implemented using mul and sum')\n\nt1 = benchmark.Timer(\n    stmt='batched_dot_bmm(x, x)',\n    setup='from __main__ import batched_dot_bmm',\n    globals={'x': x},\n    num_threads=num_threads,\n    label='Multithreaded batch dot',\n    sub_label='Implemented using bmm')\n\nprint(t0.timeit(100))\nprint(t1.timeit(100))",
      "Benchmarking on 40 threads\n <torch.utils.benchmark.utils.common.Measurement object at 0x7fb103d54080>\n Multithreaded batch dot: Implemented using mul and sum\n setup: from __main__ import batched_dot_mul_sum\n   118.47 us\n   1 measurement, 100 runs , 40 threads\n <torch.utils.benchmark.utils.common.Measurement object at 0x7fb16935d2e8>\n Multithreaded batch dot: Implemented using bmm\n setup: from __main__ import batched_dot_bmm\n   68.21 us\n   1 measurement, 100 runs , 40 threads",
      "x = torch.randn(10000, 1024, device='cuda')\n\nt0 = timeit.Timer(\n    stmt='batched_dot_mul_sum(x, x)',\n    setup='from __main__ import batched_dot_mul_sum',\n    globals={'x': x})\n\nt1 = timeit.Timer(\n    stmt='batched_dot_bmm(x, x)',\n    setup='from __main__ import batched_dot_bmm',\n    globals={'x': x})\n\n# Ran each twice to show difference before/after warm-up\nprint(f'mul_sum(x, x):  {t0.timeit(100) / 100 * 1e6:>5.1f} us')\nprint(f'mul_sum(x, x):  {t0.timeit(100) / 100 * 1e6:>5.1f} us')\nprint(f'bmm(x, x):      {t1.timeit(100) / 100 * 1e6:>5.1f} us')\nprint(f'bmm(x, x):      {t1.timeit(100) / 100 * 1e6:>5.1f} us')",
      "t0 = benchmark.Timer(\n    stmt='batched_dot_mul_sum(x, x)',\n    setup='from __main__ import batched_dot_mul_sum',\n    globals={'x': x})\n\nt1 = benchmark.Timer(\n    stmt='batched_dot_bmm(x, x)',\n    setup='from __main__ import batched_dot_bmm',\n    globals={'x': x})\n\n# Run only once since benchmark module does warm-up for us\nprint(t0.timeit(100))\nprint(t1.timeit(100))",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fb10400d080>\n batched_dot_mul_sum(x, x)\n setup: from __main__ import batched_dot_mul_sum\n   232.93 us\n   1 measurement, 100 runs , 1 thread\n <torch.utils.benchmark.utils.common.Measurement object at 0x7fb10400d0f0>\n batched_dot_bmm(x, x)\n setup: from __main__ import batched_dot_bmm\n   181.04 us\n   1 measurement, 100 runs , 1 thread",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fb10400d0f0>\n batched_dot_mul_sum(x, x)\n setup: from __main__ import batched_dot_mul_sum\n   231.79 us\n   1 measurement, 1000 runs , 1 thread\n <torch.utils.benchmark.utils.common.Measurement object at 0x7fb10400d080>\n batched_dot_bmm(x, x)\n setup: from __main__ import batched_dot_bmm\n   Median: 162.08 us\n   2 measurements, 1000 runs per measurement, 1 thread",
      "from itertools import product\n\n# Compare takes a list of measurements which we'll save in results.\nresults = []\n\nsizes = [1, 64, 1024, 10000]\nfor b, n in product(sizes, sizes):\n    # label and sub_label are the rows\n    # description is the column\n    label = 'Batched dot'\n    sub_label = f'[{b}, {n}]'\n    x = torch.ones((b, n))\n    for num_threads in [1, 4, 16, 32]:\n        results.append(benchmark.Timer(\n            stmt='batched_dot_mul_sum(x, x)',\n            setup='from __main__ import batched_dot_mul_sum',\n            globals={'x': x},\n            num_threads=num_threads,\n            label=label,\n            sub_label=sub_label,\n            description='mul/sum',\n        ).blocked_autorange(min_run_time=1))\n        results.append(benchmark.Timer(\n            stmt='batched_dot_bmm(x, x)',\n            setup='from __main__ import batched_dot_bmm',\n            globals={'x': x},\n            num_threads=num_threads,\n            label=label,\n            sub_label=sub_label,\n            description='bmm',\n        ).blocked_autorange(min_run_time=1))\n\ncompare = benchmark.Compare(results)\ncompare.print()",
      "import pickle\n\nab_test_results = []\nfor env in ('environment A: mul/sum', 'environment B: bmm'):\n    for b, n in ((1, 1), (1024, 10000), (10000, 1)):\n        x = torch.ones((b, n))\n        dot_fn = (batched_dot_mul_sum if env == 'environment A: mul/sum' else batched_dot_bmm)\n        m = benchmark.Timer(\n            stmt='batched_dot(x, x)',\n            globals={'x': x, 'batched_dot': dot_fn},\n            num_threads=1,\n            label='Batched dot',\n            description=f'[{b}, {n}]',\n            env=env,\n        ).blocked_autorange(min_run_time=1)\n        ab_test_results.append(pickle.dumps(m))\n\nab_results = [pickle.loads(i) for i in ab_test_results]\ncompare = benchmark.Compare(ab_results)\ncompare.trim_significant_figures()\ncompare.colorize()\ncompare.print()",
      "from torch.utils.benchmark import Fuzzer, FuzzedParameter, FuzzedTensor, ParameterAlias\n\n# Generates random tensors with 128 to 10000000 elements and sizes k0 and k1 chosen from a\n# ``loguniform`` distribution in [1, 10000], 40% of which will be discontiguous on average.\nexample_fuzzer = Fuzzer(\n    parameters = [\n        FuzzedParameter('k0', minval=1, maxval=10000, distribution='loguniform'),\n        FuzzedParameter('k1', minval=1, maxval=10000, distribution='loguniform'),\n    ],\n    tensors = [\n        FuzzedTensor('x', size=('k0', 'k1'), min_elements=128, max_elements=10000000, probability_contiguous=0.6)\n    ],\n    seed=0,\n)\n\nresults = []\nfor tensors, tensor_params, params in example_fuzzer.take(10):\n    # description is the column label\n    sub_label=f\"{params['k0']:<6} x {params['k1']:<4} {'' if tensor_params['x']['is_contiguous'] else '(discontiguous)'}\"\n    results.append(benchmark.Timer(\n        stmt='batched_dot_mul_sum(x, x)',\n        setup='from __main__ import batched_dot_mul_sum',\n        globals=tensors,\n        label='Batched dot',\n        sub_label=sub_label,\n        description='mul/sum',\n    ).blocked_autorange(min_run_time=1))\n    results.append(benchmark.Timer(\n        stmt='batched_dot_bmm(x, x)',\n        setup='from __main__ import batched_dot_bmm',\n        globals=tensors,\n        label='Batched dot',\n        sub_label=sub_label,\n        description='bmm',\n    ).blocked_autorange(min_run_time=1))\n\ncompare = benchmark.Compare(results)\ncompare.trim_significant_figures()\ncompare.print()",
      "from torch.utils.benchmark.op_fuzzers import binary\n\nresults = []\nfor tensors, tensor_params, params in binary.BinaryOpFuzzer(seed=0).take(10):\n    sub_label=f\"{params['k0']:<6} x {params['k1']:<4} {'' if tensor_params['x']['is_contiguous'] else '(discontiguous)'}\"\n    results.append(benchmark.Timer(\n        stmt='batched_dot_mul_sum(x, x)',\n        setup='from __main__ import batched_dot_mul_sum',\n        globals=tensors,\n        label='Batched dot',\n        sub_label=sub_label,\n        description='mul/sum',\n    ).blocked_autorange(min_run_time=1))\n    results.append(benchmark.Timer(\n        stmt='batched_dot_bmm(x, x)',\n        setup='from __main__ import batched_dot_bmm',\n        globals=tensors,\n        label='Batched dot',\n        sub_label=sub_label,\n        description='bmm',\n    ).blocked_autorange(min_run_time=1))\n\ncompare = benchmark.Compare(results)\ncompare.trim_significant_figures()\ncompare.colorize(rowwise=True)\ncompare.print()",
      "batched_dot_src = \"\"\"\\\n/* ---- Python ---- */\n// def batched_dot_mul_sum(a, b):\n//     return a.mul(b).sum(-1)\n\ntorch::Tensor batched_dot_mul_sum_v0(\n    const torch::Tensor a,\n    const torch::Tensor b) {\n  return a.mul(b).sum(-1);\n}\n\ntorch::Tensor batched_dot_mul_sum_v1(\n    const torch::Tensor& a,\n    const torch::Tensor& b) {\n  return a.mul(b).sum(-1);\n}\n\"\"\"\n\n\n# PyTorch makes it easy to test our C++ implementations by providing a utility\n# to JIT compile C++ source into Python extensions:\nimport os\nfrom torch.utils import cpp_extension\ncpp_lib = cpp_extension.load_inline(\n    name='cpp_lib',\n    cpp_sources=batched_dot_src,\n    extra_cflags=['-O3'],\n    extra_include_paths=[\n        # `load_inline` needs to know where to find ``pybind11`` headers.\n        os.path.join(os.getenv('CONDA_PREFIX'), 'include')\n    ],\n    functions=['batched_dot_mul_sum_v0', 'batched_dot_mul_sum_v1']\n)\n\n# `load_inline` will create a shared object that is loaded into Python. When we collect\n# instruction counts Timer will create a subprocess, so we need to re-import it. The\n# import process is slightly more complicated for C extensions, but that's all we're\n# doing here.\nmodule_import_str = f\"\"\"\\\n# https://stackoverflow.com/questions/67631/how-to-import-a-module-given-the-full-path\nimport importlib.util\nspec = importlib.util.spec_from_file_location(\"cpp_lib\", {repr(cpp_lib.__file__)})\ncpp_lib = importlib.util.module_from_spec(spec)\nspec.loader.exec_module(cpp_lib)\"\"\"\n\nimport textwrap\ndef pretty_print(result):\n    \"\"\"Import machinery for ``cpp_lib.so`` can get repetitive to look at.\"\"\"\n    print(repr(result).replace(textwrap.indent(module_import_str, \"  \"), \"  import cpp_lib\"))\n\n\nt_baseline = benchmark.Timer(\n    stmt='batched_dot_mul_sum(x, x)',\n    setup='''\\\nfrom __main__ import batched_dot_mul_sum\nx = torch.randn(2, 2)''')\n\nt0 = benchmark.Timer(\n    stmt='cpp_lib.batched_dot_mul_sum_v0(x, x)',\n    setup=f'''\\\n{module_import_str}\nx = torch.randn(2, 2)''')\n\nt1 = benchmark.Timer(\n    stmt='cpp_lib.batched_dot_mul_sum_v1(x, x)',\n    setup=f'''\\\n{module_import_str}\nx = torch.randn(2, 2)''')\n\n# Moving to C++ did indeed reduce overhead, but it's hard to tell which\n# calling convention is more efficient. v1 (call with references) seems to\n# be a bit faster, but it's within measurement error.\npretty_print(t_baseline.blocked_autorange())\npretty_print(t0.blocked_autorange())\npretty_print(t1.blocked_autorange())",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fb16935d2e8>\n batched_dot_mul_sum(x, x)\n setup:\n   from __main__ import batched_dot_mul_sum\n   x = torch.randn(2, 2)\n\n   6.92 us\n   1 measurement, 100000 runs , 1 thread\n <torch.utils.benchmark.utils.common.Measurement object at 0x7fb16935d2e8>\n cpp_lib.batched_dot_mul_sum_v0(x, x)\n setup:\n   import cpp_lib\n   x = torch.randn(2, 2)\n\n   5.29 us\n   1 measurement, 100000 runs , 1 thread\n <torch.utils.benchmark.utils.common.Measurement object at 0x7fb16935d2e8>\n cpp_lib.batched_dot_mul_sum_v1(x, x)\n setup:\n   import cpp_lib\n   x = torch.randn(2, 2)\n\n   5.22 us\n   1 measurement, 100000 runs , 1 thread",
      "# Let's use ``Callgrind`` to determine which is better.\nstats_v0 = t0.collect_callgrind()\nstats_v1 = t1.collect_callgrind()\n\npretty_print(stats_v0)\npretty_print(stats_v1)\n\n# `.as_standardized` removes file names and some path prefixes, and makes\n# it easier to read the function symbols.\nstats_v0 = stats_v0.as_standardized()\nstats_v1 = stats_v1.as_standardized()\n\n# `.delta` diffs the instruction counts, and `.denoise` removes several\n# functions in the Python interpreter that are known to have significant\n# jitter.\ndelta = stats_v1.delta(stats_v0).denoise()\n\n# `.transform` is a convenience API for transforming function names. It is\n# useful for increasing cancelation when ``diff-ing`` instructions, as well as\n# just generally improving readability.\nreplacements = (\n    (\"???:void pybind11\", \"pybind11\"),\n    (\"batched_dot_mul_sum_v0\", \"batched_dot_mul_sum_v1\"),\n    (\"at::Tensor, at::Tensor\", \"...\"),\n    (\"at::Tensor const&, at::Tensor const&\", \"...\"),\n    (\"auto torch::detail::wrap_pybind_function_impl_\", \"wrap_pybind_function_impl_\"),\n)\nfor before, after in replacements:\n    delta = delta.transform(lambda l: l.replace(before, after))\n\n# We can use print options to control how much of the function to display.\ntorch.set_printoptions(linewidth=160)\n\n# Once parsed, the instruction counts make clear that passing `a` and `b`\n# by reference is more efficient as it skips some ``c10::TensorImpl`` bookkeeping\n# for the intermediate Tensors, and is also works better with ``pybind11``. This\n# is consistent with our noisy wall time observations.\nprint(delta)",
      "<torch.utils.benchmark.utils.valgrind_wrapper.timer_interface.CallgrindStats object at 0x7fb0f06e7630>\ncpp_lib.batched_dot_mul_sum_v0(x, x)\nsetup:\n  import cpp_lib\n  x = torch.randn(2, 2)\n                           All          Noisy symbols removed\n    Instructions:      2392671                    2392671\n    Baseline:             4367                       4367\n100 runs per measurement, 1 thread\nWarning: PyTorch was not built with debug symbols.\n         Source information may be limited. Rebuild with\n         REL_WITH_DEB_INFO=1 for more detailed results.\n<torch.utils.benchmark.utils.valgrind_wrapper.timer_interface.CallgrindStats object at 0x7fb10400d208>\ncpp_lib.batched_dot_mul_sum_v1(x, x)\nsetup:\n  import cpp_lib\n  x = torch.randn(2, 2)\n                           All          Noisy symbols removed\n    Instructions:      2378978                    2378978\n    Baseline:             4367                       4367\n    100 runs per measurement, 1 thread\n    Warning: PyTorch was not built with debug symbols.\n             Source information may be limited. Rebuild with\n             REL_WITH_DEB_INFO=1 for more detailed results.\n    <torch.utils.benchmark.utils.valgrind_wrapper.timer_interface.FunctionCounts object at 0x7fb1000ab358>\n          86  ???:0x000000000020d9e0\n      56  ???:0x000000000020db10\n   -1100  pybind11::cpp_function::initialize<wrap_pybind_function_impl_<at::Tensor ... r (&)(...), std::integer_sequence<unsigned long, 0ul, 1ul>)::{lambda(...)\n   -1600  ???:wrap_pybind_function_impl_<at::Tensor (&)(...), 0ul, 1ul>(at::Tensor (&)(...), std::integer_sequence<unsigned long, 0ul, 1ul>)::{lambda(...)\n   -5200  ???:c10::intrusive_ptr<c10::TensorImpl, c10::UndefinedTensorImpl>::reset_()\n   -5935  ???:0x000000000022c0e0\nTotal: -13693"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/recipes/torch_export_aoti_python.html",
    "title": "torch.export AOTInductor Tutorial for Python runtime (Beta)\u00b6",
    "code_snippets": [
      "import os\nimport torch\nimport torch._inductor\nfrom torchvision.models import ResNet18_Weights, resnet18\n\nmodel = resnet18(weights=ResNet18_Weights.DEFAULT)\nmodel.eval()\n\nwith torch.inference_mode():\n    inductor_configs = {}\n\n    if torch.cuda.is_available():\n        device = \"cuda\"\n        inductor_configs[\"max_autotune\"] = True\n    else:\n        device = \"cpu\"\n\n    model = model.to(device=device)\n    example_inputs = (torch.randn(2, 3, 224, 224, device=device),)\n\n    exported_program = torch.export.export(\n        model,\n        example_inputs,\n    )\n    path = torch._inductor.aoti_compile_and_package(\n        exported_program,\n        package_path=os.path.join(os.getcwd(), \"resnet18.pt2\"),\n        inductor_configs=inductor_configs\n    )",
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /var/lib/ci-user/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n\n  0%|          | 0.00/44.7M [00:00<?, ?B/s]\n 97%|#########6| 43.2M/44.7M [00:00<00:00, 453MB/s]\n100%|##########| 44.7M/44.7M [00:00<00:00, 448MB/s]\n/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py:236: UserWarning:\n\nTensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.\n\nAUTOTUNE convolution(2x3x224x224, 64x3x7x7)\n  convolution 0.0840 ms 100.0%\n  triton_convolution2d_4 0.1516 ms 55.4% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=64, GROUPS=1, KERNEL_H=7, KERNEL_W=7, PADDING_H=3, PADDING_W=3, STRIDE_H=2, STRIDE_W=2, UNROLL=False, num_stages=2, num_warps=8\n  triton_convolution2d_0 0.1546 ms 54.3% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=64, GROUPS=1, KERNEL_H=7, KERNEL_W=7, PADDING_H=3, PADDING_W=3, STRIDE_H=2, STRIDE_W=2, UNROLL=False, num_stages=2, num_warps=4\n  triton_convolution2d_3 0.1956 ms 42.9% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=128, BLOCK_N=64, GROUPS=1, KERNEL_H=7, KERNEL_W=7, PADDING_H=3, PADDING_W=3, STRIDE_H=2, STRIDE_W=2, UNROLL=False, num_stages=2, num_warps=8\n  triton_convolution2d_5 0.2386 ms 35.2% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=256, BLOCK_N=64, GROUPS=1, KERNEL_H=7, KERNEL_W=7, PADDING_H=3, PADDING_W=3, STRIDE_H=2, STRIDE_W=2, UNROLL=False, num_stages=2, num_warps=8\n  triton_convolution2d_2 0.3041 ms 27.6% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=1024, BLOCK_N=16, GROUPS=1, KERNEL_H=7, KERNEL_W=7, PADDING_H=3, PADDING_W=3, STRIDE_H=2, STRIDE_W=2, UNROLL=False, num_stages=1, num_warps=8\n  triton_convolution2d_1 0.4076 ms 20.6% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=256, BLOCK_N=64, GROUPS=1, KERNEL_H=7, KERNEL_W=7, PADDING_H=3, PADDING_W=3, STRIDE_H=2, STRIDE_W=2, UNROLL=False, num_stages=2, num_warps=4\nSingleProcess AUTOTUNE benchmarking takes 0.4045 seconds and 0.0019 seconds precompiling for 7 choices\nAUTOTUNE convolution(2x64x56x56, 64x64x3x3)\n  triton_convolution2d_11 0.0369 ms 100.0% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, GROUPS=1, KERNEL_H=3, KERNEL_W=3, PADDING_H=1, PADDING_W=1, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=8\n  triton_convolution2d_6 0.0378 ms 97.6% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=64, GROUPS=1, KERNEL_H=3, KERNEL_W=3, PADDING_H=1, PADDING_W=1, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=4\n  triton_convolution2d_10 0.0410 ms 90.0% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, GROUPS=1, KERNEL_H=3, KERNEL_W=3, PADDING_H=1, PADDING_W=1, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=4\n  triton_convolution2d_9 0.0418 ms 88.2% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, GROUPS=1, KERNEL_H=3, KERNEL_W=3, PADDING_H=1, PADDING_W=1, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=8\n  convolution 0.0522 ms 70.6%\n  triton_convolution2d_7 0.0604 ms 61.0% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=256, BLOCK_N=64, GROUPS=1, KERNEL_H=3, KERNEL_W=3, PADDING_H=1, PADDING_W=1, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=4\n  triton_convolution2d_12 0.0665 ms 55.4% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=256, BLOCK_N=64, GROUPS=1, KERNEL_H=3, KERNEL_W=3, PADDING_H=1, PADDING_W=1, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=8\n  triton_convolution2d_8 0.1207 ms 30.5% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=1024, BLOCK_N=16, GROUPS=1, KERNEL_H=3, KERNEL_W=3, PADDING_H=1, PADDING_W=1, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=1, num_warps=8\nSingleProcess AUTOTUNE benchmarking takes 0.1782 seconds and 0.0002 seconds precompiling for 8 choices\nAUTOTUNE convolution(2x64x56x56, 128x64x3x3)\n  triton_convolution2d_38 0.0317 ms 100.0% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, GROUPS=1, KERNEL_H=3, KERNEL_W=3, PADDING_H=1, PADDING_W=1, STRIDE_H=2, STRIDE_W=2, UNROLL=False, num_stages=2, num_warps=4\n  triton_convolution2d_39 0.0358 ms 88.6% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, GROUPS=1, KERNEL_H=3, KERNEL_W=3, PADDING_H=1, PADDING_W=1, STRIDE_H=2, STRIDE_W=2, UNROLL=False, num_stages=2, num_warps=8\n  triton_convolution2d_34 0.0502 ms 63.3% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=128, GROUPS=1, KERNEL_H=3, KERNEL_W=3, PADDING_H=1, PADDING_W=1, STRIDE_H=2, STRIDE_W=2, UNROLL=False, num_stages=2, num_warps=4\n  triton_convolution2d_35 0.0573 ms 55.4% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=256, BLOCK_N=64, GROUPS=1, KERNEL_H=3, KERNEL_W=3, PADDING_H=1, PADDING_W=1, STRIDE_H=2, STRIDE_W=2, UNROLL=False, num_stages=2, num_warps=4\n  triton_convolution2d_40 0.0614 ms 51.7% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=256, BLOCK_N=64, GROUPS=1, KERNEL_H=3, KERNEL_W=3, PADDING_H=1, PADDING_W=1, STRIDE_H=2, STRIDE_W=2, UNROLL=False, num_stages=2, num_warps=8\n  triton_convolution2d_37 0.0645 ms 49.2% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, GROUPS=1, KERNEL_H=3, KERNEL_W=3, PADDING_H=1, PADDING_W=1, STRIDE_H=2, STRIDE_W=2, UNROLL=False, num_stages=2, num_warps=8\n  convolution 0.0768 ms 41.3%\n  triton_convolution2d_36 0.1126 ms 28.2% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=1024, BLOCK_N=16, GROUPS=1, KERNEL_H=3, KERNEL_W=3, PADDING_H=1, PADDING_W=1, STRIDE_H=2, STRIDE_W=2, UNROLL=False, num_stages=1, num_warps=8\nSingleProcess AUTOTUNE benchmarking takes 0.1767 seconds and 0.0002 seconds precompiling for 8 choices\nAUTOTUNE convolution(2x128x28x28, 128x128x3x3)\n  convolution 0.0451 ms 100.0%\n  triton_convolution2d_45 0.0492 ms 91.7% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, GROUPS=1, KERNEL_H=3, KERNEL_W=3, PADDING_H=1, PADDING_W=1, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=4\n  triton_convolution2d_46 0.0737 ms 61.1% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, GROUPS=1, KERNEL_H=3, KERNEL_W=3, PADDING_H=1, PADDING_W=1, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=8\n  triton_convolution2d_41 0.0891 ms 50.6% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=128, GROUPS=1, KERNEL_H=3, KERNEL_W=3, PADDING_H=1, PADDING_W=1, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=4\n  triton_convolution2d_42 0.1106 ms 40.7% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=256, BLOCK_N=64, GROUPS=1, KERNEL_H=3, KERNEL_W=3, PADDING_H=1, PADDING_W=1, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=4\n  triton_convolution2d_44 0.1147 ms 39.3% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, GROUPS=1, KERNEL_H=3, KERNEL_W=3, PADDING_H=1, PADDING_W=1, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=8\n  triton_convolution2d_47 0.1198 ms 37.6% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=256, BLOCK_N=64, GROUPS=1, KERNEL_H=3, KERNEL_W=3, PADDING_H=1, PADDING_W=1, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=8\n  triton_convolution2d_43 0.2263 ms 19.9% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=1024, BLOCK_N=16, GROUPS=1, KERNEL_H=3, KERNEL_W=3, PADDING_H=1, PADDING_W=1, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=1, num_warps=8\nSingleProcess AUTOTUNE benchmarking takes 0.2099 seconds and 0.0002 seconds precompiling for 8 choices\nAUTOTUNE convolution(2x64x56x56, 128x64x1x1)\n  triton_convolution2d_52 0.0082 ms 100.0% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=2, STRIDE_W=2, UNROLL=True, num_stages=2, num_warps=4\n  triton_convolution2d_53 0.0092 ms 88.9% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=2, STRIDE_W=2, UNROLL=True, num_stages=2, num_warps=8\n  triton_convolution2d_48 0.0113 ms 72.7% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=128, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=2, STRIDE_W=2, UNROLL=True, num_stages=2, num_warps=4\n  convolution 0.0133 ms 61.5%\n  triton_convolution2d_54 0.0133 ms 61.5% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=256, BLOCK_N=64, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=2, STRIDE_W=2, UNROLL=True, num_stages=2, num_warps=8\n  triton_convolution2d_49 0.0143 ms 57.1% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=256, BLOCK_N=64, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=2, STRIDE_W=2, UNROLL=True, num_stages=2, num_warps=4\n  triton_convolution2d_51 0.0143 ms 57.1% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=2, STRIDE_W=2, UNROLL=True, num_stages=2, num_warps=8\n  triton_convolution2d_50 0.0195 ms 42.1% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=1024, BLOCK_N=16, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=2, STRIDE_W=2, UNROLL=True, num_stages=1, num_warps=8\nSingleProcess AUTOTUNE benchmarking takes 0.1516 seconds and 0.0002 seconds precompiling for 8 choices\nAUTOTUNE convolution(2x128x28x28, 256x128x3x3)\n  convolution 0.0461 ms 100.0%\n  triton_convolution2d_73 0.0461 ms 99.9% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, GROUPS=1, KERNEL_H=3, KERNEL_W=3, PADDING_H=1, PADDING_W=1, STRIDE_H=2, STRIDE_W=2, UNROLL=False, num_stages=2, num_warps=4\n  triton_convolution2d_69 0.1065 ms 43.3% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=256, GROUPS=1, KERNEL_H=3, KERNEL_W=3, PADDING_H=1, PADDING_W=1, STRIDE_H=2, STRIDE_W=2, UNROLL=False, num_stages=2, num_warps=4\n  triton_convolution2d_74 0.1075 ms 42.9% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=256, GROUPS=1, KERNEL_H=3, KERNEL_W=3, PADDING_H=1, PADDING_W=1, STRIDE_H=2, STRIDE_W=2, UNROLL=False, num_stages=2, num_warps=8\n  triton_convolution2d_70 0.1085 ms 42.5% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=256, BLOCK_N=64, GROUPS=1, KERNEL_H=3, KERNEL_W=3, PADDING_H=1, PADDING_W=1, STRIDE_H=2, STRIDE_W=2, UNROLL=False, num_stages=2, num_warps=4\n  triton_convolution2d_75 0.1157 ms 39.8% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=256, BLOCK_N=64, GROUPS=1, KERNEL_H=3, KERNEL_W=3, PADDING_H=1, PADDING_W=1, STRIDE_H=2, STRIDE_W=2, UNROLL=False, num_stages=2, num_warps=8\n  triton_convolution2d_72 0.1208 ms 38.1% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, GROUPS=1, KERNEL_H=3, KERNEL_W=3, PADDING_H=1, PADDING_W=1, STRIDE_H=2, STRIDE_W=2, UNROLL=False, num_stages=2, num_warps=8\n  triton_convolution2d_71 0.1782 ms 25.9% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=1024, BLOCK_N=16, GROUPS=1, KERNEL_H=3, KERNEL_W=3, PADDING_H=1, PADDING_W=1, STRIDE_H=2, STRIDE_W=2, UNROLL=False, num_stages=1, num_warps=8\nSingleProcess AUTOTUNE benchmarking takes 0.2119 seconds and 0.0002 seconds precompiling for 8 choices\nAUTOTUNE convolution(2x256x14x14, 256x256x3x3)\n  convolution 0.0523 ms 100.0%\n  triton_convolution2d_80 0.0911 ms 57.3% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, GROUPS=1, KERNEL_H=3, KERNEL_W=3, PADDING_H=1, PADDING_W=1, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=4\n  triton_convolution2d_81 0.2028 ms 25.8% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=256, GROUPS=1, KERNEL_H=3, KERNEL_W=3, PADDING_H=1, PADDING_W=1, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=8\n  triton_convolution2d_76 0.2058 ms 25.4% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=256, GROUPS=1, KERNEL_H=3, KERNEL_W=3, PADDING_H=1, PADDING_W=1, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=4\n  triton_convolution2d_77 0.2120 ms 24.7% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=256, BLOCK_N=64, GROUPS=1, KERNEL_H=3, KERNEL_W=3, PADDING_H=1, PADDING_W=1, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=4\n  triton_convolution2d_78 0.2130 ms 24.5% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=512, BLOCK_N=16, GROUPS=1, KERNEL_H=3, KERNEL_W=3, PADDING_H=1, PADDING_W=1, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=1, num_warps=8\n  triton_convolution2d_79 0.2231 ms 23.4% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, GROUPS=1, KERNEL_H=3, KERNEL_W=3, PADDING_H=1, PADDING_W=1, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=8\n  triton_convolution2d_82 0.2324 ms 22.5% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=256, BLOCK_N=64, GROUPS=1, KERNEL_H=3, KERNEL_W=3, PADDING_H=1, PADDING_W=1, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=8\nSingleProcess AUTOTUNE benchmarking takes 0.2739 seconds and 0.0002 seconds precompiling for 8 choices\nAUTOTUNE convolution(2x128x28x28, 256x128x1x1)\n  triton_convolution2d_87 0.0102 ms 100.0% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=2, STRIDE_W=2, UNROLL=True, num_stages=2, num_warps=4\n  convolution 0.0184 ms 55.6%\n  triton_convolution2d_88 0.0184 ms 55.6% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=256, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=2, STRIDE_W=2, UNROLL=True, num_stages=2, num_warps=8\n  triton_convolution2d_89 0.0184 ms 55.6% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=256, BLOCK_N=64, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=2, STRIDE_W=2, UNROLL=True, num_stages=2, num_warps=8\n  triton_convolution2d_86 0.0195 ms 52.6% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=2, STRIDE_W=2, UNROLL=True, num_stages=2, num_warps=8\n  triton_convolution2d_83 0.0215 ms 47.6% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=256, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=2, STRIDE_W=2, UNROLL=True, num_stages=2, num_warps=4\n  triton_convolution2d_84 0.0215 ms 47.6% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=256, BLOCK_N=64, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=2, STRIDE_W=2, UNROLL=True, num_stages=2, num_warps=4\n  triton_convolution2d_85 0.0266 ms 38.5% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=1024, BLOCK_N=16, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=2, STRIDE_W=2, UNROLL=True, num_stages=1, num_warps=8\nSingleProcess AUTOTUNE benchmarking takes 0.1499 seconds and 0.0002 seconds precompiling for 8 choices\nAUTOTUNE convolution(2x256x14x14, 512x256x3x3)\n  convolution 0.0543 ms 100.0%\n  triton_convolution2d_108 0.0901 ms 60.2% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, GROUPS=1, KERNEL_H=3, KERNEL_W=3, PADDING_H=1, PADDING_W=1, STRIDE_H=2, STRIDE_W=2, UNROLL=False, num_stages=2, num_warps=4\n  triton_convolution2d_104 0.2068 ms 26.2% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=256, GROUPS=1, KERNEL_H=3, KERNEL_W=3, PADDING_H=1, PADDING_W=1, STRIDE_H=2, STRIDE_W=2, UNROLL=False, num_stages=2, num_warps=4\n  triton_convolution2d_106 0.2099 ms 25.9% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=512, BLOCK_N=16, GROUPS=1, KERNEL_H=3, KERNEL_W=3, PADDING_H=1, PADDING_W=1, STRIDE_H=2, STRIDE_W=2, UNROLL=False, num_stages=1, num_warps=8\n  triton_convolution2d_109 0.2140 ms 25.4% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=256, GROUPS=1, KERNEL_H=3, KERNEL_W=3, PADDING_H=1, PADDING_W=1, STRIDE_H=2, STRIDE_W=2, UNROLL=False, num_stages=2, num_warps=8\n  triton_convolution2d_110 0.2324 ms 23.3% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=256, BLOCK_N=64, GROUPS=1, KERNEL_H=3, KERNEL_W=3, PADDING_H=1, PADDING_W=1, STRIDE_H=2, STRIDE_W=2, UNROLL=False, num_stages=2, num_warps=8\n  triton_convolution2d_107 0.2355 ms 23.0% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, GROUPS=1, KERNEL_H=3, KERNEL_W=3, PADDING_H=1, PADDING_W=1, STRIDE_H=2, STRIDE_W=2, UNROLL=False, num_stages=2, num_warps=8\n  triton_convolution2d_105 0.2724 ms 19.9% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=256, BLOCK_N=64, GROUPS=1, KERNEL_H=3, KERNEL_W=3, PADDING_H=1, PADDING_W=1, STRIDE_H=2, STRIDE_W=2, UNROLL=False, num_stages=2, num_warps=4\nSingleProcess AUTOTUNE benchmarking takes 0.2772 seconds and 0.0002 seconds precompiling for 8 choices\nAUTOTUNE convolution(2x512x7x7, 512x512x3x3)\n  convolution 0.0830 ms 100.0%\n  triton_convolution2d_115 0.1812 ms 45.8% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, GROUPS=1, KERNEL_H=3, KERNEL_W=3, PADDING_H=1, PADDING_W=1, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=4\n  triton_convolution2d_113 0.2150 ms 38.6% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=128, BLOCK_N=16, GROUPS=1, KERNEL_H=3, KERNEL_W=3, PADDING_H=1, PADDING_W=1, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=1, num_warps=8\n  triton_convolution2d_117 0.2703 ms 30.7% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, GROUPS=1, KERNEL_H=3, KERNEL_W=3, PADDING_H=1, PADDING_W=1, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=8\n  triton_convolution2d_112 0.2867 ms 29.0% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=128, BLOCK_N=64, GROUPS=1, KERNEL_H=3, KERNEL_W=3, PADDING_H=1, PADDING_W=1, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=4\n  triton_convolution2d_111 0.4096 ms 20.3% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=256, GROUPS=1, KERNEL_H=3, KERNEL_W=3, PADDING_H=1, PADDING_W=1, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=4\n  triton_convolution2d_116 0.4229 ms 19.6% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=256, GROUPS=1, KERNEL_H=3, KERNEL_W=3, PADDING_H=1, PADDING_W=1, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=8\n  triton_convolution2d_114 0.4424 ms 18.8% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, GROUPS=1, KERNEL_H=3, KERNEL_W=3, PADDING_H=1, PADDING_W=1, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=8\nSingleProcess AUTOTUNE benchmarking takes 0.2969 seconds and 0.0002 seconds precompiling for 8 choices\nAUTOTUNE convolution(2x256x14x14, 512x256x1x1)\n  triton_convolution2d_122 0.0143 ms 100.0% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=2, STRIDE_W=2, UNROLL=True, num_stages=2, num_warps=4\n  triton_convolution2d_120 0.0256 ms 56.0% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=512, BLOCK_N=16, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=2, STRIDE_W=2, UNROLL=True, num_stages=1, num_warps=8\n  triton_convolution2d_119 0.0276 ms 51.9% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=256, BLOCK_N=64, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=2, STRIDE_W=2, UNROLL=True, num_stages=2, num_warps=4\n  triton_convolution2d_118 0.0287 ms 50.0% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=256, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=2, STRIDE_W=2, UNROLL=True, num_stages=2, num_warps=4\n  triton_convolution2d_121 0.0307 ms 46.7% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=2, STRIDE_W=2, UNROLL=True, num_stages=2, num_warps=8\n  triton_convolution2d_124 0.0307 ms 46.7% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=256, BLOCK_N=64, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=2, STRIDE_W=2, UNROLL=True, num_stages=2, num_warps=8\n  triton_convolution2d_123 0.0317 ms 45.2% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=256, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=2, STRIDE_W=2, UNROLL=True, num_stages=2, num_warps=8\n  convolution 0.0328 ms 43.8%\nSingleProcess AUTOTUNE benchmarking takes 0.1571 seconds and 0.0002 seconds precompiling for 8 choices\nAUTOTUNE addmm(2x1000, 2x512, 512x1000)\n  addmm 0.0154 ms 100.0%\n  triton_mm_142 0.0206 ms 74.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=2\n  triton_mm_143 0.0225 ms 68.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=2\n  triton_mm_140 0.0236 ms 65.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=2\n  triton_mm_141 0.0297 ms 51.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4\n  triton_mm_152 0.0297 ms 51.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4\n  triton_mm_153 0.0297 ms 51.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=4\n  triton_mm_146 0.0307 ms 50.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4\n  triton_mm_139 0.0328 ms 46.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=1, num_warps=2\n  triton_mm_145 0.0369 ms 41.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=4\nSingleProcess AUTOTUNE benchmarking takes 0.4912 seconds and 0.0002 seconds precompiling for 18 choices",
      "import os\nimport torch\nimport torch._inductor\n\nmodel_path = os.path.join(os.getcwd(), \"resnet18.pt2\")\n\ncompiled_model = torch._inductor.aoti_load_package(model_path)\nexample_inputs = (torch.randn(2, 3, 224, 224, device=device),)\n\nwith torch.inference_mode():\n    output = compiled_model(example_inputs)",
      "import time\ndef timed(fn):\n    # Returns the result of running `fn()` and the time it took for `fn()` to run,\n    # in seconds. We use CUDA events and synchronization for accurate\n    # measurement on CUDA enabled devices.\n    if torch.cuda.is_available():\n        start = torch.cuda.Event(enable_timing=True)\n        end = torch.cuda.Event(enable_timing=True)\n        start.record()\n    else:\n        start = time.time()\n\n    result = fn()\n    if torch.cuda.is_available():\n        end.record()\n        torch.cuda.synchronize()\n    else:\n        end = time.time()\n\n    # Measure time taken to execute the function in miliseconds\n    if torch.cuda.is_available():\n        duration = start.elapsed_time(end)\n    else:\n        duration = (end - start) * 1000\n\n    return result, duration",
      "torch._dynamo.reset()\n\nmodel = torch._inductor.aoti_load_package(model_path)\nexample_inputs = (torch.randn(1, 3, 224, 224, device=device),)\n\nwith torch.inference_mode():\n    _, time_taken = timed(lambda: model(example_inputs))\n    print(f\"Time taken for first inference for AOTInductor is {time_taken:.2f} ms\")",
      "torch._dynamo.reset()\n\nmodel = resnet18(weights=ResNet18_Weights.DEFAULT).to(device)\nmodel.eval()\n\nmodel = torch.compile(model)\nexample_inputs = torch.randn(1, 3, 224, 224, device=device)\n\nwith torch.inference_mode():\n    _, time_taken = timed(lambda: model(example_inputs))\n    print(f\"Time taken for first inference for torch.compile is {time_taken:.2f} ms\")",
      "Time taken for first inference for torch.compile is 4516.30 ms"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/recipes/regional_compilation.html",
    "title": "Reducing torch.compile cold start compilation time with regional compilation\u00b6",
    "code_snippets": [
      "from time import perf_counter",
      "import torch\nimport torch.nn as nn",
      "class Layer(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear1 = torch.nn.Linear(10, 10)\n        self.relu1 = torch.nn.ReLU()\n        self.linear2 = torch.nn.Linear(10, 10)\n        self.relu2 = torch.nn.ReLU()\n\n    def forward(self, x):\n        a = self.linear1(x)\n        a = self.relu1(a)\n        a = torch.sigmoid(a)\n        b = self.linear2(a)\n        b = self.relu2(b)\n        return b\n\n\nclass Model(torch.nn.Module):\n    def __init__(self, apply_regional_compilation):\n        super().__init__()\n        self.linear = torch.nn.Linear(10, 10)\n        # Apply compile only to the repeated layers.\n        if apply_regional_compilation:\n            self.layers = torch.nn.ModuleList(\n                [torch.compile(Layer()) for _ in range(64)]\n            )\n        else:\n            self.layers = torch.nn.ModuleList([Layer() for _ in range(64)])\n\n    def forward(self, x):\n        # In regional compilation, the self.linear is outside of the scope of `torch.compile`.\n        x = self.linear(x)\n        for layer in self.layers:\n            x = layer(x)\n        return x",
      "model = Model(apply_regional_compilation=False).cuda()\nfull_compiled_model = torch.compile(model)",
      "def measure_latency(fn, input):\n    # Reset the compiler caches to ensure no reuse between different runs\n    torch.compiler.reset()\n    with torch._inductor.utils.fresh_inductor_cache():\n        start = perf_counter()\n        fn(input)\n        torch.cuda.synchronize()\n        end = perf_counter()\n        return end - start\n\n\ninput = torch.randn(10, 10, device=\"cuda\")\nfull_model_compilation_latency = measure_latency(full_compiled_model, input)\nprint(f\"Full model compilation time = {full_model_compilation_latency:.2f} seconds\")\n\nregional_compilation_latency = measure_latency(regional_compiled_model, input)\nprint(f\"Regional compilation time = {regional_compilation_latency:.2f} seconds\")\n\nassert regional_compilation_latency < full_model_compilation_latency",
      "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py:236: UserWarning:\n\nTensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.\n\nFull model compilation time = 26.19 seconds\nRegional compilation time = 1.32 seconds"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/advanced/generic_join.html",
    "title": "Distributed Training with Uneven Inputs Using the Join Context Manager\u00b6",
    "code_snippets": [
      "import os\nimport torch\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\nfrom torch.distributed.algorithms.join import Join\nfrom torch.nn.parallel import DistributedDataParallel as DDP\n\nBACKEND = \"nccl\"\nWORLD_SIZE = 2\nNUM_INPUTS = 5\n\ndef worker(rank):\n    os.environ['MASTER_ADDR'] = 'localhost'\n    os.environ['MASTER_PORT'] = '29500'\n    dist.init_process_group(BACKEND, rank=rank, world_size=WORLD_SIZE)\n\n    model = DDP(torch.nn.Linear(1, 1).to(rank), device_ids=[rank])\n    # Rank 1 gets one more input than rank 0\n    inputs = [torch.tensor([1]).float() for _ in range(NUM_INPUTS + rank)]\n\n    num_inputs = 0\n    with Join([model]):\n        for input in inputs:\n            num_inputs += 1\n            loss = model(input).sum()\n            loss.backward()\n\n    print(f\"Rank {rank} has exhausted all {num_inputs} of its inputs!\")\n\ndef main():\n    mp.spawn(worker, nprocs=WORLD_SIZE, join=True)\n\nif __name__ == \"__main__\":\n    main()",
      "from torch.distributed.optim import ZeroRedundancyOptimizer as ZeRO\nfrom torch.optim import Adam\n\ndef worker(rank):\n    os.environ['MASTER_ADDR'] = 'localhost'\n    os.environ['MASTER_PORT'] = '29500'\n    dist.init_process_group(BACKEND, rank=rank, world_size=WORLD_SIZE)\n\n    model = DDP(torch.nn.Linear(1, 1).to(rank), device_ids=[rank])\n    optim = ZeRO(model.parameters(), Adam, lr=0.01)\n    # Rank 1 gets one more input than rank 0\n    inputs = [torch.tensor([1]).float() for _ in range(NUM_INPUTS + rank)]\n\n    num_inputs = 0\n    # Pass both `model` and `optim` into `Join()`\n    with Join([model, optim]):\n        for input in inputs:\n            num_inputs += 1\n            loss = model(input).sum()\n            loss.backward()\n            optim.step()\n\n    print(f\"Rank {rank} has exhausted all {num_inputs} of its inputs!\")",
      "import os\nimport torch\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\nfrom torch.distributed.algorithms.join import Join, Joinable, JoinHook\n\nBACKEND = \"nccl\"\nWORLD_SIZE = 2\nNUM_INPUTS = 5\n\nclass CounterJoinHook(JoinHook):\n    r\"\"\"\n    Join hook for :class:`Counter`.\n\n    Arguments:\n        counter (Counter): the :class:`Counter` object using this hook.\n        sync_max_count (bool): whether to sync the max count once all ranks\n            join.\n    \"\"\"\n    def __init__(\n        self,\n        counter,\n        sync_max_count\n    ):\n        self.counter = counter\n        self.sync_max_count = sync_max_count\n\n    def main_hook(self):\n        r\"\"\"\n        Shadows the counter's all-reduce by all-reducing a dim-1 zero tensor.\n        \"\"\"\n        t = torch.zeros(1, device=self.counter.device)\n        dist.all_reduce(t)\n\n    def post_hook(self, is_last_joiner: bool):\n        r\"\"\"\n        Synchronizes the max count across all :class:`Counter` s if\n        ``sync_max_count=True``.\n        \"\"\"\n        if not self.sync_max_count:\n            return\n        rank = dist.get_rank(self.counter.process_group)\n        common_rank = self.counter.find_common_rank(rank, is_last_joiner)\n        if rank == common_rank:\n            self.counter.max_count = self.counter.count.detach().clone()\n        dist.broadcast(self.counter.max_count, src=common_rank)\n\nclass Counter(Joinable):\n    r\"\"\"\n    Example :class:`Joinable` that counts the number of training iterations\n    that it participates in.\n    \"\"\"\n    def __init__(self, device, process_group):\n        super(Counter, self).__init__()\n        self.device = device\n        self.process_group = process_group\n        self.count = torch.tensor([0], device=device).float()\n        self.max_count = torch.tensor([0], device=device).float()\n\n    def __call__(self):\n        r\"\"\"\n        Counts the number of inputs processed on this iteration by all ranks\n        by all-reducing a dim-1 one tensor; increments its own internal count.\n        \"\"\"\n        Join.notify_join_context(self)\n        t = torch.ones(1, device=self.device).float()\n        dist.all_reduce(t)\n        self.count += t\n\n    def join_hook(self, **kwargs) -> JoinHook:\n        r\"\"\"\n        Return a join hook that shadows the all-reduce in :meth:`__call__`.\n\n        This join hook supports the following keyword arguments:\n            sync_max_count (bool, optional): whether to synchronize the maximum\n                count across all ranks once all ranks join; default is ``False``.\n        \"\"\"\n        sync_max_count = kwargs.get(\"sync_max_count\", False)\n        return CounterJoinHook(self, sync_max_count)\n\n    @property\n    def join_device(self) -> torch.device:\n        return self.device\n\n    @property\n    def join_process_group(self):\n        return self.process_group\n\n    def find_common_rank(self, rank, to_consider):\n        r\"\"\"\n        Returns the max rank of the ones to consider over the process group.\n        \"\"\"\n        common_rank = torch.tensor([rank if to_consider else -1], device=self.device)\n        dist.all_reduce(common_rank, op=dist.ReduceOp.MAX, group=self.process_group)\n        common_rank = common_rank.item()\n        return common_rank\n\ndef worker(rank):\n    assert torch.cuda.device_count() >= WORLD_SIZE\n    os.environ['MASTER_ADDR'] = 'localhost'\n    os.environ['MASTER_PORT'] = '29500'\n    dist.init_process_group(BACKEND, rank=rank, world_size=WORLD_SIZE)\n\n    counter = Counter(torch.device(f\"cuda:{rank}\"), dist.group.WORLD)\n    inputs = [torch.tensor([1]).float() for _ in range(NUM_INPUTS + rank)]\n\n    with Join([counter], sync_max_count=True):\n        for _ in inputs:\n            counter()\n\n    print(f\"{int(counter.count.item())} inputs processed before rank {rank} joined!\")\n    print(f\"{int(counter.max_count.item())} inputs processed across all ranks!\")\n\ndef main():\n    mp.spawn(worker, nprocs=WORLD_SIZE, join=True)\n\nif __name__ == \"__main__\":\n    main()"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/recipes/recipes/tensorboard_with_pytorch.html",
    "title": "How to use TensorBoard with PyTorch\u00b6",
    "code_snippets": [
      "import torch\nfrom torch.utils.tensorboard import SummaryWriter\nwriter = SummaryWriter()",
      "x = torch.arange(-5, 5, 0.1).view(-1, 1)\ny = -5 * x + 0.1 * torch.randn(x.size())\n\nmodel = torch.nn.Linear(1, 1)\ncriterion = torch.nn.MSELoss()\noptimizer = torch.optim.SGD(model.parameters(), lr = 0.1)\n\ndef train_model(iter):\n    for epoch in range(iter):\n        y1 = model(x)\n        loss = criterion(y1, y)\n        writer.add_scalar(\"Loss/train\", loss, epoch)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\ntrain_model(10)\nwriter.flush()"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/intermediate/TP_tutorial.html",
    "title": "Large Scale Transformer model training with Tensor Parallel (TP)\u00b6",
    "code_snippets": [
      "from torch.distributed.device_mesh import init_device_mesh\n\ntp_mesh = init_device_mesh(\"cuda\", (8,))",
      "# forward in the FeedForward layer\ndef forward(self, x):\n    return self.w2(F.silu(self.w1(x)) * self.w3(x))",
      "from torch.distributed.tensor.parallel import ColwiseParallel, RowwiseParallel, parallelize_module\n\nlayer_tp_plan = {\n    # by default ColwiseParallel input layouts is replicated\n    # and RowwiseParallel output layouts is replicated\n    \"feed_foward.w1\": ColwiseParallel(),\n    \"feed_forward.w2\": RowwiseParallel(),\n    \"feed_forward.w3\": ColwiseParallel(),\n}",
      "layer_tp_plan = {\n    # by default ColwiseParallel input layouts is replicated\n    # and RowwiseParallel output layouts is replicated\n    \"attention.wq\": ColwiseParallel(),\n    \"attention.wk\": ColwiseParallel(),\n    \"attention.wv\": ColwiseParallel(),\n    \"attention.wo\": RowwiseParallel(),\n    \"feed_forward.w1\": ColwiseParallel(),\n    \"feed_forward.w2\": RowwiseParallel(),\n    \"feed_forward.w3\": ColwiseParallel(),\n}",
      "# forward in a TransformerBlock\ndef forward(self, x):\n    h = x + self.attention(self.attention_norm(x))\n    out = h + self.feed_forward(self.ffn_norm(h))\n    return out",
      "from torch.distributed.tensor.parallel import (\n    PrepareModuleInput,\n    SequenceParallel,\n)",
      "import torch.nn.functional as F\nfrom torch.distributed.tensor.parallel import loss_parallel\n\npred = model(input_ids)\nwith loss_parallel():\n    # assuming pred and labels are of the shape [batch, seq, vocab]\n    loss = F.cross_entropy(pred.flatten(0, 1), labels.flatten(0, 1))\n    loss.backward()",
      "from torch.distributed.device_mesh import init_device_mesh\nfrom torch.distributed.tensor.parallel import ColwiseParallel, RowwiseParallel, parallelize_module\nfrom torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n\n# i.e. 2-D mesh is [dp, tp], training on 64 GPUs that performs 8 way DP and 8 way TP\nmesh_2d = init_device_mesh(\"cuda\", (8, 8))\ntp_mesh = mesh_2d[\"tp\"] # a submesh that connects intra-host devices\ndp_mesh = mesh_2d[\"dp\"] # a submesh that connects inter-host devices\n\nmodel = Model(...)\n\ntp_plan = {...}\n\n# apply Tensor Parallel intra-host on tp_mesh\nmodel_tp = parallelize_module(model, tp_mesh, tp_plan)\n# apply FSDP inter-host on dp_mesh\nmodel_2d = FSDP(model_tp, device_mesh=dp_mesh, use_orig_params=True, ...)"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/beginner/basics/data_tutorial.html",
    "title": "Datasets & DataLoaders\u00b6",
    "code_snippets": [
      "import torch\nfrom torch.utils.data import Dataset\nfrom torchvision import datasets\nfrom torchvision.transforms import ToTensor\nimport matplotlib.pyplot as plt\n\n\ntraining_data = datasets.FashionMNIST(\n    root=\"data\",\n    train=True,\n    download=True,\n    transform=ToTensor()\n)\n\ntest_data = datasets.FashionMNIST(\n    root=\"data\",\n    train=False,\n    download=True,\n    transform=ToTensor()\n)",
      "labels_map = {\n    0: \"T-Shirt\",\n    1: \"Trouser\",\n    2: \"Pullover\",\n    3: \"Dress\",\n    4: \"Coat\",\n    5: \"Sandal\",\n    6: \"Shirt\",\n    7: \"Sneaker\",\n    8: \"Bag\",\n    9: \"Ankle Boot\",\n}\nfigure = plt.figure(figsize=(8, 8))\ncols, rows = 3, 3\nfor i in range(1, cols * rows + 1):\n    sample_idx = torch.randint(len(training_data), size=(1,)).item()\n    img, label = training_data[sample_idx]\n    figure.add_subplot(rows, cols, i)\n    plt.title(labels_map[label])\n    plt.axis(\"off\")\n    plt.imshow(img.squeeze(), cmap=\"gray\")\nplt.show()",
      "import os\nimport pandas as pd\nfrom torchvision.io import read_image\n\nclass CustomImageDataset(Dataset):\n    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n        self.img_labels = pd.read_csv(annotations_file)\n        self.img_dir = img_dir\n        self.transform = transform\n        self.target_transform = target_transform\n\n    def __len__(self):\n        return len(self.img_labels)\n\n    def __getitem__(self, idx):\n        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n        image = read_image(img_path)\n        label = self.img_labels.iloc[idx, 1]\n        if self.transform:\n            image = self.transform(image)\n        if self.target_transform:\n            label = self.target_transform(label)\n        return image, label",
      "def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n    self.img_labels = pd.read_csv(annotations_file)\n    self.img_dir = img_dir\n    self.transform = transform\n    self.target_transform = target_transform",
      "def __len__(self):\n    return len(self.img_labels)",
      "def __getitem__(self, idx):\n    img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n    image = read_image(img_path)\n    label = self.img_labels.iloc[idx, 1]\n    if self.transform:\n        image = self.transform(image)\n    if self.target_transform:\n        label = self.target_transform(label)\n    return image, label",
      "from torch.utils.data import DataLoader\n\ntrain_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\ntest_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)",
      "Feature batch shape: torch.Size([64, 1, 28, 28])\nLabels batch shape: torch.Size([64])\nLabel: 8"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/recipes/distributed_optim_torchscript.html",
    "title": "Distributed Optimizer with TorchScript support\u00b6",
    "code_snippets": [
      "import torch\nfrom torch import Tensor\nfrom typing import List\n\n\ndef qhm_update(params: List[Tensor],\n            dp_list: List[Tensor],\n            momentum_buffer_list: List[Tensor],\n            lr: float,\n            nu: float,\n            weight_decay: float,\n            weight_decay_type: str,\n            momentum: float):\n\n    for p, d_p, momentum_buffer in zip(params, dp_list, momentum_buffer_list):\n        if weight_decay != 0:\n            if weight_decay_type == \"grad\":\n                d_p.add_(weight_decay, p)\n            elif weight_decay_type == \"direct\":\n                p.mul_(1.0 - lr * weight_decay)\n            else:\n                raise ValueError(\"Invalid weight decay type provided\")\n\n        momentum_buffer.mul_(momentum).add_(1.0 - momentum, d_p)\n\n        p.data.add_(-lr * nu, momentum_buffer)\n        p.data.add_(-lr * (1.0 - nu), d_p)",
      "import torch\nfrom torch import Tensor\nfrom typing import List, Optional, Dict\n\n# define this as a TorchScript class\n@torch.jit.script\nclass FunctionalQHM(object):\n    def __init__(self,\n                params: List[Tensor],\n                lr: float,\n                momentum: float,\n                nu: float,\n                weight_decay: float = 0.0,\n                weight_decay_type: str = \"grad\"):\n        if lr < 0.0:\n            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n        if momentum < 0.0:\n            raise ValueError(\"Invalid momentum value: {}\".format(momentum))\n        if weight_decay < 0.0:\n            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n        if weight_decay_type not in (\"grad\", \"direct\"):\n            raise ValueError(\"Invalid weight_decay_type value: {}\".format(weight_decay_type))\n\n        self.defaults = {\n            \"lr\": lr,\n            \"momentum\": momentum,\n            \"nu\": nu,\n            \"weight_decay\": weight_decay,\n        }\n        self.weight_decay_type = weight_decay_type\n\n        # NOTE: we only have one param_group here and don't allow user to add additional\n        # param group as it's not a common use case.\n        self.param_group = {\"params\": params}\n\n        self.state = torch.jit.annotate(Dict[torch.Tensor, Dict[str, torch.Tensor]], {})\n\n    def step(self, gradients: List[Optional[Tensor]]):\n        params = self.param_group['params']\n        params_with_grad = []\n        grads = []\n        momentum_buffer_list: List[Tensor] = []\n\n        if len(params) != len(gradients):\n            raise ValueError(\n                \"the gradients passed in does not equal to the size of the parameters!\"\n                + f\"Params length: {len(params)}. \"\n                + f\"Gradients length: {len(gradients)}\"\n            )\n\n        for param, gradient in zip(self.param_group['params'], gradients):\n            if gradient is not None:\n                params_with_grad.append(param)\n                grads.append(gradient)\n                state = self.state[param]\n                state['momentum_buffer'] = torch.zeros_like(param, memory_format=torch.preserve_format)\n                momentum_buffer_list.append(state['momentum_buffer'])\n\n        # calls into the update function we just defined\n        with torch.no_grad():\n            qhm_update(params_with_grad,\n                    grads,\n                    momentum_buffer_list,\n                    self.defaults['lr'],\n                    self.defaults['nu'],\n                    self.defaults['weight_decay'],\n                    self.weight_decay_type,\n                    self.defaults['momentum'])",
      "from torch.distributed.optim import DistributedOptimizer\n\nDistributedOptimizer.functional_optim_map[QHM] = FunctionalQHM"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/intermediate/custom_function_double_backward_tutorial.html",
    "title": "Double Backward with Custom Functions\u00b6",
    "code_snippets": [
      "import torch\n\nclass Square(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x):\n        # Because we are saving one of the inputs use `save_for_backward`\n        # Save non-tensors and non-inputs/non-outputs directly on ctx\n        ctx.save_for_backward(x)\n        return x**2\n\n    @staticmethod\n    def backward(ctx, grad_out):\n        # A function support double backward automatically if autograd\n        # is able to record the computations performed in backward\n        x, = ctx.saved_tensors\n        return grad_out * 2 * x\n\n# Use double precision because finite differencing method magnifies errors\nx = torch.rand(3, 3, requires_grad=True, dtype=torch.double)\ntorch.autograd.gradcheck(Square.apply, x)\n# Use gradcheck to verify second-order derivatives\ntorch.autograd.gradgradcheck(Square.apply, x)",
      "import torchviz\n\nx = torch.tensor(1., requires_grad=True).clone()\nout = Square.apply(x)\ngrad_x, = torch.autograd.grad(out, x, create_graph=True)\ntorchviz.make_dot((grad_x, x, out), {\"grad_x\": grad_x, \"x\": x, \"out\": out})",
      "class Exp(torch.autograd.Function):\n    # Simple case where everything goes well\n    @staticmethod\n    def forward(ctx, x):\n        # This time we save the output\n        result = torch.exp(x)\n        # Note that we should use `save_for_backward` here when\n        # the tensor saved is an ouptut (or an input).\n        ctx.save_for_backward(result)\n        return result\n\n    @staticmethod\n    def backward(ctx, grad_out):\n        result, = ctx.saved_tensors\n        return result * grad_out\n\nx = torch.tensor(1., requires_grad=True, dtype=torch.double).clone()\n# Validate our gradients using gradcheck\ntorch.autograd.gradcheck(Exp.apply, x)\ntorch.autograd.gradgradcheck(Exp.apply, x)",
      "out = Exp.apply(x)\ngrad_x, = torch.autograd.grad(out, x, create_graph=True)\ntorchviz.make_dot((grad_x, x, out), {\"grad_x\": grad_x, \"x\": x, \"out\": out})",
      "class Sinh(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x):\n        expx = torch.exp(x)\n        expnegx = torch.exp(-x)\n        ctx.save_for_backward(expx, expnegx)\n        # In order to be able to save the intermediate results, a trick is to\n        # include them as our outputs, so that the backward graph is constructed\n        return (expx - expnegx) / 2, expx, expnegx\n\n    @staticmethod\n    def backward(ctx, grad_out, _grad_out_exp, _grad_out_negexp):\n        expx, expnegx = ctx.saved_tensors\n        grad_input = grad_out * (expx + expnegx) / 2\n        # We cannot skip accumulating these even though we won't use the outputs\n        # directly. They will be used later in the second backward.\n        grad_input += _grad_out_exp * expx\n        grad_input -= _grad_out_negexp * expnegx\n        return grad_input\n\ndef sinh(x):\n    # Create a wrapper that only returns the first output\n    return Sinh.apply(x)[0]\n\nx = torch.rand(3, 3, requires_grad=True, dtype=torch.double)\ntorch.autograd.gradcheck(sinh, x)\ntorch.autograd.gradgradcheck(sinh, x)",
      "out = sinh(x)\ngrad_x, = torch.autograd.grad(out.sum(), x, create_graph=True)\ntorchviz.make_dot((grad_x, x, out), params={\"grad_x\": grad_x, \"x\": x, \"out\": out})",
      "class SinhBad(torch.autograd.Function):\n    # This is an example of what NOT to do!\n    @staticmethod\n    def forward(ctx, x):\n        expx = torch.exp(x)\n        expnegx = torch.exp(-x)\n        ctx.expx = expx\n        ctx.expnegx = expnegx\n        return (expx - expnegx) / 2\n\n    @staticmethod\n    def backward(ctx, grad_out):\n        expx = ctx.expx\n        expnegx = ctx.expnegx\n        grad_input = grad_out * (expx + expnegx) / 2\n        return grad_input",
      "out = SinhBad.apply(x)\ngrad_x, = torch.autograd.grad(out.sum(), x, create_graph=True)\ntorchviz.make_dot((grad_x, x, out), params={\"grad_x\": grad_x, \"x\": x, \"out\": out})",
      "def cube_forward(x):\n    return x**3\n\ndef cube_backward(grad_out, x):\n    return grad_out * 3 * x**2\n\ndef cube_backward_backward(grad_out, sav_grad_out, x):\n    return grad_out * sav_grad_out * 6 * x\n\ndef cube_backward_backward_grad_out(grad_out, x):\n    return grad_out * 3 * x**2\n\nclass Cube(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x):\n        ctx.save_for_backward(x)\n        return cube_forward(x)\n\n    @staticmethod\n    def backward(ctx, grad_out):\n        x, = ctx.saved_tensors\n        return CubeBackward.apply(grad_out, x)\n\nclass CubeBackward(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, grad_out, x):\n        ctx.save_for_backward(x, grad_out)\n        return cube_backward(grad_out, x)\n\n    @staticmethod\n    def backward(ctx, grad_out):\n        x, sav_grad_out = ctx.saved_tensors\n        dx = cube_backward_backward(grad_out, sav_grad_out, x)\n        dgrad_out = cube_backward_backward_grad_out(grad_out, x)\n        return dgrad_out, dx\n\nx = torch.tensor(2., requires_grad=True, dtype=torch.double)\n\ntorch.autograd.gradcheck(Cube.apply, x)\ntorch.autograd.gradgradcheck(Cube.apply, x)",
      "out = Cube.apply(x)\ngrad_x, = torch.autograd.grad(out, x, create_graph=True)\ntorchviz.make_dot((grad_x, x, out), params={\"grad_x\": grad_x, \"x\": x, \"out\": out})"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/recipes/torch_compile_backend_ipex.html",
    "title": "Intel\u00ae Extension for PyTorch* Backend on Intel\u00ae CPUs\u00b6",
    "code_snippets": [
      "import torch\nimport torchvision\n\nLR = 0.001\nDOWNLOAD = True\nDATA = 'datasets/cifar10/'\n\ntransform = torchvision.transforms.Compose([\n  torchvision.transforms.Resize((224, 224)),\n  torchvision.transforms.ToTensor(),\n  torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\ntrain_dataset = torchvision.datasets.CIFAR10(\n  root=DATA,\n  train=True,\n  transform=transform,\n  download=DOWNLOAD,\n)\ntrain_loader = torch.utils.data.DataLoader(\n  dataset=train_dataset,\n  batch_size=128\n)\n\nmodel = torchvision.models.resnet50()\ncriterion = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr = LR, momentum=0.9)\nmodel.train()\n\n#################### code changes ####################\nimport intel_extension_for_pytorch as ipex\n\n# Invoke the following API optionally, to apply frontend optimizations\nmodel, optimizer = ipex.optimize(model, optimizer=optimizer)\n\ncompile_model = torch.compile(model, backend=\"ipex\")\n######################################################\n\nfor batch_idx, (data, target) in enumerate(train_loader):\n    optimizer.zero_grad()\n    output = compile_model(data)\n    loss = criterion(output, target)\n    loss.backward()\n    optimizer.step()",
      "import torch\nimport torchvision\n\nLR = 0.001\nDOWNLOAD = True\nDATA = 'datasets/cifar10/'\n\ntransform = torchvision.transforms.Compose([\n  torchvision.transforms.Resize((224, 224)),\n  torchvision.transforms.ToTensor(),\n  torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\ntrain_dataset = torchvision.datasets.CIFAR10(\n  root=DATA,\n  train=True,\n  transform=transform,\n  download=DOWNLOAD,\n)\ntrain_loader = torch.utils.data.DataLoader(\n  dataset=train_dataset,\n  batch_size=128\n)\n\nmodel = torchvision.models.resnet50()\ncriterion = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr = LR, momentum=0.9)\nmodel.train()\n\n#################### code changes ####################\nimport intel_extension_for_pytorch as ipex\n\n# Invoke the following API optionally, to apply frontend optimizations\nmodel, optimizer = ipex.optimize(model, dtype=torch.bfloat16, optimizer=optimizer)\n\ncompile_model = torch.compile(model, backend=\"ipex\")\n######################################################\n\nwith torch.cpu.amp.autocast():\n    for batch_idx, (data, target) in enumerate(train_loader):\n        optimizer.zero_grad()\n        output = compile_model(data)\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()",
      "import torch\nimport torchvision.models as models\n\nmodel = models.resnet50(weights='ResNet50_Weights.DEFAULT')\nmodel.eval()\ndata = torch.rand(1, 3, 224, 224)\n\n#################### code changes ####################\nimport intel_extension_for_pytorch as ipex\n\n# Invoke the following API optionally, to apply frontend optimizations\nmodel = ipex.optimize(model, weights_prepack=False)\n\ncompile_model = torch.compile(model, backend=\"ipex\")\n######################################################\n\nwith torch.no_grad():\n    compile_model(data)",
      "import torch\nimport torchvision.models as models\n\nmodel = models.resnet50(weights='ResNet50_Weights.DEFAULT')\nmodel.eval()\ndata = torch.rand(1, 3, 224, 224)\n\n#################### code changes ####################\nimport intel_extension_for_pytorch as ipex\n\n# Invoke the following API optionally, to apply frontend optimizations\nmodel = ipex.optimize(model, dtype=torch.bfloat16, weights_prepack=False)\n\ncompile_model = torch.compile(model, backend=\"ipex\")\n######################################################\n\nwith torch.no_grad(), torch.autocast(device_type=\"cpu\", dtype=torch.bfloat16):\n    compile_model(data)"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/beginner/introyt/tensors_deeper_tutorial.html",
    "title": "Introduction to PyTorch Tensors\u00b6",
    "code_snippets": [
      "import torch\nimport math",
      "x = torch.empty(3, 4)\nprint(type(x))\nprint(x)",
      "<class 'torch.Tensor'>\ntensor([[0., 0., 0., 0.],\n        [0., 0., 0., 0.],\n        [0., 0., 0., 0.]])",
      "zeros = torch.zeros(2, 3)\nprint(zeros)\n\nones = torch.ones(2, 3)\nprint(ones)\n\ntorch.manual_seed(1729)\nrandom = torch.rand(2, 3)\nprint(random)",
      "torch.manual_seed(1729)\nrandom1 = torch.rand(2, 3)\nprint(random1)\n\nrandom2 = torch.rand(2, 3)\nprint(random2)\n\ntorch.manual_seed(1729)\nrandom3 = torch.rand(2, 3)\nprint(random3)\n\nrandom4 = torch.rand(2, 3)\nprint(random4)",
      "x = torch.empty(2, 2, 3)\nprint(x.shape)\nprint(x)\n\nempty_like_x = torch.empty_like(x)\nprint(empty_like_x.shape)\nprint(empty_like_x)\n\nzeros_like_x = torch.zeros_like(x)\nprint(zeros_like_x.shape)\nprint(zeros_like_x)\n\nones_like_x = torch.ones_like(x)\nprint(ones_like_x.shape)\nprint(ones_like_x)\n\nrand_like_x = torch.rand_like(x)\nprint(rand_like_x.shape)\nprint(rand_like_x)",
      "torch.Size([2, 2, 3])\ntensor([[[7.8676e-24, 3.0900e-41, 0.0000e+00],\n         [0.0000e+00, 8.9683e-44, 0.0000e+00]],\n\n        [[1.1210e-43, 0.0000e+00, 2.0356e-23],\n         [3.0900e-41, 1.4013e-45, 0.0000e+00]]])\ntorch.Size([2, 2, 3])\ntensor([[[ 8.9280e-24,  3.0900e-41,  2.0000e+00],\n         [ 1.7023e+00, -0.0000e+00,  1.5912e+00]],\n\n        [[ 3.6893e+19,  1.8732e+00, -2.0000e+00],\n         [ 1.7064e+00,  1.0842e-19,  1.7735e+00]]])\ntorch.Size([2, 2, 3])\ntensor([[[0., 0., 0.],\n         [0., 0., 0.]],\n\n        [[0., 0., 0.],\n         [0., 0., 0.]]])\ntorch.Size([2, 2, 3])\ntensor([[[1., 1., 1.],\n         [1., 1., 1.]],\n\n        [[1., 1., 1.],\n         [1., 1., 1.]]])\ntorch.Size([2, 2, 3])\ntensor([[[0.6128, 0.1519, 0.0453],\n         [0.5035, 0.9978, 0.3884]],\n\n        [[0.6929, 0.1703, 0.1384],\n         [0.4759, 0.7481, 0.0361]]])",
      "some_constants = torch.tensor([[3.1415926, 2.71828], [1.61803, 0.0072897]])\nprint(some_constants)\n\nsome_integers = torch.tensor((2, 3, 5, 7, 11, 13, 17, 19))\nprint(some_integers)\n\nmore_integers = torch.tensor(((2, 4, 6), [3, 6, 9]))\nprint(more_integers)",
      "a = torch.ones((2, 3), dtype=torch.int16)\nprint(a)\n\nb = torch.rand((2, 3), dtype=torch.float64) * 20.\nprint(b)\n\nc = b.to(torch.int32)\nprint(c)",
      "tensor([[1, 1, 1],\n        [1, 1, 1]], dtype=torch.int16)\ntensor([[ 0.9956,  1.4148,  5.8364],\n        [11.2406, 11.2083, 11.6692]], dtype=torch.float64)\ntensor([[ 0,  1,  5],\n        [11, 11, 11]], dtype=torch.int32)",
      "ones = torch.zeros(2, 2) + 1\ntwos = torch.ones(2, 2) * 2\nthrees = (torch.ones(2, 2) * 7 - 1) / 2\nfours = twos ** 2\nsqrt2s = twos ** 0.5\n\nprint(ones)\nprint(twos)\nprint(threes)\nprint(fours)\nprint(sqrt2s)",
      "powers2 = twos ** torch.tensor([[1, 2], [3, 4]])\nprint(powers2)\n\nfives = ones + fours\nprint(fives)\n\ndozens = threes * fours\nprint(dozens)",
      "a = torch.rand(2, 3)\nb = torch.rand(3, 2)\n\nprint(a * b)",
      "rand = torch.rand(2, 4)\ndoubled = rand * (torch.ones(1, 4) * 2)\n\nprint(rand)\nprint(doubled)",
      "a =     torch.ones(4, 3, 2)\n\nb = a * torch.rand(   3, 2) # 3rd & 2nd dims identical to a, dim 1 absent\nprint(b)\n\nc = a * torch.rand(   3, 1) # 3rd dim = 1, 2nd dim identical to a\nprint(c)\n\nd = a * torch.rand(   1, 2) # 3rd dim identical to a, 2nd dim = 1\nprint(d)",
      "a =     torch.ones(4, 3, 2)\n\nb = a * torch.rand(4, 3)    # dimensions must match last-to-first\n\nc = a * torch.rand(   2, 3) # both 3rd & 2nd dims different\n\nd = a * torch.rand((0, ))   # can't broadcast with an empty tensor",
      "# common functions\na = torch.rand(2, 4) * 2 - 1\nprint('Common functions:')\nprint(torch.abs(a))\nprint(torch.ceil(a))\nprint(torch.floor(a))\nprint(torch.clamp(a, -0.5, 0.5))\n\n# trigonometric functions and their inverses\nangles = torch.tensor([0, math.pi / 4, math.pi / 2, 3 * math.pi / 4])\nsines = torch.sin(angles)\ninverses = torch.asin(sines)\nprint('\\nSine and arcsine:')\nprint(angles)\nprint(sines)\nprint(inverses)\n\n# bitwise operations\nprint('\\nBitwise XOR:')\nb = torch.tensor([1, 5, 11])\nc = torch.tensor([2, 7, 10])\nprint(torch.bitwise_xor(b, c))\n\n# comparisons:\nprint('\\nBroadcasted, element-wise equality comparison:')\nd = torch.tensor([[1., 2.], [3., 4.]])\ne = torch.ones(1, 2)  # many comparison ops support broadcasting!\nprint(torch.eq(d, e)) # returns a tensor of type bool\n\n# reductions:\nprint('\\nReduction ops:')\nprint(torch.max(d))        # returns a single-element tensor\nprint(torch.max(d).item()) # extracts the value from the returned tensor\nprint(torch.mean(d))       # average\nprint(torch.std(d))        # standard deviation\nprint(torch.prod(d))       # product of all numbers\nprint(torch.unique(torch.tensor([1, 2, 1, 2, 1, 2]))) # filter unique elements\n\n# vector and linear algebra operations\nv1 = torch.tensor([1., 0., 0.])         # x unit vector\nv2 = torch.tensor([0., 1., 0.])         # y unit vector\nm1 = torch.rand(2, 2)                   # random matrix\nm2 = torch.tensor([[3., 0.], [0., 3.]]) # three times identity matrix\n\nprint('\\nVectors & Matrices:')\nprint(torch.linalg.cross(v2, v1)) # negative of z unit vector (v1 x v2 == -v2 x v1)\nprint(m1)\nm3 = torch.linalg.matmul(m1, m2)\nprint(m3)                  # 3 times m1\nprint(torch.linalg.svd(m3))       # singular value decomposition",
      "Common functions:\ntensor([[0.9238, 0.5724, 0.0791, 0.2629],\n        [0.1986, 0.4439, 0.6434, 0.4776]])\ntensor([[-0., -0., 1., -0.],\n        [-0., 1., 1., -0.]])\ntensor([[-1., -1.,  0., -1.],\n        [-1.,  0.,  0., -1.]])\ntensor([[-0.5000, -0.5000,  0.0791, -0.2629],\n        [-0.1986,  0.4439,  0.5000, -0.4776]])\n\nSine and arcsine:\ntensor([0.0000, 0.7854, 1.5708, 2.3562])\ntensor([0.0000, 0.7071, 1.0000, 0.7071])\ntensor([0.0000, 0.7854, 1.5708, 0.7854])\n\nBitwise XOR:\ntensor([3, 2, 1])\n\nBroadcasted, element-wise equality comparison:\ntensor([[ True, False],\n        [False, False]])\n\nReduction ops:\ntensor(4.)\n4.0\ntensor(2.5000)\ntensor(1.2910)\ntensor(24.)\ntensor([1, 2])\n\nVectors & Matrices:\ntensor([ 0.,  0., -1.])\ntensor([[0.7375, 0.8328],\n        [0.8444, 0.2941]])\ntensor([[2.2125, 2.4985],\n        [2.5332, 0.8822]])\ntorch.return_types.linalg_svd(\nU=tensor([[-0.7889, -0.6145],\n        [-0.6145,  0.7889]]),\nS=tensor([4.1498, 1.0548]),\nVh=tensor([[-0.7957, -0.6056],\n        [ 0.6056, -0.7957]]))",
      "a = torch.tensor([0, math.pi / 4, math.pi / 2, 3 * math.pi / 4])\nprint('a:')\nprint(a)\nprint(torch.sin(a))   # this operation creates a new tensor in memory\nprint(a)              # a has not changed\n\nb = torch.tensor([0, math.pi / 4, math.pi / 2, 3 * math.pi / 4])\nprint('\\nb:')\nprint(b)\nprint(torch.sin_(b))  # note the underscore\nprint(b)              # b has changed",
      "a = torch.ones(2, 2)\nb = torch.rand(2, 2)\n\nprint('Before:')\nprint(a)\nprint(b)\nprint('\\nAfter adding:')\nprint(a.add_(b))\nprint(a)\nprint(b)\nprint('\\nAfter multiplying')\nprint(b.mul_(b))\nprint(b)",
      "a = torch.rand(2, 2)\nb = torch.rand(2, 2)\nc = torch.zeros(2, 2)\nold_id = id(c)\n\nprint(c)\nd = torch.matmul(a, b, out=c)\nprint(c)                # contents of c have changed\n\nassert c is d           # test c & d are same object, not just containing equal values\nassert id(c) == old_id  # make sure that our new c is the same object as the old one\n\ntorch.rand(2, 2, out=c) # works for creation too!\nprint(c)                # c has changed again\nassert id(c) == old_id  # still the same object!",
      "a = torch.ones(2, 2)\nb = a\n\na[0][1] = 561  # we change a...\nprint(b)       # ...and b is also altered",
      "a = torch.ones(2, 2)\nb = a.clone()\n\nassert b is not a      # different objects in memory...\nprint(torch.eq(a, b))  # ...but still with the same contents!\n\na[0][1] = 561          # a changes...\nprint(b)               # ...but b is still all ones",
      "a = torch.rand(2, 2, requires_grad=True) # turn on autograd\nprint(a)\n\nb = a.clone()\nprint(b)\n\nc = a.detach().clone()\nprint(c)\n\nprint(a)",
      "if torch.accelerator.is_available():\n    print('We have an accelerator!')\nelse:\n    print('Sorry, CPU only.')",
      "if torch.accelerator.is_available():\n    gpu_rand = torch.rand(2, 2, device=torch.accelerator.current_accelerator())\n    print(gpu_rand)\nelse:\n    print('Sorry, CPU only.')",
      "my_device = torch.accelerator.current_accelerator() if torch.accelerator.is_available() else torch.device('cpu')\nprint('Device: {}'.format(my_device))\n\nx = torch.rand(2, 2, device=my_device)\nprint(x)",
      "y = torch.rand(2, 2)\ny = y.to(my_device)",
      "x = torch.rand(2, 2)\ny = torch.rand(2, 2, device='cuda')\nz = x + y  # exception will be thrown",
      "a = torch.rand(3, 226, 226)\nb = a.unsqueeze(0)\n\nprint(a.shape)\nprint(b.shape)",
      "torch.Size([3, 226, 226])\ntorch.Size([1, 3, 226, 226])",
      "c = torch.rand(1, 1, 1, 1, 1)\nprint(c)",
      "a = torch.rand(1, 20)\nprint(a.shape)\nprint(a)\n\nb = a.squeeze(0)\nprint(b.shape)\nprint(b)\n\nc = torch.rand(2, 2)\nprint(c.shape)\n\nd = c.squeeze(0)\nprint(d.shape)",
      "torch.Size([1, 20])\ntensor([[0.1899, 0.4067, 0.1519, 0.1506, 0.9585, 0.7756, 0.8973, 0.4929, 0.2367,\n         0.8194, 0.4509, 0.2690, 0.8381, 0.8207, 0.6818, 0.5057, 0.9335, 0.9769,\n         0.2792, 0.3277]])\ntorch.Size([20])\ntensor([0.1899, 0.4067, 0.1519, 0.1506, 0.9585, 0.7756, 0.8973, 0.4929, 0.2367,\n        0.8194, 0.4509, 0.2690, 0.8381, 0.8207, 0.6818, 0.5057, 0.9335, 0.9769,\n        0.2792, 0.3277])\ntorch.Size([2, 2])\ntorch.Size([2, 2])",
      "a = torch.ones(4, 3, 2)\n\nc = a * torch.rand(   3, 1) # 3rd dim = 1, 2nd dim identical to a\nprint(c)",
      "a = torch.ones(4, 3, 2)\nb = torch.rand(   3)     # trying to multiply a * b will give a runtime error\nc = b.unsqueeze(1)       # change to a 2-dimensional tensor, adding new dim at the end\nprint(c.shape)\nprint(a * c)             # broadcasting works again!",
      "torch.Size([3, 1])\ntensor([[[0.1891, 0.1891],\n         [0.3952, 0.3952],\n         [0.9176, 0.9176]],\n\n        [[0.1891, 0.1891],\n         [0.3952, 0.3952],\n         [0.9176, 0.9176]],\n\n        [[0.1891, 0.1891],\n         [0.3952, 0.3952],\n         [0.9176, 0.9176]],\n\n        [[0.1891, 0.1891],\n         [0.3952, 0.3952],\n         [0.9176, 0.9176]]])",
      "batch_me = torch.rand(3, 226, 226)\nprint(batch_me.shape)\nbatch_me.unsqueeze_(0)\nprint(batch_me.shape)",
      "torch.Size([3, 226, 226])\ntorch.Size([1, 3, 226, 226])",
      "output3d = torch.rand(6, 20, 20)\nprint(output3d.shape)\n\ninput1d = output3d.reshape(6 * 20 * 20)\nprint(input1d.shape)\n\n# can also call it as a method on the torch module:\nprint(torch.reshape(output3d, (6 * 20 * 20,)).shape)",
      "torch.Size([6, 20, 20])\ntorch.Size([2400])\ntorch.Size([2400])",
      "import numpy as np\n\nnumpy_array = np.ones((2, 3))\nprint(numpy_array)\n\npytorch_tensor = torch.from_numpy(numpy_array)\nprint(pytorch_tensor)",
      "[[1. 1. 1.]\n [1. 1. 1.]]\ntensor([[1., 1., 1.],\n        [1., 1., 1.]], dtype=torch.float64)",
      "pytorch_rand = torch.rand(2, 3)\nprint(pytorch_rand)\n\nnumpy_rand = pytorch_rand.numpy()\nprint(numpy_rand)",
      "tensor([[ 1.,  1.,  1.],\n        [ 1., 23.,  1.]], dtype=torch.float64)\n[[ 0.87163675  0.2458961   0.34993553]\n [ 0.2853077  17.          0.5695162 ]]"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/beginner/introyt/trainingyt.html",
    "title": "Training with PyTorch\u00b6",
    "code_snippets": [
      "import torch\nimport torchvision\nimport torchvision.transforms as transforms\n\n# PyTorch TensorBoard support\nfrom torch.utils.tensorboard import SummaryWriter\nfrom datetime import datetime\n\n\ntransform = transforms.Compose(\n    [transforms.ToTensor(),\n    transforms.Normalize((0.5,), (0.5,))])\n\n# Create datasets for training & validation, download if necessary\ntraining_set = torchvision.datasets.FashionMNIST('./data', train=True, transform=transform, download=True)\nvalidation_set = torchvision.datasets.FashionMNIST('./data', train=False, transform=transform, download=True)\n\n# Create data loaders for our datasets; shuffle for training, not for validation\ntraining_loader = torch.utils.data.DataLoader(training_set, batch_size=4, shuffle=True)\nvalidation_loader = torch.utils.data.DataLoader(validation_set, batch_size=4, shuffle=False)\n\n# Class labels\nclasses = ('T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n        'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot')\n\n# Report split sizes\nprint('Training set has {} instances'.format(len(training_set)))\nprint('Validation set has {} instances'.format(len(validation_set)))",
      "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Helper function for inline image display\ndef matplotlib_imshow(img, one_channel=False):\n    if one_channel:\n        img = img.mean(dim=0)\n    img = img / 2 + 0.5     # unnormalize\n    npimg = img.numpy()\n    if one_channel:\n        plt.imshow(npimg, cmap=\"Greys\")\n    else:\n        plt.imshow(np.transpose(npimg, (1, 2, 0)))\n\ndataiter = iter(training_loader)\nimages, labels = next(dataiter)\n\n# Create a grid from the images and show them\nimg_grid = torchvision.utils.make_grid(images)\nmatplotlib_imshow(img_grid, one_channel=True)\nprint('  '.join(classes[labels[j]] for j in range(4)))",
      "import torch.nn as nn\nimport torch.nn.functional as F\n\n# PyTorch models inherit from torch.nn.Module\nclass GarmentClassifier(nn.Module):\n    def __init__(self):\n        super(GarmentClassifier, self).__init__()\n        self.conv1 = nn.Conv2d(1, 6, 5)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = x.view(-1, 16 * 4 * 4)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\n\nmodel = GarmentClassifier()",
      "loss_fn = torch.nn.CrossEntropyLoss()\n\n# NB: Loss functions expect data in batches, so we're creating batches of 4\n# Represents the model's confidence in each of the 10 classes for a given input\ndummy_outputs = torch.rand(4, 10)\n# Represents the correct class among the 10 being tested\ndummy_labels = torch.tensor([1, 5, 3, 7])\n\nprint(dummy_outputs)\nprint(dummy_labels)\n\nloss = loss_fn(dummy_outputs, dummy_labels)\nprint('Total loss for this batch: {}'.format(loss.item()))",
      "# Optimizers specified in the torch.optim package\noptimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)",
      "def train_one_epoch(epoch_index, tb_writer):\n    running_loss = 0.\n    last_loss = 0.\n\n    # Here, we use enumerate(training_loader) instead of\n    # iter(training_loader) so that we can track the batch\n    # index and do some intra-epoch reporting\n    for i, data in enumerate(training_loader):\n        # Every data instance is an input + label pair\n        inputs, labels = data\n\n        # Zero your gradients for every batch!\n        optimizer.zero_grad()\n\n        # Make predictions for this batch\n        outputs = model(inputs)\n\n        # Compute the loss and its gradients\n        loss = loss_fn(outputs, labels)\n        loss.backward()\n\n        # Adjust learning weights\n        optimizer.step()\n\n        # Gather data and report\n        running_loss += loss.item()\n        if i % 1000 == 999:\n            last_loss = running_loss / 1000 # loss per batch\n            print('  batch {} loss: {}'.format(i + 1, last_loss))\n            tb_x = epoch_index * len(training_loader) + i + 1\n            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n            running_loss = 0.\n\n    return last_loss",
      "# Initializing in a separate cell so we can easily add more epochs to the same run\ntimestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\nwriter = SummaryWriter('runs/fashion_trainer_{}'.format(timestamp))\nepoch_number = 0\n\nEPOCHS = 5\n\nbest_vloss = 1_000_000.\n\nfor epoch in range(EPOCHS):\n    print('EPOCH {}:'.format(epoch_number + 1))\n\n    # Make sure gradient tracking is on, and do a pass over the data\n    model.train(True)\n    avg_loss = train_one_epoch(epoch_number, writer)\n\n\n    running_vloss = 0.0\n    # Set the model to evaluation mode, disabling dropout and using population\n    # statistics for batch normalization.\n    model.eval()\n\n    # Disable gradient computation and reduce memory consumption.\n    with torch.no_grad():\n        for i, vdata in enumerate(validation_loader):\n            vinputs, vlabels = vdata\n            voutputs = model(vinputs)\n            vloss = loss_fn(voutputs, vlabels)\n            running_vloss += vloss\n\n    avg_vloss = running_vloss / (i + 1)\n    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n\n    # Log the running loss averaged per batch\n    # for both training and validation\n    writer.add_scalars('Training vs. Validation Loss',\n                    { 'Training' : avg_loss, 'Validation' : avg_vloss },\n                    epoch_number + 1)\n    writer.flush()\n\n    # Track best performance, and save the model's state\n    if avg_vloss < best_vloss:\n        best_vloss = avg_vloss\n        model_path = 'model_{}_{}'.format(timestamp, epoch_number)\n        torch.save(model.state_dict(), model_path)\n\n    epoch_number += 1",
      "saved_model = GarmentClassifier()\nsaved_model.load_state_dict(torch.load(PATH))"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/beginner/profiler.html",
    "title": "Profiling your PyTorch Module\u00b6",
    "code_snippets": [
      "import torch\nimport numpy as np\nfrom torch import nn\nimport torch.autograd.profiler as profiler",
      "class MyModule(nn.Module):\n    def __init__(self, in_features: int, out_features: int, bias: bool = True):\n        super(MyModule, self).__init__()\n        self.linear = nn.Linear(in_features, out_features, bias)\n\n    def forward(self, input, mask):\n        with profiler.record_function(\"LINEAR PASS\"):\n            out = self.linear(input)\n\n        with profiler.record_function(\"MASK INDICES\"):\n            threshold = out.sum(axis=1).mean().item()\n            hi_idx = np.argwhere(mask.cpu().numpy() > threshold)\n            hi_idx = torch.from_numpy(hi_idx).cuda()\n\n        return out, hi_idx",
      "model = MyModule(500, 10).cuda()\ninput = torch.rand(128, 500).cuda()\nmask = torch.rand((500, 500, 500), dtype=torch.double).cuda()\n\n# warm-up\nmodel(input, mask)\n\nwith profiler.profile(with_stack=True, profile_memory=True) as prof:\n    out, idx = model(input, mask)",
      "model = MyModule(500, 10).cuda()\ninput = torch.rand(128, 500).cuda()\nmask = torch.rand((500, 500, 500), dtype=torch.float).cuda()\n\n# warm-up\nmodel(input, mask)\n\nwith profiler.profile(with_stack=True, profile_memory=True) as prof:\n    out, idx = model(input, mask)\n\nprint(prof.key_averages(group_by_stack_n=5).table(sort_by='self_cpu_time_total', row_limit=5))\n\n\"\"\"\n(Some columns are omitted)\n\n-----------------  ------------  ------------  ------------  --------------------------------\n             Name    Self CPU %      Self CPU  Self CPU Mem   Source Location\n-----------------  ------------  ------------  ------------  --------------------------------\n     MASK INDICES        93.61%        5.006s    -476.84 Mb  /mnt/xarfuse/.../torch/au\n                                                             <ipython-input-...>(10): forward\n                                                             /mnt/xarfuse/  /torch/nn\n                                                             <ipython-input-...>(9): <module>\n                                                             /mnt/xarfuse/.../IPython/\n\n      aten::copy_         6.34%     338.759ms           0 b  <ipython-input-...>(12): forward\n                                                             /mnt/xarfuse/.../torch/nn\n                                                             <ipython-input-...>(9): <module>\n                                                             /mnt/xarfuse/.../IPython/\n                                                             /mnt/xarfuse/.../IPython/\n\n aten::as_strided         0.01%     281.808us           0 b  <ipython-input-...>(11): forward\n                                                             /mnt/xarfuse/.../torch/nn\n                                                             <ipython-input-...>(9): <module>\n                                                             /mnt/xarfuse/.../IPython/\n                                                             /mnt/xarfuse/.../IPython/\n\n      aten::addmm         0.01%     275.721us           0 b  /mnt/xarfuse/.../torch/nn\n                                                             /mnt/xarfuse/.../torch/nn\n                                                             /mnt/xarfuse/.../torch/nn\n                                                             <ipython-input-...>(8): forward\n                                                             /mnt/xarfuse/.../torch/nn\n\n      aten::_local        0.01%     268.650us           0 b  <ipython-input-...>(11): forward\n      _scalar_dense                                          /mnt/xarfuse/.../torch/nn\n                                                             <ipython-input-...>(9): <module>\n                                                             /mnt/xarfuse/.../IPython/\n                                                             /mnt/xarfuse/.../IPython/\n\n-----------------  ------------  ------------  ------------  --------------------------------\nSelf CPU time total: 5.347s\n\n\"\"\"",
      "class MyModule(nn.Module):\n    def __init__(self, in_features: int, out_features: int, bias: bool = True):\n        super(MyModule, self).__init__()\n        self.linear = nn.Linear(in_features, out_features, bias)\n\n    def forward(self, input, mask):\n        with profiler.record_function(\"LINEAR PASS\"):\n            out = self.linear(input)\n\n        with profiler.record_function(\"MASK INDICES\"):\n            threshold = out.sum(axis=1).mean()\n            hi_idx = (mask > threshold).nonzero(as_tuple=True)\n\n        return out, hi_idx\n\n\nmodel = MyModule(500, 10).cuda()\ninput = torch.rand(128, 500).cuda()\nmask = torch.rand((500, 500, 500), dtype=torch.float).cuda()\n\n# warm-up\nmodel(input, mask)\n\nwith profiler.profile(with_stack=True, profile_memory=True) as prof:\n    out, idx = model(input, mask)\n\nprint(prof.key_averages(group_by_stack_n=5).table(sort_by='self_cpu_time_total', row_limit=5))\n\n\"\"\"\n(Some columns are omitted)\n\n--------------  ------------  ------------  ------------  ---------------------------------\n          Name    Self CPU %      Self CPU  Self CPU Mem   Source Location\n--------------  ------------  ------------  ------------  ---------------------------------\n      aten::gt        57.17%     129.089ms           0 b  <ipython-input-...>(12): forward\n                                                          /mnt/xarfuse/.../torch/nn\n                                                          <ipython-input-...>(25): <module>\n                                                          /mnt/xarfuse/.../IPython/\n                                                          /mnt/xarfuse/.../IPython/\n\n aten::nonzero        37.38%      84.402ms           0 b  <ipython-input-...>(12): forward\n                                                          /mnt/xarfuse/.../torch/nn\n                                                          <ipython-input-...>(25): <module>\n                                                          /mnt/xarfuse/.../IPython/\n                                                          /mnt/xarfuse/.../IPython/\n\n   INDEX SCORE         3.32%       7.491ms    -119.21 Mb  /mnt/xarfuse/.../torch/au\n                                                          <ipython-input-...>(10): forward\n                                                          /mnt/xarfuse/.../torch/nn\n                                                          <ipython-input-...>(25): <module>\n                                                          /mnt/xarfuse/.../IPython/\n\naten::as_strided         0.20%    441.587us          0 b  <ipython-input-...>(12): forward\n                                                          /mnt/xarfuse/.../torch/nn\n                                                          <ipython-input-...>(25): <module>\n                                                          /mnt/xarfuse/.../IPython/\n                                                          /mnt/xarfuse/.../IPython/\n\n aten::nonzero\n     _numpy             0.18%     395.602us           0 b  <ipython-input-...>(12): forward\n                                                          /mnt/xarfuse/.../torch/nn\n                                                          <ipython-input-...>(25): <module>\n                                                          /mnt/xarfuse/.../IPython/\n                                                          /mnt/xarfuse/.../IPython/\n--------------  ------------  ------------  ------------  ---------------------------------\nSelf CPU time total: 225.801ms\n\n\"\"\""
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/intermediate/ensembling.html",
    "title": "Model ensembling\u00b6",
    "code_snippets": [
      "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\ntorch.manual_seed(0)\n\n# Here's a simple MLP\nclass SimpleMLP(nn.Module):\n    def __init__(self):\n        super(SimpleMLP, self).__init__()\n        self.fc1 = nn.Linear(784, 128)\n        self.fc2 = nn.Linear(128, 128)\n        self.fc3 = nn.Linear(128, 10)\n\n    def forward(self, x):\n        x = x.flatten(1)\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.fc2(x)\n        x = F.relu(x)\n        x = self.fc3(x)\n        return x",
      "device = 'cuda'\nnum_models = 10\n\ndata = torch.randn(100, 64, 1, 28, 28, device=device)\ntargets = torch.randint(10, (6400,), device=device)\n\nmodels = [SimpleMLP().to(device) for _ in range(num_models)]",
      "from torch.func import stack_module_state\n\nparams, buffers = stack_module_state(models)",
      "from torch.func import functional_call\nimport copy\n\n# Construct a \"stateless\" version of one of the models. It is \"stateless\" in\n# the sense that the parameters are meta Tensors and do not have storage.\nbase_model = copy.deepcopy(models[0])\nbase_model = base_model.to('meta')\n\ndef fmodel(params, buffers, x):\n    return functional_call(base_model, (params, buffers), (x,))",
      "print([p.size(0) for p in params.values()]) # show the leading 'num_models' dimension\n\nassert minibatches.shape == (num_models, 64, 1, 28, 28) # verify minibatch has leading dimension of size 'num_models'\n\nfrom torch import vmap\n\npredictions1_vmap = vmap(fmodel)(params, buffers, minibatches)\n\n# verify the ``vmap`` predictions match the\nassert torch.allclose(predictions1_vmap, torch.stack(predictions_diff_minibatch_loop), atol=1e-3, rtol=1e-5)",
      "predictions2_vmap = vmap(fmodel, in_dims=(0, 0, None))(params, buffers, minibatch)\n\nassert torch.allclose(predictions2_vmap, torch.stack(predictions2), atol=1e-3, rtol=1e-5)",
      "from torch.utils.benchmark import Timer\nwithout_vmap = Timer(\n    stmt=\"[model(minibatch) for model, minibatch in zip(models, minibatches)]\",\n    globals=globals())\nwith_vmap = Timer(\n    stmt=\"vmap(fmodel)(params, buffers, minibatches)\",\n    globals=globals())\nprint(f'Predictions without vmap {without_vmap.timeit(100)}')\nprint(f'Predictions with vmap {with_vmap.timeit(100)}')",
      "Predictions without vmap <torch.utils.benchmark.utils.common.Measurement object at 0x7f9750198fa0>\n[model(minibatch) for model, minibatch in zip(models, minibatches)]\n  1.37 ms\n  1 measurement, 100 runs , 1 thread\nPredictions with vmap <torch.utils.benchmark.utils.common.Measurement object at 0x7f9750199360>\nvmap(fmodel)(params, buffers, minibatches)\n  473.36 us\n  1 measurement, 100 runs , 1 thread"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/recipes/recipes/swap_tensors.html",
    "title": "Extension points in nn.Module for load_state_dict and tensor subclasses\u00b6",
    "code_snippets": [
      "import torch\nimport torch.nn as nn\nt1 = torch.arange(2)\nt2 = torch.arange(3)\nprint(f\"Before swapping, t1: {t1}, t2: {t2}\")\ntorch.utils.swap_tensors(t1, t2)\nprint(f\"After swapping, t1: {t1}, t2: {t2}\")",
      "mod = torch.nn.Linear(1, 2, bias=False)\noptimizer = torch.optim.SGD(mod.parameters())\nprint(f\"weight in mod: {mod.weight}\")\nprint(f\"weight in optimizer: {optimizer.param_groups[0]['params']}\")\nmod.weight = torch.nn.Parameter(2 * mod.weight)\nprint(f\"weight in mod: {mod.weight}\")\nprint(f\"weight in optimizer: {optimizer.param_groups[0]['params']}\")",
      "aten = torch.ops.aten\n\nclass MyQuantizedLinearWeight(torch.Tensor):\n    @staticmethod\n    def __new__(cls, elem, scale):\n        return torch.Tensor._make_wrapper_subclass(\n            cls,\n            elem.shape,\n            dtype=elem.dtype,\n            layout=elem.layout,\n            device=elem.device,\n            strides=elem.stride(),\n            storage_offset=elem.storage_offset())\n\n    def __init__(self, elem: torch.Tensor, scale: float):\n        self.elem = elem\n        self.scale = scale\n\n    def __repr__(self):\n        return f\"MyQuantizedLinearWeight({self.elem}, scale={self.scale})\"\n\n    @classmethod\n    def __torch_dispatch__(cls, func, types, args, kwargs):\n        if func in (aten.detach.default, aten._to_copy.default):\n            new_elem = func(args[0].elem, *args[1:], **kwargs)\n            return cls(new_elem, args[0].scale)\n        # Implementations for certain ops would be added to ``OP_TABLE``.\n        # We omit this for brevity.\n        OP_TABLE = dict()\n        if func in OP_TABLE:\n          return OP_TABLE[func](func, args, kwargs)\n        raise NotImplementedError(f\"Unsupported function {func}\")",
      "m = nn.Linear(3, 5, dtype=torch.float32)\nm.weight = torch.nn.Parameter(MyQuantizedLinearWeight(m.weight, 0.5))\nprint(f\"Before: id(m.weight)={id(m.weight)}, id(m.bias)={id(m.bias)}\")\nm.bfloat16()\nprint(f\"After: id(m.weight)={id(m.weight)}, id(m.bias)={id(m.bias)}\")\nprint(f\"m.weight.dtype: {m.weight.dtype}\")\nprint(f\"m.weight.elem.dtype: {m.weight.elem.dtype}\")\nprint(f\"m.bias.dtype: {m.bias.dtype}\")",
      "Before: id(m.weight)=140287920621504, id(m.bias)=140287920621024\nAfter: id(m.weight)=140287920621504, id(m.bias)=140287920621024\nm.weight.dtype: torch.bfloat16\nm.weight.elem.dtype: torch.float32\nm.bias.dtype: torch.bfloat16",
      "torch.__future__.set_swap_module_params_on_conversion(True)\nm = nn.Linear(3, 5, dtype=torch.float32)\nm.weight = torch.nn.Parameter(MyQuantizedLinearWeight(m.weight, 0.5))\nprint(f\"Before: id(m.weight)={id(m.weight)}, id(m.bias)={id(m.bias)}\")\nm.bfloat16()\nprint(f\"After: id(m.weight)={id(m.weight)}, id(m.bias)={id(m.bias)}\")\nprint(f\"m.weight.dtype: {m.weight.dtype}\")\nprint(f\"m.weight.elem.dtype: {m.weight.elem.dtype}\")\nprint(f\"m.bias.dtype: {m.bias.dtype}\")\ntorch.__future__.set_swap_module_params_on_conversion(False)",
      "Before: id(m.weight)=140287920621984, id(m.bias)=140287919977664\nAfter: id(m.weight)=140287920621984, id(m.bias)=140287919977664\nm.weight.dtype: torch.bfloat16\nm.weight.elem.dtype: torch.bfloat16\nm.bias.dtype: torch.bfloat16",
      "@classmethod\ndef custom_torch_function(cls, func, types, args=(), kwargs=None):\n    kwargs = {} if kwargs is None else kwargs\n\n    if func is torch.Tensor.module_load:\n        dest, src = args[0], args[1]\n        assert type(dest) == cls and type(src) == torch.Tensor\n        return MyQuantizedLinearWeight(src, dest.scale)\n    else:\n        with torch._C.DisableTorchFunctionSubclass():\n                return func(*args, **kwargs)\n\nMyQuantizedLinearWeight.__torch_function__ = custom_torch_function",
      "def fn(m):\n    if isinstance(m, nn.Linear):\n        requires_grad = m.weight.requires_grad\n        m.weight = torch.nn.Parameter(\n                    MyQuantizedLinearWeight(m.weight, 0.5), requires_grad=requires_grad\n                   )\n\nwith torch.device(\"meta\"):\n    m = nn.Linear(3, 5)\n    m.apply(fn)",
      "torch.__future__.set_swap_module_params_on_conversion(True)\nprint(f\"Before: id(weight)={id(m.weight)}, id(bias)={id(m.bias)}\")\nprint(f\"m.state_dict() before load_state_dict():\\n {m.state_dict()}\")\nstate_dict = nn.Linear(3, 5).state_dict()\nprint(f\"state_dict:\\n {state_dict}\")\nm.load_state_dict(state_dict, assign=True)\nprint(f\"After: id(weight)={id(m.weight)}, id(bias)={id(m.bias)}\")\nprint(f\"m.state_dict() after load_state_dict():\\n {m.state_dict()}\")"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/intermediate/ax_multiobjective_nas_tutorial.html",
    "title": "Multi-Objective NAS with Ax\u00b6",
    "code_snippets": [
      "from pathlib import Path\n\nimport torchx\n\nfrom torchx import specs\nfrom torchx.components import utils\n\n\ndef trainer(\n    log_path: str,\n    hidden_size_1: int,\n    hidden_size_2: int,\n    learning_rate: float,\n    epochs: int,\n    dropout: float,\n    batch_size: int,\n    trial_idx: int = -1,\n) -> specs.AppDef:\n\n    # define the log path so we can pass it to the TorchX ``AppDef``\n    if trial_idx >= 0:\n        log_path = Path(log_path).joinpath(str(trial_idx)).absolute().as_posix()\n\n    return utils.python(\n        # command line arguments to the training script\n        \"--log_path\",\n        log_path,\n        \"--hidden_size_1\",\n        str(hidden_size_1),\n        \"--hidden_size_2\",\n        str(hidden_size_2),\n        \"--learning_rate\",\n        str(learning_rate),\n        \"--epochs\",\n        str(epochs),\n        \"--dropout\",\n        str(dropout),\n        \"--batch_size\",\n        str(batch_size),\n        # other config options\n        name=\"trainer\",\n        script=\"mnist_train_nas.py\",\n        image=torchx.version.TORCHX_IMAGE,\n    )",
      "import tempfile\nfrom ax.runners.torchx import TorchXRunner\n\n# Make a temporary dir to log our results into\nlog_dir = tempfile.mkdtemp()\n\nax_runner = TorchXRunner(\n    tracker_base=\"/tmp/\",\n    component=trainer,\n    # NOTE: To launch this job on a cluster instead of locally you can\n    # specify a different scheduler and adjust arguments appropriately.\n    scheduler=\"local_cwd\",\n    component_const_params={\"log_path\": log_dir},\n    cfg={},\n)",
      "from ax.core import (\n    ChoiceParameter,\n    ParameterType,\n    RangeParameter,\n    SearchSpace,\n)\n\nparameters = [\n    # NOTE: In a real-world setting, hidden_size_1 and hidden_size_2\n    # should probably be powers of 2, but in our simple example this\n    # would mean that ``num_params`` can't take on that many values, which\n    # in turn makes the Pareto frontier look pretty weird.\n    RangeParameter(\n        name=\"hidden_size_1\",\n        lower=16,\n        upper=128,\n        parameter_type=ParameterType.INT,\n        log_scale=True,\n    ),\n    RangeParameter(\n        name=\"hidden_size_2\",\n        lower=16,\n        upper=128,\n        parameter_type=ParameterType.INT,\n        log_scale=True,\n    ),\n    RangeParameter(\n        name=\"learning_rate\",\n        lower=1e-4,\n        upper=1e-2,\n        parameter_type=ParameterType.FLOAT,\n        log_scale=True,\n    ),\n    RangeParameter(\n        name=\"epochs\",\n        lower=1,\n        upper=4,\n        parameter_type=ParameterType.INT,\n    ),\n    RangeParameter(\n        name=\"dropout\",\n        lower=0.0,\n        upper=0.5,\n        parameter_type=ParameterType.FLOAT,\n    ),\n    ChoiceParameter(  # NOTE: ``ChoiceParameters`` don't require log-scale\n        name=\"batch_size\",\n        values=[32, 64, 128, 256],\n        parameter_type=ParameterType.INT,\n        is_ordered=True,\n        sort_values=True,\n    ),\n]\n\nsearch_space = SearchSpace(\n    parameters=parameters,\n    # NOTE: In practice, it may make sense to add a constraint\n    # hidden_size_2 <= hidden_size_1\n    parameter_constraints=[],\n)",
      "from ax.metrics.tensorboard import TensorboardMetric\nfrom tensorboard.backend.event_processing import plugin_event_multiplexer as event_multiplexer\n\nclass MyTensorboardMetric(TensorboardMetric):\n\n    # NOTE: We need to tell the new TensorBoard metric how to get the id /\n    # file handle for the TensorBoard logs from a trial. In this case\n    # our convention is to just save a separate file per trial in\n    # the prespecified log dir.\n    def _get_event_multiplexer_for_trial(self, trial):\n        mul = event_multiplexer.EventMultiplexer(max_reload_threads=20)\n        mul.AddRunsFromDirectory(Path(log_dir).joinpath(str(trial.index)).as_posix(), None)\n        mul.Reload()\n\n        return mul\n\n    # This indicates whether the metric is queryable while the trial is\n    # still running. We don't use this in the current tutorial, but Ax\n    # utilizes this to implement trial-level early-stopping functionality.\n    @classmethod\n    def is_available_while_running(cls):\n        return False",
      "from ax.core import MultiObjective, Objective, ObjectiveThreshold\nfrom ax.core.optimization_config import MultiObjectiveOptimizationConfig\n\n\nopt_config = MultiObjectiveOptimizationConfig(\n    objective=MultiObjective(\n        objectives=[\n            Objective(metric=val_acc, minimize=False),\n            Objective(metric=model_num_params, minimize=True),\n        ],\n    ),\n    objective_thresholds=[\n        ObjectiveThreshold(metric=val_acc, bound=0.94, relative=False),\n        ObjectiveThreshold(metric=model_num_params, bound=80_000, relative=False),\n    ],\n)",
      "from ax.core import Experiment\n\nexperiment = Experiment(\n    name=\"torchx_mnist\",\n    search_space=search_space,\n    optimization_config=opt_config,\n    runner=ax_runner,\n)",
      "total_trials = 48  # total evaluation budget\n\nfrom ax.modelbridge.dispatch_utils import choose_generation_strategy\n\ngs = choose_generation_strategy(\n    search_space=experiment.search_space,\n    optimization_config=experiment.optimization_config,\n    num_trials=total_trials,\n  )",
      "from ax.service.scheduler import Scheduler, SchedulerOptions\n\nscheduler = Scheduler(\n    experiment=experiment,\n    generation_strategy=gs,\n    options=SchedulerOptions(\n        total_trials=total_trials, max_pending_trials=4\n    ),\n)",
      "/usr/local/lib/python3.10/dist-packages/ax/modelbridge/cross_validation.py:439: UserWarning:\n\nEncountered exception in computing model fit quality: RandomModelBridge does not support prediction.\n\n[INFO 05-23 19:42:42] Scheduler: Running trials [0]...\n/usr/local/lib/python3.10/dist-packages/ax/modelbridge/cross_validation.py:439: UserWarning:\n\nEncountered exception in computing model fit quality: RandomModelBridge does not support prediction.\n\n[INFO 05-23 19:42:43] Scheduler: Running trials [1]...\n/usr/local/lib/python3.10/dist-packages/ax/modelbridge/cross_validation.py:439: UserWarning:\n\nEncountered exception in computing model fit quality: RandomModelBridge does not support prediction.\n\n[INFO 05-23 19:42:44] Scheduler: Running trials [2]...\n/usr/local/lib/python3.10/dist-packages/ax/modelbridge/cross_validation.py:439: UserWarning:\n\nEncountered exception in computing model fit quality: RandomModelBridge does not support prediction.\n\n[INFO 05-23 19:42:45] Scheduler: Running trials [3]...\n[INFO 05-23 19:42:46] Scheduler: Waiting for completed trials (for 1 sec, currently running trials: 4).\n[INFO 05-23 19:42:47] Scheduler: Waiting for completed trials (for 1.5 sec, currently running trials: 4).\n[INFO 05-23 19:42:49] Scheduler: Waiting for completed trials (for 2 sec, currently running trials: 4).\n[INFO 05-23 19:42:51] Scheduler: Waiting for completed trials (for 3 sec, currently running trials: 4).\n[INFO 05-23 19:42:55] Scheduler: Waiting for completed trials (for 5 sec, currently running trials: 4).\n[INFO 05-23 19:43:00] Scheduler: Waiting for completed trials (for 7 sec, currently running trials: 4).\n[INFO 05-23 19:43:07] Scheduler: Waiting for completed trials (for 11 sec, currently running trials: 4).\n[INFO 05-23 19:43:19] Scheduler: Retrieved COMPLETED trials: [0, 2].\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n/usr/local/lib/python3.10/dist-packages/ax/modelbridge/cross_validation.py:439: UserWarning:\n\nEncountered exception in computing model fit quality: RandomModelBridge does not support prediction.\n\n[INFO 05-23 19:43:19] Scheduler: Running trials [4]...\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n/usr/local/lib/python3.10/dist-packages/ax/modelbridge/cross_validation.py:439: UserWarning:\n\nEncountered exception in computing model fit quality: RandomModelBridge does not support prediction.\n\n[INFO 05-23 19:43:20] Scheduler: Running trials [5]...\n[INFO 05-23 19:43:21] Scheduler: Waiting for completed trials (for 1 sec, currently running trials: 4).\n[INFO 05-23 19:43:22] Scheduler: Waiting for completed trials (for 1.5 sec, currently running trials: 4).\n[INFO 05-23 19:43:23] Scheduler: Waiting for completed trials (for 2 sec, currently running trials: 4).\n[INFO 05-23 19:43:26] Scheduler: Waiting for completed trials (for 3 sec, currently running trials: 4).\n[INFO 05-23 19:43:29] Scheduler: Waiting for completed trials (for 5 sec, currently running trials: 4).\n[INFO 05-23 19:43:34] Scheduler: Waiting for completed trials (for 7 sec, currently running trials: 4).\n[INFO 05-23 19:43:42] Scheduler: Retrieved COMPLETED trials: [1].\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n/usr/local/lib/python3.10/dist-packages/ax/modelbridge/cross_validation.py:439: UserWarning:\n\nEncountered exception in computing model fit quality: RandomModelBridge does not support prediction.\n\n[INFO 05-23 19:43:42] Scheduler: Running trials [6]...\n[INFO 05-23 19:43:43] Scheduler: Waiting for completed trials (for 1 sec, currently running trials: 4).\n[INFO 05-23 19:43:44] Scheduler: Retrieved COMPLETED trials: [5].\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n/usr/local/lib/python3.10/dist-packages/ax/modelbridge/cross_validation.py:439: UserWarning:\n\nEncountered exception in computing model fit quality: RandomModelBridge does not support prediction.\n\n[INFO 05-23 19:43:44] Scheduler: Running trials [7]...\n[INFO 05-23 19:43:45] Scheduler: Retrieved COMPLETED trials: [3].\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n/usr/local/lib/python3.10/dist-packages/ax/modelbridge/cross_validation.py:439: UserWarning:\n\nEncountered exception in computing model fit quality: RandomModelBridge does not support prediction.\n\n[INFO 05-23 19:43:45] Scheduler: Running trials [8]...\n[INFO 05-23 19:43:46] Scheduler: Waiting for completed trials (for 1 sec, currently running trials: 4).\n[INFO 05-23 19:43:47] Scheduler: Waiting for completed trials (for 1.5 sec, currently running trials: 4).\n[INFO 05-23 19:43:48] Scheduler: Waiting for completed trials (for 2 sec, currently running trials: 4).\n[INFO 05-23 19:43:51] Scheduler: Waiting for completed trials (for 3 sec, currently running trials: 4).\n[INFO 05-23 19:43:54] Scheduler: Waiting for completed trials (for 5 sec, currently running trials: 4).\n[INFO 05-23 19:43:59] Scheduler: Waiting for completed trials (for 7 sec, currently running trials: 4).\n[INFO 05-23 19:44:07] Scheduler: Retrieved COMPLETED trials: [8].\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n/usr/local/lib/python3.10/dist-packages/botorch/optim/optimize_mixed.py:702: OptimizationWarning:\n\nFailed to initialize using continuous relaxation. Using `sample_feasible_points` for initialization. Original error message: 1\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n[INFO 05-23 19:44:23] Scheduler: Running trials [9]...\n[INFO 05-23 19:44:24] Scheduler: Retrieved COMPLETED trials: [4, 7].\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n/usr/local/lib/python3.10/dist-packages/botorch/optim/optimize_mixed.py:702: OptimizationWarning:\n\nFailed to initialize using continuous relaxation. Using `sample_feasible_points` for initialization. Original error message: 2\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n[INFO 05-23 19:44:40] Scheduler: Running trials [10]...\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n/usr/local/lib/python3.10/dist-packages/botorch/optim/optimize_mixed.py:702: OptimizationWarning:\n\nFailed to initialize using continuous relaxation. Using `sample_feasible_points` for initialization. Original error message: 2\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n[INFO 05-23 19:44:58] Scheduler: Running trials [11]...\n[INFO 05-23 19:44:59] Scheduler: Retrieved COMPLETED trials: [6].\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n[INFO 05-23 19:45:02] Scheduler: Waiting for completed trials (for 1 sec, currently running trials: 3).\n[INFO 05-23 19:45:03] Scheduler: Waiting for completed trials (for 1.5 sec, currently running trials: 3).\n[INFO 05-23 19:45:05] Scheduler: Waiting for completed trials (for 2 sec, currently running trials: 3).\n[INFO 05-23 19:45:07] Scheduler: Waiting for completed trials (for 3 sec, currently running trials: 3).\n[INFO 05-23 19:45:10] Scheduler: Retrieved COMPLETED trials: [9].\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n/usr/local/lib/python3.10/dist-packages/botorch/optim/optimize_mixed.py:702: OptimizationWarning:\n\nFailed to initialize using continuous relaxation. Using `sample_feasible_points` for initialization. Original error message: 2\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n[INFO 05-23 19:45:21] Scheduler: Running trials [12]...\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n[INFO 05-23 19:45:22] Scheduler: Generated all trials that can be generated currently. Max parallelism currently reached.\n[INFO 05-23 19:45:22] Scheduler: Retrieved COMPLETED trials: [11].\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n/usr/local/lib/python3.10/dist-packages/botorch/optim/optimize_mixed.py:702: OptimizationWarning:\n\nFailed to initialize using continuous relaxation. Using `sample_feasible_points` for initialization. Original error message: 2\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n[INFO 05-23 19:45:40] Scheduler: Running trials [13]...\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n[INFO 05-23 19:45:40] Scheduler: Generated all trials that can be generated currently. Max parallelism currently reached.\n[INFO 05-23 19:45:40] Scheduler: Retrieved COMPLETED trials: [10].\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n/usr/local/lib/python3.10/dist-packages/botorch/optim/optimize_mixed.py:702: OptimizationWarning:\n\nFailed to initialize using continuous relaxation. Using `sample_feasible_points` for initialization. Original error message: 2\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n[INFO 05-23 19:45:51] Scheduler: Running trials [14]...\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n[INFO 05-23 19:45:52] Scheduler: Generated all trials that can be generated currently. Max parallelism currently reached.\n[INFO 05-23 19:45:52] Scheduler: Retrieved COMPLETED trials: [12].\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n/usr/local/lib/python3.10/dist-packages/botorch/optim/optimize_mixed.py:702: OptimizationWarning:\n\nFailed to initialize using continuous relaxation. Using `sample_feasible_points` for initialization. Original error message: 1\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n[INFO 05-23 19:46:10] Scheduler: Running trials [15]...\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n[INFO 05-23 19:46:11] Scheduler: Generated all trials that can be generated currently. Max parallelism currently reached.\n[INFO 05-23 19:46:11] Scheduler: Waiting for completed trials (for 1 sec, currently running trials: 3).\n[INFO 05-23 19:46:12] Scheduler: Waiting for completed trials (for 1.5 sec, currently running trials: 3).\n[INFO 05-23 19:46:14] Scheduler: Waiting for completed trials (for 2 sec, currently running trials: 3).\n[INFO 05-23 19:46:16] Scheduler: Waiting for completed trials (for 3 sec, currently running trials: 3).\n[INFO 05-23 19:46:19] Scheduler: Waiting for completed trials (for 5 sec, currently running trials: 3).\n[INFO 05-23 19:46:24] Scheduler: Waiting for completed trials (for 7 sec, currently running trials: 3).\n[INFO 05-23 19:46:32] Scheduler: Waiting for completed trials (for 11 sec, currently running trials: 3).\n[INFO 05-23 19:46:43] Scheduler: Retrieved COMPLETED trials: [13].\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n/usr/local/lib/python3.10/dist-packages/botorch/optim/optimize_mixed.py:702: OptimizationWarning:\n\nFailed to initialize using continuous relaxation. Using `sample_feasible_points` for initialization. Original error message: 1\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n[INFO 05-23 19:47:00] Scheduler: Running trials [16]...\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n[INFO 05-23 19:47:01] Scheduler: Generated all trials that can be generated currently. Max parallelism currently reached.\n[INFO 05-23 19:47:01] Scheduler: Retrieved COMPLETED trials: [14].\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n/usr/local/lib/python3.10/dist-packages/botorch/optim/optimize_mixed.py:702: OptimizationWarning:\n\nFailed to initialize using continuous relaxation. Using `sample_feasible_points` for initialization. Original error message: 1\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n[INFO 05-23 19:47:21] Scheduler: Running trials [17]...\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n[INFO 05-23 19:47:22] Scheduler: Generated all trials that can be generated currently. Max parallelism currently reached.\n[INFO 05-23 19:47:22] Scheduler: Retrieved COMPLETED trials: [15].\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n/usr/local/lib/python3.10/dist-packages/botorch/optim/optimize_mixed.py:702: OptimizationWarning:\n\nFailed to initialize using continuous relaxation. Using `sample_feasible_points` for initialization. Original error message: 1\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n[INFO 05-23 19:47:42] Scheduler: Running trials [18]...\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n[INFO 05-23 19:47:43] Scheduler: Generated all trials that can be generated currently. Max parallelism currently reached.\n[INFO 05-23 19:47:43] Scheduler: Waiting for completed trials (for 1 sec, currently running trials: 3).\n[INFO 05-23 19:47:44] Scheduler: Waiting for completed trials (for 1.5 sec, currently running trials: 3).\n[INFO 05-23 19:47:45] Scheduler: Waiting for completed trials (for 2 sec, currently running trials: 3).\n[INFO 05-23 19:47:47] Scheduler: Waiting for completed trials (for 3 sec, currently running trials: 3).\n[INFO 05-23 19:47:51] Scheduler: Waiting for completed trials (for 5 sec, currently running trials: 3).\n[INFO 05-23 19:47:56] Scheduler: Waiting for completed trials (for 7 sec, currently running trials: 3).\n[INFO 05-23 19:48:03] Scheduler: Retrieved COMPLETED trials: [16].\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n/usr/local/lib/python3.10/dist-packages/botorch/optim/optimize_mixed.py:702: OptimizationWarning:\n\nFailed to initialize using continuous relaxation. Using `sample_feasible_points` for initialization. Original error message: 2\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n[INFO 05-23 19:48:27] Scheduler: Running trials [19]...\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n[INFO 05-23 19:48:28] Scheduler: Generated all trials that can be generated currently. Max parallelism currently reached.\n[INFO 05-23 19:48:28] Scheduler: Retrieved COMPLETED trials: [17].\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n/usr/local/lib/python3.10/dist-packages/botorch/optim/optimize_mixed.py:702: OptimizationWarning:\n\nFailed to initialize using continuous relaxation. Using `sample_feasible_points` for initialization. Original error message: 1\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n[INFO 05-23 19:48:48] Scheduler: Running trials [20]...\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n[INFO 05-23 19:48:49] Scheduler: Generated all trials that can be generated currently. Max parallelism currently reached.\n[INFO 05-23 19:48:50] Scheduler: Retrieved COMPLETED trials: [18].\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n/usr/local/lib/python3.10/dist-packages/botorch/optim/optimize_mixed.py:702: OptimizationWarning:\n\nFailed to initialize using continuous relaxation. Using `sample_feasible_points` for initialization. Original error message: 1\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n[INFO 05-23 19:49:14] Scheduler: Running trials [21]...\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n[INFO 05-23 19:49:16] Scheduler: Generated all trials that can be generated currently. Max parallelism currently reached.\n[INFO 05-23 19:49:16] Scheduler: Waiting for completed trials (for 1 sec, currently running trials: 3).\n[INFO 05-23 19:49:17] Scheduler: Waiting for completed trials (for 1.5 sec, currently running trials: 3).\n[INFO 05-23 19:49:18] Scheduler: Waiting for completed trials (for 2 sec, currently running trials: 3).\n[INFO 05-23 19:49:20] Scheduler: Waiting for completed trials (for 3 sec, currently running trials: 3).\n[INFO 05-23 19:49:24] Scheduler: Waiting for completed trials (for 5 sec, currently running trials: 3).\n[INFO 05-23 19:49:29] Scheduler: Waiting for completed trials (for 7 sec, currently running trials: 3).\n[INFO 05-23 19:49:36] Scheduler: Retrieved COMPLETED trials: [19].\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n/usr/local/lib/python3.10/dist-packages/botorch/optim/optimize_mixed.py:702: OptimizationWarning:\n\nFailed to initialize using continuous relaxation. Using `sample_feasible_points` for initialization. Original error message: 0\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n[INFO 05-23 19:50:00] Scheduler: Running trials [22]...\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n[INFO 05-23 19:50:00] Scheduler: Generated all trials that can be generated currently. Max parallelism currently reached.\n[INFO 05-23 19:50:00] Scheduler: Retrieved COMPLETED trials: [20].\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n/usr/local/lib/python3.10/dist-packages/botorch/optim/optimize_mixed.py:702: OptimizationWarning:\n\nFailed to initialize using continuous relaxation. Using `sample_feasible_points` for initialization. Original error message: 2\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n[INFO 05-23 19:50:25] Scheduler: Running trials [23]...\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n[INFO 05-23 19:50:26] Scheduler: Generated all trials that can be generated currently. Max parallelism currently reached.\n[INFO 05-23 19:50:26] Scheduler: Retrieved COMPLETED trials: [21].\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n/usr/local/lib/python3.10/dist-packages/botorch/optim/optimize_mixed.py:702: OptimizationWarning:\n\nFailed to initialize using continuous relaxation. Using `sample_feasible_points` for initialization. Original error message: 1\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n[INFO 05-23 19:50:48] Scheduler: Running trials [24]...\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n[INFO 05-23 19:50:49] Scheduler: Generated all trials that can be generated currently. Max parallelism currently reached.\n[INFO 05-23 19:50:49] Scheduler: Waiting for completed trials (for 1 sec, currently running trials: 3).\n[INFO 05-23 19:50:50] Scheduler: Waiting for completed trials (for 1.5 sec, currently running trials: 3).\n[INFO 05-23 19:50:51] Scheduler: Waiting for completed trials (for 2 sec, currently running trials: 3).\n[INFO 05-23 19:50:53] Scheduler: Waiting for completed trials (for 3 sec, currently running trials: 3).\n[INFO 05-23 19:50:57] Scheduler: Waiting for completed trials (for 5 sec, currently running trials: 3).\n[INFO 05-23 19:51:02] Scheduler: Waiting for completed trials (for 7 sec, currently running trials: 3).\n[INFO 05-23 19:51:09] Scheduler: Waiting for completed trials (for 11 sec, currently running trials: 3).\n[INFO 05-23 19:51:21] Scheduler: Waiting for completed trials (for 17 sec, currently running trials: 3).\n[INFO 05-23 19:51:38] Scheduler: Retrieved COMPLETED trials: 22 - 23.\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n/usr/local/lib/python3.10/dist-packages/botorch/optim/optimize_mixed.py:702: OptimizationWarning:\n\nFailed to initialize using continuous relaxation. Using `sample_feasible_points` for initialization. Original error message: 0\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n[INFO 05-23 19:51:56] Scheduler: Running trials [25]...\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n/usr/local/lib/python3.10/dist-packages/botorch/optim/optimize_mixed.py:702: OptimizationWarning:\n\nFailed to initialize using continuous relaxation. Using `sample_feasible_points` for initialization. Original error message: 1\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n[INFO 05-23 19:52:12] Scheduler: Running trials [26]...\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n[INFO 05-23 19:52:13] Scheduler: Generated all trials that can be generated currently. Max parallelism currently reached.\n[INFO 05-23 19:52:13] Scheduler: Retrieved COMPLETED trials: [24].\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n/usr/local/lib/python3.10/dist-packages/botorch/optim/optimize_mixed.py:702: OptimizationWarning:\n\nFailed to initialize using continuous relaxation. Using `sample_feasible_points` for initialization. Original error message: 2\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n[INFO 05-23 19:52:31] Scheduler: Running trials [27]...\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n[INFO 05-23 19:52:32] Scheduler: Generated all trials that can be generated currently. Max parallelism currently reached.\n[INFO 05-23 19:52:32] Scheduler: Waiting for completed trials (for 1 sec, currently running trials: 3).\n[INFO 05-23 19:52:33] Scheduler: Waiting for completed trials (for 1.5 sec, currently running trials: 3).\n[INFO 05-23 19:52:35] Scheduler: Waiting for completed trials (for 2 sec, currently running trials: 3).\n[INFO 05-23 19:52:37] Scheduler: Waiting for completed trials (for 3 sec, currently running trials: 3).\n[INFO 05-23 19:52:40] Scheduler: Waiting for completed trials (for 5 sec, currently running trials: 3).\n[INFO 05-23 19:52:46] Scheduler: Waiting for completed trials (for 7 sec, currently running trials: 3).\n[INFO 05-23 19:52:53] Scheduler: Retrieved COMPLETED trials: [25].\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n/usr/local/lib/python3.10/dist-packages/botorch/optim/optimize_mixed.py:702: OptimizationWarning:\n\nFailed to initialize using continuous relaxation. Using `sample_feasible_points` for initialization. Original error message: 2\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n[INFO 05-23 19:53:15] Scheduler: Running trials [28]...\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n[INFO 05-23 19:53:16] Scheduler: Generated all trials that can be generated currently. Max parallelism currently reached.\n[INFO 05-23 19:53:16] Scheduler: Retrieved COMPLETED trials: [26].\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n[INFO 05-23 19:53:30] Scheduler: Running trials [29]...\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n[INFO 05-23 19:53:31] Scheduler: Generated all trials that can be generated currently. Max parallelism currently reached.\n[INFO 05-23 19:53:31] Scheduler: Retrieved COMPLETED trials: [27].\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n/usr/local/lib/python3.10/dist-packages/botorch/optim/optimize_mixed.py:702: OptimizationWarning:\n\nFailed to initialize using continuous relaxation. Using `sample_feasible_points` for initialization. Original error message: 0\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n[INFO 05-23 19:53:51] Scheduler: Running trials [30]...\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n[INFO 05-23 19:53:52] Scheduler: Generated all trials that can be generated currently. Max parallelism currently reached.\n[INFO 05-23 19:53:52] Scheduler: Waiting for completed trials (for 1 sec, currently running trials: 3).\n[INFO 05-23 19:53:53] Scheduler: Waiting for completed trials (for 1.5 sec, currently running trials: 3).\n[INFO 05-23 19:53:55] Scheduler: Waiting for completed trials (for 2 sec, currently running trials: 3).\n[INFO 05-23 19:53:57] Scheduler: Waiting for completed trials (for 3 sec, currently running trials: 3).\n[INFO 05-23 19:54:01] Scheduler: Waiting for completed trials (for 5 sec, currently running trials: 3).\n[INFO 05-23 19:54:06] Scheduler: Waiting for completed trials (for 7 sec, currently running trials: 3).\n[INFO 05-23 19:54:13] Scheduler: Waiting for completed trials (for 11 sec, currently running trials: 3).\n[INFO 05-23 19:54:25] Scheduler: Retrieved COMPLETED trials: [28].\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n/usr/local/lib/python3.10/dist-packages/botorch/optim/optimize_mixed.py:702: OptimizationWarning:\n\nFailed to initialize using continuous relaxation. Using `sample_feasible_points` for initialization. Original error message: 2\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n[INFO 05-23 19:54:48] Scheduler: Running trials [31]...\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n[INFO 05-23 19:54:48] Scheduler: Generated all trials that can be generated currently. Max parallelism currently reached.\n[INFO 05-23 19:54:48] Scheduler: Retrieved COMPLETED trials: 29 - 30.\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n/usr/local/lib/python3.10/dist-packages/botorch/optim/optimize_mixed.py:702: OptimizationWarning:\n\nFailed to initialize using continuous relaxation. Using `sample_feasible_points` for initialization. Original error message: 2\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n[INFO 05-23 19:55:10] Scheduler: Running trials [32]...\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n/usr/local/lib/python3.10/dist-packages/botorch/optim/optimize_mixed.py:702: OptimizationWarning:\n\nFailed to initialize using continuous relaxation. Using `sample_feasible_points` for initialization. Original error message: 1\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n[INFO 05-23 19:55:35] Scheduler: Running trials [33]...\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n[INFO 05-23 19:55:36] Scheduler: Generated all trials that can be generated currently. Max parallelism currently reached.\n[INFO 05-23 19:55:36] Scheduler: Waiting for completed trials (for 1 sec, currently running trials: 3).\n[INFO 05-23 19:55:37] Scheduler: Waiting for completed trials (for 1.5 sec, currently running trials: 3).\n[INFO 05-23 19:55:38] Scheduler: Waiting for completed trials (for 2 sec, currently running trials: 3).\n[INFO 05-23 19:55:41] Scheduler: Waiting for completed trials (for 3 sec, currently running trials: 3).\n[INFO 05-23 19:55:44] Scheduler: Retrieved COMPLETED trials: [31].\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n/usr/local/lib/python3.10/dist-packages/botorch/optim/optimize_mixed.py:702: OptimizationWarning:\n\nFailed to initialize using continuous relaxation. Using `sample_feasible_points` for initialization. Original error message: 2\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n[INFO 05-23 19:56:09] Scheduler: Running trials [34]...\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n[INFO 05-23 19:56:10] Scheduler: Generated all trials that can be generated currently. Max parallelism currently reached.\n[INFO 05-23 19:56:10] Scheduler: Waiting for completed trials (for 1 sec, currently running trials: 3).\n[INFO 05-23 19:56:11] Scheduler: Waiting for completed trials (for 1.5 sec, currently running trials: 3).\n[INFO 05-23 19:56:12] Scheduler: Waiting for completed trials (for 2 sec, currently running trials: 3).\n[INFO 05-23 19:56:14] Scheduler: Waiting for completed trials (for 3 sec, currently running trials: 3).\n[INFO 05-23 19:56:18] Scheduler: Retrieved COMPLETED trials: [32].\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n/usr/local/lib/python3.10/dist-packages/botorch/optim/optimize_mixed.py:702: OptimizationWarning:\n\nFailed to initialize using continuous relaxation. Using `sample_feasible_points` for initialization. Original error message: 0\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n[INFO 05-23 19:56:39] Scheduler: Running trials [35]...\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n[INFO 05-23 19:56:40] Scheduler: Generated all trials that can be generated currently. Max parallelism currently reached.\n[INFO 05-23 19:56:41] Scheduler: Retrieved COMPLETED trials: [33].\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n/usr/local/lib/python3.10/dist-packages/botorch/optim/optimize_mixed.py:702: OptimizationWarning:\n\nFailed to initialize using continuous relaxation. Using `sample_feasible_points` for initialization. Original error message: 1\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n[INFO 05-23 19:57:06] Scheduler: Running trials [36]...\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n[INFO 05-23 19:57:06] Scheduler: Generated all trials that can be generated currently. Max parallelism currently reached.\n[INFO 05-23 19:57:06] Scheduler: Waiting for completed trials (for 1 sec, currently running trials: 3).\n[INFO 05-23 19:57:07] Scheduler: Waiting for completed trials (for 1.5 sec, currently running trials: 3).\n[INFO 05-23 19:57:09] Scheduler: Retrieved COMPLETED trials: [34].\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n/usr/local/lib/python3.10/dist-packages/botorch/optim/optimize_mixed.py:702: OptimizationWarning:\n\nFailed to initialize using continuous relaxation. Using `sample_feasible_points` for initialization. Original error message: 2\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n[INFO 05-23 19:57:35] Scheduler: Running trials [37]...\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n[INFO 05-23 19:57:36] Scheduler: Generated all trials that can be generated currently. Max parallelism currently reached.\n[INFO 05-23 19:57:36] Scheduler: Waiting for completed trials (for 1 sec, currently running trials: 3).\n[INFO 05-23 19:57:37] Scheduler: Waiting for completed trials (for 1.5 sec, currently running trials: 3).\n[INFO 05-23 19:57:39] Scheduler: Retrieved COMPLETED trials: [35].\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n/usr/local/lib/python3.10/dist-packages/botorch/optim/optimize_mixed.py:702: OptimizationWarning:\n\nFailed to initialize using continuous relaxation. Using `sample_feasible_points` for initialization. Original error message: 0\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n[INFO 05-23 19:58:04] Scheduler: Running trials [38]...\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n[INFO 05-23 19:58:04] Scheduler: Generated all trials that can be generated currently. Max parallelism currently reached.\n[INFO 05-23 19:58:04] Scheduler: Waiting for completed trials (for 1 sec, currently running trials: 3).\n[INFO 05-23 19:58:05] Scheduler: Waiting for completed trials (for 1.5 sec, currently running trials: 3).\n[INFO 05-23 19:58:06] Scheduler: Waiting for completed trials (for 2 sec, currently running trials: 3).\n[INFO 05-23 19:58:09] Scheduler: Waiting for completed trials (for 3 sec, currently running trials: 3).\n[INFO 05-23 19:58:12] Scheduler: Waiting for completed trials (for 5 sec, currently running trials: 3).\n[INFO 05-23 19:58:17] Scheduler: Waiting for completed trials (for 7 sec, currently running trials: 3).\n[INFO 05-23 19:58:25] Scheduler: Waiting for completed trials (for 11 sec, currently running trials: 3).\n[INFO 05-23 19:58:36] Scheduler: Retrieved COMPLETED trials: 36 - 37.\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n/usr/local/lib/python3.10/dist-packages/botorch/optim/optimize_mixed.py:702: OptimizationWarning:\n\nFailed to initialize using continuous relaxation. Using `sample_feasible_points` for initialization. Original error message: 2\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n[INFO 05-23 19:59:01] Scheduler: Running trials [39]...\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n/usr/local/lib/python3.10/dist-packages/botorch/optim/optimize_mixed.py:702: OptimizationWarning:\n\nFailed to initialize using continuous relaxation. Using `sample_feasible_points` for initialization. Original error message: 2\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n[INFO 05-23 19:59:18] Scheduler: Running trials [40]...\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n[INFO 05-23 19:59:19] Scheduler: Generated all trials that can be generated currently. Max parallelism currently reached.\n[INFO 05-23 19:59:19] Scheduler: Retrieved COMPLETED trials: [38].\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n/usr/local/lib/python3.10/dist-packages/botorch/optim/optimize_mixed.py:702: OptimizationWarning:\n\nFailed to initialize using continuous relaxation. Using `sample_feasible_points` for initialization. Original error message: 2\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n[INFO 05-23 19:59:45] Scheduler: Running trials [41]...\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n[INFO 05-23 19:59:46] Scheduler: Generated all trials that can be generated currently. Max parallelism currently reached.\n[INFO 05-23 19:59:46] Scheduler: Waiting for completed trials (for 1 sec, currently running trials: 3).\n[INFO 05-23 19:59:47] Scheduler: Waiting for completed trials (for 1.5 sec, currently running trials: 3).\n[INFO 05-23 19:59:48] Scheduler: Waiting for completed trials (for 2 sec, currently running trials: 3).\n[INFO 05-23 19:59:50] Scheduler: Waiting for completed trials (for 3 sec, currently running trials: 3).\n[INFO 05-23 19:59:54] Scheduler: Waiting for completed trials (for 5 sec, currently running trials: 3).\n[INFO 05-23 19:59:59] Scheduler: Retrieved COMPLETED trials: [39].\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n/usr/local/lib/python3.10/dist-packages/botorch/optim/optimize_mixed.py:702: OptimizationWarning:\n\nFailed to initialize using continuous relaxation. Using `sample_feasible_points` for initialization. Original error message: 2\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n[INFO 05-23 20:00:25] Scheduler: Running trials [42]...\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n[INFO 05-23 20:00:26] Scheduler: Generated all trials that can be generated currently. Max parallelism currently reached.\n[INFO 05-23 20:00:26] Scheduler: Retrieved COMPLETED trials: [40].\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n/usr/local/lib/python3.10/dist-packages/botorch/optim/optimize_mixed.py:702: OptimizationWarning:\n\nFailed to initialize using continuous relaxation. Using `sample_feasible_points` for initialization. Original error message: 2\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n[INFO 05-23 20:00:50] Scheduler: Running trials [43]...\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n[INFO 05-23 20:00:51] Scheduler: Generated all trials that can be generated currently. Max parallelism currently reached.\n[INFO 05-23 20:00:51] Scheduler: Retrieved COMPLETED trials: [41].\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n/usr/local/lib/python3.10/dist-packages/botorch/optim/optimize_mixed.py:702: OptimizationWarning:\n\nFailed to initialize using continuous relaxation. Using `sample_feasible_points` for initialization. Original error message: 2\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n[INFO 05-23 20:01:18] Scheduler: Running trials [44]...\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n[INFO 05-23 20:01:19] Scheduler: Generated all trials that can be generated currently. Max parallelism currently reached.\n[INFO 05-23 20:01:19] Scheduler: Waiting for completed trials (for 1 sec, currently running trials: 3).\n[INFO 05-23 20:01:20] Scheduler: Waiting for completed trials (for 1.5 sec, currently running trials: 3).\n[INFO 05-23 20:01:22] Scheduler: Waiting for completed trials (for 2 sec, currently running trials: 3).\n[INFO 05-23 20:01:24] Scheduler: Waiting for completed trials (for 3 sec, currently running trials: 3).\n[INFO 05-23 20:01:27] Scheduler: Retrieved COMPLETED trials: [42].\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n/usr/local/lib/python3.10/dist-packages/botorch/optim/optimize_mixed.py:702: OptimizationWarning:\n\nFailed to initialize using continuous relaxation. Using `sample_feasible_points` for initialization. Original error message: 2\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n[INFO 05-23 20:01:56] Scheduler: Running trials [45]...\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n[INFO 05-23 20:01:57] Scheduler: Generated all trials that can be generated currently. Max parallelism currently reached.\n[INFO 05-23 20:01:57] Scheduler: Retrieved COMPLETED trials: [43].\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n/usr/local/lib/python3.10/dist-packages/botorch/optim/optimize_mixed.py:702: OptimizationWarning:\n\nFailed to initialize using continuous relaxation. Using `sample_feasible_points` for initialization. Original error message: 2\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n[INFO 05-23 20:02:27] Scheduler: Running trials [46]...\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n[INFO 05-23 20:02:28] Scheduler: Generated all trials that can be generated currently. Max parallelism currently reached.\n[INFO 05-23 20:02:28] Scheduler: Retrieved COMPLETED trials: [44].\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n/usr/local/lib/python3.10/dist-packages/botorch/optim/optimize_mixed.py:702: OptimizationWarning:\n\nFailed to initialize using continuous relaxation. Using `sample_feasible_points` for initialization. Original error message: 2\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n/usr/lib/python3.10/dataclasses.py:1453: RuntimeWarning:\n\nIf using a 2-dim `batch_initial_conditions` botorch will default to old behavior of ignoring `num_restarts` and just use the given `batch_initial_conditions` by setting `raw_samples` to None.\n\n[INFO 05-23 20:02:55] Scheduler: Running trials [47]...\n[INFO 05-23 20:02:56] Scheduler: Waiting for completed trials (for 1 sec, currently running trials: 3).\n[INFO 05-23 20:02:57] Scheduler: Waiting for completed trials (for 1.5 sec, currently running trials: 3).\n[INFO 05-23 20:02:58] Scheduler: Retrieved COMPLETED trials: [45].\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n[INFO 05-23 20:02:58] Scheduler: Done submitting trials, waiting for remaining 2 running trials...\n[INFO 05-23 20:02:58] Scheduler: Waiting for completed trials (for 1 sec, currently running trials: 2).\n[INFO 05-23 20:02:59] Scheduler: Waiting for completed trials (for 1.5 sec, currently running trials: 2).\n[INFO 05-23 20:03:01] Scheduler: Waiting for completed trials (for 2 sec, currently running trials: 2).\n[INFO 05-23 20:03:03] Scheduler: Waiting for completed trials (for 3 sec, currently running trials: 2).\n[INFO 05-23 20:03:06] Scheduler: Waiting for completed trials (for 5 sec, currently running trials: 2).\n[INFO 05-23 20:03:12] Scheduler: Waiting for completed trials (for 7 sec, currently running trials: 2).\n[INFO 05-23 20:03:19] Scheduler: Waiting for completed trials (for 11 sec, currently running trials: 2).\n[INFO 05-23 20:03:31] Scheduler: Retrieved COMPLETED trials: [46].\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n[INFO 05-23 20:03:31] Scheduler: Waiting for completed trials (for 1 sec, currently running trials: 1).\n[INFO 05-23 20:03:32] Scheduler: Waiting for completed trials (for 1.5 sec, currently running trials: 1).\n[INFO 05-23 20:03:33] Scheduler: Waiting for completed trials (for 2 sec, currently running trials: 1).\n[INFO 05-23 20:03:35] Scheduler: Waiting for completed trials (for 3 sec, currently running trials: 1).\n[INFO 05-23 20:03:39] Scheduler: Waiting for completed trials (for 5 sec, currently running trials: 1).\n[INFO 05-23 20:03:44] Scheduler: Waiting for completed trials (for 7 sec, currently running trials: 1).\n[INFO 05-23 20:03:51] Scheduler: Waiting for completed trials (for 11 sec, currently running trials: 1).\n[INFO 05-23 20:04:03] Scheduler: Retrieved COMPLETED trials: [47].\n/usr/local/lib/python3.10/dist-packages/ax/core/map_data.py:216: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n\nOptimizationResult()",
      "from ax.service.utils.report_utils import exp_to_df\n\ndf = exp_to_df(experiment)\ndf.head(10)",
      "from ax.service.utils.report_utils import _pareto_frontier_scatter_2d_plotly\n\n_pareto_frontier_scatter_2d_plotly(experiment)",
      "from ax.modelbridge.cross_validation import compute_diagnostics, cross_validate\nfrom ax.plot.diagnostic import interact_cross_validation_plotly\nfrom ax.utils.notebook.plotting import init_notebook_plotting, render\n\ncv = cross_validate(model=gs.model)  # The surrogate model is stored on the ``GenerationStrategy``\ncompute_diagnostics(cv)\n\ninteract_cross_validation_plotly(cv)",
      "from ax.plot.contour import interact_contour_plotly\n\ninteract_contour_plotly(model=gs.model, metric_name=\"val_acc\")"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/recipes/recipes/defining_a_neural_network.html",
    "title": "Defining a Neural Network in PyTorch\u00b6",
    "code_snippets": [
      "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F",
      "class Net(nn.Module):\n    def __init__(self):\n      super(Net, self).__init__()\n\n      # First 2D convolutional layer, taking in 1 input channel (image),\n      # outputting 32 convolutional features, with a square kernel size of 3\n      self.conv1 = nn.Conv2d(1, 32, 3, 1)\n      # Second 2D convolutional layer, taking in the 32 input layers,\n      # outputting 64 convolutional features, with a square kernel size of 3\n      self.conv2 = nn.Conv2d(32, 64, 3, 1)\n\n      # Designed to ensure that adjacent pixels are either all 0s or all active\n      # with an input probability\n      self.dropout1 = nn.Dropout2d(0.25)\n      self.dropout2 = nn.Dropout2d(0.5)\n\n      # First fully connected layer\n      self.fc1 = nn.Linear(9216, 128)\n      # Second fully connected layer that outputs our 10 labels\n      self.fc2 = nn.Linear(128, 10)\n\nmy_nn = Net()\nprint(my_nn)",
      "class Net(nn.Module):\n    def __init__(self):\n      super(Net, self).__init__()\n      self.conv1 = nn.Conv2d(1, 32, 3, 1)\n      self.conv2 = nn.Conv2d(32, 64, 3, 1)\n      self.dropout1 = nn.Dropout2d(0.25)\n      self.dropout2 = nn.Dropout2d(0.5)\n      self.fc1 = nn.Linear(9216, 128)\n      self.fc2 = nn.Linear(128, 10)\n\n    # x represents our data\n    def forward(self, x):\n      # Pass data through conv1\n      x = self.conv1(x)\n      # Use the rectified-linear activation function over x\n      x = F.relu(x)\n\n      x = self.conv2(x)\n      x = F.relu(x)\n\n      # Run max pooling over x\n      x = F.max_pool2d(x, 2)\n      # Pass data through dropout1\n      x = self.dropout1(x)\n      # Flatten x with start_dim=1\n      x = torch.flatten(x, 1)\n      # Pass data through ``fc1``\n      x = self.fc1(x)\n      x = F.relu(x)\n      x = self.dropout2(x)\n      x = self.fc2(x)\n\n      # Apply softmax to x\n      output = F.log_softmax(x, dim=1)\n      return output",
      "# Equates to one random 28x28 image\nrandom_data = torch.rand((1, 1, 28, 28))\n\nmy_nn = Net()\nresult = my_nn(random_data)\nprint (result)"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/advanced/pendulum.html",
    "title": "Pendulum: Writing your environment and transforms with TorchRL\u00b6",
    "code_snippets": [
      "from collections import defaultdict\nfrom typing import Optional\n\nimport numpy as np\nimport torch\nimport tqdm\nfrom tensordict import TensorDict, TensorDictBase\nfrom tensordict.nn import TensorDictModule\nfrom torch import nn\n\nfrom torchrl.data import BoundedTensorSpec, CompositeSpec, UnboundedContinuousTensorSpec\nfrom torchrl.envs import (\n    CatTensors,\n    EnvBase,\n    Transform,\n    TransformedEnv,\n    UnsqueezeTransform,\n)\nfrom torchrl.envs.transforms.transforms import _apply_to_composite\nfrom torchrl.envs.utils import check_env_specs, step_mdp\n\nDEFAULT_X = np.pi\nDEFAULT_Y = 1.0",
      ">>> policy(env.reset())\n>>> print(tensordict)\nTensorDict(\n    fields={\n        action: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, is_shared=False),\n        done: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n        observation: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},\n    batch_size=torch.Size([]),\n    device=cpu,\n    is_shared=False)\n>>> env.step(tensordict)\n>>> print(tensordict)\nTensorDict(\n    fields={\n        action: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, is_shared=False),\n        done: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n        next: TensorDict(\n            fields={\n                done: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n                observation: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),\n                reward: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, is_shared=False)},\n            batch_size=torch.Size([]),\n            device=cpu,\n            is_shared=False),\n        observation: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},\n    batch_size=torch.Size([]),\n    device=cpu,\n    is_shared=False)",
      "def _step(tensordict):\n    th, thdot = tensordict[\"th\"], tensordict[\"thdot\"]  # th := theta\n\n    g_force = tensordict[\"params\", \"g\"]\n    mass = tensordict[\"params\", \"m\"]\n    length = tensordict[\"params\", \"l\"]\n    dt = tensordict[\"params\", \"dt\"]\n    u = tensordict[\"action\"].squeeze(-1)\n    u = u.clamp(-tensordict[\"params\", \"max_torque\"], tensordict[\"params\", \"max_torque\"])\n    costs = angle_normalize(th) ** 2 + 0.1 * thdot**2 + 0.001 * (u**2)\n\n    new_thdot = (\n        thdot\n        + (3 * g_force / (2 * length) * th.sin() + 3.0 / (mass * length**2) * u) * dt\n    )\n    new_thdot = new_thdot.clamp(\n        -tensordict[\"params\", \"max_speed\"], tensordict[\"params\", \"max_speed\"]\n    )\n    new_th = th + new_thdot * dt\n    reward = -costs.view(*tensordict.shape, 1)\n    done = torch.zeros_like(reward, dtype=torch.bool)\n    out = TensorDict(\n        {\n            \"th\": new_th,\n            \"thdot\": new_thdot,\n            \"params\": tensordict[\"params\"],\n            \"reward\": reward,\n            \"done\": done,\n        },\n        tensordict.shape,\n    )\n    return out\n\n\ndef angle_normalize(x):\n    return ((x + torch.pi) % (2 * torch.pi)) - torch.pi",
      "def _reset(self, tensordict):\n    if tensordict is None or tensordict.is_empty():\n        # if no ``tensordict`` is passed, we generate a single set of hyperparameters\n        # Otherwise, we assume that the input ``tensordict`` contains all the relevant\n        # parameters to get started.\n        tensordict = self.gen_params(batch_size=self.batch_size)\n\n    high_th = torch.tensor(DEFAULT_X, device=self.device)\n    high_thdot = torch.tensor(DEFAULT_Y, device=self.device)\n    low_th = -high_th\n    low_thdot = -high_thdot\n\n    # for non batch-locked environments, the input ``tensordict`` shape dictates the number\n    # of simulators run simultaneously. In other contexts, the initial\n    # random state's shape will depend upon the environment batch-size instead.\n    th = (\n        torch.rand(tensordict.shape, generator=self.rng, device=self.device)\n        * (high_th - low_th)\n        + low_th\n    )\n    thdot = (\n        torch.rand(tensordict.shape, generator=self.rng, device=self.device)\n        * (high_thdot - low_thdot)\n        + low_thdot\n    )\n    out = TensorDict(\n        {\n            \"th\": th,\n            \"thdot\": thdot,\n            \"params\": tensordict[\"params\"],\n        },\n        batch_size=tensordict.shape,\n    )\n    return out",
      "def _make_spec(self, td_params):\n    # Under the hood, this will populate self.output_spec[\"observation\"]\n    self.observation_spec = CompositeSpec(\n        th=BoundedTensorSpec(\n            low=-torch.pi,\n            high=torch.pi,\n            shape=(),\n            dtype=torch.float32,\n        ),\n        thdot=BoundedTensorSpec(\n            low=-td_params[\"params\", \"max_speed\"],\n            high=td_params[\"params\", \"max_speed\"],\n            shape=(),\n            dtype=torch.float32,\n        ),\n        # we need to add the ``params`` to the observation specs, as we want\n        # to pass it at each step during a rollout\n        params=make_composite_from_td(td_params[\"params\"]),\n        shape=(),\n    )\n    # since the environment is stateless, we expect the previous output as input.\n    # For this, ``EnvBase`` expects some state_spec to be available\n    self.state_spec = self.observation_spec.clone()\n    # action-spec will be automatically wrapped in input_spec when\n    # `self.action_spec = spec` will be called supported\n    self.action_spec = BoundedTensorSpec(\n        low=-td_params[\"params\", \"max_torque\"],\n        high=td_params[\"params\", \"max_torque\"],\n        shape=(1,),\n        dtype=torch.float32,\n    )\n    self.reward_spec = UnboundedContinuousTensorSpec(shape=(*td_params.shape, 1))\n\n\ndef make_composite_from_td(td):\n    # custom function to convert a ``tensordict`` in a similar spec structure\n    # of unbounded values.\n    composite = CompositeSpec(\n        {\n            key: make_composite_from_td(tensor)\n            if isinstance(tensor, TensorDictBase)\n            else UnboundedContinuousTensorSpec(\n                dtype=tensor.dtype, device=tensor.device, shape=tensor.shape\n            )\n            for key, tensor in td.items()\n        },\n        shape=td.shape,\n    )\n    return composite",
      "def _set_seed(self, seed: Optional[int]):\n    rng = torch.manual_seed(seed)\n    self.rng = rng",
      "def gen_params(g=10.0, batch_size=None) -> TensorDictBase:\n    \"\"\"Returns a ``tensordict`` containing the physical parameters such as gravitational force and torque or speed limits.\"\"\"\n    if batch_size is None:\n        batch_size = []\n    td = TensorDict(\n        {\n            \"params\": TensorDict(\n                {\n                    \"max_speed\": 8,\n                    \"max_torque\": 2.0,\n                    \"dt\": 0.05,\n                    \"g\": g,\n                    \"m\": 1.0,\n                    \"l\": 1.0,\n                },\n                [],\n            )\n        },\n        [],\n    )\n    if batch_size:\n        td = td.expand(batch_size).contiguous()\n    return td",
      "class PendulumEnv(EnvBase):\n    metadata = {\n        \"render_modes\": [\"human\", \"rgb_array\"],\n        \"render_fps\": 30,\n    }\n    batch_locked = False\n\n    def __init__(self, td_params=None, seed=None, device=\"cpu\"):\n        if td_params is None:\n            td_params = self.gen_params()\n\n        super().__init__(device=device, batch_size=[])\n        self._make_spec(td_params)\n        if seed is None:\n            seed = torch.empty((), dtype=torch.int64).random_().item()\n        self.set_seed(seed)\n\n    # Helpers: _make_step and gen_params\n    gen_params = staticmethod(gen_params)\n    _make_spec = _make_spec\n\n    # Mandatory methods: _step, _reset and _set_seed\n    _reset = _reset\n    _step = staticmethod(_step)\n    _set_seed = _set_seed",
      "observation_spec: Composite(\n    th: BoundedContinuous(\n        shape=torch.Size([]),\n        space=ContinuousBox(\n            low=Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, contiguous=True),\n            high=Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, contiguous=True)),\n        device=cpu,\n        dtype=torch.float32,\n        domain=continuous),\n    thdot: BoundedContinuous(\n        shape=torch.Size([]),\n        space=ContinuousBox(\n            low=Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, contiguous=True),\n            high=Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, contiguous=True)),\n        device=cpu,\n        dtype=torch.float32,\n        domain=continuous),\n    params: Composite(\n        max_speed: UnboundedDiscrete(\n            shape=torch.Size([]),\n            space=ContinuousBox(\n                low=Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, contiguous=True),\n                high=Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, contiguous=True)),\n            device=cpu,\n            dtype=torch.int64,\n            domain=discrete),\n        max_torque: UnboundedContinuous(\n            shape=torch.Size([]),\n            space=ContinuousBox(\n                low=Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, contiguous=True),\n                high=Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, contiguous=True)),\n            device=cpu,\n            dtype=torch.float32,\n            domain=continuous),\n        dt: UnboundedContinuous(\n            shape=torch.Size([]),\n            space=ContinuousBox(\n                low=Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, contiguous=True),\n                high=Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, contiguous=True)),\n            device=cpu,\n            dtype=torch.float32,\n            domain=continuous),\n        g: UnboundedContinuous(\n            shape=torch.Size([]),\n            space=ContinuousBox(\n                low=Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, contiguous=True),\n                high=Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, contiguous=True)),\n            device=cpu,\n            dtype=torch.float32,\n            domain=continuous),\n        m: UnboundedContinuous(\n            shape=torch.Size([]),\n            space=ContinuousBox(\n                low=Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, contiguous=True),\n                high=Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, contiguous=True)),\n            device=cpu,\n            dtype=torch.float32,\n            domain=continuous),\n        l: UnboundedContinuous(\n            shape=torch.Size([]),\n            space=ContinuousBox(\n                low=Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, contiguous=True),\n                high=Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, contiguous=True)),\n            device=cpu,\n            dtype=torch.float32,\n            domain=continuous),\n        device=cpu,\n        shape=torch.Size([])),\n    device=cpu,\n    shape=torch.Size([]))\nstate_spec: Composite(\n    th: BoundedContinuous(\n        shape=torch.Size([]),\n        space=ContinuousBox(\n            low=Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, contiguous=True),\n            high=Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, contiguous=True)),\n        device=cpu,\n        dtype=torch.float32,\n        domain=continuous),\n    thdot: BoundedContinuous(\n        shape=torch.Size([]),\n        space=ContinuousBox(\n            low=Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, contiguous=True),\n            high=Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, contiguous=True)),\n        device=cpu,\n        dtype=torch.float32,\n        domain=continuous),\n    params: Composite(\n        max_speed: UnboundedDiscrete(\n            shape=torch.Size([]),\n            space=ContinuousBox(\n                low=Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, contiguous=True),\n                high=Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, contiguous=True)),\n            device=cpu,\n            dtype=torch.int64,\n            domain=discrete),\n        max_torque: UnboundedContinuous(\n            shape=torch.Size([]),\n            space=ContinuousBox(\n                low=Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, contiguous=True),\n                high=Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, contiguous=True)),\n            device=cpu,\n            dtype=torch.float32,\n            domain=continuous),\n        dt: UnboundedContinuous(\n            shape=torch.Size([]),\n            space=ContinuousBox(\n                low=Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, contiguous=True),\n                high=Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, contiguous=True)),\n            device=cpu,\n            dtype=torch.float32,\n            domain=continuous),\n        g: UnboundedContinuous(\n            shape=torch.Size([]),\n            space=ContinuousBox(\n                low=Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, contiguous=True),\n                high=Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, contiguous=True)),\n            device=cpu,\n            dtype=torch.float32,\n            domain=continuous),\n        m: UnboundedContinuous(\n            shape=torch.Size([]),\n            space=ContinuousBox(\n                low=Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, contiguous=True),\n                high=Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, contiguous=True)),\n            device=cpu,\n            dtype=torch.float32,\n            domain=continuous),\n        l: UnboundedContinuous(\n            shape=torch.Size([]),\n            space=ContinuousBox(\n                low=Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, contiguous=True),\n                high=Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, contiguous=True)),\n            device=cpu,\n            dtype=torch.float32,\n            domain=continuous),\n        device=cpu,\n        shape=torch.Size([])),\n    device=cpu,\n    shape=torch.Size([]))\nreward_spec: UnboundedContinuous(\n    shape=torch.Size([1]),\n    space=ContinuousBox(\n        low=Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, contiguous=True),\n        high=Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, contiguous=True)),\n    device=cpu,\n    dtype=torch.float32,\n    domain=continuous)",
      "reset tensordict TensorDict(\n    fields={\n        done: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n        params: TensorDict(\n            fields={\n                dt: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),\n                g: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),\n                l: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),\n                m: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),\n                max_speed: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, is_shared=False),\n                max_torque: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},\n            batch_size=torch.Size([]),\n            device=None,\n            is_shared=False),\n        terminated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n        th: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),\n        thdot: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},\n    batch_size=torch.Size([]),\n    device=None,\n    is_shared=False)",
      "random step tensordict TensorDict(\n    fields={\n        action: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, is_shared=False),\n        done: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n        next: TensorDict(\n            fields={\n                done: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n                params: TensorDict(\n                    fields={\n                        dt: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),\n                        g: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),\n                        l: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),\n                        m: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),\n                        max_speed: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, is_shared=False),\n                        max_torque: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},\n                    batch_size=torch.Size([]),\n                    device=None,\n                    is_shared=False),\n                reward: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, is_shared=False),\n                terminated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n                th: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),\n                thdot: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},\n            batch_size=torch.Size([]),\n            device=None,\n            is_shared=False),\n        params: TensorDict(\n            fields={\n                dt: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),\n                g: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),\n                l: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),\n                m: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),\n                max_speed: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, is_shared=False),\n                max_torque: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},\n            batch_size=torch.Size([]),\n            device=None,\n            is_shared=False),\n        terminated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n        th: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),\n        thdot: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},\n    batch_size=torch.Size([]),\n    device=None,\n    is_shared=False)",
      "class Transform(nn.Module):\n    def forward(self, tensordict):\n        ...\n    def _apply_transform(self, tensordict):\n        ...\n    def _step(self, tensordict):\n        ...\n    def _call(self, tensordict):\n        ...\n    def inv(self, tensordict):\n        ...\n    def _inv_apply_transform(self, tensordict):\n        ...",
      "class SinTransform(Transform):\n    def _apply_transform(self, obs: torch.Tensor) -> None:\n        return obs.sin()\n\n    # The transform must also modify the data at reset time\n    def _reset(\n        self, tensordict: TensorDictBase, tensordict_reset: TensorDictBase\n    ) -> TensorDictBase:\n        return self._call(tensordict_reset)\n\n    # _apply_to_composite will execute the observation spec transform across all\n    # in_keys/out_keys pairs and write the result in the observation_spec which\n    # is of type ``Composite``\n    @_apply_to_composite\n    def transform_observation_spec(self, observation_spec):\n        return BoundedTensorSpec(\n            low=-1,\n            high=1,\n            shape=observation_spec.shape,\n            dtype=observation_spec.dtype,\n            device=observation_spec.device,\n        )\n\n\nclass CosTransform(Transform):\n    def _apply_transform(self, obs: torch.Tensor) -> None:\n        return obs.cos()\n\n    # The transform must also modify the data at reset time\n    def _reset(\n        self, tensordict: TensorDictBase, tensordict_reset: TensorDictBase\n    ) -> TensorDictBase:\n        return self._call(tensordict_reset)\n\n    # _apply_to_composite will execute the observation spec transform across all\n    # in_keys/out_keys pairs and write the result in the observation_spec which\n    # is of type ``Composite``\n    @_apply_to_composite\n    def transform_observation_spec(self, observation_spec):\n        return BoundedTensorSpec(\n            low=-1,\n            high=1,\n            shape=observation_spec.shape,\n            dtype=observation_spec.dtype,\n            device=observation_spec.device,\n        )\n\n\nt_sin = SinTransform(in_keys=[\"th\"], out_keys=[\"sin\"])\nt_cos = CosTransform(in_keys=[\"th\"], out_keys=[\"cos\"])\nenv.append_transform(t_sin)\nenv.append_transform(t_cos)",
      "def simple_rollout(steps=100):\n    # preallocate:\n    data = TensorDict({}, [steps])\n    # reset\n    _data = env.reset()\n    for i in range(steps):\n        _data[\"action\"] = env.action_spec.rand()\n        _data = env.step(_data)\n        data[i] = _data\n        _data = step_mdp(_data, keep_other=True)\n    return data\n\n\nprint(\"data from rollout:\", simple_rollout(100))",
      "data from rollout: TensorDict(\n    fields={\n        action: Tensor(shape=torch.Size([100, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n        cos: Tensor(shape=torch.Size([100, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n        done: Tensor(shape=torch.Size([100, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n        next: TensorDict(\n            fields={\n                cos: Tensor(shape=torch.Size([100, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n                done: Tensor(shape=torch.Size([100, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n                observation: Tensor(shape=torch.Size([100, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n                params: TensorDict(\n                    fields={\n                        dt: Tensor(shape=torch.Size([100]), device=cpu, dtype=torch.float32, is_shared=False),\n                        g: Tensor(shape=torch.Size([100]), device=cpu, dtype=torch.float32, is_shared=False),\n                        l: Tensor(shape=torch.Size([100]), device=cpu, dtype=torch.float32, is_shared=False),\n                        m: Tensor(shape=torch.Size([100]), device=cpu, dtype=torch.float32, is_shared=False),\n                        max_speed: Tensor(shape=torch.Size([100]), device=cpu, dtype=torch.int64, is_shared=False),\n                        max_torque: Tensor(shape=torch.Size([100]), device=cpu, dtype=torch.float32, is_shared=False)},\n                    batch_size=torch.Size([100]),\n                    device=None,\n                    is_shared=False),\n                reward: Tensor(shape=torch.Size([100, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n                sin: Tensor(shape=torch.Size([100, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n                terminated: Tensor(shape=torch.Size([100, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n                th: Tensor(shape=torch.Size([100, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n                thdot: Tensor(shape=torch.Size([100, 1]), device=cpu, dtype=torch.float32, is_shared=False)},\n            batch_size=torch.Size([100]),\n            device=None,\n            is_shared=False),\n        observation: Tensor(shape=torch.Size([100, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n        params: TensorDict(\n            fields={\n                dt: Tensor(shape=torch.Size([100]), device=cpu, dtype=torch.float32, is_shared=False),\n                g: Tensor(shape=torch.Size([100]), device=cpu, dtype=torch.float32, is_shared=False),\n                l: Tensor(shape=torch.Size([100]), device=cpu, dtype=torch.float32, is_shared=False),\n                m: Tensor(shape=torch.Size([100]), device=cpu, dtype=torch.float32, is_shared=False),\n                max_speed: Tensor(shape=torch.Size([100]), device=cpu, dtype=torch.int64, is_shared=False),\n                max_torque: Tensor(shape=torch.Size([100]), device=cpu, dtype=torch.float32, is_shared=False)},\n            batch_size=torch.Size([100]),\n            device=None,\n            is_shared=False),\n        sin: Tensor(shape=torch.Size([100, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n        terminated: Tensor(shape=torch.Size([100, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n        th: Tensor(shape=torch.Size([100, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n        thdot: Tensor(shape=torch.Size([100, 1]), device=cpu, dtype=torch.float32, is_shared=False)},\n    batch_size=torch.Size([100]),\n    device=None,\n    is_shared=False)",
      "reset (batch size of 10) TensorDict(\n    fields={\n        cos: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n        done: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n        observation: Tensor(shape=torch.Size([10, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n        params: TensorDict(\n            fields={\n                dt: Tensor(shape=torch.Size([10]), device=cpu, dtype=torch.float32, is_shared=False),\n                g: Tensor(shape=torch.Size([10]), device=cpu, dtype=torch.float32, is_shared=False),\n                l: Tensor(shape=torch.Size([10]), device=cpu, dtype=torch.float32, is_shared=False),\n                m: Tensor(shape=torch.Size([10]), device=cpu, dtype=torch.float32, is_shared=False),\n                max_speed: Tensor(shape=torch.Size([10]), device=cpu, dtype=torch.int64, is_shared=False),\n                max_torque: Tensor(shape=torch.Size([10]), device=cpu, dtype=torch.float32, is_shared=False)},\n            batch_size=torch.Size([10]),\n            device=None,\n            is_shared=False),\n        sin: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n        terminated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n        th: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n        thdot: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False)},\n    batch_size=torch.Size([10]),\n    device=None,\n    is_shared=False)\nrand step (batch size of 10) TensorDict(\n    fields={\n        action: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n        cos: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n        done: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n        next: TensorDict(\n            fields={\n                cos: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n                done: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n                observation: Tensor(shape=torch.Size([10, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n                params: TensorDict(\n                    fields={\n                        dt: Tensor(shape=torch.Size([10]), device=cpu, dtype=torch.float32, is_shared=False),\n                        g: Tensor(shape=torch.Size([10]), device=cpu, dtype=torch.float32, is_shared=False),\n                        l: Tensor(shape=torch.Size([10]), device=cpu, dtype=torch.float32, is_shared=False),\n                        m: Tensor(shape=torch.Size([10]), device=cpu, dtype=torch.float32, is_shared=False),\n                        max_speed: Tensor(shape=torch.Size([10]), device=cpu, dtype=torch.int64, is_shared=False),\n                        max_torque: Tensor(shape=torch.Size([10]), device=cpu, dtype=torch.float32, is_shared=False)},\n                    batch_size=torch.Size([10]),\n                    device=None,\n                    is_shared=False),\n                reward: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n                sin: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n                terminated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n                th: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n                thdot: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False)},\n            batch_size=torch.Size([10]),\n            device=None,\n            is_shared=False),\n        observation: Tensor(shape=torch.Size([10, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n        params: TensorDict(\n            fields={\n                dt: Tensor(shape=torch.Size([10]), device=cpu, dtype=torch.float32, is_shared=False),\n                g: Tensor(shape=torch.Size([10]), device=cpu, dtype=torch.float32, is_shared=False),\n                l: Tensor(shape=torch.Size([10]), device=cpu, dtype=torch.float32, is_shared=False),\n                m: Tensor(shape=torch.Size([10]), device=cpu, dtype=torch.float32, is_shared=False),\n                max_speed: Tensor(shape=torch.Size([10]), device=cpu, dtype=torch.int64, is_shared=False),\n                max_torque: Tensor(shape=torch.Size([10]), device=cpu, dtype=torch.float32, is_shared=False)},\n            batch_size=torch.Size([10]),\n            device=None,\n            is_shared=False),\n        sin: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n        terminated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n        th: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n        thdot: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False)},\n    batch_size=torch.Size([10]),\n    device=None,\n    is_shared=False)",
      "rollout of len 3 (batch size of 10): TensorDict(\n    fields={\n        action: Tensor(shape=torch.Size([10, 3, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n        cos: Tensor(shape=torch.Size([10, 3, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n        done: Tensor(shape=torch.Size([10, 3, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n        next: TensorDict(\n            fields={\n                cos: Tensor(shape=torch.Size([10, 3, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n                done: Tensor(shape=torch.Size([10, 3, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n                observation: Tensor(shape=torch.Size([10, 3, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n                params: TensorDict(\n                    fields={\n                        dt: Tensor(shape=torch.Size([10, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n                        g: Tensor(shape=torch.Size([10, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n                        l: Tensor(shape=torch.Size([10, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n                        m: Tensor(shape=torch.Size([10, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n                        max_speed: Tensor(shape=torch.Size([10, 3]), device=cpu, dtype=torch.int64, is_shared=False),\n                        max_torque: Tensor(shape=torch.Size([10, 3]), device=cpu, dtype=torch.float32, is_shared=False)},\n                    batch_size=torch.Size([10, 3]),\n                    device=None,\n                    is_shared=False),\n                reward: Tensor(shape=torch.Size([10, 3, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n                sin: Tensor(shape=torch.Size([10, 3, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n                terminated: Tensor(shape=torch.Size([10, 3, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n                th: Tensor(shape=torch.Size([10, 3, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n                thdot: Tensor(shape=torch.Size([10, 3, 1]), device=cpu, dtype=torch.float32, is_shared=False)},\n            batch_size=torch.Size([10, 3]),\n            device=None,\n            is_shared=False),\n        observation: Tensor(shape=torch.Size([10, 3, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n        params: TensorDict(\n            fields={\n                dt: Tensor(shape=torch.Size([10, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n                g: Tensor(shape=torch.Size([10, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n                l: Tensor(shape=torch.Size([10, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n                m: Tensor(shape=torch.Size([10, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n                max_speed: Tensor(shape=torch.Size([10, 3]), device=cpu, dtype=torch.int64, is_shared=False),\n                max_torque: Tensor(shape=torch.Size([10, 3]), device=cpu, dtype=torch.float32, is_shared=False)},\n            batch_size=torch.Size([10, 3]),\n            device=None,\n            is_shared=False),\n        sin: Tensor(shape=torch.Size([10, 3, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n        terminated: Tensor(shape=torch.Size([10, 3, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n        th: Tensor(shape=torch.Size([10, 3, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n        thdot: Tensor(shape=torch.Size([10, 3, 1]), device=cpu, dtype=torch.float32, is_shared=False)},\n    batch_size=torch.Size([10, 3]),\n    device=None,\n    is_shared=False)",
      "torch.manual_seed(0)\nenv.set_seed(0)\n\nnet = nn.Sequential(\n    nn.LazyLinear(64),\n    nn.Tanh(),\n    nn.LazyLinear(64),\n    nn.Tanh(),\n    nn.LazyLinear(64),\n    nn.Tanh(),\n    nn.LazyLinear(1),\n)\npolicy = TensorDictModule(\n    net,\n    in_keys=[\"observation\"],\n    out_keys=[\"action\"],\n)",
      "optim = torch.optim.Adam(policy.parameters(), lr=2e-3)",
      "batch_size = 32\npbar = tqdm.tqdm(range(20_000 // batch_size))\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, 20_000)\nlogs = defaultdict(list)\n\nfor _ in pbar:\n    init_td = env.reset(env.gen_params(batch_size=[batch_size]))\n    rollout = env.rollout(100, policy, tensordict=init_td, auto_reset=False)\n    traj_return = rollout[\"next\", \"reward\"].mean()\n    (-traj_return).backward()\n    gn = torch.nn.utils.clip_grad_norm_(net.parameters(), 1.0)\n    optim.step()\n    optim.zero_grad()\n    pbar.set_description(\n        f\"reward: {traj_return: 4.4f}, \"\n        f\"last reward: {rollout[..., -1]['next', 'reward'].mean(): 4.4f}, gradient norm: {gn: 4.4}\"\n    )\n    logs[\"return\"].append(traj_return.item())\n    logs[\"last_reward\"].append(rollout[..., -1][\"next\", \"reward\"].mean().item())\n    scheduler.step()\n\n\ndef plot():\n    import matplotlib\n    from matplotlib import pyplot as plt\n\n    is_ipython = \"inline\" in matplotlib.get_backend()\n    if is_ipython:\n        from IPython import display\n\n    with plt.ion():\n        plt.figure(figsize=(10, 5))\n        plt.subplot(1, 2, 1)\n        plt.plot(logs[\"return\"])\n        plt.title(\"returns\")\n        plt.xlabel(\"iteration\")\n        plt.subplot(1, 2, 2)\n        plt.plot(logs[\"last_reward\"])\n        plt.title(\"last reward\")\n        plt.xlabel(\"iteration\")\n        if is_ipython:\n            display.display(plt.gcf())\n            display.clear_output(wait=True)\n        plt.show()\n\n\nplot()"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/recipes/xeon_run_cpu.html",
    "title": "Optimizing CPU Performance on Intel\u00ae Xeon\u00ae with run_cpu Script\u00b6",
    "code_snippets": [
      "$ python -m torch.backends.xeon.run_cpu --ninstances 1 --ncores-per-instance 1 <program.py> [program_args]",
      "$ python -m torch.backends.xeon.run_cpu --node-id 0 <program.py> [program_args]",
      "$ python -m torch.backends.xeon.run_cpu --ninstances 8 --ncores-per-instance 14 <program.py> [program_args]",
      "$ python -m torch.backends.xeon.run_cpu --throughput-mode <program.py> [program_args]",
      "$ python -m torch.backends.xeon.run_cpu \u2013h\nusage: run_cpu.py [-h] [--multi-instance] [-m] [--no-python] [--enable-tcmalloc] [--enable-jemalloc] [--use-default-allocator] [--disable-iomp] [--ncores-per-instance] [--ninstances] [--skip-cross-node-cores] [--rank] [--latency-mode] [--throughput-mode] [--node-id] [--use-logical-core] [--disable-numactl] [--disable-taskset] [--core-list] [--log-path] [--log-file-prefix] <program> [program_args]"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/beginner/introyt/modelsyt_tutorial.html",
    "title": "Building Models with PyTorch\u00b6",
    "code_snippets": [
      "import torch\n\nclass TinyModel(torch.nn.Module):\n\n    def __init__(self):\n        super(TinyModel, self).__init__()\n\n        self.linear1 = torch.nn.Linear(100, 200)\n        self.activation = torch.nn.ReLU()\n        self.linear2 = torch.nn.Linear(200, 10)\n        self.softmax = torch.nn.Softmax()\n\n    def forward(self, x):\n        x = self.linear1(x)\n        x = self.activation(x)\n        x = self.linear2(x)\n        x = self.softmax(x)\n        return x\n\ntinymodel = TinyModel()\n\nprint('The model:')\nprint(tinymodel)\n\nprint('\\n\\nJust one layer:')\nprint(tinymodel.linear2)\n\nprint('\\n\\nModel params:')\nfor param in tinymodel.parameters():\n    print(param)\n\nprint('\\n\\nLayer params:')\nfor param in tinymodel.linear2.parameters():\n    print(param)",
      "lin = torch.nn.Linear(3, 2)\nx = torch.rand(1, 3)\nprint('Input:')\nprint(x)\n\nprint('\\n\\nWeight and Bias parameters:')\nfor param in lin.parameters():\n    print(param)\n\ny = lin(x)\nprint('\\n\\nOutput:')\nprint(y)",
      "import torch.functional as F\n\n\nclass LeNet(torch.nn.Module):\n\n    def __init__(self):\n        super(LeNet, self).__init__()\n        # 1 input image channel (black & white), 6 output channels, 5x5 square convolution\n        # kernel\n        self.conv1 = torch.nn.Conv2d(1, 6, 5)\n        self.conv2 = torch.nn.Conv2d(6, 16, 3)\n        # an affine operation: y = Wx + b\n        self.fc1 = torch.nn.Linear(16 * 6 * 6, 120)  # 6*6 from image dimension\n        self.fc2 = torch.nn.Linear(120, 84)\n        self.fc3 = torch.nn.Linear(84, 10)\n\n    def forward(self, x):\n        # Max pooling over a (2, 2) window\n        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n        # If the size is a square you can only specify a single number\n        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n        x = x.view(-1, self.num_flat_features(x))\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\n    def num_flat_features(self, x):\n        size = x.size()[1:]  # all dimensions except the batch dimension\n        num_features = 1\n        for s in size:\n            num_features *= s\n        return num_features",
      "class LSTMTagger(torch.nn.Module):\n\n    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n        super(LSTMTagger, self).__init__()\n        self.hidden_dim = hidden_dim\n\n        self.word_embeddings = torch.nn.Embedding(vocab_size, embedding_dim)\n\n        # The LSTM takes word embeddings as inputs, and outputs hidden states\n        # with dimensionality hidden_dim.\n        self.lstm = torch.nn.LSTM(embedding_dim, hidden_dim)\n\n        # The linear layer that maps from hidden state space to tag space\n        self.hidden2tag = torch.nn.Linear(hidden_dim, tagset_size)\n\n    def forward(self, sentence):\n        embeds = self.word_embeddings(sentence)\n        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n        tag_scores = F.log_softmax(tag_space, dim=1)\n        return tag_scores",
      "my_tensor = torch.rand(1, 6, 6)\nprint(my_tensor)\n\nmaxpool_layer = torch.nn.MaxPool2d(3)\nprint(maxpool_layer(my_tensor))",
      "my_tensor = torch.rand(1, 4, 4) * 20 + 5\nprint(my_tensor)\n\nprint(my_tensor.mean())\n\nnorm_layer = torch.nn.BatchNorm1d(4)\nnormed_tensor = norm_layer(my_tensor)\nprint(normed_tensor)\n\nprint(normed_tensor.mean())",
      "my_tensor = torch.rand(1, 4, 4)\n\ndropout = torch.nn.Dropout(p=0.4)\nprint(dropout(my_tensor))\nprint(dropout(my_tensor))"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/beginner/Intro_to_TorchScript_tutorial.html",
    "title": "Introduction to TorchScript\u00b6",
    "code_snippets": [
      "import torch  # This is all you need to use both PyTorch and TorchScript!\nprint(torch.__version__)\ntorch.manual_seed(191009)  # set the seed for reproducibility",
      "2.7.0+cu126\n\n<torch._C.Generator object at 0x7f0947b8e570>",
      "class MyCell(torch.nn.Module):\n    def __init__(self):\n        super(MyCell, self).__init__()\n\n    def forward(self, x, h):\n        new_h = torch.tanh(x + h)\n        return new_h, new_h\n\nmy_cell = MyCell()\nx = torch.rand(3, 4)\nh = torch.rand(3, 4)\nprint(my_cell(x, h))",
      "class MyCell(torch.nn.Module):\n    def __init__(self):\n        super(MyCell, self).__init__()\n        self.linear = torch.nn.Linear(4, 4)\n\n    def forward(self, x, h):\n        new_h = torch.tanh(self.linear(x) + h)\n        return new_h, new_h\n\nmy_cell = MyCell()\nprint(my_cell)\nprint(my_cell(x, h))",
      "class MyDecisionGate(torch.nn.Module):\n    def forward(self, x):\n        if x.sum() > 0:\n            return x\n        else:\n            return -x\n\nclass MyCell(torch.nn.Module):\n    def __init__(self):\n        super(MyCell, self).__init__()\n        self.dg = MyDecisionGate()\n        self.linear = torch.nn.Linear(4, 4)\n\n    def forward(self, x, h):\n        new_h = torch.tanh(self.dg(self.linear(x)) + h)\n        return new_h, new_h\n\nmy_cell = MyCell()\nprint(my_cell)\nprint(my_cell(x, h))",
      "class MyCell(torch.nn.Module):\n    def __init__(self):\n        super(MyCell, self).__init__()\n        self.linear = torch.nn.Linear(4, 4)\n\n    def forward(self, x, h):\n        new_h = torch.tanh(self.linear(x) + h)\n        return new_h, new_h\n\nmy_cell = MyCell()\nx, h = torch.rand(3, 4), torch.rand(3, 4)\ntraced_cell = torch.jit.trace(my_cell, (x, h))\nprint(traced_cell)\ntraced_cell(x, h)",
      "graph(%self.1 : __torch__.MyCell,\n      %x : Float(3, 4, strides=[4, 1], requires_grad=0, device=cpu),\n      %h : Float(3, 4, strides=[4, 1], requires_grad=0, device=cpu)):\n  %linear : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name=\"linear\"](%self.1)\n  %20 : Tensor = prim::CallMethod[name=\"forward\"](%linear, %x)\n  %11 : int = prim::Constant[value=1]() # /var/lib/workspace/beginner_source/Intro_to_TorchScript_tutorial.py:191:0\n  %12 : Float(3, 4, strides=[4, 1], requires_grad=1, device=cpu) = aten::add(%20, %h, %11) # /var/lib/workspace/beginner_source/Intro_to_TorchScript_tutorial.py:191:0\n  %13 : Float(3, 4, strides=[4, 1], requires_grad=1, device=cpu) = aten::tanh(%12) # /var/lib/workspace/beginner_source/Intro_to_TorchScript_tutorial.py:191:0\n  %14 : (Float(3, 4, strides=[4, 1], requires_grad=1, device=cpu), Float(3, 4, strides=[4, 1], requires_grad=1, device=cpu)) = prim::TupleConstruct(%13, %13)\n  return (%14)",
      "def forward(self,\n    x: Tensor,\n    h: Tensor) -> Tuple[Tensor, Tensor]:\n  linear = self.linear\n  _0 = torch.tanh(torch.add((linear).forward(x, ), h))\n  return (_0, _0)",
      "class MyDecisionGate(torch.nn.Module):\n    def forward(self, x):\n        if x.sum() > 0:\n            return x\n        else:\n            return -x\n\nclass MyCell(torch.nn.Module):\n    def __init__(self, dg):\n        super(MyCell, self).__init__()\n        self.dg = dg\n        self.linear = torch.nn.Linear(4, 4)\n\n    def forward(self, x, h):\n        new_h = torch.tanh(self.dg(self.linear(x)) + h)\n        return new_h, new_h\n\nmy_cell = MyCell(MyDecisionGate())\ntraced_cell = torch.jit.trace(my_cell, (x, h))\n\nprint(traced_cell.dg.code)\nprint(traced_cell.code)",
      "/var/lib/workspace/beginner_source/Intro_to_TorchScript_tutorial.py:263: TracerWarning:\n\nConverting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n\ndef forward(self,\n    argument_1: Tensor) -> NoneType:\n  return None\n\ndef forward(self,\n    x: Tensor,\n    h: Tensor) -> Tuple[Tensor, Tensor]:\n  dg = self.dg\n  linear = self.linear\n  _0 = (linear).forward(x, )\n  _1 = (dg).forward(_0, )\n  _2 = torch.tanh(torch.add(_0, h))\n  return (_2, _2)",
      "scripted_gate = torch.jit.script(MyDecisionGate())\n\nmy_cell = MyCell(scripted_gate)\nscripted_cell = torch.jit.script(my_cell)\n\nprint(scripted_gate.code)\nprint(scripted_cell.code)",
      "def forward(self,\n    x: Tensor) -> Tensor:\n  if bool(torch.gt(torch.sum(x), 0)):\n    _0 = x\n  else:\n    _0 = torch.neg(x)\n  return _0\n\ndef forward(self,\n    x: Tensor,\n    h: Tensor) -> Tuple[Tensor, Tensor]:\n  dg = self.dg\n  linear = self.linear\n  _0 = torch.add((dg).forward((linear).forward(x, ), ), h)\n  new_h = torch.tanh(_0)\n  return (new_h, new_h)",
      "# New inputs\nx, h = torch.rand(3, 4), torch.rand(3, 4)\nprint(scripted_cell(x, h))",
      "class MyRNNLoop(torch.nn.Module):\n    def __init__(self):\n        super(MyRNNLoop, self).__init__()\n        self.cell = torch.jit.trace(MyCell(scripted_gate), (x, h))\n\n    def forward(self, xs):\n        h, y = torch.zeros(3, 4), torch.zeros(3, 4)\n        for i in range(xs.size(0)):\n            y, h = self.cell(xs[i], h)\n        return y, h\n\nrnn_loop = torch.jit.script(MyRNNLoop())\nprint(rnn_loop.code)",
      "def forward(self,\n    xs: Tensor) -> Tuple[Tensor, Tensor]:\n  h = torch.zeros([3, 4])\n  y = torch.zeros([3, 4])\n  y0 = y\n  h0 = h\n  for i in range(torch.size(xs, 0)):\n    cell = self.cell\n    _0 = (cell).forward(torch.select(xs, 0, i), h0, )\n    y1, h1, = _0\n    y0, h0 = y1, h1\n  return (y0, h0)",
      "class WrapRNN(torch.nn.Module):\n    def __init__(self):\n        super(WrapRNN, self).__init__()\n        self.loop = torch.jit.script(MyRNNLoop())\n\n    def forward(self, xs):\n        y, h = self.loop(xs)\n        return torch.relu(y)\n\ntraced = torch.jit.trace(WrapRNN(), (torch.rand(10, 3, 4)))\nprint(traced.code)",
      "def forward(self,\n    xs: Tensor) -> Tensor:\n  loop = self.loop\n  _0, y, = (loop).forward(xs, )\n  return torch.relu(y)",
      "traced.save('wrapped_rnn.pt')\n\nloaded = torch.jit.load('wrapped_rnn.pt')\n\nprint(loaded)\nprint(loaded.code)",
      "RecursiveScriptModule(\n  original_name=WrapRNN\n  (loop): RecursiveScriptModule(\n    original_name=MyRNNLoop\n    (cell): RecursiveScriptModule(\n      original_name=MyCell\n      (dg): RecursiveScriptModule(original_name=MyDecisionGate)\n      (linear): RecursiveScriptModule(original_name=Linear)\n    )\n  )\n)\ndef forward(self,\n    xs: Tensor) -> Tensor:\n  loop = self.loop\n  _0, y, = (loop).forward(xs, )\n  return torch.relu(y)"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html",
    "title": "Getting Started with Fully Sharded Data Parallel (FSDP2)\u00b6",
    "code_snippets": [
      "from torch.distributed.fsdp import fully_shard, FSDPModule\nmodel = Transformer()\nfor layer in model.layers:\n    fully_shard(layer)\nfully_shard(model)\n\nassert isinstance(model, Transformer)\nassert isinstance(model, FSDPModule)\nprint(model)\n#  FSDPTransformer(\n#    (tok_embeddings): Embedding(...)\n#    ...\n#    (layers): 3 x FSDPTransformerBlock(...)\n#    (output): Linear(...)\n#  )",
      "from torch.distributed.tensor import DTensor\nfor param in model.parameters():\n    assert isinstance(param, DTensor)\n    assert param.placements == (Shard(0),)\n    # inspect sharded parameters with param.to_local()\n\noptim = torch.optim.Adam(model.parameters(), lr=1e-2)",
      "for _ in range(epochs):\n    x = torch.randint(0, vocab_size, (batch_size, seq_len), device=device)\n    loss = model(x).sum()\n    loss.backward()\n    optim.step()\n    optim.zero_grad()",
      "num_to_forward_prefetch = 2\nfor i, layer in enumerate(model.layers):\n    if i >= len(model.layers) - num_to_forward_prefetch:\n        break\n    layers_to_prefetch = [\n        model.layers[i + j] for j in range(1, num_to_forward_prefetch + 1)\n    ]\n    layer.set_modules_to_forward_prefetch(layers_to_prefetch)\n\nnum_to_backward_prefetch = 2\nfor i, layer in enumerate(model.layers):\n    if i < num_to_backward_prefetch:\n        continue\n    layers_to_prefetch = [\n        model.layers[i - j] for j in range(1, num_to_backward_prefetch + 1)\n    ]\n    layer.set_modules_to_backward_prefetch(layers_to_prefetch)\n\nfor _ in range(epochs):\n    # trigger 1st all-gather earlier\n    # this overlaps all-gather with any computation before model(x)\n    model.unshard()\n    x = torch.randint(0, vocab_size, (batch_size, seq_len), device=device)\n    loss = model(x).sum()\n    loss.backward()\n    optim.step()\n    optim.zero_grad()",
      "model = Transformer(model_args)\nfsdp_kwargs = {\n    \"mp_policy\": MixedPrecisionPolicy(\n        param_dtype=torch.bfloat16,\n        reduce_dtype=torch.float32,\n    )\n}\nfor layer in model.layers:\n    fully_shard(layer, **fsdp_kwargs)\nfully_shard(model, **fsdp_kwargs)\n\n# sharded parameters are float32\nfor param in model.parameters():\n    assert param.dtype == torch.float32\n\n# unsharded parameters are bfloat16\nmodel.unshard()\nfor param in model.parameters(recurse=False):\n    assert param.dtype == torch.bfloat16\nmodel.reshard()\n\n# optimizer states are in float32\noptim = torch.optim.Adam(model.parameters(), lr=1e-2)\n\n# training loop\n# ...",
      "# optim is constructed base on DTensor model parameters\noptim = torch.optim.Adam(model.parameters(), lr=1e-2)\nfor _ in range(epochs):\n    x = torch.randint(0, vocab_size, (batch_size, seq_len), device=device)\n    loss = model(x).sum()\n    loss.backward()\n    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=max_norm)\n    optim.step()\n    optim.zero_grad()",
      "from torch.distributed.tensor import distribute_tensor\n\n# mmap=True reduces CPU memory usage\nfull_sd = torch.load(\n    \"checkpoints/model_state_dict.pt\",\n    mmap=True,\n    weights_only=True,\n    map_location='cpu',\n)\nmeta_sharded_sd = model.state_dict()\nsharded_sd = {}\nfor param_name, full_tensor in full_sd.items():\n    sharded_meta_param = meta_sharded_sd.get(param_name)\n    sharded_tensor = distribute_tensor(\n        full_tensor,\n        sharded_meta_param.device_mesh,\n        sharded_meta_param.placements,\n    )\n    sharded_sd[param_name] = nn.Parameter(sharded_tensor)\n# `assign=True` since we cannot call `copy_` on meta tensor\nmodel.load_state_dict(sharded_sd, assign=True)",
      "sharded_sd = model.state_dict()\ncpu_state_dict = {}\nfor param_name, sharded_param in sharded_sd.items():\n    full_param = sharded_param.full_tensor()\n    if torch.distributed.get_rank() == 0:\n        cpu_state_dict[param_name] = full_param.cpu()\n    else:\n        del full_param\ntorch.save(cpu_state_dict, \"checkpoints/model_state_dict.pt\")",
      "from torch.distributed.checkpoint.state_dict import set_model_state_dict\nset_model_state_dict(\n    model=model,\n    model_state_dict=full_sd,\n    options=StateDictOptions(\n        full_state_dict=True,\n        broadcast_from_rank0=True,\n    ),\n)",
      "from torch.distributed.checkpoint.state_dict import get_model_state_dict\nmodel_state_dict = get_model_state_dict(\n    model=model,\n    options=StateDictOptions(\n        full_state_dict=True,\n        cpu_offload=True,\n    )\n)\ntorch.save(model_state_dict, \"model_state_dict.pt\")",
      "from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\nwith torch.device(\"meta\"):\n    model = Transformer()\npolicy = ModuleWrapPolicy({TransformerBlock})\nmodel = FSDP(model, auto_wrap_policy=policy)\ndef param_init_fn(module: nn.Module) -> None: ...\nmodel = FSDP(model, auto_wrap_policy=policy, param_init_fn=param_init_fn)",
      "with torch.device(\"meta\"):\n    model = Transformer()\nfor module in model.modules():\n    if isinstance(module, TransformerBlock):\n        fully_shard(module)\nfully_shard(model)\nfor tensor in itertools.chain(model.parameters(), model.buffers()):\n    assert tensor.device == torch.device(\"meta\")\n\n\n# Initialize the model after sharding\nmodel.to_empty(device=\"cuda\")\nmodel.reset_parameters()"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/recipes/inference_tuning_on_aws_graviton.html",
    "title": "(Beta) PyTorch Inference Performance Tuning on AWS Graviton Processors\u00b6",
    "code_snippets": [
      "import torch\nimport torch.nn as nn\nfrom torch.profiler import profile, record_function, ProfilerActivity\n\n# AWS Graviton3 cpu\ndevice = (\"cpu\")\nprint(f\"Using {device} device\")",
      "class MyNeuralNetwork(nn.Module):\n  def __init__(self):\n      super().__init__()\n      self.flatten = nn.Flatten()\n      self.linear_relu_stack = nn.Sequential(\n          nn.Linear(4096, 4096),\n          nn.ReLU(),\n          nn.Linear(4096, 11008),\n          nn.ReLU(),\n          nn.Linear(11008, 10),\n      )\n\n  def forward(self, x):\n      x = self.flatten(x)\n      logits = self.linear_relu_stack(x)\n      return logits",
      "X = torch.rand(1, 64, 64, device=device)\nlogits = model(X)\npred_probab = nn.Softmax(dim=1)(logits)\ny_pred = pred_probab.argmax(1)\nprint(f\"Predicted class: {y_pred}\")",
      "Predicted class: tensor([2])",
      "# warm it up first and loop over multiple times to have enough execution time\n\nX = torch.rand(256, 64, 64, device=device)\n\nwith torch.set_grad_enabled(False):\n    for _ in range(50):\n        model(X) #Warmup\n    with profile(activities=[ProfilerActivity.CPU]) as prof:\n        with record_function(\"mymodel_inference\"):\n            for _ in range(100):\n                model(X)\n\nprint(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))",
      "X = torch.rand(32, 64, 64, device=device)\nwith torch.set_grad_enabled(False):\n    for _ in range(50):\n        model(X) #Warmup\n    with profile(activities=[ProfilerActivity.CPU]) as prof:\n        with record_function(\"mymodel_inference\"):\n            for _ in range(100):\n                model(X)\n\nprint(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))",
      "X = torch.rand(10, 64, 64, device=device)\nwith torch.set_grad_enabled(False):\n    for _ in range(50):\n        model(X) #Warmup\n    with profile(activities=[ProfilerActivity.CPU]) as prof:\n        with record_function(\"mymodel_inference\"):\n            for _ in range(100):\n                model(X)\n\nprint(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))",
      "X = torch.rand(256, 64, 64, device=device)\nwith torch.set_grad_enabled(False):\n    for _ in range(50):\n        model(X) #Warmup\n    with profile(activities=[ProfilerActivity.CPU]) as prof:\n        with record_function(\"mymodel_inference\"):\n            for _ in range(100):\n                model(X)\n\nprint(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/intermediate/memory_format_tutorial.html",
    "title": "(beta) Channels Last Memory Format in PyTorch\u00b6",
    "code_snippets": [
      "import torch\n\nN, C, H, W = 10, 3, 32, 32\nx = torch.empty(N, C, H, W)\nprint(x.stride())  # Outputs: (3072, 1024, 32, 1)",
      "x = x.to(memory_format=torch.channels_last)\nprint(x.shape)  # Outputs: (10, 3, 32, 32) as dimensions order preserved\nprint(x.stride())  # Outputs: (3072, 1, 96, 3)",
      "torch.Size([10, 3, 32, 32])\n(3072, 1, 96, 3)",
      "x = x.to(memory_format=torch.contiguous_format)\nprint(x.stride())  # Outputs: (3072, 1024, 32, 1)",
      "x = x.contiguous(memory_format=torch.channels_last)\nprint(x.stride())  # Outputs: (3072, 1, 96, 3)",
      "print(x.is_contiguous(memory_format=torch.channels_last))  # Outputs: True",
      "special_x = torch.empty(4, 1, 4, 4)\nprint(special_x.is_contiguous(memory_format=torch.channels_last))  # Outputs: True\nprint(special_x.is_contiguous(memory_format=torch.contiguous_format))  # Outputs: True",
      "x = torch.empty(N, C, H, W, memory_format=torch.channels_last)\nprint(x.stride())  # Outputs: (3072, 1, 96, 3)",
      "if torch.cuda.is_available():\n    y = x.cuda()\n    print(y.stride())  # Outputs: (3072, 1, 96, 3)",
      "y = torch.empty_like(x)\nprint(y.stride())  # Outputs: (3072, 1, 96, 3)",
      "if torch.backends.cudnn.is_available() and torch.backends.cudnn.version() >= 7603:\n    model = torch.nn.Conv2d(8, 4, 3).cuda().half()\n    model = model.to(memory_format=torch.channels_last)  # Module parameters need to be channels last\n\n    input = torch.randint(1, 10, (2, 8, 4, 4), dtype=torch.float32, requires_grad=True)\n    input = input.to(device=\"cuda\", memory_format=torch.channels_last, dtype=torch.float16)\n\n    out = model(input)\n    print(out.is_contiguous(memory_format=torch.channels_last))  # Outputs: True",
      "# opt_level = O2\n# keep_batchnorm_fp32 = None <class 'NoneType'>\n# loss_scale = None <class 'NoneType'>\n# CUDNN VERSION: 7603\n# => creating model 'resnet50'\n# Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n# Defaults for this optimization level are:\n# enabled                : True\n# opt_level              : O2\n# cast_model_type        : torch.float16\n# patch_torch_functions  : False\n# keep_batchnorm_fp32    : True\n# master_weights         : True\n# loss_scale             : dynamic\n# Processing user overrides (additional kwargs that are not None)...\n# After processing overrides, optimization options are:\n# enabled                : True\n# opt_level              : O2\n# cast_model_type        : torch.float16\n# patch_torch_functions  : False\n# keep_batchnorm_fp32    : True\n# master_weights         : True\n# loss_scale             : dynamic\n# Epoch: [0][10/125] Time 0.866 (0.866) Speed 230.949 (230.949) Loss 0.6735125184 (0.6735) Prec@1 61.000 (61.000) Prec@5 100.000 (100.000)\n# Epoch: [0][20/125] Time 0.259 (0.562) Speed 773.481 (355.693) Loss 0.6968704462 (0.6852) Prec@1 55.000 (58.000) Prec@5 100.000 (100.000)\n# Epoch: [0][30/125] Time 0.258 (0.461) Speed 775.089 (433.965) Loss 0.7877287269 (0.7194) Prec@1 51.500 (55.833) Prec@5 100.000 (100.000)\n# Epoch: [0][40/125] Time 0.259 (0.410) Speed 771.710 (487.281) Loss 0.8285319805 (0.7467) Prec@1 48.500 (54.000) Prec@5 100.000 (100.000)\n# Epoch: [0][50/125] Time 0.260 (0.380) Speed 770.090 (525.908) Loss 0.7370464802 (0.7447) Prec@1 56.500 (54.500) Prec@5 100.000 (100.000)\n# Epoch: [0][60/125] Time 0.258 (0.360) Speed 775.623 (555.728) Loss 0.7592862844 (0.7472) Prec@1 51.000 (53.917) Prec@5 100.000 (100.000)\n# Epoch: [0][70/125] Time 0.258 (0.345) Speed 774.746 (579.115) Loss 1.9698858261 (0.9218) Prec@1 49.500 (53.286) Prec@5 100.000 (100.000)\n# Epoch: [0][80/125] Time 0.260 (0.335) Speed 770.324 (597.659) Loss 2.2505953312 (1.0879) Prec@1 50.500 (52.938) Prec@5 100.000 (100.000)",
      "# opt_level = O2\n# keep_batchnorm_fp32 = None <class 'NoneType'>\n# loss_scale = None <class 'NoneType'>\n#\n# CUDNN VERSION: 7603\n#\n# => creating model 'resnet50'\n# Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n#\n# Defaults for this optimization level are:\n# enabled                : True\n# opt_level              : O2\n# cast_model_type        : torch.float16\n# patch_torch_functions  : False\n# keep_batchnorm_fp32    : True\n# master_weights         : True\n# loss_scale             : dynamic\n# Processing user overrides (additional kwargs that are not None)...\n# After processing overrides, optimization options are:\n# enabled                : True\n# opt_level              : O2\n# cast_model_type        : torch.float16\n# patch_torch_functions  : False\n# keep_batchnorm_fp32    : True\n# master_weights         : True\n# loss_scale             : dynamic\n#\n# Epoch: [0][10/125] Time 0.767 (0.767) Speed 260.785 (260.785) Loss 0.7579724789 (0.7580) Prec@1 53.500 (53.500) Prec@5 100.000 (100.000)\n# Epoch: [0][20/125] Time 0.198 (0.482) Speed 1012.135 (414.716) Loss 0.7007197738 (0.7293) Prec@1 49.000 (51.250) Prec@5 100.000 (100.000)\n# Epoch: [0][30/125] Time 0.198 (0.387) Speed 1010.977 (516.198) Loss 0.7113101482 (0.7233) Prec@1 55.500 (52.667) Prec@5 100.000 (100.000)\n# Epoch: [0][40/125] Time 0.197 (0.340) Speed 1013.023 (588.333) Loss 0.8943189979 (0.7661) Prec@1 54.000 (53.000) Prec@5 100.000 (100.000)\n# Epoch: [0][50/125] Time 0.198 (0.312) Speed 1010.541 (641.977) Loss 1.7113249302 (0.9551) Prec@1 51.000 (52.600) Prec@5 100.000 (100.000)\n# Epoch: [0][60/125] Time 0.198 (0.293) Speed 1011.163 (683.574) Loss 5.8537774086 (1.7716) Prec@1 50.500 (52.250) Prec@5 100.000 (100.000)\n# Epoch: [0][70/125] Time 0.198 (0.279) Speed 1011.453 (716.767) Loss 5.7595844269 (2.3413) Prec@1 46.500 (51.429) Prec@5 100.000 (100.000)\n# Epoch: [0][80/125] Time 0.198 (0.269) Speed 1011.827 (743.883) Loss 2.8196096420 (2.4011) Prec@1 47.500 (50.938) Prec@5 100.000 (100.000)",
      "# Need to be done once, after model initialization (or load)\nmodel = model.to(memory_format=torch.channels_last)  # Replace with your model\n\n# Need to be done for every input\ninput = input.to(memory_format=torch.channels_last)  # Replace with your input\noutput = model(input)",
      "def contains_cl(args):\n    for t in args:\n        if isinstance(t, torch.Tensor):\n            if t.is_contiguous(memory_format=torch.channels_last) and not t.is_contiguous():\n                return True\n        elif isinstance(t, list) or isinstance(t, tuple):\n            if contains_cl(list(t)):\n                return True\n    return False\n\n\ndef print_inputs(args, indent=\"\"):\n    for t in args:\n        if isinstance(t, torch.Tensor):\n            print(indent, t.stride(), t.shape, t.device, t.dtype)\n        elif isinstance(t, list) or isinstance(t, tuple):\n            print(indent, type(t))\n            print_inputs(list(t), indent=indent + \"    \")\n        else:\n            print(indent, t)\n\n\ndef check_wrapper(fn):\n    name = fn.__name__\n\n    def check_cl(*args, **kwargs):\n        was_cl = contains_cl(args)\n        try:\n            result = fn(*args, **kwargs)\n        except Exception as e:\n            print(\"`{}` inputs are:\".format(name))\n            print_inputs(args)\n            print(\"-------------------\")\n            raise e\n        failed = False\n        if was_cl:\n            if isinstance(result, torch.Tensor):\n                if result.dim() == 4 and not result.is_contiguous(memory_format=torch.channels_last):\n                    print(\n                        \"`{}` got channels_last input, but output is not channels_last:\".format(name),\n                        result.shape,\n                        result.stride(),\n                        result.device,\n                        result.dtype,\n                    )\n                    failed = True\n        if failed and True:\n            print(\"`{}` inputs are:\".format(name))\n            print_inputs(args)\n            raise Exception(\"Operator `{}` lost channels_last property\".format(name))\n        return result\n\n    return check_cl\n\n\nold_attrs = dict()\n\n\ndef attribute(m):\n    old_attrs[m] = dict()\n    for i in dir(m):\n        e = getattr(m, i)\n        exclude_functions = [\"is_cuda\", \"has_names\", \"numel\", \"stride\", \"Tensor\", \"is_contiguous\", \"__class__\"]\n        if i not in exclude_functions and not i.startswith(\"_\") and \"__call__\" in dir(e):\n            try:\n                old_attrs[m][i] = e\n                setattr(m, i, check_wrapper(e))\n            except Exception as e:\n                print(i)\n                print(e)\n\n\nattribute(torch.Tensor)\nattribute(torch.nn.functional)\nattribute(torch)"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/beginner/introyt/tensorboardyt_tutorial.html",
    "title": "PyTorch TensorBoard Support\u00b6",
    "code_snippets": [
      "# PyTorch model and training necessities\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\n# Image datasets and image manipulation\nimport torchvision\nimport torchvision.transforms as transforms\n\n# Image display\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# PyTorch TensorBoard support\nfrom torch.utils.tensorboard import SummaryWriter\n\n# In case you are using an environment that has TensorFlow installed,\n# such as Google Colab, uncomment the following code to avoid\n# a bug with saving embeddings to your TensorBoard directory\n\n# import tensorflow as tf\n# import tensorboard as tb\n# tf.io.gfile = tb.compat.tensorflow_stub.io.gfile",
      "# Gather datasets and prepare them for consumption\ntransform = transforms.Compose(\n    [transforms.ToTensor(),\n    transforms.Normalize((0.5,), (0.5,))])\n\n# Store separate training and validations splits in ./data\ntraining_set = torchvision.datasets.FashionMNIST('./data',\n    download=True,\n    train=True,\n    transform=transform)\nvalidation_set = torchvision.datasets.FashionMNIST('./data',\n    download=True,\n    train=False,\n    transform=transform)\n\ntraining_loader = torch.utils.data.DataLoader(training_set,\n                                              batch_size=4,\n                                              shuffle=True,\n                                              num_workers=2)\n\n\nvalidation_loader = torch.utils.data.DataLoader(validation_set,\n                                                batch_size=4,\n                                                shuffle=False,\n                                                num_workers=2)\n\n# Class labels\nclasses = ('T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n        'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot')\n\n# Helper function for inline image display\ndef matplotlib_imshow(img, one_channel=False):\n    if one_channel:\n        img = img.mean(dim=0)\n    img = img / 2 + 0.5     # unnormalize\n    npimg = img.numpy()\n    if one_channel:\n        plt.imshow(npimg, cmap=\"Greys\")\n    else:\n        plt.imshow(np.transpose(npimg, (1, 2, 0)))\n\n# Extract a batch of 4 images\ndataiter = iter(training_loader)\nimages, labels = next(dataiter)\n\n# Create a grid from the images and show them\nimg_grid = torchvision.utils.make_grid(images)\nmatplotlib_imshow(img_grid, one_channel=True)",
      "# Default log_dir argument is \"runs\" - but it's good to be specific\n# torch.utils.tensorboard.SummaryWriter is imported above\nwriter = SummaryWriter('runs/fashion_mnist_experiment_1')\n\n# Write image data to TensorBoard log dir\nwriter.add_image('Four Fashion-MNIST Images', img_grid)\nwriter.flush()\n\n# To view, start TensorBoard on the command line with:\n#   tensorboard --logdir=runs\n# ...and open a browser tab to http://localhost:6006/",
      "class Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 6, 5)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = x.view(-1, 16 * 4 * 4)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\n\nnet = Net()\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)",
      "# Select a random subset of data and corresponding labels\ndef select_n_random(data, labels, n=100):\n    assert len(data) == len(labels)\n\n    perm = torch.randperm(len(data))\n    return data[perm][:n], labels[perm][:n]\n\n# Extract a random subset of data\nimages, labels = select_n_random(training_set.data, training_set.targets)\n\n# get the class labels for each image\nclass_labels = [classes[label] for label in labels]\n\n# log embeddings\nfeatures = images.view(-1, 28 * 28)\nwriter.add_embedding(features,\n                    metadata=class_labels,\n                    label_img=images.unsqueeze(1))\nwriter.flush()\nwriter.close()"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/intermediate/tensorboard_profiler_tutorial.html",
    "title": "PyTorch Profiler With TensorBoard\u00b6",
    "code_snippets": [
      "import torch\nimport torch.nn\nimport torch.optim\nimport torch.profiler\nimport torch.utils.data\nimport torchvision.datasets\nimport torchvision.models\nimport torchvision.transforms as T",
      "transform = T.Compose(\n    [T.Resize(224),\n     T.ToTensor(),\n     T.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\ntrain_set = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\ntrain_loader = torch.utils.data.DataLoader(train_set, batch_size=32, shuffle=True)",
      "device = torch.device(\"cuda:0\")\nmodel = torchvision.models.resnet18(weights='IMAGENET1K_V1').cuda(device)\ncriterion = torch.nn.CrossEntropyLoss().cuda(device)\noptimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\nmodel.train()",
      "def train(data):\n    inputs, labels = data[0].to(device=device), data[1].to(device=device)\n    outputs = model(inputs)\n    loss = criterion(outputs, labels)\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()",
      "with torch.profiler.profile(\n        schedule=torch.profiler.schedule(wait=1, warmup=1, active=3, repeat=1),\n        on_trace_ready=torch.profiler.tensorboard_trace_handler('./log/resnet18'),\n        record_shapes=True,\n        profile_memory=True,\n        with_stack=True\n) as prof:\n    for step, batch_data in enumerate(train_loader):\n        prof.step()  # Need to call this at each step to notify profiler of steps' boundary.\n        if step >= 1 + 1 + 3:\n            break\n        train(batch_data)",
      "prof = torch.profiler.profile(\n        schedule=torch.profiler.schedule(wait=1, warmup=1, active=3, repeat=1),\n        on_trace_ready=torch.profiler.tensorboard_trace_handler('./log/resnet18'),\n        record_shapes=True,\n        with_stack=True)\nprof.start()\nfor step, batch_data in enumerate(train_loader):\n    prof.step()\n    if step >= 1 + 1 + 3:\n        break\n    train(batch_data)\nprof.stop()",
      "train_loader = torch.utils.data.DataLoader(train_set, batch_size=32, shuffle=True, num_workers=4)",
      "pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/rocm5.6"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/advanced/torch_script_custom_classes.html",
    "title": "Extending TorchScript with Custom C++ Classes\u00b6",
    "code_snippets": [
      "// This header is all you need to do the C++ portions of this\n// tutorial\n#include <torch/script.h>\n// This header is what defines the custom class registration\n// behavior specifically. script.h already includes this, but\n// we include it here so you know it exists in case you want\n// to look at the API or implementation.\n#include <torch/custom_class.h>\n\n#include <string>\n#include <vector>\n\ntemplate <class T>\nstruct MyStackClass : torch::CustomClassHolder {\n  std::vector<T> stack_;\n  MyStackClass(std::vector<T> init) : stack_(init.begin(), init.end()) {}\n\n  void push(T x) {\n    stack_.push_back(x);\n  }\n  T pop() {\n    auto val = stack_.back();\n    stack_.pop_back();\n    return val;\n  }\n\n  c10::intrusive_ptr<MyStackClass> clone() const {\n    return c10::make_intrusive<MyStackClass>(stack_);\n  }\n\n  void merge(const c10::intrusive_ptr<MyStackClass>& c) {\n    for (auto& elem : c->stack_) {\n      push(elem);\n    }\n  }\n};",
      "// Notice a few things:\n// - We pass the class to be registered as a template parameter to\n//   `torch::class_`. In this instance, we've passed the\n//   specialization of the MyStackClass class ``MyStackClass<std::string>``.\n//   In general, you cannot register a non-specialized template\n//   class. For non-templated classes, you can just pass the\n//   class name directly as the template parameter.\n// - The arguments passed to the constructor make up the \"qualified name\"\n//   of the class. In this case, the registered class will appear in\n//   Python and C++ as `torch.classes.my_classes.MyStackClass`. We call\n//   the first argument the \"namespace\" and the second argument the\n//   actual class name.\nTORCH_LIBRARY(my_classes, m) {\n  m.class_<MyStackClass<std::string>>(\"MyStackClass\")\n    // The following line registers the contructor of our MyStackClass\n    // class that takes a single `std::vector<std::string>` argument,\n    // i.e. it exposes the C++ method `MyStackClass(std::vector<T> init)`.\n    // Currently, we do not support registering overloaded\n    // constructors, so for now you can only `def()` one instance of\n    // `torch::init`.\n    .def(torch::init<std::vector<std::string>>())\n    // The next line registers a stateless (i.e. no captures) C++ lambda\n    // function as a method. Note that a lambda function must take a\n    // `c10::intrusive_ptr<YourClass>` (or some const/ref version of that)\n    // as the first argument. Other arguments can be whatever you want.\n    .def(\"top\", [](const c10::intrusive_ptr<MyStackClass<std::string>>& self) {\n      return self->stack_.back();\n    })\n    // The following four lines expose methods of the MyStackClass<std::string>\n    // class as-is. `torch::class_` will automatically examine the\n    // argument and return types of the passed-in method pointers and\n    // expose these to Python and TorchScript accordingly. Finally, notice\n    // that we must take the *address* of the fully-qualified method name,\n    // i.e. use the unary `&` operator, due to C++ typing rules.\n    .def(\"push\", &MyStackClass<std::string>::push)\n    .def(\"pop\", &MyStackClass<std::string>::pop)\n    .def(\"clone\", &MyStackClass<std::string>::clone)\n    .def(\"merge\", &MyStackClass<std::string>::merge)\n  ;\n}",
      "cmake_minimum_required(VERSION 3.1 FATAL_ERROR)\nproject(custom_class)\n\nfind_package(Torch REQUIRED)\n\n# Define our library target\nadd_library(custom_class SHARED class.cpp)\nset(CMAKE_CXX_STANDARD 14)\n# Link against LibTorch\ntarget_link_libraries(custom_class \"${TORCH_LIBRARIES}\")",
      "custom_class_project/\n  class.cpp\n  CMakeLists.txt\n  build/",
      "$ cd build\n$ cmake -DCMAKE_PREFIX_PATH=\"$(python -c 'import torch.utils; print(torch.utils.cmake_prefix_path)')\" ..\n  -- The C compiler identification is GNU 7.3.1\n  -- The CXX compiler identification is GNU 7.3.1\n  -- Check for working C compiler: /opt/rh/devtoolset-7/root/usr/bin/cc\n  -- Check for working C compiler: /opt/rh/devtoolset-7/root/usr/bin/cc -- works\n  -- Detecting C compiler ABI info\n  -- Detecting C compiler ABI info - done\n  -- Detecting C compile features\n  -- Detecting C compile features - done\n  -- Check for working CXX compiler: /opt/rh/devtoolset-7/root/usr/bin/c++\n  -- Check for working CXX compiler: /opt/rh/devtoolset-7/root/usr/bin/c++ -- works\n  -- Detecting CXX compiler ABI info\n  -- Detecting CXX compiler ABI info - done\n  -- Detecting CXX compile features\n  -- Detecting CXX compile features - done\n  -- Looking for pthread.h\n  -- Looking for pthread.h - found\n  -- Looking for pthread_create\n  -- Looking for pthread_create - not found\n  -- Looking for pthread_create in pthreads\n  -- Looking for pthread_create in pthreads - not found\n  -- Looking for pthread_create in pthread\n  -- Looking for pthread_create in pthread - found\n  -- Found Threads: TRUE\n  -- Found torch: /torchbind_tutorial/libtorch/lib/libtorch.so\n  -- Configuring done\n  -- Generating done\n  -- Build files have been written to: /torchbind_tutorial/build\n$ make -j\n  Scanning dependencies of target custom_class\n  [ 50%] Building CXX object CMakeFiles/custom_class.dir/class.cpp.o\n  [100%] Linking CXX shared library libcustom_class.so\n  [100%] Built target custom_class",
      "custom_class_project/\n  class.cpp\n  CMakeLists.txt\n  build/\n    libcustom_class.so",
      "import torch\n\n# `torch.classes.load_library()` allows you to pass the path to your .so file\n# to load it in and make the custom C++ classes available to both Python and\n# TorchScript\ntorch.classes.load_library(\"build/libcustom_class.so\")\n# You can query the loaded libraries like this:\nprint(torch.classes.loaded_libraries)\n# prints {'/custom_class_project/build/libcustom_class.so'}\n\n# We can find and instantiate our custom C++ class in python by using the\n# `torch.classes` namespace:\n#\n# This instantiation will invoke the MyStackClass(std::vector<T> init)\n# constructor we registered earlier\ns = torch.classes.my_classes.MyStackClass([\"foo\", \"bar\"])\n\n# We can call methods in Python\ns.push(\"pushed\")\nassert s.pop() == \"pushed\"\n\n# Test custom operator\ns.push(\"pushed\")\ntorch.ops.my_classes.manipulate_instance(s)  # acting as s.pop()\nassert s.top() == \"bar\" \n\n# Returning and passing instances of custom classes works as you'd expect\ns2 = s.clone()\ns.merge(s2)\nfor expected in [\"bar\", \"foo\", \"bar\", \"foo\"]:\n    assert s.pop() == expected\n\n# We can also use the class in TorchScript\n# For now, we need to assign the class's type to a local in order to\n# annotate the type on the TorchScript function. This may change\n# in the future.\nMyStackClass = torch.classes.my_classes.MyStackClass\n\n\n@torch.jit.script\ndef do_stacks(s: MyStackClass):  # We can pass a custom class instance\n    # We can instantiate the class\n    s2 = torch.classes.my_classes.MyStackClass([\"hi\", \"mom\"])\n    s2.merge(s)  # We can call a method on the class\n    # We can also return instances of the class\n    # from TorchScript function/methods\n    return s2.clone(), s2.top()\n\n\nstack, top = do_stacks(torch.classes.my_classes.MyStackClass([\"wow\"]))\nassert top == \"wow\"\nfor expected in [\"wow\", \"mom\", \"hi\"]:\n    assert stack.pop() == expected",
      "import torch\n\ntorch.classes.load_library('build/libcustom_class.so')\n\n\nclass Foo(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, s: str) -> str:\n        stack = torch.classes.my_classes.MyStackClass([\"hi\", \"mom\"])\n        return stack.pop() + s\n\n\nscripted_foo = torch.jit.script(Foo())\nprint(scripted_foo.graph)\n\nscripted_foo.save('foo.pt')",
      "cpp_inference_example/\n  infer.cpp\n  CMakeLists.txt\n  foo.pt\n  build/\n  custom_class_project/\n    class.cpp\n    CMakeLists.txt\n    build/",
      "cmake_minimum_required(VERSION 3.1 FATAL_ERROR)\nproject(infer)\n\nfind_package(Torch REQUIRED)\n\nadd_subdirectory(custom_class_project)\n\n# Define our library target\nadd_executable(infer infer.cpp)\nset(CMAKE_CXX_STANDARD 14)\n# Link against LibTorch\ntarget_link_libraries(infer \"${TORCH_LIBRARIES}\")\n# This is where we link in our libcustom_class code, making our\n# custom class available in our binary.\ntarget_link_libraries(infer -Wl,--no-as-needed custom_class)",
      "$ cd build\n$ cmake -DCMAKE_PREFIX_PATH=\"$(python -c 'import torch.utils; print(torch.utils.cmake_prefix_path)')\" ..\n  -- The C compiler identification is GNU 7.3.1\n  -- The CXX compiler identification is GNU 7.3.1\n  -- Check for working C compiler: /opt/rh/devtoolset-7/root/usr/bin/cc\n  -- Check for working C compiler: /opt/rh/devtoolset-7/root/usr/bin/cc -- works\n  -- Detecting C compiler ABI info\n  -- Detecting C compiler ABI info - done\n  -- Detecting C compile features\n  -- Detecting C compile features - done\n  -- Check for working CXX compiler: /opt/rh/devtoolset-7/root/usr/bin/c++\n  -- Check for working CXX compiler: /opt/rh/devtoolset-7/root/usr/bin/c++ -- works\n  -- Detecting CXX compiler ABI info\n  -- Detecting CXX compiler ABI info - done\n  -- Detecting CXX compile features\n  -- Detecting CXX compile features - done\n  -- Looking for pthread.h\n  -- Looking for pthread.h - found\n  -- Looking for pthread_create\n  -- Looking for pthread_create - not found\n  -- Looking for pthread_create in pthreads\n  -- Looking for pthread_create in pthreads - not found\n  -- Looking for pthread_create in pthread\n  -- Looking for pthread_create in pthread - found\n  -- Found Threads: TRUE\n  -- Found torch: /local/miniconda3/lib/python3.7/site-packages/torch/lib/libtorch.so\n  -- Configuring done\n  -- Generating done\n  -- Build files have been written to: /cpp_inference_example/build\n$ make -j\n  Scanning dependencies of target custom_class\n  [ 25%] Building CXX object custom_class_project/CMakeFiles/custom_class.dir/class.cpp.o\n  [ 50%] Linking CXX shared library libcustom_class.so\n  [ 50%] Built target custom_class\n  Scanning dependencies of target infer\n  [ 75%] Building CXX object CMakeFiles/infer.dir/infer.cpp.o\n  [100%] Linking CXX executable infer\n  [100%] Built target infer",
      "# export_attr.py\nimport torch\n\ntorch.classes.load_library('build/libcustom_class.so')\n\n\nclass Foo(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.stack = torch.classes.my_classes.MyStackClass([\"just\", \"testing\"])\n\n    def forward(self, s: str) -> str:\n        return self.stack.pop() + s\n\n\nscripted_foo = torch.jit.script(Foo())\n\nscripted_foo.save('foo.pt')\nloaded = torch.jit.load('foo.pt')\n\nprint(loaded.stack.pop())",
      "$ python export_attr.py\nRuntimeError: Cannot serialize custom bound C++ class __torch__.torch.classes.my_classes.MyStackClass. Please define serialization methods via def_pickle for this class. (pushIValueImpl at ../torch/csrc/jit/pickler.cpp:128)",
      "// class_<>::def_pickle allows you to define the serialization\n    // and deserialization methods for your C++ class.\n    // Currently, we only support passing stateless lambda functions\n    // as arguments to def_pickle\n    .def_pickle(\n          // __getstate__\n          // This function defines what data structure should be produced\n          // when we serialize an instance of this class. The function\n          // must take a single `self` argument, which is an intrusive_ptr\n          // to the instance of the object. The function can return\n          // any type that is supported as a return value of the TorchScript\n          // custom operator API. In this instance, we've chosen to return\n          // a std::vector<std::string> as the salient data to preserve\n          // from the class.\n          [](const c10::intrusive_ptr<MyStackClass<std::string>>& self)\n              -> std::vector<std::string> {\n            return self->stack_;\n          },\n          // __setstate__\n          // This function defines how to create a new instance of the C++\n          // class when we are deserializing. The function must take a\n          // single argument of the same type as the return value of\n          // `__getstate__`. The function must return an intrusive_ptr\n          // to a new instance of the C++ class, initialized however\n          // you would like given the serialized state.\n          [](std::vector<std::string> state)\n              -> c10::intrusive_ptr<MyStackClass<std::string>> {\n            // A convenient way to instantiate an object and get an\n            // intrusive_ptr to it is via `make_intrusive`. We use\n            // that here to allocate an instance of MyStackClass<std::string>\n            // and call the single-argument std::vector<std::string>\n            // constructor with the serialized state.\n            return c10::make_intrusive<MyStackClass<std::string>>(std::move(state));\n          });",
      "m.def(\n      \"manipulate_instance(__torch__.torch.classes.my_classes.MyStackClass x) -> __torch__.torch.classes.my_classes.MyStackClass Y\",\n      manipulate_instance\n    );",
      "class TryCustomOp(torch.nn.Module):\n    def __init__(self):\n        super(TryCustomOp, self).__init__()\n        self.f = torch.classes.my_classes.MyStackClass([\"foo\", \"bar\"])\n\n    def forward(self):\n        return torch.ops.my_classes.manipulate_instance(self.f)"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html",
    "title": "Build the Neural Network\u00b6",
    "code_snippets": [
      "import os\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms",
      "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\nprint(f\"Using {device} device\")",
      "class NeuralNetwork(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.flatten = nn.Flatten()\n        self.linear_relu_stack = nn.Sequential(\n            nn.Linear(28*28, 512),\n            nn.ReLU(),\n            nn.Linear(512, 512),\n            nn.ReLU(),\n            nn.Linear(512, 10),\n        )\n\n    def forward(self, x):\n        x = self.flatten(x)\n        logits = self.linear_relu_stack(x)\n        return logits",
      "X = torch.rand(1, 28, 28, device=device)\nlogits = model(X)\npred_probab = nn.Softmax(dim=1)(logits)\ny_pred = pred_probab.argmax(1)\nprint(f\"Predicted class: {y_pred}\")",
      "Predicted class: tensor([9], device='cuda:0')",
      "input_image = torch.rand(3,28,28)\nprint(input_image.size())",
      "torch.Size([3, 28, 28])",
      "torch.Size([3, 784])",
      "torch.Size([3, 20])",
      "seq_modules = nn.Sequential(\n    flatten,\n    layer1,\n    nn.ReLU(),\n    nn.Linear(20, 10)\n)\ninput_image = torch.rand(3,28,28)\nlogits = seq_modules(input_image)",
      "Model structure: NeuralNetwork(\n  (flatten): Flatten(start_dim=1, end_dim=-1)\n  (linear_relu_stack): Sequential(\n    (0): Linear(in_features=784, out_features=512, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=512, out_features=512, bias=True)\n    (3): ReLU()\n    (4): Linear(in_features=512, out_features=10, bias=True)\n  )\n)\n\n\nLayer: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values : tensor([[ 0.0112, -0.0107,  0.0107,  ...,  0.0025,  0.0020, -0.0080],\n        [-0.0212, -0.0289, -0.0342,  ...,  0.0296,  0.0090,  0.0253]],\n       device='cuda:0', grad_fn=<SliceBackward0>)\n\nLayer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values : tensor([-0.0064, -0.0080], device='cuda:0', grad_fn=<SliceBackward0>)\n\nLayer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values : tensor([[-0.0029, -0.0369,  0.0377,  ..., -0.0052, -0.0395,  0.0037],\n        [ 0.0122,  0.0287, -0.0040,  ...,  0.0237, -0.0357, -0.0297]],\n       device='cuda:0', grad_fn=<SliceBackward0>)\n\nLayer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values : tensor([-0.0130, -0.0037], device='cuda:0', grad_fn=<SliceBackward0>)\n\nLayer: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Values : tensor([[-0.0128, -0.0234,  0.0242,  ...,  0.0200,  0.0136, -0.0300],\n        [-0.0245, -0.0337,  0.0080,  ..., -0.0001,  0.0327, -0.0370]],\n       device='cuda:0', grad_fn=<SliceBackward0>)\n\nLayer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values : tensor([ 0.0047, -0.0385], device='cuda:0', grad_fn=<SliceBackward0>)"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html",
    "title": "Tensors\u00b6",
    "code_snippets": [
      "import torch\nimport numpy as np",
      "data = [[1, 2],[3, 4]]\nx_data = torch.tensor(data)",
      "np_array = np.array(data)\nx_np = torch.from_numpy(np_array)",
      "x_ones = torch.ones_like(x_data) # retains the properties of x_data\nprint(f\"Ones Tensor: \\n {x_ones} \\n\")\n\nx_rand = torch.rand_like(x_data, dtype=torch.float) # overrides the datatype of x_data\nprint(f\"Random Tensor: \\n {x_rand} \\n\")",
      "shape = (2,3,)\nrand_tensor = torch.rand(shape)\nones_tensor = torch.ones(shape)\nzeros_tensor = torch.zeros(shape)\n\nprint(f\"Random Tensor: \\n {rand_tensor} \\n\")\nprint(f\"Ones Tensor: \\n {ones_tensor} \\n\")\nprint(f\"Zeros Tensor: \\n {zeros_tensor}\")",
      "tensor = torch.rand(3,4)\n\nprint(f\"Shape of tensor: {tensor.shape}\")\nprint(f\"Datatype of tensor: {tensor.dtype}\")\nprint(f\"Device tensor is stored on: {tensor.device}\")",
      "Shape of tensor: torch.Size([3, 4])\nDatatype of tensor: torch.float32\nDevice tensor is stored on: cpu",
      "# We move our tensor to the current accelerator if available\nif torch.accelerator.is_available():\n    tensor = tensor.to(torch.accelerator.current_accelerator())",
      "tensor = torch.ones(4, 4)\nprint(f\"First row: {tensor[0]}\")\nprint(f\"First column: {tensor[:, 0]}\")\nprint(f\"Last column: {tensor[..., -1]}\")\ntensor[:,1] = 0\nprint(tensor)",
      "t1 = torch.cat([tensor, tensor, tensor], dim=1)\nprint(t1)",
      "# This computes the matrix multiplication between two tensors. y1, y2, y3 will have the same value\n# ``tensor.T`` returns the transpose of a tensor\ny1 = tensor @ tensor.T\ny2 = tensor.matmul(tensor.T)\n\ny3 = torch.rand_like(y1)\ntorch.matmul(tensor, tensor.T, out=y3)\n\n\n# This computes the element-wise product. z1, z2, z3 will have the same value\nz1 = tensor * tensor\nz2 = tensor.mul(tensor)\n\nz3 = torch.rand_like(tensor)\ntorch.mul(tensor, tensor, out=z3)",
      "12.0 <class 'float'>",
      "t = torch.ones(5)\nprint(f\"t: {t}\")\nn = t.numpy()\nprint(f\"n: {n}\")",
      "n = np.ones(5)\nt = torch.from_numpy(n)",
      "t: tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\nn: [2. 2. 2. 2. 2.]"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/recipes/torch_compiler_set_stance_tutorial.html",
    "title": "Dynamic Compilation Control with torch.compiler.set_stance\u00b6",
    "code_snippets": [
      "import torch\n\n\n@torch.compile\ndef foo(x):\n    if torch.compiler.is_compiling():\n        # torch.compile is active\n        return x + 1\n    else:\n        # torch.compile is not active\n        return x - 1\n\n\ninp = torch.zeros(3)\n\nprint(foo(inp))  # compiled, prints 1",
      "@torch.compiler.set_stance(\"force_eager\")\ndef bar(x):\n    # force disable the compiler\n    return foo(x)\n\n\nprint(bar(inp))  # not compiled, prints -1",
      "with torch.compiler.set_stance(\"force_eager\"):\n    print(foo(inp))  # not compiled, prints -1",
      "torch.compiler.set_stance(\"force_eager\")\nprint(foo(inp))  # not compiled, prints -1\ntorch.compiler.set_stance(\"default\")\n\nprint(foo(inp))  # compiled, prints 1",
      "@torch.compile\ndef baz(x):\n    # error!\n    with torch.compiler.set_stance(\"force_eager\"):\n        return x + 1\n\n\ntry:\n    baz(inp)\nexcept Exception as e:\n    print(e)\n\n\n@torch.compiler.set_stance(\"force_eager\")\ndef inner(x):\n    return x + 1\n\n\n@torch.compile\ndef outer(x):\n    # error!\n    return inner(x)\n\n\ntry:\n    outer(inp)\nexcept Exception as e:\n    print(e)",
      "Attempt to trace forbidden callable <function set_stance at 0x7f40d2c84dc0>\n\nfrom user code:\n   File \"/var/lib/workspace/recipes_source/torch_compiler_set_stance_tutorial.py\", line 85, in baz\n    with torch.compiler.set_stance(\"force_eager\"):\n\nSet TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=\"+dynamo\"\n\nAttempt to trace forbidden callable <function inner at 0x7f40eb42f490>\n\nfrom user code:\n   File \"/var/lib/workspace/recipes_source/torch_compiler_set_stance_tutorial.py\", line 103, in outer\n    return inner(x)\n\nSet TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=\"+dynamo\"",
      "@torch.compile\ndef my_big_model(x):\n    return torch.relu(x)\n\n\n# first compilation\nmy_big_model(torch.randn(3))\n\nwith torch.compiler.set_stance(\"fail_on_recompile\"):\n    my_big_model(torch.randn(3))  # no recompilation - OK\n    try:\n        my_big_model(torch.randn(4))  # recompilation - error\n    except Exception as e:\n        print(e)",
      "Detected recompile when torch.compile stance is 'fail_on_recompile'",
      "@torch.compile\ndef my_huge_model(x):\n    if torch.compiler.is_compiling():\n        return x + 1\n    else:\n        return x - 1\n\n\n# first compilation\nprint(my_huge_model(torch.zeros(3)))  # 1\n\nwith torch.compiler.set_stance(\"eager_on_recompile\"):\n    print(my_huge_model(torch.zeros(3)))  # 1\n    print(my_huge_model(torch.zeros(4)))  # -1\n    print(my_huge_model(torch.zeros(3)))  # 1",
      "# Returns the result of running `fn()` and the time it took for `fn()` to run,\n# in seconds. We use CUDA events and synchronization for the most accurate\n# measurements.\ndef timed(fn):\n    start = torch.cuda.Event(enable_timing=True)\n    end = torch.cuda.Event(enable_timing=True)\n    start.record()\n    result = fn()\n    end.record()\n    torch.cuda.synchronize()\n    return result, start.elapsed_time(end) / 1000\n\n\n@torch.compile\ndef my_gigantic_model(x, y):\n    x = x @ y\n    x = x @ y\n    x = x @ y\n    return x\n\n\ninps = torch.randn(5, 5), torch.randn(5, 5)\n\nwith torch.compiler.set_stance(\"force_eager\"):\n    print(\"eager:\", timed(lambda: my_gigantic_model(*inps))[1])\n\n# warmups\nfor _ in range(3):\n    my_gigantic_model(*inps)\n\nprint(\"compiled:\", timed(lambda: my_gigantic_model(*inps))[1])",
      "@torch.compile\ndef my_humongous_model(x):\n    return torch.sin(x, x)\n\n\ntry:\n    with torch.compiler.set_stance(\"force_eager\"):\n        print(my_humongous_model(torch.randn(3)))\n    # this call to the compiled model won't run\n    print(my_humongous_model(torch.randn(3)))\nexcept Exception as e:\n    print(e)"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/advanced/cpp_autograd.html",
    "title": "Autograd in C++ Frontend\u00b6",
    "code_snippets": [
      "#include <torch/torch.h>\n\nauto model = torch::nn::Linear(4, 3);\n\nauto input = torch::randn({3, 4}).requires_grad_(true);\nauto output = model(input);\n\n// Calculate loss\nauto target = torch::randn({3, 3});\nauto loss = torch::nn::MSELoss()(output, target);\n\n// Use norm of gradients as penalty\nauto grad_output = torch::ones_like(output);\nauto gradient = torch::autograd::grad({output}, {input}, /*grad_outputs=*/{grad_output}, /*create_graph=*/true)[0];\nauto gradient_penalty = torch::pow((gradient.norm(2, /*dim=*/1) - 1), 2).mean();\n\n// Add gradient penalty to loss\nauto combined_loss = loss + gradient_penalty;\ncombined_loss.backward();\n\nstd::cout << input.grad() << std::endl;",
      "#include <torch/torch.h>\n\nusing namespace torch::autograd;\n\n// Inherit from Function\nclass LinearFunction : public Function<LinearFunction> {\n public:\n  // Note that both forward and backward are static functions\n\n  // bias is an optional argument\n  static torch::Tensor forward(\n      AutogradContext *ctx, torch::Tensor input, torch::Tensor weight, torch::Tensor bias = torch::Tensor()) {\n    ctx->save_for_backward({input, weight, bias});\n    auto output = input.mm(weight.t());\n    if (bias.defined()) {\n      output += bias.unsqueeze(0).expand_as(output);\n    }\n    return output;\n  }\n\n  static tensor_list backward(AutogradContext *ctx, tensor_list grad_outputs) {\n    auto saved = ctx->get_saved_variables();\n    auto input = saved[0];\n    auto weight = saved[1];\n    auto bias = saved[2];\n\n    auto grad_output = grad_outputs[0];\n    auto grad_input = grad_output.mm(weight);\n    auto grad_weight = grad_output.t().mm(input);\n    auto grad_bias = torch::Tensor();\n    if (bias.defined()) {\n      grad_bias = grad_output.sum(0);\n    }\n\n    return {grad_input, grad_weight, grad_bias};\n  }\n};",
      "#include <torch/torch.h>\n\nusing namespace torch::autograd;\n\nclass MulConstant : public Function<MulConstant> {\n public:\n  static torch::Tensor forward(AutogradContext *ctx, torch::Tensor tensor, double constant) {\n    // ctx is a context object that can be used to stash information\n    // for backward computation\n    ctx->saved_data[\"constant\"] = constant;\n    return tensor * constant;\n  }\n\n  static tensor_list backward(AutogradContext *ctx, tensor_list grad_outputs) {\n    // We return as many input gradients as there were arguments.\n    // Gradients of non-tensor arguments to forward must be `torch::Tensor()`.\n    return {grad_outputs[0] * ctx->saved_data[\"constant\"].toDouble(), torch::Tensor()};\n  }\n};"
    ]
  },
  {
    "url": "https://pytorch.org/tutorials/intermediate/flask_rest_api_tutorial.html",
    "title": "Deploying PyTorch in Python via a REST API with Flask\u00b6",
    "code_snippets": [
      "{\"class_id\": \"n02124075\", \"class_name\": \"Egyptian_cat\"}",
      "from flask import Flask\napp = Flask(__name__)\n\n\n@app.route('/')\ndef hello():\n    return 'Hello World!'",
      "from flask import Flask, jsonify\napp = Flask(__name__)\n\n@app.route('/predict', methods=['POST'])\ndef predict():\n    return jsonify({'class_id': 'IMAGE_NET_XXX', 'class_name': 'Cat'})",
      "import io\n\nimport torchvision.transforms as transforms\nfrom PIL import Image\n\ndef transform_image(image_bytes):\n    my_transforms = transforms.Compose([transforms.Resize(255),\n                                        transforms.CenterCrop(224),\n                                        transforms.ToTensor(),\n                                        transforms.Normalize(\n                                            [0.485, 0.456, 0.406],\n                                            [0.229, 0.224, 0.225])])\n    image = Image.open(io.BytesIO(image_bytes))\n    return my_transforms(image).unsqueeze(0)",
      "from torchvision import models\n\n# Make sure to set `weights` as `'IMAGENET1K_V1'` to use the pretrained weights:\nmodel = models.densenet121(weights='IMAGENET1K_V1')\n# Since we are using our model only for inference, switch to `eval` mode:\nmodel.eval()\n\n\ndef get_prediction(image_bytes):\n    tensor = transform_image(image_bytes=image_bytes)\n    outputs = model.forward(tensor)\n    _, y_hat = outputs.max(1)\n    return y_hat",
      "import json\n\nimagenet_class_index = json.load(open('../_static/imagenet_class_index.json'))\n\ndef get_prediction(image_bytes):\n    tensor = transform_image(image_bytes=image_bytes)\n    outputs = model.forward(tensor)\n    _, y_hat = outputs.max(1)\n    predicted_idx = str(y_hat.item())\n    return imagenet_class_index[predicted_idx]",
      "from flask import request\n\n@app.route('/predict', methods=['POST'])\ndef predict():\n    if request.method == 'POST':\n        # we will get the file from the request\n        file = request.files['file']\n        # convert that to bytes\n        img_bytes = file.read()\n        class_id, class_name = get_prediction(image_bytes=img_bytes)\n        return jsonify({'class_id': class_id, 'class_name': class_name})",
      "import io\nimport json\n\nfrom torchvision import models\nimport torchvision.transforms as transforms\nfrom PIL import Image\nfrom flask import Flask, jsonify, request\n\n\napp = Flask(__name__)\nimagenet_class_index = json.load(open('<PATH/TO/.json/FILE>/imagenet_class_index.json'))\nmodel = models.densenet121(weights='IMAGENET1K_V1')\nmodel.eval()\n\n\ndef transform_image(image_bytes):\n    my_transforms = transforms.Compose([transforms.Resize(255),\n                                        transforms.CenterCrop(224),\n                                        transforms.ToTensor(),\n                                        transforms.Normalize(\n                                            [0.485, 0.456, 0.406],\n                                            [0.229, 0.224, 0.225])])\n    image = Image.open(io.BytesIO(image_bytes))\n    return my_transforms(image).unsqueeze(0)\n\n\ndef get_prediction(image_bytes):\n    tensor = transform_image(image_bytes=image_bytes)\n    outputs = model.forward(tensor)\n    _, y_hat = outputs.max(1)\n    predicted_idx = str(y_hat.item())\n    return imagenet_class_index[predicted_idx]\n\n\n@app.route('/predict', methods=['POST'])\ndef predict():\n    if request.method == 'POST':\n        file = request.files['file']\n        img_bytes = file.read()\n        class_id, class_name = get_prediction(image_bytes=img_bytes)\n        return jsonify({'class_id': class_id, 'class_name': class_name})\n\n\nif __name__ == '__main__':\n    app.run()",
      "import requests\n\nresp = requests.post(\"http://localhost:5000/predict\",\n                     files={\"file\": open('<PATH/TO/.jpg/FILE>/cat.jpg','rb')})",
      "{\"class_id\": \"n02124075\", \"class_name\": \"Egyptian_cat\"}"
    ]
  }
]