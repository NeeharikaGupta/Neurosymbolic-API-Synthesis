[
  {
    "url": "https://pandas.pydata.org/docs/user_guide/10min.html",
    "title": "10 minutes to pandas#",
    "code_snippets": [
      "In [1]: import numpy as np\n\nIn [2]: import pandas as pd",
      "In [3]: s = pd.Series([1, 3, 5, np.nan, 6, 8])\n\nIn [4]: s\nOut[4]: \n0    1.0\n1    3.0\n2    5.0\n3    NaN\n4    6.0\n5    8.0\ndtype: float64",
      "In [5]: dates = pd.date_range(\"20130101\", periods=6)\n\nIn [6]: dates\nOut[6]: \nDatetimeIndex(['2013-01-01', '2013-01-02', '2013-01-03', '2013-01-04',\n               '2013-01-05', '2013-01-06'],\n              dtype='datetime64[ns]', freq='D')\n\nIn [7]: df = pd.DataFrame(np.random.randn(6, 4), index=dates, columns=list(\"ABCD\"))\n\nIn [8]: df\nOut[8]: \n                   A         B         C         D\n2013-01-01  0.469112 -0.282863 -1.509059 -1.135632\n2013-01-02  1.212112 -0.173215  0.119209 -1.044236\n2013-01-03 -0.861849 -2.104569 -0.494929  1.071804\n2013-01-04  0.721555 -0.706771 -1.039575  0.271860\n2013-01-05 -0.424972  0.567020  0.276232 -1.087401\n2013-01-06 -0.673690  0.113648 -1.478427  0.524988",
      "In [9]: df2 = pd.DataFrame(\n   ...:     {\n   ...:         \"A\": 1.0,\n   ...:         \"B\": pd.Timestamp(\"20130102\"),\n   ...:         \"C\": pd.Series(1, index=list(range(4)), dtype=\"float32\"),\n   ...:         \"D\": np.array([3] * 4, dtype=\"int32\"),\n   ...:         \"E\": pd.Categorical([\"test\", \"train\", \"test\", \"train\"]),\n   ...:         \"F\": \"foo\",\n   ...:     }\n   ...: )\n   ...: \n\nIn [10]: df2\nOut[10]: \n     A          B    C  D      E    F\n0  1.0 2013-01-02  1.0  3   test  foo\n1  1.0 2013-01-02  1.0  3  train  foo\n2  1.0 2013-01-02  1.0  3   test  foo\n3  1.0 2013-01-02  1.0  3  train  foo",
      "In [45]: s1 = pd.Series([1, 2, 3, 4, 5, 6], index=pd.date_range(\"20130102\", periods=6))\n\nIn [46]: s1\nOut[46]: \n2013-01-02    1\n2013-01-03    2\n2013-01-04    3\n2013-01-05    4\n2013-01-06    5\n2013-01-07    6\nFreq: D, dtype: int64\n\nIn [47]: df[\"F\"] = s1",
      "In [60]: pd.isna(df1)\nOut[60]: \n                A      B      C      D      F      E\n2013-01-01  False  False  False  False   True  False\n2013-01-02  False  False  False  False  False  False\n2013-01-03  False  False  False  False  False   True\n2013-01-04  False  False  False  False  False   True",
      "In [63]: s = pd.Series([1, 3, 5, np.nan, 6, 8], index=dates).shift(2)\n\nIn [64]: s\nOut[64]: \n2013-01-01    NaN\n2013-01-02    NaN\n2013-01-03    1.0\n2013-01-04    3.0\n2013-01-05    5.0\n2013-01-06    NaN\nFreq: D, dtype: float64\n\nIn [65]: df.sub(s, axis=\"index\")\nOut[65]: \n                   A         B         C    D    F\n2013-01-01       NaN       NaN       NaN  NaN  NaN\n2013-01-02       NaN       NaN       NaN  NaN  NaN\n2013-01-03 -1.861849 -3.104569 -1.494929  4.0  1.0\n2013-01-04 -2.278445 -3.706771 -4.039575  2.0  0.0\n2013-01-05 -5.424972 -4.432980 -4.723768  0.0 -1.0\n2013-01-06       NaN       NaN       NaN  NaN  NaN",
      "In [68]: s = pd.Series(np.random.randint(0, 7, size=10))\n\nIn [69]: s\nOut[69]: \n0    4\n1    2\n2    1\n3    2\n4    6\n5    4\n6    4\n7    6\n8    4\n9    4\ndtype: int64\n\nIn [70]: s.value_counts()\nOut[70]: \n4    5\n2    2\n6    2\n1    1\nName: count, dtype: int64",
      "In [71]: s = pd.Series([\"A\", \"B\", \"C\", \"Aaba\", \"Baca\", np.nan, \"CABA\", \"dog\", \"cat\"])\n\nIn [72]: s.str.lower()\nOut[72]: \n0       a\n1       b\n2       c\n3    aaba\n4    baca\n5     NaN\n6    caba\n7     dog\n8     cat\ndtype: object",
      "In [73]: df = pd.DataFrame(np.random.randn(10, 4))\n\nIn [74]: df\nOut[74]: \n          0         1         2         3\n0 -0.548702  1.467327 -1.015962 -0.483075\n1  1.637550 -1.217659 -0.291519 -1.745505\n2 -0.263952  0.991460 -0.919069  0.266046\n3 -0.709661  1.669052  1.037882 -1.705775\n4 -0.919854 -0.042379  1.247642 -0.009920\n5  0.290213  0.495767  0.362949  1.548106\n6 -1.131345 -0.089329  0.337863 -0.945867\n7 -0.932132  1.956030  0.017587 -0.016692\n8 -0.575247  0.254161 -1.143704  0.215897\n9  1.193555 -0.077118 -0.408530 -0.862495\n\n# break it into pieces\nIn [75]: pieces = [df[:3], df[3:7], df[7:]]\n\nIn [76]: pd.concat(pieces)\nOut[76]: \n          0         1         2         3\n0 -0.548702  1.467327 -1.015962 -0.483075\n1  1.637550 -1.217659 -0.291519 -1.745505\n2 -0.263952  0.991460 -0.919069  0.266046\n3 -0.709661  1.669052  1.037882 -1.705775\n4 -0.919854 -0.042379  1.247642 -0.009920\n5  0.290213  0.495767  0.362949  1.548106\n6 -1.131345 -0.089329  0.337863 -0.945867\n7 -0.932132  1.956030  0.017587 -0.016692\n8 -0.575247  0.254161 -1.143704  0.215897\n9  1.193555 -0.077118 -0.408530 -0.862495",
      "In [77]: left = pd.DataFrame({\"key\": [\"foo\", \"foo\"], \"lval\": [1, 2]})\n\nIn [78]: right = pd.DataFrame({\"key\": [\"foo\", \"foo\"], \"rval\": [4, 5]})\n\nIn [79]: left\nOut[79]: \n   key  lval\n0  foo     1\n1  foo     2\n\nIn [80]: right\nOut[80]: \n   key  rval\n0  foo     4\n1  foo     5\n\nIn [81]: pd.merge(left, right, on=\"key\")\nOut[81]: \n   key  lval  rval\n0  foo     1     4\n1  foo     1     5\n2  foo     2     4\n3  foo     2     5",
      "In [82]: left = pd.DataFrame({\"key\": [\"foo\", \"bar\"], \"lval\": [1, 2]})\n\nIn [83]: right = pd.DataFrame({\"key\": [\"foo\", \"bar\"], \"rval\": [4, 5]})\n\nIn [84]: left\nOut[84]: \n   key  lval\n0  foo     1\n1  bar     2\n\nIn [85]: right\nOut[85]: \n   key  rval\n0  foo     4\n1  bar     5\n\nIn [86]: pd.merge(left, right, on=\"key\")\nOut[86]: \n   key  lval  rval\n0  foo     1     4\n1  bar     2     5",
      "In [87]: df = pd.DataFrame(\n   ....:     {\n   ....:         \"A\": [\"foo\", \"bar\", \"foo\", \"bar\", \"foo\", \"bar\", \"foo\", \"foo\"],\n   ....:         \"B\": [\"one\", \"one\", \"two\", \"three\", \"two\", \"two\", \"one\", \"three\"],\n   ....:         \"C\": np.random.randn(8),\n   ....:         \"D\": np.random.randn(8),\n   ....:     }\n   ....: )\n   ....: \n\nIn [88]: df\nOut[88]: \n     A      B         C         D\n0  foo    one  1.346061 -1.577585\n1  bar    one  1.511763  0.396823\n2  foo    two  1.627081 -0.105381\n3  bar  three -0.990582 -0.532532\n4  foo    two -0.441652  1.453749\n5  bar    two  1.211526  1.208843\n6  foo    one  0.268520 -0.080952\n7  foo  three  0.024580 -0.264610",
      "In [91]: arrays = [\n   ....:    [\"bar\", \"bar\", \"baz\", \"baz\", \"foo\", \"foo\", \"qux\", \"qux\"],\n   ....:    [\"one\", \"two\", \"one\", \"two\", \"one\", \"two\", \"one\", \"two\"],\n   ....: ]\n   ....: \n\nIn [92]: index = pd.MultiIndex.from_arrays(arrays, names=[\"first\", \"second\"])\n\nIn [93]: df = pd.DataFrame(np.random.randn(8, 2), index=index, columns=[\"A\", \"B\"])\n\nIn [94]: df2 = df[:4]\n\nIn [95]: df2\nOut[95]: \n                     A         B\nfirst second                    \nbar   one    -0.727965 -0.589346\n      two     0.339969 -0.693205\nbaz   one    -0.339355  0.593616\n      two     0.884345  1.591431",
      "In [101]: df = pd.DataFrame(\n   .....:     {\n   .....:         \"A\": [\"one\", \"one\", \"two\", \"three\"] * 3,\n   .....:         \"B\": [\"A\", \"B\", \"C\"] * 4,\n   .....:         \"C\": [\"foo\", \"foo\", \"foo\", \"bar\", \"bar\", \"bar\"] * 2,\n   .....:         \"D\": np.random.randn(12),\n   .....:         \"E\": np.random.randn(12),\n   .....:     }\n   .....: )\n   .....: \n\nIn [102]: df\nOut[102]: \n        A  B    C         D         E\n0     one  A  foo -1.202872  0.047609\n1     one  B  foo -1.814470 -0.136473\n2     two  C  foo  1.018601 -0.561757\n3   three  A  bar -0.595447 -1.623033\n4     one  B  bar  1.395433  0.029399\n5     one  C  bar -0.392670 -0.542108\n6     two  A  foo  0.007207  0.282696\n7   three  B  foo  1.928123 -0.087302\n8     one  C  foo -0.055224 -1.575170\n9     one  A  bar  2.395985  1.771208\n10    two  B  bar  1.552825  0.816482\n11  three  C  bar  0.166599  1.100230",
      "In [103]: pd.pivot_table(df, values=\"D\", index=[\"A\", \"B\"], columns=[\"C\"])\nOut[103]: \nC             bar       foo\nA     B                    \none   A  2.395985 -1.202872\n      B  1.395433 -1.814470\n      C -0.392670 -0.055224\nthree A -0.595447       NaN\n      B       NaN  1.928123\n      C  0.166599       NaN\ntwo   A       NaN  0.007207\n      B  1.552825       NaN\n      C       NaN  1.018601",
      "In [104]: rng = pd.date_range(\"1/1/2012\", periods=100, freq=\"s\")\n\nIn [105]: ts = pd.Series(np.random.randint(0, 500, len(rng)), index=rng)\n\nIn [106]: ts.resample(\"5Min\").sum()\nOut[106]: \n2012-01-01    24182\nFreq: 5min, dtype: int64",
      "In [107]: rng = pd.date_range(\"3/6/2012 00:00\", periods=5, freq=\"D\")\n\nIn [108]: ts = pd.Series(np.random.randn(len(rng)), rng)\n\nIn [109]: ts\nOut[109]: \n2012-03-06    1.857704\n2012-03-07   -1.193545\n2012-03-08    0.677510\n2012-03-09   -0.153931\n2012-03-10    0.520091\nFreq: D, dtype: float64\n\nIn [110]: ts_utc = ts.tz_localize(\"UTC\")\n\nIn [111]: ts_utc\nOut[111]: \n2012-03-06 00:00:00+00:00    1.857704\n2012-03-07 00:00:00+00:00   -1.193545\n2012-03-08 00:00:00+00:00    0.677510\n2012-03-09 00:00:00+00:00   -0.153931\n2012-03-10 00:00:00+00:00    0.520091\nFreq: D, dtype: float64",
      "In [113]: rng\nOut[113]: \nDatetimeIndex(['2012-03-06', '2012-03-07', '2012-03-08', '2012-03-09',\n               '2012-03-10'],\n              dtype='datetime64[ns]', freq='D')\n\nIn [114]: rng + pd.offsets.BusinessDay(5)\nOut[114]: \nDatetimeIndex(['2012-03-13', '2012-03-14', '2012-03-15', '2012-03-16',\n               '2012-03-16'],\n              dtype='datetime64[ns]', freq=None)",
      "In [115]: df = pd.DataFrame(\n   .....:     {\"id\": [1, 2, 3, 4, 5, 6], \"raw_grade\": [\"a\", \"b\", \"b\", \"a\", \"a\", \"e\"]}\n   .....: )\n   .....:",
      "In [124]: import matplotlib.pyplot as plt\n\nIn [125]: plt.close(\"all\")",
      "In [126]: ts = pd.Series(np.random.randn(1000), index=pd.date_range(\"1/1/2000\", periods=1000))\n\nIn [127]: ts = ts.cumsum()\n\nIn [128]: ts.plot();",
      "In [129]: df = pd.DataFrame(\n   .....:     np.random.randn(1000, 4), index=ts.index, columns=[\"A\", \"B\", \"C\", \"D\"]\n   .....: )\n   .....: \n\nIn [130]: df = df.cumsum()\n\nIn [131]: plt.figure();\n\nIn [132]: df.plot();\n\nIn [133]: plt.legend(loc='best');",
      "In [134]: df = pd.DataFrame(np.random.randint(0, 5, (10, 5)))\n\nIn [135]: df.to_csv(\"foo.csv\")",
      "In [136]: pd.read_csv(\"foo.csv\")\nOut[136]: \n   Unnamed: 0  0  1  2  3  4\n0           0  4  3  1  1  2\n1           1  1  0  2  3  2\n2           2  1  4  2  1  2\n3           3  0  4  0  2  2\n4           4  4  2  2  3  4\n5           5  4  0  4  3  1\n6           6  2  1  2  0  3\n7           7  4  0  4  4  4\n8           8  4  4  1  0  1\n9           9  0  4  3  0  3",
      "In [138]: pd.read_parquet(\"foo.parquet\")\nOut[138]: \n   0  1  2  3  4\n0  4  3  1  1  2\n1  1  0  2  3  2\n2  1  4  2  1  2\n3  0  4  0  2  2\n4  4  2  2  3  4\n5  4  0  4  3  1\n6  2  1  2  0  3\n7  4  0  4  4  4\n8  4  4  1  0  1\n9  0  4  3  0  3",
      "In [140]: pd.read_excel(\"foo.xlsx\", \"Sheet1\", index_col=None, na_values=[\"NA\"])\nOut[140]: \n   Unnamed: 0  0  1  2  3  4\n0           0  4  3  1  1  2\n1           1  1  0  2  3  2\n2           2  1  4  2  1  2\n3           3  0  4  0  2  2\n4           4  4  2  2  3  4\n5           5  4  0  4  3  1\n6           6  2  1  2  0  3\n7           7  4  0  4  4  4\n8           8  4  4  1  0  1\n9           9  0  4  3  0  3",
      "In [141]: if pd.Series([False, True, False]):\n   .....:      print(\"I was true\")\n   .....: \n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n<ipython-input-141-b27eb9c1dfc0> in ?()\n----> 1 if pd.Series([False, True, False]):\n      2      print(\"I was true\")\n\n~/work/pandas/pandas/pandas/core/generic.py in ?(self)\n   1575     @final\n   1576     def __nonzero__(self) -> NoReturn:\n-> 1577         raise ValueError(\n   1578             f\"The truth value of a {type(self).__name__} is ambiguous. \"\n   1579             \"Use a.empty, a.bool(), a.item(), a.any() or a.all().\"\n   1580         )\n\nValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()."
    ]
  },
  {
    "url": "https://pandas.pydata.org/docs/user_guide/dsintro.html",
    "title": "Intro to data structures#",
    "code_snippets": [
      "In [1]: import numpy as np\n\nIn [2]: import pandas as pd",
      "s = pd.Series(data, index=index)",
      "In [3]: s = pd.Series(np.random.randn(5), index=[\"a\", \"b\", \"c\", \"d\", \"e\"])\n\nIn [4]: s\nOut[4]: \na    0.469112\nb   -0.282863\nc   -1.509059\nd   -1.135632\ne    1.212112\ndtype: float64\n\nIn [5]: s.index\nOut[5]: Index(['a', 'b', 'c', 'd', 'e'], dtype='object')\n\nIn [6]: pd.Series(np.random.randn(5))\nOut[6]: \n0   -0.173215\n1    0.119209\n2   -1.044236\n3   -0.861849\n4   -2.104569\ndtype: float64",
      "In [7]: d = {\"b\": 1, \"a\": 0, \"c\": 2}\n\nIn [8]: pd.Series(d)\nOut[8]: \nb    1\na    0\nc    2\ndtype: int64",
      "In [9]: d = {\"a\": 0.0, \"b\": 1.0, \"c\": 2.0}\n\nIn [10]: pd.Series(d)\nOut[10]: \na    0.0\nb    1.0\nc    2.0\ndtype: float64\n\nIn [11]: pd.Series(d, index=[\"b\", \"c\", \"d\", \"a\"])\nOut[11]: \nb    1.0\nc    2.0\nd    NaN\na    0.0\ndtype: float64",
      "In [12]: pd.Series(5.0, index=[\"a\", \"b\", \"c\", \"d\", \"e\"])\nOut[12]: \na    5.0\nb    5.0\nc    5.0\nd    5.0\ne    5.0\ndtype: float64",
      "In [26]: s[\"f\"]\n---------------------------------------------------------------------------\nKeyError                                  Traceback (most recent call last)\nFile ~/work/pandas/pandas/pandas/core/indexes/base.py:3805, in Index.get_loc(self, key)\n   3804 try:\n-> 3805     return self._engine.get_loc(casted_key)\n   3806 except KeyError as err:\n\nFile index.pyx:167, in pandas._libs.index.IndexEngine.get_loc()\n\nFile index.pyx:196, in pandas._libs.index.IndexEngine.get_loc()\n\nFile pandas/_libs/hashtable_class_helper.pxi:7081, in pandas._libs.hashtable.PyObjectHashTable.get_item()\n\nFile pandas/_libs/hashtable_class_helper.pxi:7089, in pandas._libs.hashtable.PyObjectHashTable.get_item()\n\nKeyError: 'f'\n\nThe above exception was the direct cause of the following exception:\n\nKeyError                                  Traceback (most recent call last)\nCell In[26], line 1\n----> 1 s[\"f\"]\n\nFile ~/work/pandas/pandas/pandas/core/series.py:1121, in Series.__getitem__(self, key)\n   1118     return self._values[key]\n   1120 elif key_is_scalar:\n-> 1121     return self._get_value(key)\n   1123 # Convert generator to list before going through hashable part\n   1124 # (We will iterate through the generator there to check for slices)\n   1125 if is_iterator(key):\n\nFile ~/work/pandas/pandas/pandas/core/series.py:1237, in Series._get_value(self, label, takeable)\n   1234     return self._values[label]\n   1236 # Similar to Index.get_value, but we do not fall back to positional\n-> 1237 loc = self.index.get_loc(label)\n   1239 if is_integer(loc):\n   1240     return self._values[loc]\n\nFile ~/work/pandas/pandas/pandas/core/indexes/base.py:3812, in Index.get_loc(self, key)\n   3807     if isinstance(casted_key, slice) or (\n   3808         isinstance(casted_key, abc.Iterable)\n   3809         and any(isinstance(x, slice) for x in casted_key)\n   3810     ):\n   3811         raise InvalidIndexError(key)\n-> 3812     raise KeyError(key) from err\n   3813 except TypeError:\n   3814     # If we have a listlike key, _check_indexing_error will raise\n   3815     #  InvalidIndexError. Otherwise we fall through and re-raise\n   3816     #  the TypeError.\n   3817     self._check_indexing_error(key)\n\nKeyError: 'f'",
      "In [33]: s = pd.Series(np.random.randn(5), name=\"something\")\n\nIn [34]: s\nOut[34]: \n0   -0.494929\n1    1.071804\n2    0.721555\n3   -0.706771\n4   -1.039575\nName: something, dtype: float64\n\nIn [35]: s.name\nOut[35]: 'something'",
      "In [38]: d = {\n   ....:     \"one\": pd.Series([1.0, 2.0, 3.0], index=[\"a\", \"b\", \"c\"]),\n   ....:     \"two\": pd.Series([1.0, 2.0, 3.0, 4.0], index=[\"a\", \"b\", \"c\", \"d\"]),\n   ....: }\n   ....: \n\nIn [39]: df = pd.DataFrame(d)\n\nIn [40]: df\nOut[40]: \n   one  two\na  1.0  1.0\nb  2.0  2.0\nc  3.0  3.0\nd  NaN  4.0\n\nIn [41]: pd.DataFrame(d, index=[\"d\", \"b\", \"a\"])\nOut[41]: \n   one  two\nd  NaN  4.0\nb  2.0  2.0\na  1.0  1.0\n\nIn [42]: pd.DataFrame(d, index=[\"d\", \"b\", \"a\"], columns=[\"two\", \"three\"])\nOut[42]: \n   two three\nd  4.0   NaN\nb  2.0   NaN\na  1.0   NaN",
      "In [45]: d = {\"one\": [1.0, 2.0, 3.0, 4.0], \"two\": [4.0, 3.0, 2.0, 1.0]}\n\nIn [46]: pd.DataFrame(d)\nOut[46]: \n   one  two\n0  1.0  4.0\n1  2.0  3.0\n2  3.0  2.0\n3  4.0  1.0\n\nIn [47]: pd.DataFrame(d, index=[\"a\", \"b\", \"c\", \"d\"])\nOut[47]: \n   one  two\na  1.0  4.0\nb  2.0  3.0\nc  3.0  2.0\nd  4.0  1.0",
      "In [48]: data = np.zeros((2,), dtype=[(\"A\", \"i4\"), (\"B\", \"f4\"), (\"C\", \"a10\")])\n\nIn [49]: data[:] = [(1, 2.0, \"Hello\"), (2, 3.0, \"World\")]\n\nIn [50]: pd.DataFrame(data)\nOut[50]: \n   A    B         C\n0  1  2.0  b'Hello'\n1  2  3.0  b'World'\n\nIn [51]: pd.DataFrame(data, index=[\"first\", \"second\"])\nOut[51]: \n        A    B         C\nfirst   1  2.0  b'Hello'\nsecond  2  3.0  b'World'\n\nIn [52]: pd.DataFrame(data, columns=[\"C\", \"A\", \"B\"])\nOut[52]: \n          C  A    B\n0  b'Hello'  1  2.0\n1  b'World'  2  3.0",
      "In [53]: data2 = [{\"a\": 1, \"b\": 2}, {\"a\": 5, \"b\": 10, \"c\": 20}]\n\nIn [54]: pd.DataFrame(data2)\nOut[54]: \n   a   b     c\n0  1   2   NaN\n1  5  10  20.0\n\nIn [55]: pd.DataFrame(data2, index=[\"first\", \"second\"])\nOut[55]: \n        a   b     c\nfirst   1   2   NaN\nsecond  5  10  20.0\n\nIn [56]: pd.DataFrame(data2, columns=[\"a\", \"b\"])\nOut[56]: \n   a   b\n0  1   2\n1  5  10",
      "In [57]: pd.DataFrame(\n   ....:     {\n   ....:         (\"a\", \"b\"): {(\"A\", \"B\"): 1, (\"A\", \"C\"): 2},\n   ....:         (\"a\", \"a\"): {(\"A\", \"C\"): 3, (\"A\", \"B\"): 4},\n   ....:         (\"a\", \"c\"): {(\"A\", \"B\"): 5, (\"A\", \"C\"): 6},\n   ....:         (\"b\", \"a\"): {(\"A\", \"C\"): 7, (\"A\", \"B\"): 8},\n   ....:         (\"b\", \"b\"): {(\"A\", \"D\"): 9, (\"A\", \"B\"): 10},\n   ....:     }\n   ....: )\n   ....: \nOut[57]: \n       a              b      \n       b    a    c    a     b\nA B  1.0  4.0  5.0  8.0  10.0\n  C  2.0  3.0  6.0  7.0   NaN\n  D  NaN  NaN  NaN  NaN   9.0",
      "In [58]: ser = pd.Series(range(3), index=list(\"abc\"), name=\"ser\")\n\nIn [59]: pd.DataFrame(ser)\nOut[59]: \n   ser\na    0\nb    1\nc    2",
      "In [60]: from collections import namedtuple\n\nIn [61]: Point = namedtuple(\"Point\", \"x y\")\n\nIn [62]: pd.DataFrame([Point(0, 0), Point(0, 3), (2, 3)])\nOut[62]: \n   x  y\n0  0  0\n1  0  3\n2  2  3\n\nIn [63]: Point3D = namedtuple(\"Point3D\", \"x y z\")\n\nIn [64]: pd.DataFrame([Point3D(0, 0, 0), Point3D(0, 3, 5), Point(2, 3)])\nOut[64]: \n   x  y    z\n0  0  0  0.0\n1  0  3  5.0\n2  2  3  NaN",
      "In [65]: from dataclasses import make_dataclass\n\nIn [66]: Point = make_dataclass(\"Point\", [(\"x\", int), (\"y\", int)])\n\nIn [67]: pd.DataFrame([Point(0, 0), Point(0, 3), Point(2, 3)])\nOut[67]: \n   x  y\n0  0  0\n1  0  3\n2  2  3",
      "In [68]: pd.DataFrame.from_dict(dict([(\"A\", [1, 2, 3]), (\"B\", [4, 5, 6])]))\nOut[68]: \n   A  B\n0  1  4\n1  2  5\n2  3  6",
      "In [69]: pd.DataFrame.from_dict(\n   ....:     dict([(\"A\", [1, 2, 3]), (\"B\", [4, 5, 6])]),\n   ....:     orient=\"index\",\n   ....:     columns=[\"one\", \"two\", \"three\"],\n   ....: )\n   ....: \nOut[69]: \n   one  two  three\nA    1    2      3\nB    4    5      6",
      "In [70]: data\nOut[70]: \narray([(1, 2., b'Hello'), (2, 3., b'World')],\n      dtype=[('A', '<i4'), ('B', '<f4'), ('C', 'S10')])\n\nIn [71]: pd.DataFrame.from_records(data, index=\"C\")\nOut[71]: \n          A    B\nC               \nb'Hello'  1  2.0\nb'World'  2  3.0",
      "In [85]: iris = pd.read_csv(\"data/iris.data\")\n\nIn [86]: iris.head()\nOut[86]: \n   SepalLength  SepalWidth  PetalLength  PetalWidth         Name\n0          5.1         3.5          1.4         0.2  Iris-setosa\n1          4.9         3.0          1.4         0.2  Iris-setosa\n2          4.7         3.2          1.3         0.2  Iris-setosa\n3          4.6         3.1          1.5         0.2  Iris-setosa\n4          5.0         3.6          1.4         0.2  Iris-setosa\n\nIn [87]: iris.assign(sepal_ratio=iris[\"SepalWidth\"] / iris[\"SepalLength\"]).head()\nOut[87]: \n   SepalLength  SepalWidth  PetalLength  PetalWidth         Name  sepal_ratio\n0          5.1         3.5          1.4         0.2  Iris-setosa     0.686275\n1          4.9         3.0          1.4         0.2  Iris-setosa     0.612245\n2          4.7         3.2          1.3         0.2  Iris-setosa     0.680851\n3          4.6         3.1          1.5         0.2  Iris-setosa     0.673913\n4          5.0         3.6          1.4         0.2  Iris-setosa     0.720000",
      "In [90]: dfa = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\n\nIn [91]: dfa.assign(C=lambda x: x[\"A\"] + x[\"B\"], D=lambda x: x[\"A\"] + x[\"C\"])\nOut[91]: \n   A  B  C   D\n0  1  4  5   6\n1  2  5  7   9\n2  3  6  9  12",
      "In [94]: df = pd.DataFrame(np.random.randn(10, 4), columns=[\"A\", \"B\", \"C\", \"D\"])\n\nIn [95]: df2 = pd.DataFrame(np.random.randn(7, 3), columns=[\"A\", \"B\", \"C\"])\n\nIn [96]: df + df2\nOut[96]: \n          A         B         C   D\n0  0.045691 -0.014138  1.380871 NaN\n1 -0.955398 -1.501007  0.037181 NaN\n2 -0.662690  1.534833 -0.859691 NaN\n3 -2.452949  1.237274 -0.133712 NaN\n4  1.414490  1.951676 -2.320422 NaN\n5 -0.494922 -1.649727 -1.084601 NaN\n6 -1.047551 -0.748572 -0.805479 NaN\n7       NaN       NaN       NaN NaN\n8       NaN       NaN       NaN NaN\n9       NaN       NaN       NaN NaN",
      "In [101]: df1 = pd.DataFrame({\"a\": [1, 0, 1], \"b\": [0, 1, 1]}, dtype=bool)\n\nIn [102]: df2 = pd.DataFrame({\"a\": [0, 1, 1], \"b\": [1, 1, 0]}, dtype=bool)\n\nIn [103]: df1 & df2\nOut[103]: \n       a      b\n0  False  False\n1  False   True\n2   True  False\n\nIn [104]: df1 | df2\nOut[104]: \n      a     b\n0  True  True\n1  True  True\n2  True  True\n\nIn [105]: df1 ^ df2\nOut[105]: \n       a      b\n0   True   True\n1   True  False\n2  False   True\n\nIn [106]: -df1\nOut[106]: \n       a      b\n0  False   True\n1   True  False\n2  False  False",
      "In [110]: ser = pd.Series([1, 2, 3, 4])\n\nIn [111]: np.exp(ser)\nOut[111]: \n0     2.718282\n1     7.389056\n2    20.085537\n3    54.598150\ndtype: float64",
      "In [112]: ser1 = pd.Series([1, 2, 3], index=[\"a\", \"b\", \"c\"])\n\nIn [113]: ser2 = pd.Series([1, 3, 5], index=[\"b\", \"a\", \"c\"])\n\nIn [114]: ser1\nOut[114]: \na    1\nb    2\nc    3\ndtype: int64\n\nIn [115]: ser2\nOut[115]: \nb    1\na    3\nc    5\ndtype: int64\n\nIn [116]: np.remainder(ser1, ser2)\nOut[116]: \na    1\nb    0\nc    3\ndtype: int64",
      "In [117]: ser3 = pd.Series([2, 4, 6], index=[\"b\", \"c\", \"d\"])\n\nIn [118]: ser3\nOut[118]: \nb    2\nc    4\nd    6\ndtype: int64\n\nIn [119]: np.remainder(ser1, ser3)\nOut[119]: \na    NaN\nb    0.0\nc    3.0\nd    NaN\ndtype: float64",
      "In [120]: ser = pd.Series([1, 2, 3])\n\nIn [121]: idx = pd.Index([4, 5, 6])\n\nIn [122]: np.maximum(ser, idx)\nOut[122]: \n0    4\n1    5\n2    6\ndtype: int64",
      "In [123]: baseball = pd.read_csv(\"data/baseball.csv\")\n\nIn [124]: print(baseball)\n       id     player  year  stint team  lg  ...    so  ibb  hbp   sh   sf  gidp\n0   88641  womacto01  2006      2  CHN  NL  ...   4.0  0.0  0.0  3.0  0.0   0.0\n1   88643  schilcu01  2006      1  BOS  AL  ...   1.0  0.0  0.0  0.0  0.0   0.0\n..    ...        ...   ...    ...  ...  ..  ...   ...  ...  ...  ...  ...   ...\n98  89533   aloumo01  2007      1  NYN  NL  ...  30.0  5.0  2.0  0.0  3.0  13.0\n99  89534  alomasa02  2007      1  NYN  NL  ...   3.0  0.0  0.0  0.0  0.0   0.0\n\n[100 rows x 23 columns]\n\nIn [125]: baseball.info()\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 100 entries, 0 to 99\nData columns (total 23 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   id      100 non-null    int64  \n 1   player  100 non-null    object \n 2   year    100 non-null    int64  \n 3   stint   100 non-null    int64  \n 4   team    100 non-null    object \n 5   lg      100 non-null    object \n 6   g       100 non-null    int64  \n 7   ab      100 non-null    int64  \n 8   r       100 non-null    int64  \n 9   h       100 non-null    int64  \n 10  X2b     100 non-null    int64  \n 11  X3b     100 non-null    int64  \n 12  hr      100 non-null    int64  \n 13  rbi     100 non-null    float64\n 14  sb      100 non-null    float64\n 15  cs      100 non-null    float64\n 16  bb      100 non-null    int64  \n 17  so      100 non-null    float64\n 18  ibb     100 non-null    float64\n 19  hbp     100 non-null    float64\n 20  sh      100 non-null    float64\n 21  sf      100 non-null    float64\n 22  gidp    100 non-null    float64\ndtypes: float64(9), int64(11), object(3)\nmemory usage: 18.1+ KB",
      "In [127]: pd.DataFrame(np.random.randn(3, 12))\nOut[127]: \n         0         1         2   ...        9         10        11\n0 -1.226825  0.769804 -1.281247  ... -1.110336 -0.619976  0.149748\n1 -0.732339  0.687738  0.176444  ...  1.462696 -1.743161 -0.826591\n2 -0.345352  1.314232  0.690579  ...  0.896171 -0.487602 -0.082240\n\n[3 rows x 12 columns]",
      "In [128]: pd.set_option(\"display.width\", 40)  # default is 80\n\nIn [129]: pd.DataFrame(np.random.randn(3, 12))\nOut[129]: \n         0         1         2   ...        9         10        11\n0 -2.182937  0.380396  0.084844  ... -0.023688  2.410179  1.450520\n1  0.206053 -0.251905 -2.213588  ... -0.025747 -0.988387  0.094055\n2  1.262731  1.289997  0.082423  ... -0.281461  0.030711  0.109121\n\n[3 rows x 12 columns]",
      "In [130]: datafile = {\n   .....:     \"filename\": [\"filename_01\", \"filename_02\"],\n   .....:     \"path\": [\n   .....:         \"media/user_name/storage/folder_01/filename_01\",\n   .....:         \"media/user_name/storage/folder_02/filename_02\",\n   .....:     ],\n   .....: }\n   .....: \n\nIn [131]: pd.set_option(\"display.max_colwidth\", 30)\n\nIn [132]: pd.DataFrame(datafile)\nOut[132]: \n      filename                           path\n0  filename_01  media/user_name/storage/fo...\n1  filename_02  media/user_name/storage/fo...\n\nIn [133]: pd.set_option(\"display.max_colwidth\", 100)\n\nIn [134]: pd.DataFrame(datafile)\nOut[134]: \n      filename                                           path\n0  filename_01  media/user_name/storage/folder_01/filename_01\n1  filename_02  media/user_name/storage/folder_02/filename_02",
      "In [135]: df = pd.DataFrame({\"foo1\": np.random.randn(5), \"foo2\": np.random.randn(5)})\n\nIn [136]: df\nOut[136]: \n       foo1      foo2\n0  1.126203  0.781836\n1 -0.977349 -1.071357\n2  1.474071  0.441153\n3 -0.064034  2.353925\n4 -1.282782  0.583787\n\nIn [137]: df.foo1\nOut[137]: \n0    1.126203\n1   -0.977349\n2    1.474071\n3   -0.064034\n4   -1.282782\nName: foo1, dtype: float64"
    ]
  },
  {
    "url": "https://pandas.pydata.org/docs/user_guide/basics.html",
    "title": "Essential basic functionality#",
    "code_snippets": [
      "In [1]: index = pd.date_range(\"1/1/2000\", periods=8)\n\nIn [2]: s = pd.Series(np.random.randn(5), index=[\"a\", \"b\", \"c\", \"d\", \"e\"])\n\nIn [3]: df = pd.DataFrame(np.random.randn(8, 3), index=index, columns=[\"A\", \"B\", \"C\"])",
      "In [4]: long_series = pd.Series(np.random.randn(1000))\n\nIn [5]: long_series.head()\nOut[5]: \n0   -1.157892\n1   -1.344312\n2    0.844885\n3    1.075770\n4   -0.109050\ndtype: float64\n\nIn [6]: long_series.tail(3)\nOut[6]: \n997   -0.289388\n998   -1.020544\n999    0.589993\ndtype: float64",
      "In [14]: ser = pd.Series(pd.date_range(\"2000\", periods=2, tz=\"CET\"))\n\nIn [15]: ser.to_numpy(dtype=object)\nOut[15]: \narray([Timestamp('2000-01-01 00:00:00+0100', tz='CET'),\n       Timestamp('2000-01-02 00:00:00+0100', tz='CET')], dtype=object)",
      "pd.set_option(\"compute.use_bottleneck\", False)\npd.set_option(\"compute.use_numexpr\", False)",
      "In [18]: df = pd.DataFrame(\n   ....:     {\n   ....:         \"one\": pd.Series(np.random.randn(3), index=[\"a\", \"b\", \"c\"]),\n   ....:         \"two\": pd.Series(np.random.randn(4), index=[\"a\", \"b\", \"c\", \"d\"]),\n   ....:         \"three\": pd.Series(np.random.randn(3), index=[\"b\", \"c\", \"d\"]),\n   ....:     }\n   ....: )\n   ....: \n\nIn [19]: df\nOut[19]: \n        one       two     three\na  1.394981  1.772517       NaN\nb  0.343054  1.912123 -0.050390\nc  0.695246  1.478369  1.227435\nd       NaN  0.279344 -0.613172\n\nIn [20]: row = df.iloc[1]\n\nIn [21]: column = df[\"two\"]\n\nIn [22]: df.sub(row, axis=\"columns\")\nOut[22]: \n        one       two     three\na  1.051928 -0.139606       NaN\nb  0.000000  0.000000  0.000000\nc  0.352192 -0.433754  1.277825\nd       NaN -1.632779 -0.562782\n\nIn [23]: df.sub(row, axis=1)\nOut[23]: \n        one       two     three\na  1.051928 -0.139606       NaN\nb  0.000000  0.000000  0.000000\nc  0.352192 -0.433754  1.277825\nd       NaN -1.632779 -0.562782\n\nIn [24]: df.sub(column, axis=\"index\")\nOut[24]: \n        one  two     three\na -0.377535  0.0       NaN\nb -1.569069  0.0 -1.962513\nc -0.783123  0.0 -0.250933\nd       NaN  0.0 -0.892516\n\nIn [25]: df.sub(column, axis=0)\nOut[25]: \n        one  two     three\na -0.377535  0.0       NaN\nb -1.569069  0.0 -1.962513\nc -0.783123  0.0 -0.250933\nd       NaN  0.0 -0.892516",
      "In [26]: dfmi = df.copy()\n\nIn [27]: dfmi.index = pd.MultiIndex.from_tuples(\n   ....:     [(1, \"a\"), (1, \"b\"), (1, \"c\"), (2, \"a\")], names=[\"first\", \"second\"]\n   ....: )\n   ....: \n\nIn [28]: dfmi.sub(column, axis=0, level=\"second\")\nOut[28]: \n                   one       two     three\nfirst second                              \n1     a      -0.377535  0.000000       NaN\n      b      -1.569069  0.000000 -1.962513\n      c      -0.783123  0.000000 -0.250933\n2     a            NaN -1.493173 -2.385688",
      "In [29]: s = pd.Series(np.arange(10))\n\nIn [30]: s\nOut[30]: \n0    0\n1    1\n2    2\n3    3\n4    4\n5    5\n6    6\n7    7\n8    8\n9    9\ndtype: int64\n\nIn [31]: div, rem = divmod(s, 3)\n\nIn [32]: div\nOut[32]: \n0    0\n1    0\n2    0\n3    1\n4    1\n5    1\n6    2\n7    2\n8    2\n9    3\ndtype: int64\n\nIn [33]: rem\nOut[33]: \n0    0\n1    1\n2    2\n3    0\n4    1\n5    2\n6    0\n7    1\n8    2\n9    0\ndtype: int64\n\nIn [34]: idx = pd.Index(np.arange(10))\n\nIn [35]: idx\nOut[35]: Index([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype='int64')\n\nIn [36]: div, rem = divmod(idx, 3)\n\nIn [37]: div\nOut[37]: Index([0, 0, 0, 1, 1, 1, 2, 2, 2, 3], dtype='int64')\n\nIn [38]: rem\nOut[38]: Index([0, 1, 2, 0, 1, 2, 0, 1, 2, 0], dtype='int64')",
      "In [53]: df.empty\nOut[53]: False\n\nIn [54]: pd.DataFrame(columns=list(\"ABC\")).empty\nOut[54]: True",
      "In [55]: if df:\n   ....:     print(True)\n   ....: \n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n<ipython-input-55-318d08b2571a> in ?()\n----> 1 if df:\n      2     print(True)\n\n~/work/pandas/pandas/pandas/core/generic.py in ?(self)\n   1575     @final\n   1576     def __nonzero__(self) -> NoReturn:\n-> 1577         raise ValueError(\n   1578             f\"The truth value of a {type(self).__name__} is ambiguous. \"\n   1579             \"Use a.empty, a.bool(), a.item(), a.any() or a.all().\"\n   1580         )\n\nValueError: The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().",
      "In [56]: df and df2\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n<ipython-input-56-b241b64bb471> in ?()\n----> 1 df and df2\n\n~/work/pandas/pandas/pandas/core/generic.py in ?(self)\n   1575     @final\n   1576     def __nonzero__(self) -> NoReturn:\n-> 1577         raise ValueError(\n   1578             f\"The truth value of a {type(self).__name__} is ambiguous. \"\n   1579             \"Use a.empty, a.bool(), a.item(), a.any() or a.all().\"\n   1580         )\n\nValueError: The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().",
      "In [61]: df1 = pd.DataFrame({\"col\": [\"foo\", 0, np.nan]})\n\nIn [62]: df2 = pd.DataFrame({\"col\": [np.nan, 0, \"foo\"]}, index=[2, 1, 0])\n\nIn [63]: df1.equals(df2)\nOut[63]: False\n\nIn [64]: df1.equals(df2.sort_index())\nOut[64]: True",
      "In [65]: pd.Series([\"foo\", \"bar\", \"baz\"]) == \"foo\"\nOut[65]: \n0     True\n1    False\n2    False\ndtype: bool\n\nIn [66]: pd.Index([\"foo\", \"bar\", \"baz\"]) == \"foo\"\nOut[66]: array([ True, False, False])",
      "In [67]: pd.Series([\"foo\", \"bar\", \"baz\"]) == pd.Index([\"foo\", \"bar\", \"qux\"])\nOut[67]: \n0     True\n1     True\n2    False\ndtype: bool\n\nIn [68]: pd.Series([\"foo\", \"bar\", \"baz\"]) == np.array([\"foo\", \"bar\", \"qux\"])\nOut[68]: \n0     True\n1     True\n2    False\ndtype: bool",
      "In [69]: pd.Series(['foo', 'bar', 'baz']) == pd.Series(['foo', 'bar'])\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nCell In[69], line 1\n----> 1 pd.Series(['foo', 'bar', 'baz']) == pd.Series(['foo', 'bar'])\n\nFile ~/work/pandas/pandas/pandas/core/ops/common.py:76, in _unpack_zerodim_and_defer.<locals>.new_method(self, other)\n     72             return NotImplemented\n     74 other = item_from_zerodim(other)\n---> 76 return method(self, other)\n\nFile ~/work/pandas/pandas/pandas/core/arraylike.py:40, in OpsMixin.__eq__(self, other)\n     38 @unpack_zerodim_and_defer(\"__eq__\")\n     39 def __eq__(self, other):\n---> 40     return self._cmp_method(other, operator.eq)\n\nFile ~/work/pandas/pandas/pandas/core/series.py:6114, in Series._cmp_method(self, other, op)\n   6111 res_name = ops.get_op_result_name(self, other)\n   6113 if isinstance(other, Series) and not self._indexed_same(other):\n-> 6114     raise ValueError(\"Can only compare identically-labeled Series objects\")\n   6116 lvalues = self._values\n   6117 rvalues = extract_array(other, extract_numpy=True, extract_range=True)\n\nValueError: Can only compare identically-labeled Series objects\n\nIn [70]: pd.Series(['foo', 'bar', 'baz']) == pd.Series(['foo'])\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nCell In[70], line 1\n----> 1 pd.Series(['foo', 'bar', 'baz']) == pd.Series(['foo'])\n\nFile ~/work/pandas/pandas/pandas/core/ops/common.py:76, in _unpack_zerodim_and_defer.<locals>.new_method(self, other)\n     72             return NotImplemented\n     74 other = item_from_zerodim(other)\n---> 76 return method(self, other)\n\nFile ~/work/pandas/pandas/pandas/core/arraylike.py:40, in OpsMixin.__eq__(self, other)\n     38 @unpack_zerodim_and_defer(\"__eq__\")\n     39 def __eq__(self, other):\n---> 40     return self._cmp_method(other, operator.eq)\n\nFile ~/work/pandas/pandas/pandas/core/series.py:6114, in Series._cmp_method(self, other, op)\n   6111 res_name = ops.get_op_result_name(self, other)\n   6113 if isinstance(other, Series) and not self._indexed_same(other):\n-> 6114     raise ValueError(\"Can only compare identically-labeled Series objects\")\n   6116 lvalues = self._values\n   6117 rvalues = extract_array(other, extract_numpy=True, extract_range=True)\n\nValueError: Can only compare identically-labeled Series objects",
      "In [71]: df1 = pd.DataFrame(\n   ....:     {\"A\": [1.0, np.nan, 3.0, 5.0, np.nan], \"B\": [np.nan, 2.0, 3.0, np.nan, 6.0]}\n   ....: )\n   ....: \n\nIn [72]: df2 = pd.DataFrame(\n   ....:     {\n   ....:         \"A\": [5.0, 2.0, 4.0, np.nan, 3.0, 7.0],\n   ....:         \"B\": [np.nan, np.nan, 3.0, 4.0, 6.0, 8.0],\n   ....:     }\n   ....: )\n   ....: \n\nIn [73]: df1\nOut[73]: \n     A    B\n0  1.0  NaN\n1  NaN  2.0\n2  3.0  3.0\n3  5.0  NaN\n4  NaN  6.0\n\nIn [74]: df2\nOut[74]: \n     A    B\n0  5.0  NaN\n1  2.0  NaN\n2  4.0  3.0\n3  NaN  4.0\n4  3.0  6.0\n5  7.0  8.0\n\nIn [75]: df1.combine_first(df2)\nOut[75]: \n     A    B\n0  1.0  NaN\n1  2.0  2.0\n2  3.0  3.0\n3  5.0  4.0\n4  3.0  6.0\n5  7.0  8.0",
      "In [76]: def combiner(x, y):\n   ....:     return np.where(pd.isna(x), y, x)\n   ....: \n\nIn [77]: df1.combine(df2, combiner)\nOut[77]: \n     A    B\n0  1.0  NaN\n1  2.0  2.0\n2  3.0  3.0\n3  5.0  4.0\n4  3.0  6.0\n5  7.0  8.0",
      "In [90]: series = pd.Series(np.random.randn(500))\n\nIn [91]: series[20:500] = np.nan\n\nIn [92]: series[10:20] = 5\n\nIn [93]: series.nunique()\nOut[93]: 11",
      "In [94]: series = pd.Series(np.random.randn(1000))\n\nIn [95]: series[::2] = np.nan\n\nIn [96]: series.describe()\nOut[96]: \ncount    500.000000\nmean      -0.021292\nstd        1.015906\nmin       -2.683763\n25%       -0.699070\n50%       -0.069718\n75%        0.714483\nmax        3.160915\ndtype: float64\n\nIn [97]: frame = pd.DataFrame(np.random.randn(1000, 5), columns=[\"a\", \"b\", \"c\", \"d\", \"e\"])\n\nIn [98]: frame.iloc[::2] = np.nan\n\nIn [99]: frame.describe()\nOut[99]: \n                a           b           c           d           e\ncount  500.000000  500.000000  500.000000  500.000000  500.000000\nmean     0.033387    0.030045   -0.043719   -0.051686    0.005979\nstd      1.017152    0.978743    1.025270    1.015988    1.006695\nmin     -3.000951   -2.637901   -3.303099   -3.159200   -3.188821\n25%     -0.647623   -0.576449   -0.712369   -0.691338   -0.691115\n50%      0.047578   -0.021499   -0.023888   -0.032652   -0.025363\n75%      0.729907    0.775880    0.618896    0.670047    0.649748\nmax      2.740139    2.752332    3.004229    2.728702    3.240991",
      "In [101]: s = pd.Series([\"a\", \"a\", \"b\", \"b\", \"a\", \"a\", np.nan, \"c\", \"d\", \"a\"])\n\nIn [102]: s.describe()\nOut[102]: \ncount     9\nunique    4\ntop       a\nfreq      5\ndtype: object",
      "In [103]: frame = pd.DataFrame({\"a\": [\"Yes\", \"Yes\", \"No\", \"No\"], \"b\": range(4)})\n\nIn [104]: frame.describe()\nOut[104]: \n              b\ncount  4.000000\nmean   1.500000\nstd    1.290994\nmin    0.000000\n25%    0.750000\n50%    1.500000\n75%    2.250000\nmax    3.000000",
      "In [108]: s1 = pd.Series(np.random.randn(5))\n\nIn [109]: s1\nOut[109]: \n0    1.118076\n1   -0.352051\n2   -1.242883\n3   -1.277155\n4   -0.641184\ndtype: float64\n\nIn [110]: s1.idxmin(), s1.idxmax()\nOut[110]: (3, 0)\n\nIn [111]: df1 = pd.DataFrame(np.random.randn(5, 3), columns=[\"A\", \"B\", \"C\"])\n\nIn [112]: df1\nOut[112]: \n          A         B         C\n0 -0.327863 -0.946180 -0.137570\n1 -0.186235 -0.257213 -0.486567\n2 -0.507027 -0.871259 -0.111110\n3  2.000339 -2.430505  0.089759\n4 -0.321434 -0.033695  0.096271\n\nIn [113]: df1.idxmin(axis=0)\nOut[113]: \nA    2\nB    3\nC    1\ndtype: int64\n\nIn [114]: df1.idxmax(axis=1)\nOut[114]: \n0    C\n1    A\n2    C\n3    A\n4    C\ndtype: object",
      "In [115]: df3 = pd.DataFrame([2, 1, 1, 3, np.nan], columns=[\"A\"], index=list(\"edcba\"))\n\nIn [116]: df3\nOut[116]: \n     A\ne  2.0\nd  1.0\nc  1.0\nb  3.0\na  NaN\n\nIn [117]: df3[\"A\"].idxmin()\nOut[117]: 'd'",
      "In [118]: data = np.random.randint(0, 7, size=50)\n\nIn [119]: data\nOut[119]: \narray([6, 6, 2, 3, 5, 3, 2, 5, 4, 5, 4, 3, 4, 5, 0, 2, 0, 4, 2, 0, 3, 2,\n       2, 5, 6, 5, 3, 4, 6, 4, 3, 5, 6, 4, 3, 6, 2, 6, 6, 2, 3, 4, 2, 1,\n       6, 2, 6, 1, 5, 4])\n\nIn [120]: s = pd.Series(data)\n\nIn [121]: s.value_counts()\nOut[121]: \n6    10\n2    10\n4     9\n3     8\n5     8\n0     3\n1     2\nName: count, dtype: int64",
      "In [122]: data = {\"a\": [1, 2, 3, 4], \"b\": [\"x\", \"x\", \"y\", \"y\"]}\n\nIn [123]: frame = pd.DataFrame(data)\n\nIn [124]: frame.value_counts()\nOut[124]: \na  b\n1  x    1\n2  x    1\n3  y    1\n4  y    1\nName: count, dtype: int64",
      "In [125]: s5 = pd.Series([1, 1, 3, 3, 3, 5, 5, 7, 7, 7])\n\nIn [126]: s5.mode()\nOut[126]: \n0    3\n1    7\ndtype: int64\n\nIn [127]: df5 = pd.DataFrame(\n   .....:     {\n   .....:         \"A\": np.random.randint(0, 7, size=50),\n   .....:         \"B\": np.random.randint(-10, 15, size=50),\n   .....:     }\n   .....: )\n   .....: \n\nIn [128]: df5.mode()\nOut[128]: \n     A   B\n0  1.0  -9\n1  NaN  10\n2  NaN  13",
      "In [129]: arr = np.random.randn(20)\n\nIn [130]: factor = pd.cut(arr, 4)\n\nIn [131]: factor\nOut[131]: \n[(-0.251, 0.464], (-0.968, -0.251], (0.464, 1.179], (-0.251, 0.464], (-0.968, -0.251], ..., (-0.251, 0.464], (-0.968, -0.251], (-0.968, -0.251], (-0.968, -0.251], (-0.968, -0.251]]\nLength: 20\nCategories (4, interval[float64, right]): [(-0.968, -0.251] < (-0.251, 0.464] < (0.464, 1.179] <\n                                           (1.179, 1.893]]\n\nIn [132]: factor = pd.cut(arr, [-5, -1, 0, 1, 5])\n\nIn [133]: factor\nOut[133]: \n[(0, 1], (-1, 0], (0, 1], (0, 1], (-1, 0], ..., (-1, 0], (-1, 0], (-1, 0], (-1, 0], (-1, 0]]\nLength: 20\nCategories (4, interval[int64, right]): [(-5, -1] < (-1, 0] < (0, 1] < (1, 5]]",
      "In [134]: arr = np.random.randn(30)\n\nIn [135]: factor = pd.qcut(arr, [0, 0.25, 0.5, 0.75, 1])\n\nIn [136]: factor\nOut[136]: \n[(0.569, 1.184], (-2.278, -0.301], (-2.278, -0.301], (0.569, 1.184], (0.569, 1.184], ..., (-0.301, 0.569], (1.184, 2.346], (1.184, 2.346], (-0.301, 0.569], (-2.278, -0.301]]\nLength: 30\nCategories (4, interval[float64, right]): [(-2.278, -0.301] < (-0.301, 0.569] < (0.569, 1.184] <\n                                           (1.184, 2.346]]",
      "In [137]: arr = np.random.randn(20)\n\nIn [138]: factor = pd.cut(arr, [-np.inf, 0, np.inf])\n\nIn [139]: factor\nOut[139]: \n[(-inf, 0.0], (0.0, inf], (0.0, inf], (-inf, 0.0], (-inf, 0.0], ..., (-inf, 0.0], (-inf, 0.0], (-inf, 0.0], (0.0, inf], (0.0, inf]]\nLength: 20\nCategories (2, interval[float64, right]): [(-inf, 0.0] < (0.0, inf]]",
      "In [140]: def extract_city_name(df):\n   .....:     \"\"\"\n   .....:     Chicago, IL -> Chicago for city_name column\n   .....:     \"\"\"\n   .....:     df[\"city_name\"] = df[\"city_and_code\"].str.split(\",\").str.get(0)\n   .....:     return df\n   .....: \n\nIn [141]: def add_country_name(df, country_name=None):\n   .....:     \"\"\"\n   .....:     Chicago -> Chicago-US for city_name column\n   .....:     \"\"\"\n   .....:     col = \"city_name\"\n   .....:     df[\"city_and_country\"] = df[col] + country_name\n   .....:     return df\n   .....: \n\nIn [142]: df_p = pd.DataFrame({\"city_and_code\": [\"Chicago, IL\"]})",
      "In [147]: import statsmodels.formula.api as sm\n\nIn [148]: bb = pd.read_csv(\"data/baseball.csv\", index_col=\"id\")\n\nIn [149]: (\n   .....:     bb.query(\"h > 0\")\n   .....:     .assign(ln_h=lambda df: np.log(df.h))\n   .....:     .pipe((sm.ols, \"data\"), \"hr ~ ln_h + year + g + C(lg)\")\n   .....:     .fit()\n   .....:     .summary()\n   .....: )\n   .....:\nOut[149]:\n<class 'statsmodels.iolib.summary.Summary'>\n\"\"\"\n                           OLS Regression Results\n==============================================================================\nDep. Variable:                     hr   R-squared:                       0.685\nModel:                            OLS   Adj. R-squared:                  0.665\nMethod:                 Least Squares   F-statistic:                     34.28\nDate:                Tue, 22 Nov 2022   Prob (F-statistic):           3.48e-15\nTime:                        05:34:17   Log-Likelihood:                -205.92\nNo. Observations:                  68   AIC:                             421.8\nDf Residuals:                      63   BIC:                             432.9\nDf Model:                           4\nCovariance Type:            nonrobust\n===============================================================================\n                  coef    std err          t      P>|t|      [0.025      0.975]\n-------------------------------------------------------------------------------\nIntercept   -8484.7720   4664.146     -1.819      0.074   -1.78e+04     835.780\nC(lg)[T.NL]    -2.2736      1.325     -1.716      0.091      -4.922       0.375\nln_h           -1.3542      0.875     -1.547      0.127      -3.103       0.395\nyear            4.2277      2.324      1.819      0.074      -0.417       8.872\ng               0.1841      0.029      6.258      0.000       0.125       0.243\n==============================================================================\nOmnibus:                       10.875   Durbin-Watson:                   1.999\nProb(Omnibus):                  0.004   Jarque-Bera (JB):               17.298\nSkew:                           0.537   Prob(JB):                     0.000175\nKurtosis:                       5.225   Cond. No.                     1.49e+07\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 1.49e+07. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\"\"\"",
      "In [152]: tsdf = pd.DataFrame(\n   .....:     np.random.randn(1000, 3),\n   .....:     columns=[\"A\", \"B\", \"C\"],\n   .....:     index=pd.date_range(\"1/1/2000\", periods=1000),\n   .....: )\n   .....: \n\nIn [153]: tsdf.apply(lambda x: x.idxmax())\nOut[153]: \nA   2000-08-06\nB   2001-01-18\nC   2001-07-18\ndtype: datetime64[ns]",
      "In [154]: def subtract_and_divide(x, sub, divide=1):\n   .....:     return (x - sub) / divide\n   .....: \n\nIn [155]: df_udf = pd.DataFrame(np.ones((2, 2)))\n\nIn [156]: df_udf.apply(subtract_and_divide, args=(5,), divide=3)\nOut[156]: \n          0         1\n0 -1.333333 -1.333333\n1 -1.333333 -1.333333",
      "In [157]: tsdf = pd.DataFrame(\n   .....:     np.random.randn(10, 3),\n   .....:     columns=[\"A\", \"B\", \"C\"],\n   .....:     index=pd.date_range(\"1/1/2000\", periods=10),\n   .....: )\n   .....: \n\nIn [158]: tsdf.iloc[3:7] = np.nan\n\nIn [159]: tsdf\nOut[159]: \n                   A         B         C\n2000-01-01 -0.158131 -0.232466  0.321604\n2000-01-02 -1.810340 -3.105758  0.433834\n2000-01-03 -1.209847 -1.156793 -0.136794\n2000-01-04       NaN       NaN       NaN\n2000-01-05       NaN       NaN       NaN\n2000-01-06       NaN       NaN       NaN\n2000-01-07       NaN       NaN       NaN\n2000-01-08 -0.653602  0.178875  1.008298\n2000-01-09  1.007996  0.462824  0.254472\n2000-01-10  0.307473  0.600337  1.643950\n\nIn [160]: tsdf.apply(pd.Series.interpolate)\nOut[160]: \n                   A         B         C\n2000-01-01 -0.158131 -0.232466  0.321604\n2000-01-02 -1.810340 -3.105758  0.433834\n2000-01-03 -1.209847 -1.156793 -0.136794\n2000-01-04 -1.098598 -0.889659  0.092225\n2000-01-05 -0.987349 -0.622526  0.321243\n2000-01-06 -0.876100 -0.355392  0.550262\n2000-01-07 -0.764851 -0.088259  0.779280\n2000-01-08 -0.653602  0.178875  1.008298\n2000-01-09  1.007996  0.462824  0.254472\n2000-01-10  0.307473  0.600337  1.643950",
      "In [161]: tsdf = pd.DataFrame(\n   .....:     np.random.randn(10, 3),\n   .....:     columns=[\"A\", \"B\", \"C\"],\n   .....:     index=pd.date_range(\"1/1/2000\", periods=10),\n   .....: )\n   .....: \n\nIn [162]: tsdf.iloc[3:7] = np.nan\n\nIn [163]: tsdf\nOut[163]: \n                   A         B         C\n2000-01-01  1.257606  1.004194  0.167574\n2000-01-02 -0.749892  0.288112 -0.757304\n2000-01-03 -0.207550 -0.298599  0.116018\n2000-01-04       NaN       NaN       NaN\n2000-01-05       NaN       NaN       NaN\n2000-01-06       NaN       NaN       NaN\n2000-01-07       NaN       NaN       NaN\n2000-01-08  0.814347 -0.257623  0.869226\n2000-01-09 -0.250663 -1.206601  0.896839\n2000-01-10  2.169758 -1.333363  0.283157",
      "In [172]: def mymean(x):\n   .....:     return x.mean()\n   .....: \n\nIn [173]: tsdf[\"A\"].agg([\"sum\", mymean])\nOut[173]: \nsum       3.033606\nmymean    0.505601\nName: A, dtype: float64",
      "In [176]: from functools import partial\n\nIn [177]: q_25 = partial(pd.Series.quantile, q=0.25)\n\nIn [178]: q_25.__name__ = \"25%\"\n\nIn [179]: q_75 = partial(pd.Series.quantile, q=0.75)\n\nIn [180]: q_75.__name__ = \"75%\"\n\nIn [181]: tsdf.agg([\"count\", \"mean\", \"std\", \"min\", q_25, \"median\", q_75, \"max\"])\nOut[181]: \n               A         B         C\ncount   6.000000  6.000000  6.000000\nmean    0.505601 -0.300647  0.262585\nstd     1.103362  0.887508  0.606860\nmin    -0.749892 -1.333363 -0.757304\n25%    -0.239885 -0.979600  0.128907\nmedian  0.303398 -0.278111  0.225365\n75%     1.146791  0.151678  0.722709\nmax     2.169758  1.004194  0.896839",
      "In [182]: tsdf = pd.DataFrame(\n   .....:     np.random.randn(10, 3),\n   .....:     columns=[\"A\", \"B\", \"C\"],\n   .....:     index=pd.date_range(\"1/1/2000\", periods=10),\n   .....: )\n   .....: \n\nIn [183]: tsdf.iloc[3:7] = np.nan\n\nIn [184]: tsdf\nOut[184]: \n                   A         B         C\n2000-01-01 -0.428759 -0.864890 -0.675341\n2000-01-02 -0.168731  1.338144 -1.279321\n2000-01-03 -1.621034  0.438107  0.903794\n2000-01-04       NaN       NaN       NaN\n2000-01-05       NaN       NaN       NaN\n2000-01-06       NaN       NaN       NaN\n2000-01-07       NaN       NaN       NaN\n2000-01-08  0.254374 -1.240447 -0.201052\n2000-01-09 -0.157795  0.791197 -1.144209\n2000-01-10 -0.030876  0.371900  0.061932",
      "In [194]: df4 = df.copy()\n\nIn [195]: df4\nOut[195]: \n        one       two     three\na  1.394981  1.772517       NaN\nb  0.343054  1.912123 -0.050390\nc  0.695246  1.478369  1.227435\nd       NaN  0.279344 -0.613172\n\nIn [196]: def f(x):\n   .....:     return len(str(x))\n   .....: \n\nIn [197]: df4[\"one\"].map(f)\nOut[197]: \na    18\nb    19\nc    18\nd     3\nName: one, dtype: int64\n\nIn [198]: df4.map(f)\nOut[198]: \n   one  two  three\na   18   17      3\nb   19   18     20\nc   18   18     16\nd    3   19     19",
      "In [199]: s = pd.Series(\n   .....:     [\"six\", \"seven\", \"six\", \"seven\", \"six\"], index=[\"a\", \"b\", \"c\", \"d\", \"e\"]\n   .....: )\n   .....: \n\nIn [200]: t = pd.Series({\"six\": 6.0, \"seven\": 7.0})\n\nIn [201]: s\nOut[201]: \na      six\nb    seven\nc      six\nd    seven\ne      six\ndtype: object\n\nIn [202]: s.map(t)\nOut[202]: \na    6.0\nb    7.0\nc    6.0\nd    7.0\ne    6.0\ndtype: float64",
      "In [203]: s = pd.Series(np.random.randn(5), index=[\"a\", \"b\", \"c\", \"d\", \"e\"])\n\nIn [204]: s\nOut[204]: \na    1.695148\nb    1.328614\nc    1.234686\nd   -0.385845\ne   -1.326508\ndtype: float64\n\nIn [205]: s.reindex([\"e\", \"b\", \"f\", \"d\"])\nOut[205]: \ne   -1.326508\nb    1.328614\nf         NaN\nd   -0.385845\ndtype: float64",
      "In [218]: s = pd.Series(np.random.randn(5), index=[\"a\", \"b\", \"c\", \"d\", \"e\"])\n\nIn [219]: s1 = s[:4]\n\nIn [220]: s2 = s[1:]\n\nIn [221]: s1.align(s2)\nOut[221]: \n(a   -0.186646\n b   -1.692424\n c   -0.303893\n d   -1.425662\n e         NaN\n dtype: float64,\n a         NaN\n b   -1.692424\n c   -0.303893\n d   -1.425662\n e    1.114285\n dtype: float64)\n\nIn [222]: s1.align(s2, join=\"inner\")\nOut[222]: \n(b   -1.692424\n c   -0.303893\n d   -1.425662\n dtype: float64,\n b   -1.692424\n c   -0.303893\n d   -1.425662\n dtype: float64)\n\nIn [223]: s1.align(s2, join=\"left\")\nOut[223]: \n(a   -0.186646\n b   -1.692424\n c   -0.303893\n d   -1.425662\n dtype: float64,\n a         NaN\n b   -1.692424\n c   -0.303893\n d   -1.425662\n dtype: float64)",
      "In [227]: rng = pd.date_range(\"1/3/2000\", periods=8)\n\nIn [228]: ts = pd.Series(np.random.randn(8), index=rng)\n\nIn [229]: ts2 = ts.iloc[[0, 3, 6]]\n\nIn [230]: ts\nOut[230]: \n2000-01-03    0.183051\n2000-01-04    0.400528\n2000-01-05   -0.015083\n2000-01-06    2.395489\n2000-01-07    1.414806\n2000-01-08    0.118428\n2000-01-09    0.733639\n2000-01-10   -0.936077\nFreq: D, dtype: float64\n\nIn [231]: ts2\nOut[231]: \n2000-01-03    0.183051\n2000-01-06    2.395489\n2000-01-09    0.733639\nFreq: 3D, dtype: float64\n\nIn [232]: ts2.reindex(ts.index)\nOut[232]: \n2000-01-03    0.183051\n2000-01-04         NaN\n2000-01-05         NaN\n2000-01-06    2.395489\n2000-01-07         NaN\n2000-01-08         NaN\n2000-01-09    0.733639\n2000-01-10         NaN\nFreq: D, dtype: float64\n\nIn [233]: ts2.reindex(ts.index, method=\"ffill\")\nOut[233]: \n2000-01-03    0.183051\n2000-01-04    0.183051\n2000-01-05    0.183051\n2000-01-06    2.395489\n2000-01-07    2.395489\n2000-01-08    2.395489\n2000-01-09    0.733639\n2000-01-10    0.733639\nFreq: D, dtype: float64\n\nIn [234]: ts2.reindex(ts.index, method=\"bfill\")\nOut[234]: \n2000-01-03    0.183051\n2000-01-04    2.395489\n2000-01-05    2.395489\n2000-01-06    2.395489\n2000-01-07    0.733639\n2000-01-08    0.733639\n2000-01-09    0.733639\n2000-01-10         NaN\nFreq: D, dtype: float64\n\nIn [235]: ts2.reindex(ts.index, method=\"nearest\")\nOut[235]: \n2000-01-03    0.183051\n2000-01-04    0.183051\n2000-01-05    2.395489\n2000-01-06    2.395489\n2000-01-07    2.395489\n2000-01-08    0.733639\n2000-01-09    0.733639\n2000-01-10    0.733639\nFreq: D, dtype: float64",
      "In [249]: df = pd.DataFrame(\n   .....:     {\"x\": [1, 2, 3, 4, 5, 6], \"y\": [10, 20, 30, 40, 50, 60]},\n   .....:     index=pd.MultiIndex.from_product(\n   .....:         [[\"a\", \"b\", \"c\"], [1, 2]], names=[\"let\", \"num\"]\n   .....:     ),\n   .....: )\n   .....: \n\nIn [250]: df\nOut[250]: \n         x   y\nlet num       \na   1    1  10\n    2    2  20\nb   1    3  30\n    2    4  40\nc   1    5  50\n    2    6  60\n\nIn [251]: df.rename_axis(index={\"let\": \"abc\"})\nOut[251]: \n         x   y\nabc num       \na   1    1  10\n    2    2  20\nb   1    3  30\n    2    4  40\nc   1    5  50\n    2    6  60\n\nIn [252]: df.rename_axis(index=str.upper)\nOut[252]: \n         x   y\nLET NUM       \na   1    1  10\n    2    2  20\nb   1    3  30\n    2    4  40\nc   1    5  50\n    2    6  60",
      "In [253]: df = pd.DataFrame(\n   .....:     {\"col1\": np.random.randn(3), \"col2\": np.random.randn(3)}, index=[\"a\", \"b\", \"c\"]\n   .....: )\n   .....: \n\nIn [254]: for col in df:\n   .....:     print(col)\n   .....: \ncol1\ncol2",
      "In [255]: df = pd.DataFrame({\"a\": [1, 2, 3], \"b\": [\"a\", \"b\", \"c\"]})\n\nIn [256]: for index, row in df.iterrows():\n   .....:     row[\"a\"] = 10\n   .....: \n\nIn [257]: df\nOut[257]: \n   a  b\n0  1  a\n1  2  b\n2  3  c",
      "In [260]: df_orig = pd.DataFrame([[1, 1.5]], columns=[\"int\", \"float\"])\n\nIn [261]: df_orig.dtypes\nOut[261]: \nint        int64\nfloat    float64\ndtype: object\n\nIn [262]: row = next(df_orig.iterrows())[1]\n\nIn [263]: row\nOut[263]: \nint      1.0\nfloat    1.5\nName: 0, dtype: float64",
      "In [266]: df2 = pd.DataFrame({\"x\": [1, 2, 3], \"y\": [4, 5, 6]})\n\nIn [267]: print(df2)\n   x  y\n0  1  4\n1  2  5\n2  3  6\n\nIn [268]: print(df2.T)\n   0  1  2\nx  1  2  3\ny  4  5  6\n\nIn [269]: df2_t = pd.DataFrame({idx: values for idx, values in df2.iterrows()})\n\nIn [270]: print(df2_t)\n   0  1  2\nx  1  2  3\ny  4  5  6",
      "# datetime\nIn [272]: s = pd.Series(pd.date_range(\"20130101 09:10:12\", periods=4))\n\nIn [273]: s\nOut[273]: \n0   2013-01-01 09:10:12\n1   2013-01-02 09:10:12\n2   2013-01-03 09:10:12\n3   2013-01-04 09:10:12\ndtype: datetime64[ns]\n\nIn [274]: s.dt.hour\nOut[274]: \n0    9\n1    9\n2    9\n3    9\ndtype: int32\n\nIn [275]: s.dt.second\nOut[275]: \n0    12\n1    12\n2    12\n3    12\ndtype: int32\n\nIn [276]: s.dt.day\nOut[276]: \n0    1\n1    2\n2    3\n3    4\ndtype: int32",
      "# DatetimeIndex\nIn [282]: s = pd.Series(pd.date_range(\"20130101\", periods=4))\n\nIn [283]: s\nOut[283]: \n0   2013-01-01\n1   2013-01-02\n2   2013-01-03\n3   2013-01-04\ndtype: datetime64[ns]\n\nIn [284]: s.dt.strftime(\"%Y/%m/%d\")\nOut[284]: \n0    2013/01/01\n1    2013/01/02\n2    2013/01/03\n3    2013/01/04\ndtype: object",
      "# PeriodIndex\nIn [285]: s = pd.Series(pd.period_range(\"20130101\", periods=4))\n\nIn [286]: s\nOut[286]: \n0    2013-01-01\n1    2013-01-02\n2    2013-01-03\n3    2013-01-04\ndtype: period[D]\n\nIn [287]: s.dt.strftime(\"%Y/%m/%d\")\nOut[287]: \n0    2013/01/01\n1    2013/01/02\n2    2013/01/03\n3    2013/01/04\ndtype: object",
      "# period\nIn [288]: s = pd.Series(pd.period_range(\"20130101\", periods=4, freq=\"D\"))\n\nIn [289]: s\nOut[289]: \n0    2013-01-01\n1    2013-01-02\n2    2013-01-03\n3    2013-01-04\ndtype: period[D]\n\nIn [290]: s.dt.year\nOut[290]: \n0    2013\n1    2013\n2    2013\n3    2013\ndtype: int64\n\nIn [291]: s.dt.day\nOut[291]: \n0    1\n1    2\n2    3\n3    4\ndtype: int64",
      "# timedelta\nIn [292]: s = pd.Series(pd.timedelta_range(\"1 day 00:00:05\", periods=4, freq=\"s\"))\n\nIn [293]: s\nOut[293]: \n0   1 days 00:00:05\n1   1 days 00:00:06\n2   1 days 00:00:07\n3   1 days 00:00:08\ndtype: timedelta64[ns]\n\nIn [294]: s.dt.days\nOut[294]: \n0    1\n1    1\n2    1\n3    1\ndtype: int64\n\nIn [295]: s.dt.seconds\nOut[295]: \n0    5\n1    6\n2    7\n3    8\ndtype: int32\n\nIn [296]: s.dt.components\nOut[296]: \n   days  hours  minutes  seconds  milliseconds  microseconds  nanoseconds\n0     1      0        0        5             0             0            0\n1     1      0        0        6             0             0            0\n2     1      0        0        7             0             0            0\n3     1      0        0        8             0             0            0",
      "In [297]: s = pd.Series(\n   .....:     [\"A\", \"B\", \"C\", \"Aaba\", \"Baca\", np.nan, \"CABA\", \"dog\", \"cat\"], dtype=\"string\"\n   .....: )\n   .....: \n\nIn [298]: s.str.lower()\nOut[298]: \n0       a\n1       b\n2       c\n3    aaba\n4    baca\n5    <NA>\n6    caba\n7     dog\n8     cat\ndtype: string",
      "In [299]: df = pd.DataFrame(\n   .....:     {\n   .....:         \"one\": pd.Series(np.random.randn(3), index=[\"a\", \"b\", \"c\"]),\n   .....:         \"two\": pd.Series(np.random.randn(4), index=[\"a\", \"b\", \"c\", \"d\"]),\n   .....:         \"three\": pd.Series(np.random.randn(3), index=[\"b\", \"c\", \"d\"]),\n   .....:     }\n   .....: )\n   .....: \n\nIn [300]: unsorted_df = df.reindex(\n   .....:     index=[\"a\", \"d\", \"c\", \"b\"], columns=[\"three\", \"two\", \"one\"]\n   .....: )\n   .....: \n\nIn [301]: unsorted_df\nOut[301]: \n      three       two       one\na       NaN -1.152244  0.562973\nd -0.252916 -0.109597       NaN\nc  1.273388 -0.167123  0.640382\nb -0.098217  0.009797 -1.299504\n\n# DataFrame\nIn [302]: unsorted_df.sort_index()\nOut[302]: \n      three       two       one\na       NaN -1.152244  0.562973\nb -0.098217  0.009797 -1.299504\nc  1.273388 -0.167123  0.640382\nd -0.252916 -0.109597       NaN\n\nIn [303]: unsorted_df.sort_index(ascending=False)\nOut[303]: \n      three       two       one\nd -0.252916 -0.109597       NaN\nc  1.273388 -0.167123  0.640382\nb -0.098217  0.009797 -1.299504\na       NaN -1.152244  0.562973\n\nIn [304]: unsorted_df.sort_index(axis=1)\nOut[304]: \n        one     three       two\na  0.562973       NaN -1.152244\nd       NaN -0.252916 -0.109597\nc  0.640382  1.273388 -0.167123\nb -1.299504 -0.098217  0.009797\n\n# Series\nIn [305]: unsorted_df[\"three\"].sort_index()\nOut[305]: \na         NaN\nb   -0.098217\nc    1.273388\nd   -0.252916\nName: three, dtype: float64",
      "In [306]: s1 = pd.DataFrame({\"a\": [\"B\", \"a\", \"C\"], \"b\": [1, 2, 3], \"c\": [2, 3, 4]}).set_index(\n   .....:     list(\"ab\")\n   .....: )\n   .....: \n\nIn [307]: s1\nOut[307]: \n     c\na b   \nB 1  2\na 2  3\nC 3  4",
      "In [310]: df1 = pd.DataFrame(\n   .....:     {\"one\": [2, 1, 1, 1], \"two\": [1, 3, 2, 4], \"three\": [5, 4, 3, 2]}\n   .....: )\n   .....: \n\nIn [311]: df1.sort_values(by=\"two\")\nOut[311]: \n   one  two  three\n0    2    1      5\n2    1    2      3\n1    1    3      4\n3    1    4      2",
      "In [316]: s1 = pd.Series([\"B\", \"a\", \"C\"])",
      "In [319]: df = pd.DataFrame({\"a\": [\"B\", \"a\", \"C\"], \"b\": [1, 2, 3]})",
      "# Build MultiIndex\nIn [322]: idx = pd.MultiIndex.from_tuples(\n   .....:     [(\"a\", 1), (\"a\", 2), (\"a\", 2), (\"b\", 2), (\"b\", 1), (\"b\", 1)]\n   .....: )\n   .....: \n\nIn [323]: idx.names = [\"first\", \"second\"]\n\n# Build DataFrame\nIn [324]: df_multi = pd.DataFrame({\"A\": np.arange(6, 0, -1)}, index=idx)\n\nIn [325]: df_multi\nOut[325]: \n              A\nfirst second   \na     1       6\n      2       5\n      2       4\nb     2       3\n      1       2\n      1       1",
      "In [327]: ser = pd.Series([1, 2, 3])\n\nIn [328]: ser.searchsorted([0, 3])\nOut[328]: array([0, 2])\n\nIn [329]: ser.searchsorted([0, 4])\nOut[329]: array([0, 3])\n\nIn [330]: ser.searchsorted([1, 3], side=\"right\")\nOut[330]: array([1, 3])\n\nIn [331]: ser.searchsorted([1, 3], side=\"left\")\nOut[331]: array([0, 2])\n\nIn [332]: ser = pd.Series([3, 1, 2])\n\nIn [333]: ser.searchsorted([0, 3], sorter=np.argsort(ser))\nOut[333]: array([0, 2])",
      "In [334]: s = pd.Series(np.random.permutation(10))\n\nIn [335]: s\nOut[335]: \n0    2\n1    0\n2    3\n3    7\n4    1\n5    5\n6    9\n7    6\n8    8\n9    4\ndtype: int64\n\nIn [336]: s.sort_values()\nOut[336]: \n1    0\n4    1\n0    2\n2    3\n9    4\n5    5\n7    6\n3    7\n8    8\n6    9\ndtype: int64\n\nIn [337]: s.nsmallest(3)\nOut[337]: \n1    0\n4    1\n0    2\ndtype: int64\n\nIn [338]: s.nlargest(3)\nOut[338]: \n6    9\n8    8\n3    7\ndtype: int64",
      "In [339]: df = pd.DataFrame(\n   .....:     {\n   .....:         \"a\": [-2, -1, 1, 10, 8, 11, -1],\n   .....:         \"b\": list(\"abdceff\"),\n   .....:         \"c\": [1.0, 2.0, 4.0, 3.2, np.nan, 3.0, 4.0],\n   .....:     }\n   .....: )\n   .....: \n\nIn [340]: df.nlargest(3, \"a\")\nOut[340]: \n    a  b    c\n5  11  f  3.0\n3  10  c  3.2\n4   8  e  NaN\n\nIn [341]: df.nlargest(5, [\"a\", \"c\"])\nOut[341]: \n    a  b    c\n5  11  f  3.0\n3  10  c  3.2\n4   8  e  NaN\n2   1  d  4.0\n6  -1  f  4.0\n\nIn [342]: df.nsmallest(3, \"a\")\nOut[342]: \n   a  b    c\n0 -2  a  1.0\n1 -1  b  2.0\n6 -1  f  4.0\n\nIn [343]: df.nsmallest(5, [\"a\", \"c\"])\nOut[343]: \n   a  b    c\n0 -2  a  1.0\n1 -1  b  2.0\n6 -1  f  4.0\n2  1  d  4.0\n4  8  e  NaN",
      "In [344]: df1.columns = pd.MultiIndex.from_tuples(\n   .....:     [(\"a\", \"one\"), (\"a\", \"two\"), (\"b\", \"three\")]\n   .....: )\n   .....: \n\nIn [345]: df1.sort_values(by=(\"a\", \"two\"))\nOut[345]: \n    a         b\n  one two three\n0   2   1     5\n2   1   2     3\n1   1   3     4\n3   1   4     2",
      "In [346]: dft = pd.DataFrame(\n   .....:     {\n   .....:         \"A\": np.random.rand(3),\n   .....:         \"B\": 1,\n   .....:         \"C\": \"foo\",\n   .....:         \"D\": pd.Timestamp(\"20010102\"),\n   .....:         \"E\": pd.Series([1.0] * 3).astype(\"float32\"),\n   .....:         \"F\": False,\n   .....:         \"G\": pd.Series([1] * 3, dtype=\"int8\"),\n   .....:     }\n   .....: )\n   .....: \n\nIn [347]: dft\nOut[347]: \n          A  B    C          D    E      F  G\n0  0.035962  1  foo 2001-01-02  1.0  False  1\n1  0.701379  1  foo 2001-01-02  1.0  False  1\n2  0.281885  1  foo 2001-01-02  1.0  False  1\n\nIn [348]: dft.dtypes\nOut[348]: \nA          float64\nB            int64\nC           object\nD    datetime64[s]\nE          float32\nF             bool\nG             int8\ndtype: object",
      "# these ints are coerced to floats\nIn [350]: pd.Series([1, 2, 3, 4, 5, 6.0])\nOut[350]: \n0    1.0\n1    2.0\n2    3.0\n3    4.0\n4    5.0\n5    6.0\ndtype: float64\n\n# string data forces an ``object`` dtype\nIn [351]: pd.Series([1, 2, 3, 6.0, \"foo\"])\nOut[351]: \n0      1\n1      2\n2      3\n3    6.0\n4    foo\ndtype: object",
      "In [353]: df1 = pd.DataFrame(np.random.randn(8, 1), columns=[\"A\"], dtype=\"float32\")\n\nIn [354]: df1\nOut[354]: \n          A\n0  0.224364\n1  1.890546\n2  0.182879\n3  0.787847\n4 -0.188449\n5  0.667715\n6 -0.011736\n7 -0.399073\n\nIn [355]: df1.dtypes\nOut[355]: \nA    float32\ndtype: object\n\nIn [356]: df2 = pd.DataFrame(\n   .....:     {\n   .....:         \"A\": pd.Series(np.random.randn(8), dtype=\"float16\"),\n   .....:         \"B\": pd.Series(np.random.randn(8)),\n   .....:         \"C\": pd.Series(np.random.randint(0, 255, size=8), dtype=\"uint8\"),  # [0,255] (range of uint8)\n   .....:     }\n   .....: )\n   .....: \n\nIn [357]: df2\nOut[357]: \n          A         B    C\n0  0.823242  0.256090   26\n1  1.607422  1.426469   86\n2 -0.333740 -0.416203   46\n3 -0.063477  1.139976  212\n4 -1.014648 -1.193477   26\n5  0.678711  0.096706    7\n6 -0.040863 -1.956850  184\n7 -0.357422 -0.714337  206\n\nIn [358]: df2.dtypes\nOut[358]: \nA    float16\nB    float64\nC      uint8\ndtype: object",
      "In [359]: pd.DataFrame([1, 2], columns=[\"a\"]).dtypes\nOut[359]: \na    int64\ndtype: object\n\nIn [360]: pd.DataFrame({\"a\": [1, 2]}).dtypes\nOut[360]: \na    int64\ndtype: object\n\nIn [361]: pd.DataFrame({\"a\": 1}, index=list(range(2))).dtypes\nOut[361]: \na    int64\ndtype: object",
      "In [362]: frame = pd.DataFrame(np.array([1, 2]))",
      "In [370]: dft = pd.DataFrame({\"a\": [1, 2, 3], \"b\": [4, 5, 6], \"c\": [7, 8, 9]})\n\nIn [371]: dft[[\"a\", \"b\"]] = dft[[\"a\", \"b\"]].astype(np.uint8)\n\nIn [372]: dft\nOut[372]: \n   a  b  c\n0  1  4  7\n1  2  5  8\n2  3  6  9\n\nIn [373]: dft.dtypes\nOut[373]: \na    uint8\nb    uint8\nc    int64\ndtype: object",
      "In [374]: dft1 = pd.DataFrame({\"a\": [1, 0, 1], \"b\": [4, 5, 6], \"c\": [7, 8, 9]})\n\nIn [375]: dft1 = dft1.astype({\"a\": np.bool_, \"c\": np.float64})\n\nIn [376]: dft1\nOut[376]: \n       a  b    c\n0   True  4  7.0\n1  False  5  8.0\n2   True  6  9.0\n\nIn [377]: dft1.dtypes\nOut[377]: \na       bool\nb      int64\nc    float64\ndtype: object",
      "In [378]: dft = pd.DataFrame({\"a\": [1, 2, 3], \"b\": [4, 5, 6], \"c\": [7, 8, 9]})\n\nIn [379]: dft.loc[:, [\"a\", \"b\"]].astype(np.uint8).dtypes\nOut[379]: \na    uint8\nb    uint8\ndtype: object\n\nIn [380]: dft.loc[:, [\"a\", \"b\"]] = dft.loc[:, [\"a\", \"b\"]].astype(np.uint8)\n\nIn [381]: dft.dtypes\nOut[381]: \na    int64\nb    int64\nc    int64\ndtype: object",
      "In [382]: import datetime\n\nIn [383]: df = pd.DataFrame(\n   .....:     [\n   .....:         [1, 2],\n   .....:         [\"a\", \"b\"],\n   .....:         [datetime.datetime(2016, 3, 2), datetime.datetime(2016, 3, 2)],\n   .....:     ]\n   .....: )\n   .....: \n\nIn [384]: df = df.T\n\nIn [385]: df\nOut[385]: \n   0  1                    2\n0  1  a  2016-03-02 00:00:00\n1  2  b  2016-03-02 00:00:00\n\nIn [386]: df.dtypes\nOut[386]: \n0    object\n1    object\n2    object\ndtype: object",
      "In [388]: m = [\"1.1\", 2, 3]\n\nIn [389]: pd.to_numeric(m)\nOut[389]: array([1.1, 2. , 3. ])",
      "In [390]: import datetime\n\nIn [391]: m = [\"2016-07-09\", datetime.datetime(2016, 3, 2)]\n\nIn [392]: pd.to_datetime(m)\nOut[392]: DatetimeIndex(['2016-07-09', '2016-03-02'], dtype='datetime64[ns]', freq=None)",
      "In [393]: m = [\"5us\", pd.Timedelta(\"1day\")]\n\nIn [394]: pd.to_timedelta(m)\nOut[394]: TimedeltaIndex(['0 days 00:00:00.000005', '1 days 00:00:00'], dtype='timedelta64[ns]', freq=None)",
      "In [395]: import datetime\n\nIn [396]: m = [\"apple\", datetime.datetime(2016, 3, 2)]\n\nIn [397]: pd.to_datetime(m, errors=\"coerce\")\nOut[397]: DatetimeIndex(['NaT', '2016-03-02'], dtype='datetime64[ns]', freq=None)\n\nIn [398]: m = [\"apple\", 2, 3]\n\nIn [399]: pd.to_numeric(m, errors=\"coerce\")\nOut[399]: array([nan,  2.,  3.])\n\nIn [400]: m = [\"apple\", pd.Timedelta(\"1day\")]\n\nIn [401]: pd.to_timedelta(m, errors=\"coerce\")\nOut[401]: TimedeltaIndex([NaT, '1 days'], dtype='timedelta64[ns]', freq=None)",
      "In [402]: m = [\"1\", 2, 3]\n\nIn [403]: pd.to_numeric(m, downcast=\"integer\")  # smallest signed int dtype\nOut[403]: array([1, 2, 3], dtype=int8)\n\nIn [404]: pd.to_numeric(m, downcast=\"signed\")  # same as 'integer'\nOut[404]: array([1, 2, 3], dtype=int8)\n\nIn [405]: pd.to_numeric(m, downcast=\"unsigned\")  # smallest unsigned int dtype\nOut[405]: array([1, 2, 3], dtype=uint8)\n\nIn [406]: pd.to_numeric(m, downcast=\"float\")  # smallest float dtype\nOut[406]: array([1., 2., 3.], dtype=float32)",
      "In [407]: import datetime\n\nIn [408]: df = pd.DataFrame([[\"2016-07-09\", datetime.datetime(2016, 3, 2)]] * 2, dtype=\"O\")\n\nIn [409]: df\nOut[409]: \n            0                    1\n0  2016-07-09  2016-03-02 00:00:00\n1  2016-07-09  2016-03-02 00:00:00\n\nIn [410]: df.apply(pd.to_datetime)\nOut[410]: \n           0          1\n0 2016-07-09 2016-03-02\n1 2016-07-09 2016-03-02\n\nIn [411]: df = pd.DataFrame([[\"1.1\", 2, 3]] * 2, dtype=\"O\")\n\nIn [412]: df\nOut[412]: \n     0  1  2\n0  1.1  2  3\n1  1.1  2  3\n\nIn [413]: df.apply(pd.to_numeric)\nOut[413]: \n     0  1  2\n0  1.1  2  3\n1  1.1  2  3\n\nIn [414]: df = pd.DataFrame([[\"5us\", pd.Timedelta(\"1day\")]] * 2, dtype=\"O\")\n\nIn [415]: df\nOut[415]: \n     0                1\n0  5us  1 days 00:00:00\n1  5us  1 days 00:00:00\n\nIn [416]: df.apply(pd.to_timedelta)\nOut[416]: \n                       0      1\n0 0 days 00:00:00.000005 1 days\n1 0 days 00:00:00.000005 1 days",
      "In [430]: df = pd.DataFrame(\n   .....:     {\n   .....:         \"string\": list(\"abc\"),\n   .....:         \"int64\": list(range(1, 4)),\n   .....:         \"uint8\": np.arange(3, 6).astype(\"u1\"),\n   .....:         \"float64\": np.arange(4.0, 7.0),\n   .....:         \"bool1\": [True, False, True],\n   .....:         \"bool2\": [False, True, False],\n   .....:         \"dates\": pd.date_range(\"now\", periods=3),\n   .....:         \"category\": pd.Series(list(\"ABC\")).astype(\"category\"),\n   .....:     }\n   .....: )\n   .....: \n\nIn [431]: df[\"tdeltas\"] = df.dates.diff()\n\nIn [432]: df[\"uint64\"] = np.arange(3, 6).astype(\"u8\")\n\nIn [433]: df[\"other_dates\"] = pd.date_range(\"20130101\", periods=3)\n\nIn [434]: df[\"tz_aware_dates\"] = pd.date_range(\"20130101\", periods=3, tz=\"US/Eastern\")\n\nIn [435]: df\nOut[435]: \n  string  int64  uint8  ...  uint64  other_dates            tz_aware_dates\n0      a      1      3  ...       3   2013-01-01 2013-01-01 00:00:00-05:00\n1      b      2      4  ...       4   2013-01-02 2013-01-02 00:00:00-05:00\n2      c      3      5  ...       5   2013-01-03 2013-01-03 00:00:00-05:00\n\n[3 rows x 12 columns]",
      "In [441]: def subdtypes(dtype):\n   .....:     subs = dtype.__subclasses__()\n   .....:     if not subs:\n   .....:         return dtype\n   .....:     return [dtype, [subdtypes(dt) for dt in subs]]\n   .....:"
    ]
  },
  {
    "url": "https://pandas.pydata.org/docs/user_guide/indexing.html",
    "title": "Indexing and selecting data#",
    "code_snippets": [
      "In [1]: ser = pd.Series(range(5), index=list(\"abcde\"))\n\nIn [2]: ser.loc[[\"a\", \"c\", \"e\"]]\nOut[2]: \na    0\nc    2\ne    4\ndtype: int64\n\nIn [3]: df = pd.DataFrame(np.arange(25).reshape(5, 5), index=list(\"abcde\"), columns=list(\"abcde\"))\n\nIn [4]: df.loc[[\"a\", \"c\", \"e\"], [\"b\", \"d\"]]\nOut[4]: \n    b   d\na   1   3\nc  11  13\ne  21  23",
      "In [5]: dates = pd.date_range('1/1/2000', periods=8)\n\nIn [6]: df = pd.DataFrame(np.random.randn(8, 4),\n   ...:                   index=dates, columns=['A', 'B', 'C', 'D'])\n   ...: \n\nIn [7]: df\nOut[7]: \n                   A         B         C         D\n2000-01-01  0.469112 -0.282863 -1.509059 -1.135632\n2000-01-02  1.212112 -0.173215  0.119209 -1.044236\n2000-01-03 -0.861849 -2.104569 -0.494929  1.071804\n2000-01-04  0.721555 -0.706771 -1.039575  0.271860\n2000-01-05 -0.424972  0.567020  0.276232 -1.087401\n2000-01-06 -0.673690  0.113648 -1.478427  0.524988\n2000-01-07  0.404705  0.577046 -1.715002 -1.039268\n2000-01-08 -0.370647 -1.157892 -1.344312  0.844885",
      "In [21]: sa = pd.Series([1, 2, 3], index=list('abc'))\n\nIn [22]: dfa = df.copy()",
      "In [31]: x = pd.DataFrame({'x': [1, 2, 3], 'y': [3, 4, 5]})\n\nIn [32]: x.iloc[1] = {'x': 9, 'y': 99}\n\nIn [33]: x\nOut[33]: \n   x   y\n0  1   3\n1  9  99\n2  3   5",
      "In [34]: df_new = pd.DataFrame({'one': [1., 2., 3.]})\n\nIn [35]: df_new.two = [4, 5, 6]\n\nIn [36]: df_new\nOut[36]: \n   one\n0  1.0\n1  2.0\n2  3.0",
      "In [45]: dfl = pd.DataFrame(np.random.randn(5, 4),\n   ....:                    columns=list('ABCD'),\n   ....:                    index=pd.date_range('20130101', periods=5))\n   ....: \n\nIn [46]: dfl\nOut[46]: \n                   A         B         C         D\n2013-01-01  1.075770 -0.109050  1.643563 -1.469388\n2013-01-02  0.357021 -0.674600 -1.776904 -0.968914\n2013-01-03 -1.294524  0.413738  0.276662 -0.472035\n2013-01-04 -0.013960 -0.362543 -0.006154 -0.923061\n2013-01-05  0.895717  0.805244 -1.206412  2.565646\n\nIn [47]: dfl.loc[2:3]\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[47], line 1\n----> 1 dfl.loc[2:3]\n\nFile ~/work/pandas/pandas/pandas/core/indexing.py:1191, in _LocationIndexer.__getitem__(self, key)\n   1189 maybe_callable = com.apply_if_callable(key, self.obj)\n   1190 maybe_callable = self._check_deprecated_callable_usage(key, maybe_callable)\n-> 1191 return self._getitem_axis(maybe_callable, axis=axis)\n\nFile ~/work/pandas/pandas/pandas/core/indexing.py:1411, in _LocIndexer._getitem_axis(self, key, axis)\n   1409 if isinstance(key, slice):\n   1410     self._validate_key(key, axis)\n-> 1411     return self._get_slice_axis(key, axis=axis)\n   1412 elif com.is_bool_indexer(key):\n   1413     return self._getbool_axis(key, axis=axis)\n\nFile ~/work/pandas/pandas/pandas/core/indexing.py:1443, in _LocIndexer._get_slice_axis(self, slice_obj, axis)\n   1440     return obj.copy(deep=False)\n   1442 labels = obj._get_axis(axis)\n-> 1443 indexer = labels.slice_indexer(slice_obj.start, slice_obj.stop, slice_obj.step)\n   1445 if isinstance(indexer, slice):\n   1446     return self.obj._slice(indexer, axis=axis)\n\nFile ~/work/pandas/pandas/pandas/core/indexes/datetimes.py:682, in DatetimeIndex.slice_indexer(self, start, end, step)\n    674 # GH#33146 if start and end are combinations of str and None and Index is not\n    675 # monotonic, we can not use Index.slice_indexer because it does not honor the\n    676 # actual elements, is only searching for start and end\n    677 if (\n    678     check_str_or_none(start)\n    679     or check_str_or_none(end)\n    680     or self.is_monotonic_increasing\n    681 ):\n--> 682     return Index.slice_indexer(self, start, end, step)\n    684 mask = np.array(True)\n    685 in_index = True\n\nFile ~/work/pandas/pandas/pandas/core/indexes/base.py:6662, in Index.slice_indexer(self, start, end, step)\n   6618 def slice_indexer(\n   6619     self,\n   6620     start: Hashable | None = None,\n   6621     end: Hashable | None = None,\n   6622     step: int | None = None,\n   6623 ) -> slice:\n   6624     \"\"\"\n   6625     Compute the slice indexer for input labels and step.\n   6626 \n   (...)\n   6660     slice(1, 3, None)\n   6661     \"\"\"\n-> 6662     start_slice, end_slice = self.slice_locs(start, end, step=step)\n   6664     # return a slice\n   6665     if not is_scalar(start_slice):\n\nFile ~/work/pandas/pandas/pandas/core/indexes/base.py:6879, in Index.slice_locs(self, start, end, step)\n   6877 start_slice = None\n   6878 if start is not None:\n-> 6879     start_slice = self.get_slice_bound(start, \"left\")\n   6880 if start_slice is None:\n   6881     start_slice = 0\n\nFile ~/work/pandas/pandas/pandas/core/indexes/base.py:6794, in Index.get_slice_bound(self, label, side)\n   6790 original_label = label\n   6792 # For datetime indices label may be a string that has to be converted\n   6793 # to datetime boundary according to its resolution.\n-> 6794 label = self._maybe_cast_slice_bound(label, side)\n   6796 # we need to look up the label\n   6797 try:\n\nFile ~/work/pandas/pandas/pandas/core/indexes/datetimes.py:642, in DatetimeIndex._maybe_cast_slice_bound(self, label, side)\n    637 if isinstance(label, dt.date) and not isinstance(label, dt.datetime):\n    638     # Pandas supports slicing with dates, treated as datetimes at midnight.\n    639     # https://github.com/pandas-dev/pandas/issues/31501\n    640     label = Timestamp(label).to_pydatetime()\n--> 642 label = super()._maybe_cast_slice_bound(label, side)\n    643 self._data._assert_tzawareness_compat(label)\n    644 return Timestamp(label)\n\nFile ~/work/pandas/pandas/pandas/core/indexes/datetimelike.py:378, in DatetimeIndexOpsMixin._maybe_cast_slice_bound(self, label, side)\n    376     return lower if side == \"left\" else upper\n    377 elif not isinstance(label, self._data._recognized_scalars):\n--> 378     self._raise_invalid_indexer(\"slice\", label)\n    380 return label\n\nFile ~/work/pandas/pandas/pandas/core/indexes/base.py:4301, in Index._raise_invalid_indexer(self, form, key, reraise)\n   4299 if reraise is not lib.no_default:\n   4300     raise TypeError(msg) from reraise\n-> 4301 raise TypeError(msg)\n\nTypeError: cannot do slice indexing on DatetimeIndex with these indexers [2] of type int",
      "In [49]: s1 = pd.Series(np.random.randn(6), index=list('abcdef'))\n\nIn [50]: s1\nOut[50]: \na    1.431256\nb    1.340309\nc   -1.170299\nd   -0.226169\ne    0.410835\nf    0.813850\ndtype: float64\n\nIn [51]: s1.loc['c':]\nOut[51]: \nc   -1.170299\nd   -0.226169\ne    0.410835\nf    0.813850\ndtype: float64\n\nIn [52]: s1.loc['b']\nOut[52]: 1.3403088497993827",
      "In [55]: df1 = pd.DataFrame(np.random.randn(6, 4),\n   ....:                    index=list('abcdef'),\n   ....:                    columns=list('ABCD'))\n   ....: \n\nIn [56]: df1\nOut[56]: \n          A         B         C         D\na  0.132003 -0.827317 -0.076467 -1.187678\nb  1.130127 -1.436737 -1.413681  1.607920\nc  1.024180  0.569605  0.875906 -2.211372\nd  0.974466 -2.006747 -0.410001 -0.078638\ne  0.545952 -1.219217 -1.226825  0.769804\nf -1.281247 -0.727707 -0.121306 -0.097883\n\nIn [57]: df1.loc[['a', 'b', 'd'], :]\nOut[57]: \n          A         B         C         D\na  0.132003 -0.827317 -0.076467 -1.187678\nb  1.130127 -1.436737 -1.413681  1.607920\nd  0.974466 -2.006747 -0.410001 -0.078638",
      "In [62]: mask = pd.array([True, False, True, False, pd.NA, False], dtype=\"boolean\")\n\nIn [63]: mask\nOut[63]: \n<BooleanArray>\n[True, False, True, False, <NA>, False]\nLength: 6, dtype: boolean\n\nIn [64]: df1[mask]\nOut[64]: \n          A         B         C         D\na  0.132003 -0.827317 -0.076467 -1.187678\nc  1.024180  0.569605  0.875906 -2.211372",
      "In [66]: s = pd.Series(list('abcde'), index=[0, 3, 2, 5, 4])\n\nIn [67]: s.loc[3:5]\nOut[67]: \n3    b\n2    c\n5    d\ndtype: object",
      "In [70]: s = pd.Series(list('abcdef'), index=[0, 3, 2, 5, 4, 2])\n\nIn [71]: s.loc[3:5]\nOut[71]: \n3    b\n2    c\n5    d\ndtype: object",
      "In [72]: s1 = pd.Series(np.random.randn(5), index=list(range(0, 10, 2)))\n\nIn [73]: s1\nOut[73]: \n0    0.695775\n2    0.341734\n4    0.959726\n6   -1.110336\n8   -0.619976\ndtype: float64\n\nIn [74]: s1.iloc[:3]\nOut[74]: \n0    0.695775\n2    0.341734\n4    0.959726\ndtype: float64\n\nIn [75]: s1.iloc[3]\nOut[75]: -1.110336102891167",
      "In [78]: df1 = pd.DataFrame(np.random.randn(6, 4),\n   ....:                    index=list(range(0, 12, 2)),\n   ....:                    columns=list(range(0, 8, 2)))\n   ....: \n\nIn [79]: df1\nOut[79]: \n           0         2         4         6\n0   0.149748 -0.732339  0.687738  0.176444\n2   0.403310 -0.154951  0.301624 -2.179861\n4  -1.369849 -0.954208  1.462696 -1.743161\n6  -0.826591 -0.345352  1.314232  0.690579\n8   0.995761  2.396780  0.014871  3.357427\n10 -0.317441 -1.236269  0.896171 -0.487602",
      "# these are allowed in Python/NumPy.\nIn [87]: x = list('abcdef')\n\nIn [88]: x\nOut[88]: ['a', 'b', 'c', 'd', 'e', 'f']\n\nIn [89]: x[4:10]\nOut[89]: ['e', 'f']\n\nIn [90]: x[8:10]\nOut[90]: []\n\nIn [91]: s = pd.Series(x)\n\nIn [92]: s\nOut[92]: \n0    a\n1    b\n2    c\n3    d\n4    e\n5    f\ndtype: object\n\nIn [93]: s.iloc[4:10]\nOut[93]: \n4    e\n5    f\ndtype: object\n\nIn [94]: s.iloc[8:10]\nOut[94]: Series([], dtype: object)",
      "In [95]: dfl = pd.DataFrame(np.random.randn(5, 2), columns=list('AB'))\n\nIn [96]: dfl\nOut[96]: \n          A         B\n0 -0.082240 -2.182937\n1  0.380396  0.084844\n2  0.432390  1.519970\n3 -0.493662  0.600178\n4  0.274230  0.132885\n\nIn [97]: dfl.iloc[:, 2:3]\nOut[97]: \nEmpty DataFrame\nColumns: []\nIndex: [0, 1, 2, 3, 4]\n\nIn [98]: dfl.iloc[:, 1:3]\nOut[98]: \n          B\n0 -2.182937\n1  0.084844\n2  1.519970\n3  0.600178\n4  0.132885\n\nIn [99]: dfl.iloc[4:6]\nOut[99]: \n         A         B\n4  0.27423  0.132885",
      "In [101]: dfl.iloc[:, 4]\n---------------------------------------------------------------------------\nIndexError                                Traceback (most recent call last)\nCell In[101], line 1\n----> 1 dfl.iloc[:, 4]\n\nFile ~/work/pandas/pandas/pandas/core/indexing.py:1184, in _LocationIndexer.__getitem__(self, key)\n   1182     if self._is_scalar_access(key):\n   1183         return self.obj._get_value(*key, takeable=self._takeable)\n-> 1184     return self._getitem_tuple(key)\n   1185 else:\n   1186     # we by definition only have the 0th axis\n   1187     axis = self.axis or 0\n\nFile ~/work/pandas/pandas/pandas/core/indexing.py:1690, in _iLocIndexer._getitem_tuple(self, tup)\n   1689 def _getitem_tuple(self, tup: tuple):\n-> 1690     tup = self._validate_tuple_indexer(tup)\n   1691     with suppress(IndexingError):\n   1692         return self._getitem_lowerdim(tup)\n\nFile ~/work/pandas/pandas/pandas/core/indexing.py:966, in _LocationIndexer._validate_tuple_indexer(self, key)\n    964 for i, k in enumerate(key):\n    965     try:\n--> 966         self._validate_key(k, i)\n    967     except ValueError as err:\n    968         raise ValueError(\n    969             \"Location based indexing can only have \"\n    970             f\"[{self._valid_types}] types\"\n    971         ) from err\n\nFile ~/work/pandas/pandas/pandas/core/indexing.py:1592, in _iLocIndexer._validate_key(self, key, axis)\n   1590     return\n   1591 elif is_integer(key):\n-> 1592     self._validate_integer(key, axis)\n   1593 elif isinstance(key, tuple):\n   1594     # a tuple should already have been caught by this point\n   1595     # so don't treat a tuple as a valid indexer\n   1596     raise IndexingError(\"Too many indexers\")\n\nFile ~/work/pandas/pandas/pandas/core/indexing.py:1685, in _iLocIndexer._validate_integer(self, key, axis)\n   1683 len_axis = len(self.obj._get_axis(axis))\n   1684 if key >= len_axis or key < -len_axis:\n-> 1685     raise IndexError(\"single positional indexer is out-of-bounds\")\n\nIndexError: single positional indexer is out-of-bounds",
      "In [102]: df1 = pd.DataFrame(np.random.randn(6, 4),\n   .....:                    index=list('abcdef'),\n   .....:                    columns=list('ABCD'))\n   .....: \n\nIn [103]: df1\nOut[103]: \n          A         B         C         D\na -0.023688  2.410179  1.450520  0.206053\nb -0.251905 -2.213588  1.063327  1.266143\nc  0.299368 -0.863838  0.408204 -1.048089\nd -0.025747 -0.988387  0.094055  1.262731\ne  1.289997  0.082423 -0.055758  0.536580\nf -0.489682  0.369374 -0.034571 -2.484478\n\nIn [104]: df1.loc[lambda df: df['A'] > 0, :]\nOut[104]: \n          A         B         C         D\nc  0.299368 -0.863838  0.408204 -1.048089\ne  1.289997  0.082423 -0.055758  0.536580\n\nIn [105]: df1.loc[:, lambda df: ['A', 'B']]\nOut[105]: \n          A         B\na -0.023688  2.410179\nb -0.251905 -2.213588\nc  0.299368 -0.863838\nd -0.025747 -0.988387\ne  1.289997  0.082423\nf -0.489682  0.369374\n\nIn [106]: df1.iloc[:, lambda df: [0, 1]]\nOut[106]: \n          A         B\na -0.023688  2.410179\nb -0.251905 -2.213588\nc  0.299368 -0.863838\nd -0.025747 -0.988387\ne  1.289997  0.082423\nf -0.489682  0.369374\n\nIn [107]: df1[lambda df: df.columns[0]]\nOut[107]: \na   -0.023688\nb   -0.251905\nc    0.299368\nd   -0.025747\ne    1.289997\nf   -0.489682\nName: A, dtype: float64",
      "In [109]: bb = pd.read_csv('data/baseball.csv', index_col='id')\n\nIn [110]: (bb.groupby(['year', 'team']).sum(numeric_only=True)\n   .....:    .loc[lambda df: df['r'] > 100])\n   .....: \nOut[110]: \n           stint    g    ab    r    h  X2b  ...     so   ibb   hbp    sh    sf  gidp\nyear team                                   ...                                     \n2007 CIN       6  379   745  101  203   35  ...  127.0  14.0   1.0   1.0  15.0  18.0\n     DET       5  301  1062  162  283   54  ...  176.0   3.0  10.0   4.0   8.0  28.0\n     HOU       4  311   926  109  218   47  ...  212.0   3.0   9.0  16.0   6.0  17.0\n     LAN      11  413  1021  153  293   61  ...  141.0   8.0   9.0   3.0   8.0  29.0\n     NYN      13  622  1854  240  509  101  ...  310.0  24.0  23.0  18.0  15.0  48.0\n     SFN       5  482  1305  198  337   67  ...  188.0  51.0   8.0  16.0   6.0  41.0\n     TEX       2  198   729  115  200   40  ...  140.0   4.0   5.0   2.0   8.0  16.0\n     TOR       4  459  1408  187  378   96  ...  265.0  16.0  12.0   4.0  16.0  38.0\n\n[8 rows x 18 columns]",
      "In [111]: dfd = pd.DataFrame({'A': [1, 2, 3],\n   .....:                     'B': [4, 5, 6]},\n   .....:                    index=list('abc'))\n   .....: \n\nIn [112]: dfd\nOut[112]: \n   A  B\na  1  4\nb  2  5\nc  3  6\n\nIn [113]: dfd.loc[dfd.index[[0, 2]], 'A']\nOut[113]: \na    1\nc    3\nName: A, dtype: int64",
      "In [116]: s = pd.Series([1, 2, 3])\n\nIn [117]: s.reindex([1, 2, 3])\nOut[117]: \n1    2.0\n2    3.0\n3    NaN\ndtype: float64",
      "In [120]: s = pd.Series(np.arange(4), index=['a', 'a', 'b', 'c'])\n\nIn [121]: labels = ['c', 'd']\n\nIn [122]: s.reindex(labels)\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nCell In[122], line 1\n----> 1 s.reindex(labels)\n\nFile ~/work/pandas/pandas/pandas/core/series.py:5153, in Series.reindex(self, index, axis, method, copy, level, fill_value, limit, tolerance)\n   5136 @doc(\n   5137     NDFrame.reindex,  # type: ignore[has-type]\n   5138     klass=_shared_doc_kwargs[\"klass\"],\n   (...)\n   5151     tolerance=None,\n   5152 ) -> Series:\n-> 5153     return super().reindex(\n   5154         index=index,\n   5155         method=method,\n   5156         copy=copy,\n   5157         level=level,\n   5158         fill_value=fill_value,\n   5159         limit=limit,\n   5160         tolerance=tolerance,\n   5161     )\n\nFile ~/work/pandas/pandas/pandas/core/generic.py:5610, in NDFrame.reindex(self, labels, index, columns, axis, method, copy, level, fill_value, limit, tolerance)\n   5607     return self._reindex_multi(axes, copy, fill_value)\n   5609 # perform the reindex on the axes\n-> 5610 return self._reindex_axes(\n   5611     axes, level, limit, tolerance, method, fill_value, copy\n   5612 ).__finalize__(self, method=\"reindex\")\n\nFile ~/work/pandas/pandas/pandas/core/generic.py:5633, in NDFrame._reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy)\n   5630     continue\n   5632 ax = self._get_axis(a)\n-> 5633 new_index, indexer = ax.reindex(\n   5634     labels, level=level, limit=limit, tolerance=tolerance, method=method\n   5635 )\n   5637 axis = self._get_axis_number(a)\n   5638 obj = obj._reindex_with_indexers(\n   5639     {axis: [new_index, indexer]},\n   5640     fill_value=fill_value,\n   5641     copy=copy,\n   5642     allow_dups=False,\n   5643 )\n\nFile ~/work/pandas/pandas/pandas/core/indexes/base.py:4429, in Index.reindex(self, target, method, level, limit, tolerance)\n   4426     raise ValueError(\"cannot handle a non-unique multi-index!\")\n   4427 elif not self.is_unique:\n   4428     # GH#42568\n-> 4429     raise ValueError(\"cannot reindex on an axis with duplicate labels\")\n   4430 else:\n   4431     indexer, _ = self.get_indexer_non_unique(target)\n\nValueError: cannot reindex on an axis with duplicate labels",
      "In [126]: s = pd.Series([0, 1, 2, 3, 4, 5])\n\n# When no arguments are passed, returns 1 row.\nIn [127]: s.sample()\nOut[127]: \n4    4\ndtype: int64\n\n# One may specify either a number of rows:\nIn [128]: s.sample(n=3)\nOut[128]: \n0    0\n4    4\n1    1\ndtype: int64\n\n# Or a fraction of the rows:\nIn [129]: s.sample(frac=0.5)\nOut[129]: \n5    5\n3    3\n1    1\ndtype: int64",
      "In [130]: s = pd.Series([0, 1, 2, 3, 4, 5])\n\n# Without replacement (default):\nIn [131]: s.sample(n=6, replace=False)\nOut[131]: \n0    0\n1    1\n5    5\n3    3\n2    2\n4    4\ndtype: int64\n\n# With replacement:\nIn [132]: s.sample(n=6, replace=True)\nOut[132]: \n0    0\n4    4\n3    3\n2    2\n4    4\n4    4\ndtype: int64",
      "In [133]: s = pd.Series([0, 1, 2, 3, 4, 5])\n\nIn [134]: example_weights = [0, 0, 0.2, 0.2, 0.2, 0.4]\n\nIn [135]: s.sample(n=3, weights=example_weights)\nOut[135]: \n5    5\n4    4\n3    3\ndtype: int64\n\n# Weights will be re-normalized automatically\nIn [136]: example_weights2 = [0.5, 0, 0, 0, 0, 0]\n\nIn [137]: s.sample(n=1, weights=example_weights2)\nOut[137]: \n0    0\ndtype: int64",
      "In [138]: df2 = pd.DataFrame({'col1': [9, 8, 7, 6],\n   .....:                     'weight_column': [0.5, 0.4, 0.1, 0]})\n   .....: \n\nIn [139]: df2.sample(n=3, weights='weight_column')\nOut[139]: \n   col1  weight_column\n1     8            0.4\n0     9            0.5\n2     7            0.1",
      "In [140]: df3 = pd.DataFrame({'col1': [1, 2, 3], 'col2': [2, 3, 4]})\n\nIn [141]: df3.sample(n=1, axis=1)\nOut[141]: \n   col1\n0     1\n1     2\n2     3",
      "In [142]: df4 = pd.DataFrame({'col1': [1, 2, 3], 'col2': [2, 3, 4]})\n\n# With a given seed, the sample will always draw the same rows.\nIn [143]: df4.sample(n=2, random_state=2)\nOut[143]: \n   col1  col2\n2     3     4\n1     2     3\n\nIn [144]: df4.sample(n=2, random_state=2)\nOut[144]: \n   col1  col2\n2     3     4\n1     2     3",
      "In [145]: se = pd.Series([1, 2, 3])\n\nIn [146]: se\nOut[146]: \n0    1\n1    2\n2    3\ndtype: int64\n\nIn [147]: se[5] = 5.\n\nIn [148]: se\nOut[148]: \n0    1.0\n1    2.0\n2    3.0\n5    5.0\ndtype: float64",
      "In [149]: dfi = pd.DataFrame(np.arange(6).reshape(3, 2),\n   .....:                    columns=['A', 'B'])\n   .....: \n\nIn [150]: dfi\nOut[150]: \n   A  B\n0  0  1\n1  2  3\n2  4  5\n\nIn [151]: dfi.loc[:, 'C'] = dfi.loc[:, 'A']\n\nIn [152]: dfi\nOut[152]: \n   A  B  C\n0  0  1  0\n1  2  3  2\n2  4  5  4",
      "In [160]: df.at[dates[-1] + pd.Timedelta('1 day'), 0] = 7\n\nIn [161]: df\nOut[161]: \n                   A         B         C         D    E    0\n2000-01-01 -0.282863  0.469112 -1.509059 -1.135632  NaN  NaN\n2000-01-02 -0.173215  1.212112  0.119209 -1.044236  NaN  NaN\n2000-01-03 -2.104569 -0.861849 -0.494929  1.071804  NaN  NaN\n2000-01-04  7.000000  0.721555 -1.039575  0.271860  NaN  NaN\n2000-01-05  0.567020 -0.424972  0.276232 -1.087401  NaN  NaN\n2000-01-06  0.113648 -0.673690 -1.478427  0.524988  7.0  NaN\n2000-01-07  0.577046  0.404705 -1.715002 -1.039268  NaN  NaN\n2000-01-08 -1.157892 -0.370647 -1.344312  0.844885  NaN  NaN\n2000-01-09       NaN       NaN       NaN       NaN  NaN  7.0",
      "In [162]: s = pd.Series(range(-3, 4))\n\nIn [163]: s\nOut[163]: \n0   -3\n1   -2\n2   -1\n3    0\n4    1\n5    2\n6    3\ndtype: int64\n\nIn [164]: s[s > 0]\nOut[164]: \n4    1\n5    2\n6    3\ndtype: int64\n\nIn [165]: s[(s < -1) | (s > 0.5)]\nOut[165]: \n0   -3\n1   -2\n4    1\n5    2\n6    3\ndtype: int64\n\nIn [166]: s[~(s < 0)]\nOut[166]: \n3    0\n4    1\n5    2\n6    3\ndtype: int64",
      "In [168]: df2 = pd.DataFrame({'a': ['one', 'one', 'two', 'three', 'two', 'one', 'six'],\n   .....:                     'b': ['x', 'y', 'y', 'x', 'y', 'x', 'x'],\n   .....:                     'c': np.random.randn(7)})\n   .....: \n\n# only want 'two' or 'three'\nIn [169]: criterion = df2['a'].map(lambda x: x.startswith('t'))\n\nIn [170]: df2[criterion]\nOut[170]: \n       a  b         c\n2    two  y  0.041290\n3  three  x  0.361719\n4    two  y -0.238075\n\n# equivalent but slower\nIn [171]: df2[[x.startswith('t') for x in df2['a']]]\nOut[171]: \n       a  b         c\n2    two  y  0.041290\n3  three  x  0.361719\n4    two  y -0.238075\n\n# Multiple criteria\nIn [172]: df2[criterion & (df2['b'] == 'x')]\nOut[172]: \n       a  b         c\n3  three  x  0.361719",
      "In [174]: df = pd.DataFrame([[1, 2], [3, 4], [5, 6]],\n   .....:                   index=list('abc'),\n   .....:                   columns=['A', 'B'])\n   .....: \n\nIn [175]: s = (df['A'] > 2)\n\nIn [176]: s\nOut[176]: \na    False\nb     True\nc     True\nName: A, dtype: bool\n\nIn [177]: df.loc[s, 'B']\nOut[177]: \nb    4\nc    6\nName: B, dtype: int64\n\nIn [178]: df.iloc[s.values, 1]\nOut[178]: \nb    4\nc    6\nName: B, dtype: int64",
      "In [179]: s = pd.Series(np.arange(5), index=np.arange(5)[::-1], dtype='int64')\n\nIn [180]: s\nOut[180]: \n4    0\n3    1\n2    2\n1    3\n0    4\ndtype: int64\n\nIn [181]: s.isin([2, 4, 6])\nOut[181]: \n4    False\n3    False\n2     True\n1    False\n0     True\ndtype: bool\n\nIn [182]: s[s.isin([2, 4, 6])]\nOut[182]: \n2    2\n0    4\ndtype: int64",
      "In [185]: s_mi = pd.Series(np.arange(6),\n   .....:                  index=pd.MultiIndex.from_product([[0, 1], ['a', 'b', 'c']]))\n   .....: \n\nIn [186]: s_mi\nOut[186]: \n0  a    0\n   b    1\n   c    2\n1  a    3\n   b    4\n   c    5\ndtype: int64\n\nIn [187]: s_mi.iloc[s_mi.index.isin([(1, 'a'), (2, 'b'), (0, 'c')])]\nOut[187]: \n0  c    2\n1  a    3\ndtype: int64\n\nIn [188]: s_mi.iloc[s_mi.index.isin(['a', 'c', 'e'], level=1)]\nOut[188]: \n0  a    0\n   c    2\n1  a    3\n   c    5\ndtype: int64",
      "In [189]: df = pd.DataFrame({'vals': [1, 2, 3, 4], 'ids': ['a', 'b', 'f', 'n'],\n   .....:                    'ids2': ['a', 'n', 'c', 'n']})\n   .....: \n\nIn [190]: values = ['a', 'b', 1, 3]\n\nIn [191]: df.isin(values)\nOut[191]: \n    vals    ids   ids2\n0   True   True   True\n1  False   True  False\n2   True  False  False\n3  False  False  False",
      "In [201]: dates = pd.date_range('1/1/2000', periods=8)\n\nIn [202]: df = pd.DataFrame(np.random.randn(8, 4),\n   .....:                   index=dates, columns=['A', 'B', 'C', 'D'])\n   .....: \n\nIn [203]: df[df < 0]\nOut[203]: \n                   A         B         C         D\n2000-01-01 -2.104139 -1.309525       NaN       NaN\n2000-01-02 -0.352480       NaN -1.192319       NaN\n2000-01-03 -0.864883       NaN -0.227870       NaN\n2000-01-04       NaN -1.222082       NaN -1.233203\n2000-01-05       NaN -0.605656 -1.169184       NaN\n2000-01-06       NaN -0.948458       NaN -0.684718\n2000-01-07 -2.670153 -0.114722       NaN -0.048048\n2000-01-08       NaN       NaN -0.048788 -0.808838",
      "In [219]: df3 = pd.DataFrame({'A': [1, 2, 3],\n   .....:                     'B': [4, 5, 6],\n   .....:                     'C': [7, 8, 9]})\n   .....: \n\nIn [220]: df3.where(lambda x: x > 4, lambda x: x + 10)\nOut[220]: \n    A   B  C\n0  11  14  7\n1  12   5  8\n2  13   6  9",
      "In [223]: df = pd.DataFrame({'col1': list('ABBC'), 'col2': list('ZZXY')})\n\nIn [224]: df['color'] = np.where(df['col2'] == 'Z', 'green', 'red')\n\nIn [225]: df\nOut[225]: \n  col1 col2  color\n0    A    Z  green\n1    B    Z  green\n2    B    X    red\n3    C    Y    red",
      "In [226]: conditions = [\n   .....:     (df['col2'] == 'Z') & (df['col1'] == 'A'),\n   .....:     (df['col2'] == 'Z') & (df['col1'] == 'B'),\n   .....:     (df['col1'] == 'B')\n   .....: ]\n   .....: \n\nIn [227]: choices = ['yellow', 'blue', 'purple']\n\nIn [228]: df['color'] = np.select(conditions, choices, default='black')\n\nIn [229]: df\nOut[229]: \n  col1 col2   color\n0    A    Z  yellow\n1    B    Z    blue\n2    B    X  purple\n3    C    Y   black",
      "In [230]: n = 10\n\nIn [231]: df = pd.DataFrame(np.random.rand(n, 3), columns=list('abc'))\n\nIn [232]: df\nOut[232]: \n          a         b         c\n0  0.438921  0.118680  0.863670\n1  0.138138  0.577363  0.686602\n2  0.595307  0.564592  0.520630\n3  0.913052  0.926075  0.616184\n4  0.078718  0.854477  0.898725\n5  0.076404  0.523211  0.591538\n6  0.792342  0.216974  0.564056\n7  0.397890  0.454131  0.915716\n8  0.074315  0.437913  0.019794\n9  0.559209  0.502065  0.026437\n\n# pure python\nIn [233]: df[(df['a'] < df['b']) & (df['b'] < df['c'])]\nOut[233]: \n          a         b         c\n1  0.138138  0.577363  0.686602\n4  0.078718  0.854477  0.898725\n5  0.076404  0.523211  0.591538\n7  0.397890  0.454131  0.915716\n\n# query\nIn [234]: df.query('(a < b) & (b < c)')\nOut[234]: \n          a         b         c\n1  0.138138  0.577363  0.686602\n4  0.078718  0.854477  0.898725\n5  0.076404  0.523211  0.591538\n7  0.397890  0.454131  0.915716",
      "In [235]: df = pd.DataFrame(np.random.randint(n / 2, size=(n, 2)), columns=list('bc'))\n\nIn [236]: df.index.name = 'a'\n\nIn [237]: df\nOut[237]: \n   b  c\na      \n0  0  4\n1  0  1\n2  3  4\n3  4  3\n4  1  4\n5  0  3\n6  0  1\n7  3  4\n8  2  3\n9  1  1\n\nIn [238]: df.query('a < b and b < c')\nOut[238]: \n   b  c\na      \n2  3  4",
      "In [239]: df = pd.DataFrame(np.random.randint(n, size=(n, 2)), columns=list('bc'))\n\nIn [240]: df\nOut[240]: \n   b  c\n0  3  1\n1  3  0\n2  5  6\n3  5  2\n4  7  4\n5  0  1\n6  2  5\n7  0  1\n8  6  0\n9  7  9\n\nIn [241]: df.query('index < b < c')\nOut[241]: \n   b  c\n2  5  6",
      "In [242]: df = pd.DataFrame({'a': np.random.randint(5, size=5)})\n\nIn [243]: df.index.name = 'a'\n\nIn [244]: df.query('a > 2')  # uses the column 'a', not the index\nOut[244]: \n   a\na   \n1  3\n3  3",
      "In [246]: n = 10\n\nIn [247]: colors = np.random.choice(['red', 'green'], size=n)\n\nIn [248]: foods = np.random.choice(['eggs', 'ham'], size=n)\n\nIn [249]: colors\nOut[249]: \narray(['red', 'red', 'red', 'green', 'green', 'green', 'green', 'green',\n       'green', 'green'], dtype='<U5')\n\nIn [250]: foods\nOut[250]: \narray(['ham', 'ham', 'eggs', 'eggs', 'eggs', 'ham', 'ham', 'eggs', 'eggs',\n       'eggs'], dtype='<U4')\n\nIn [251]: index = pd.MultiIndex.from_arrays([colors, foods], names=['color', 'food'])\n\nIn [252]: df = pd.DataFrame(np.random.randn(n, 2), index=index)\n\nIn [253]: df\nOut[253]: \n                   0         1\ncolor food                    \nred   ham   0.194889 -0.381994\n      ham   0.318587  2.089075\n      eggs -0.728293 -0.090255\ngreen eggs -0.748199  1.318931\n      eggs -2.029766  0.792652\n      ham   0.461007 -0.542749\n      ham  -0.305384 -0.479195\n      eggs  0.095031 -0.270099\n      eggs -0.707140 -0.773882\n      eggs  0.229453  0.304418\n\nIn [254]: df.query('color == \"red\"')\nOut[254]: \n                   0         1\ncolor food                    \nred   ham   0.194889 -0.381994\n      ham   0.318587  2.089075\n      eggs -0.728293 -0.090255",
      "In [258]: df = pd.DataFrame(np.random.rand(n, 3), columns=list('abc'))\n\nIn [259]: df\nOut[259]: \n          a         b         c\n0  0.224283  0.736107  0.139168\n1  0.302827  0.657803  0.713897\n2  0.611185  0.136624  0.984960\n3  0.195246  0.123436  0.627712\n4  0.618673  0.371660  0.047902\n5  0.480088  0.062993  0.185760\n6  0.568018  0.483467  0.445289\n7  0.309040  0.274580  0.587101\n8  0.258993  0.477769  0.370255\n9  0.550459  0.840870  0.304611\n\nIn [260]: df2 = pd.DataFrame(np.random.rand(n + 2, 3), columns=df.columns)\n\nIn [261]: df2\nOut[261]: \n           a         b         c\n0   0.357579  0.229800  0.596001\n1   0.309059  0.957923  0.965663\n2   0.123102  0.336914  0.318616\n3   0.526506  0.323321  0.860813\n4   0.518736  0.486514  0.384724\n5   0.190804  0.505723  0.614533\n6   0.891939  0.623977  0.676639\n7   0.480559  0.378528  0.460858\n8   0.420223  0.136404  0.141295\n9   0.732206  0.419540  0.604675\n10  0.604466  0.848974  0.896165\n11  0.589168  0.920046  0.732716\n\nIn [262]: expr = '0.0 <= a <= c <= 0.5'\n\nIn [263]: map(lambda frame: frame.query(expr), [df, df2])\nOut[263]: <map at 0x7fe8c904cca0>",
      "In [264]: df = pd.DataFrame(np.random.randint(n, size=(n, 3)), columns=list('abc'))\n\nIn [265]: df\nOut[265]: \n   a  b  c\n0  7  8  9\n1  1  0  7\n2  2  7  2\n3  6  2  2\n4  2  6  3\n5  3  8  2\n6  1  7  2\n7  5  1  5\n8  9  8  0\n9  1  5  0\n\nIn [266]: df.query('(a < b) & (b < c)')\nOut[266]: \n   a  b  c\n0  7  8  9\n\nIn [267]: df[(df['a'] < df['b']) & (df['b'] < df['c'])]\nOut[267]: \n   a  b  c\n0  7  8  9",
      "# get all rows where columns \"a\" and \"b\" have overlapping values\nIn [271]: df = pd.DataFrame({'a': list('aabbccddeeff'), 'b': list('aaaabbbbcccc'),\n   .....:                    'c': np.random.randint(5, size=12),\n   .....:                    'd': np.random.randint(9, size=12)})\n   .....: \n\nIn [272]: df\nOut[272]: \n    a  b  c  d\n0   a  a  2  6\n1   a  a  4  7\n2   b  a  1  6\n3   b  a  2  1\n4   c  b  3  6\n5   c  b  0  2\n6   d  b  3  3\n7   d  b  2  1\n8   e  c  4  3\n9   e  c  2  0\n10  f  c  0  6\n11  f  c  1  2\n\nIn [273]: df.query('a in b')\nOut[273]: \n   a  b  c  d\n0  a  a  2  6\n1  a  a  4  7\n2  b  a  1  6\n3  b  a  2  1\n4  c  b  3  6\n5  c  b  0  2\n\n# How you'd do it in pure Python\nIn [274]: df[df['a'].isin(df['b'])]\nOut[274]: \n   a  b  c  d\n0  a  a  2  6\n1  a  a  4  7\n2  b  a  1  6\n3  b  a  2  1\n4  c  b  3  6\n5  c  b  0  2\n\nIn [275]: df.query('a not in b')\nOut[275]: \n    a  b  c  d\n6   d  b  3  3\n7   d  b  2  1\n8   e  c  4  3\n9   e  c  2  0\n10  f  c  0  6\n11  f  c  1  2\n\n# pure Python\nIn [276]: df[~df['a'].isin(df['b'])]\nOut[276]: \n    a  b  c  d\n6   d  b  3  3\n7   d  b  2  1\n8   e  c  4  3\n9   e  c  2  0\n10  f  c  0  6\n11  f  c  1  2",
      "In [286]: df = pd.DataFrame(np.random.rand(n, 3), columns=list('abc'))\n\nIn [287]: df['bools'] = np.random.rand(len(df)) > 0.5\n\nIn [288]: df.query('~bools')\nOut[288]: \n          a         b         c  bools\n2  0.697753  0.212799  0.329209  False\n7  0.275396  0.691034  0.826619  False\n8  0.190649  0.558748  0.262467  False\n\nIn [289]: df.query('not bools')\nOut[289]: \n          a         b         c  bools\n2  0.697753  0.212799  0.329209  False\n7  0.275396  0.691034  0.826619  False\n8  0.190649  0.558748  0.262467  False\n\nIn [290]: df.query('not bools') == df[~df['bools']]\nOut[290]: \n      a     b     c  bools\n2  True  True  True   True\n7  True  True  True   True\n8  True  True  True   True",
      "In [296]: df = pd.DataFrame(np.random.randn(8, 4),\n   .....:                   index=dates, columns=['A', 'B', 'C', 'D'])\n   .....: \n\nIn [297]: df2 = df.copy()",
      "In [298]: df2 = pd.DataFrame({'a': ['one', 'one', 'two', 'two', 'two', 'three', 'four'],\n   .....:                     'b': ['x', 'y', 'x', 'y', 'x', 'x', 'x'],\n   .....:                     'c': np.random.randn(7)})\n   .....: \n\nIn [299]: df2\nOut[299]: \n       a  b         c\n0    one  x -1.067137\n1    one  y  0.309500\n2    two  x -0.211056\n3    two  y -1.842023\n4    two  x -0.390820\n5  three  x -1.964475\n6   four  x  1.298329\n\nIn [300]: df2.duplicated('a')\nOut[300]: \n0    False\n1     True\n2    False\n3     True\n4     True\n5    False\n6    False\ndtype: bool\n\nIn [301]: df2.duplicated('a', keep='last')\nOut[301]: \n0     True\n1    False\n2     True\n3     True\n4    False\n5    False\n6    False\ndtype: bool\n\nIn [302]: df2.duplicated('a', keep=False)\nOut[302]: \n0     True\n1     True\n2     True\n3     True\n4     True\n5    False\n6    False\ndtype: bool\n\nIn [303]: df2.drop_duplicates('a')\nOut[303]: \n       a  b         c\n0    one  x -1.067137\n2    two  x -0.211056\n5  three  x -1.964475\n6   four  x  1.298329\n\nIn [304]: df2.drop_duplicates('a', keep='last')\nOut[304]: \n       a  b         c\n1    one  y  0.309500\n4    two  x -0.390820\n5  three  x -1.964475\n6   four  x  1.298329\n\nIn [305]: df2.drop_duplicates('a', keep=False)\nOut[305]: \n       a  b         c\n5  three  x -1.964475\n6   four  x  1.298329",
      "In [308]: df3 = pd.DataFrame({'a': np.arange(6),\n   .....:                     'b': np.random.randn(6)},\n   .....:                    index=['a', 'a', 'b', 'c', 'b', 'a'])\n   .....: \n\nIn [309]: df3\nOut[309]: \n   a         b\na  0  1.440455\na  1  2.456086\nb  2  1.038402\nc  3 -0.894409\nb  4  0.683536\na  5  3.082764\n\nIn [310]: df3.index.duplicated()\nOut[310]: array([False,  True, False, False,  True,  True])\n\nIn [311]: df3[~df3.index.duplicated()]\nOut[311]: \n   a         b\na  0  1.440455\nb  2  1.038402\nc  3 -0.894409\n\nIn [312]: df3[~df3.index.duplicated(keep='last')]\nOut[312]: \n   a         b\nc  3 -0.894409\nb  4  0.683536\na  5  3.082764\n\nIn [313]: df3[~df3.index.duplicated(keep=False)]\nOut[313]: \n   a         b\nc  3 -0.894409",
      "In [314]: s = pd.Series([1, 2, 3], index=['a', 'b', 'c'])\n\nIn [315]: s.get('a')  # equivalent to s['a']\nOut[315]: 1\n\nIn [316]: s.get('x', default=-1)\nOut[316]: -1",
      "In [317]: df = pd.DataFrame({'col': [\"A\", \"A\", \"B\", \"B\"],\n   .....:                    'A': [80, 23, np.nan, 22],\n   .....:                    'B': [80, 55, 76, 67]})\n   .....: \n\nIn [318]: df\nOut[318]: \n  col     A   B\n0   A  80.0  80\n1   A  23.0  55\n2   B   NaN  76\n3   B  22.0  67\n\nIn [319]: idx, cols = pd.factorize(df['col'])\n\nIn [320]: df.reindex(cols, axis=1).to_numpy()[np.arange(len(df)), idx]\nOut[320]: array([80., 23., 76., 67.])",
      "In [321]: index = pd.Index(['e', 'd', 'a', 'b'])\n\nIn [322]: index\nOut[322]: Index(['e', 'd', 'a', 'b'], dtype='object')\n\nIn [323]: 'd' in index\nOut[323]: True",
      "In [324]: index = pd.Index([1, 5, 12])\n\nIn [325]: index\nOut[325]: Index([1, 5, 12], dtype='int64')\n\nIn [326]: 5 in index\nOut[326]: True",
      "In [327]: index = pd.Index(['e', 'd', 'a', 'b'], dtype=\"string\")\n\nIn [328]: index\nOut[328]: Index(['e', 'd', 'a', 'b'], dtype='string')\n\nIn [329]: index = pd.Index([1, 5, 12], dtype=\"int8\")\n\nIn [330]: index\nOut[330]: Index([1, 5, 12], dtype='int8')\n\nIn [331]: index = pd.Index([1, 5, 12], dtype=\"float32\")\n\nIn [332]: index\nOut[332]: Index([1.0, 5.0, 12.0], dtype='float32')",
      "In [333]: index = pd.Index(['e', 'd', 'a', 'b'], name='something')\n\nIn [334]: index.name\nOut[334]: 'something'",
      "In [335]: index = pd.Index(list(range(5)), name='rows')\n\nIn [336]: columns = pd.Index(['A', 'B', 'C'], name='cols')\n\nIn [337]: df = pd.DataFrame(np.random.randn(5, 3), index=index, columns=columns)\n\nIn [338]: df\nOut[338]: \ncols         A         B         C\nrows                              \n0     1.295989 -1.051694  1.340429\n1    -2.366110  0.428241  0.387275\n2     0.433306  0.929548  0.278094\n3     2.154730 -0.315628  0.264223\n4     1.126818  1.132290 -0.353310\n\nIn [339]: df['A']\nOut[339]: \nrows\n0    1.295989\n1   -2.366110\n2    0.433306\n3    2.154730\n4    1.126818\nName: A, dtype: float64",
      "In [340]: ind = pd.Index([1, 2, 3])\n\nIn [341]: ind.rename(\"apple\")\nOut[341]: Index([1, 2, 3], dtype='int64', name='apple')\n\nIn [342]: ind\nOut[342]: Index([1, 2, 3], dtype='int64')\n\nIn [343]: ind = ind.set_names([\"apple\"])\n\nIn [344]: ind.name = \"bob\"\n\nIn [345]: ind\nOut[345]: Index([1, 2, 3], dtype='int64', name='bob')",
      "In [346]: index = pd.MultiIndex.from_product([range(3), ['one', 'two']], names=['first', 'second'])\n\nIn [347]: index\nOut[347]: \nMultiIndex([(0, 'one'),\n            (0, 'two'),\n            (1, 'one'),\n            (1, 'two'),\n            (2, 'one'),\n            (2, 'two')],\n           names=['first', 'second'])\n\nIn [348]: index.levels[1]\nOut[348]: Index(['one', 'two'], dtype='object', name='second')\n\nIn [349]: index.set_levels([\"a\", \"b\"], level=1)\nOut[349]: \nMultiIndex([(0, 'a'),\n            (0, 'b'),\n            (1, 'a'),\n            (1, 'b'),\n            (2, 'a'),\n            (2, 'b')],\n           names=['first', 'second'])",
      "In [350]: a = pd.Index(['c', 'b', 'a'])\n\nIn [351]: b = pd.Index(['c', 'e', 'd'])\n\nIn [352]: a.difference(b)\nOut[352]: Index(['a', 'b'], dtype='object')",
      "In [353]: idx1 = pd.Index([1, 2, 3, 4])\n\nIn [354]: idx2 = pd.Index([2, 3, 4, 5])\n\nIn [355]: idx1.symmetric_difference(idx2)\nOut[355]: Index([1, 5], dtype='int64')",
      "In [356]: idx1 = pd.Index([0, 1, 2])\n\nIn [357]: idx2 = pd.Index([0.5, 1.5])\n\nIn [358]: idx1.union(idx2)\nOut[358]: Index([0.0, 0.5, 1.0, 1.5, 2.0], dtype='float64')",
      "In [359]: idx1 = pd.Index([1, np.nan, 3, 4])\n\nIn [360]: idx1\nOut[360]: Index([1.0, nan, 3.0, 4.0], dtype='float64')\n\nIn [361]: idx1.fillna(2)\nOut[361]: Index([1.0, 2.0, 3.0, 4.0], dtype='float64')\n\nIn [362]: idx2 = pd.DatetimeIndex([pd.Timestamp('2011-01-01'),\n   .....:                          pd.NaT,\n   .....:                          pd.Timestamp('2011-01-03')])\n   .....: \n\nIn [363]: idx2\nOut[363]: DatetimeIndex(['2011-01-01', 'NaT', '2011-01-03'], dtype='datetime64[ns]', freq=None)\n\nIn [364]: idx2.fillna(pd.Timestamp('2011-01-02'))\nOut[364]: DatetimeIndex(['2011-01-01', '2011-01-02', '2011-01-03'], dtype='datetime64[ns]', freq=None)",
      "In [365]: data = pd.DataFrame({'a': ['bar', 'bar', 'foo', 'foo'],\n   .....:                      'b': ['one', 'two', 'one', 'two'],\n   .....:                      'c': ['z', 'y', 'x', 'w'],\n   .....:                      'd': [1., 2., 3, 4]})\n   .....: \n\nIn [366]: data\nOut[366]: \n     a    b  c    d\n0  bar  one  z  1.0\n1  bar  two  y  2.0\n2  foo  one  x  3.0\n3  foo  two  w  4.0\n\nIn [367]: indexed1 = data.set_index('c')\n\nIn [368]: indexed1\nOut[368]: \n     a    b    d\nc               \nz  bar  one  1.0\ny  bar  two  2.0\nx  foo  one  3.0\nw  foo  two  4.0\n\nIn [369]: indexed2 = data.set_index(['a', 'b'])\n\nIn [370]: indexed2\nOut[370]: \n         c    d\na   b          \nbar one  z  1.0\n    two  y  2.0\nfoo one  x  3.0\n    two  w  4.0",
      "In [379]: df_idx = pd.DataFrame(range(4))\n\nIn [380]: df_idx.index = pd.Index([10, 20, 30, 40], name=\"a\")\n\nIn [381]: df_idx\nOut[381]: \n    0\na    \n10  0\n20  1\n30  2\n40  3",
      "In [382]: dfmi = pd.DataFrame([list('abcd'),\n   .....:                      list('efgh'),\n   .....:                      list('ijkl'),\n   .....:                      list('mnop')],\n   .....:                     columns=pd.MultiIndex.from_product([['one', 'two'],\n   .....:                                                         ['first', 'second']]))\n   .....: \n\nIn [383]: dfmi\nOut[383]: \n    one          two       \n  first second first second\n0     a      b     c      d\n1     e      f     g      h\n2     i      j     k      l\n3     m      n     o      p",
      "def do_something(df):\n    foo = df[['bar', 'baz']]  # Is foo a view? A copy? Nobody knows!\n    # ... many lines here ...\n    # We don't know whether this will modify df or not!\n    foo['quux'] = value\n    return foo",
      "In [386]: dfb = pd.DataFrame({'a': ['one', 'one', 'two',\n   .....:                           'three', 'two', 'one', 'six'],\n   .....:                     'c': np.arange(7)})\n   .....: \n\n# This will show the SettingWithCopyWarning\n# but the frame values will be set\nIn [387]: dfb['c'][dfb['a'].str.startswith('o')] = 42",
      "In [388]: with pd.option_context('mode.chained_assignment','warn'):\n   .....:     dfb[dfb['a'].str.startswith('o')]['c'] = 42\n   .....:",
      "In [389]: dfc = pd.DataFrame({'a': ['one', 'one', 'two',\n   .....:                           'three', 'two', 'one', 'six'],\n   .....:                     'c': np.arange(7)})\n   .....: \n\nIn [390]: dfd = dfc.copy()\n\n# Setting multiple items using a mask\nIn [391]: mask = dfd['a'].str.startswith('o')\n\nIn [392]: dfd.loc[mask, 'c'] = 42\n\nIn [393]: dfd\nOut[393]: \n       a   c\n0    one  42\n1    one  42\n2    two   2\n3  three   3\n4    two   4\n5    one  42\n6    six   6\n\n# Setting a single item\nIn [394]: dfd = dfc.copy()\n\nIn [395]: dfd.loc[2, 'a'] = 11\n\nIn [396]: dfd\nOut[396]: \n       a  c\n0    one  0\n1    one  1\n2     11  2\n3  three  3\n4    two  4\n5    one  5\n6    six  6",
      "In [400]: with pd.option_context('mode.chained_assignment','raise'):\n   .....:     dfd.loc[0]['a'] = 1111\n   .....: \n---------------------------------------------------------------------------\nSettingWithCopyError                      Traceback (most recent call last)\n<ipython-input-400-32ce785aaa5b> in ?()\n      1 with pd.option_context('mode.chained_assignment','raise'):\n----> 2     dfd.loc[0]['a'] = 1111\n\n~/work/pandas/pandas/pandas/core/series.py in ?(self, key, value)\n   1284                 )\n   1285 \n   1286         check_dict_or_set_indexers(key)\n   1287         key = com.apply_if_callable(key, self)\n-> 1288         cacher_needs_updating = self._check_is_chained_assignment_possible()\n   1289 \n   1290         if key is Ellipsis:\n   1291             key = slice(None)\n\n~/work/pandas/pandas/pandas/core/series.py in ?(self)\n   1489             ref = self._get_cacher()\n   1490             if ref is not None and ref._is_mixed_type:\n   1491                 self._check_setitem_copy(t=\"referent\", force=True)\n   1492             return True\n-> 1493         return super()._check_is_chained_assignment_possible()\n\n~/work/pandas/pandas/pandas/core/generic.py in ?(self)\n   4395         single-dtype meaning that the cacher should be updated following\n   4396         setting.\n   4397         \"\"\"\n   4398         if self._is_copy:\n-> 4399             self._check_setitem_copy(t=\"referent\")\n   4400         return False\n\n~/work/pandas/pandas/pandas/core/generic.py in ?(self, t, force)\n   4469                 \"indexing.html#returning-a-view-versus-a-copy\"\n   4470             )\n   4471 \n   4472         if value == \"raise\":\n-> 4473             raise SettingWithCopyError(t)\n   4474         if value == \"warn\":\n   4475             warnings.warn(t, SettingWithCopyWarning, stacklevel=find_stack_level())\n\nSettingWithCopyError: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy"
    ]
  },
  {
    "url": "https://pandas.pydata.org/docs/user_guide/io.html",
    "title": "IO tools (text, CSV, HDF5, \u2026)#",
    "code_snippets": [
      "In [1]: import pandas as pd\n\nIn [2]: from io import StringIO\n\nIn [3]: data = \"col1,col2,col3\\na,b,1\\na,b,2\\nc,d,3\"\n\nIn [4]: pd.read_csv(StringIO(data))\nOut[4]: \n  col1 col2  col3\n0    a    b     1\n1    a    b     2\n2    c    d     3\n\nIn [5]: pd.read_csv(StringIO(data), usecols=lambda x: x.upper() in [\"COL1\", \"COL3\"])\nOut[5]: \n  col1  col3\n0    a     1\n1    a     2\n2    c     3",
      "In [6]: data = \"col1,col2,col3\\na,b,1\\na,b,2\\nc,d,3\"\n\nIn [7]: pd.read_csv(StringIO(data))\nOut[7]: \n  col1 col2  col3\n0    a    b     1\n1    a    b     2\n2    c    d     3\n\nIn [8]: pd.read_csv(StringIO(data), skiprows=lambda x: x % 2 != 0)\nOut[8]: \n  col1 col2  col3\n0    a    b     2",
      "In [9]: import numpy as np\n\nIn [10]: data = \"a,b,c,d\\n1,2,3,4\\n5,6,7,8\\n9,10,11\"\n\nIn [11]: print(data)\na,b,c,d\n1,2,3,4\n5,6,7,8\n9,10,11\n\nIn [12]: df = pd.read_csv(StringIO(data), dtype=object)\n\nIn [13]: df\nOut[13]: \n   a   b   c    d\n0  1   2   3    4\n1  5   6   7    8\n2  9  10  11  NaN\n\nIn [14]: df[\"a\"][0]\nOut[14]: '1'\n\nIn [15]: df = pd.read_csv(StringIO(data), dtype={\"b\": object, \"c\": np.float64, \"d\": \"Int64\"})\n\nIn [16]: df.dtypes\nOut[16]: \na      int64\nb     object\nc    float64\nd      Int64\ndtype: object",
      "In [17]: data = \"col_1\\n1\\n2\\n'A'\\n4.22\"\n\nIn [18]: df = pd.read_csv(StringIO(data), converters={\"col_1\": str})\n\nIn [19]: df\nOut[19]: \n  col_1\n0     1\n1     2\n2   'A'\n3  4.22\n\nIn [20]: df[\"col_1\"].apply(type).value_counts()\nOut[20]: \ncol_1\n<class 'str'>    4\nName: count, dtype: int64",
      "In [21]: df2 = pd.read_csv(StringIO(data))\n\nIn [22]: df2[\"col_1\"] = pd.to_numeric(df2[\"col_1\"], errors=\"coerce\")\n\nIn [23]: df2\nOut[23]: \n   col_1\n0   1.00\n1   2.00\n2    NaN\n3   4.22\n\nIn [24]: df2[\"col_1\"].apply(type).value_counts()\nOut[24]: \ncol_1\n<class 'float'>    4\nName: count, dtype: int64",
      "In [25]: col_1 = list(range(500000)) + [\"a\", \"b\"] + list(range(500000))\n\nIn [26]: df = pd.DataFrame({\"col_1\": col_1})\n\nIn [27]: df.to_csv(\"foo.csv\")\n\nIn [28]: mixed_df = pd.read_csv(\"foo.csv\")\n\nIn [29]: mixed_df[\"col_1\"].apply(type).value_counts()\nOut[29]: \ncol_1\n<class 'int'>    737858\n<class 'str'>    262144\nName: count, dtype: int64\n\nIn [30]: mixed_df[\"col_1\"].dtype\nOut[30]: dtype('O')",
      "In [31]: data = \"\"\"a,b,c,d,e,f,g,h,i,j\n   ....: 1,2.5,True,a,,,,,12-31-2019,\n   ....: 3,4.5,False,b,6,7.5,True,a,12-31-2019,\n   ....: \"\"\"\n   ....: \n\nIn [32]: df = pd.read_csv(StringIO(data), dtype_backend=\"numpy_nullable\", parse_dates=[\"i\"])\n\nIn [33]: df\nOut[33]: \n   a    b      c  d     e     f     g     h          i     j\n0  1  2.5   True  a  <NA>  <NA>  <NA>  <NA> 2019-12-31  <NA>\n1  3  4.5  False  b     6   7.5  True     a 2019-12-31  <NA>\n\nIn [34]: df.dtypes\nOut[34]: \na             Int64\nb           Float64\nc           boolean\nd    string[python]\ne             Int64\nf           Float64\ng           boolean\nh    string[python]\ni    datetime64[ns]\nj             Int64\ndtype: object",
      "In [35]: data = \"col1,col2,col3\\na,b,1\\na,b,2\\nc,d,3\"\n\nIn [36]: pd.read_csv(StringIO(data))\nOut[36]: \n  col1 col2  col3\n0    a    b     1\n1    a    b     2\n2    c    d     3\n\nIn [37]: pd.read_csv(StringIO(data)).dtypes\nOut[37]: \ncol1    object\ncol2    object\ncol3     int64\ndtype: object\n\nIn [38]: pd.read_csv(StringIO(data), dtype=\"category\").dtypes\nOut[38]: \ncol1    category\ncol2    category\ncol3    category\ndtype: object",
      "In [39]: pd.read_csv(StringIO(data), dtype={\"col1\": \"category\"}).dtypes\nOut[39]: \ncol1    category\ncol2      object\ncol3       int64\ndtype: object",
      "In [40]: from pandas.api.types import CategoricalDtype\n\nIn [41]: dtype = CategoricalDtype([\"d\", \"c\", \"b\", \"a\"], ordered=True)\n\nIn [42]: pd.read_csv(StringIO(data), dtype={\"col1\": dtype}).dtypes\nOut[42]: \ncol1    category\ncol2      object\ncol3       int64\ndtype: object",
      "In [43]: dtype = CategoricalDtype([\"a\", \"b\", \"d\"])  # No 'c'\n\nIn [44]: pd.read_csv(StringIO(data), dtype={\"col1\": dtype}).col1\nOut[44]: \n0      a\n1      a\n2    NaN\nName: col1, dtype: category\nCategories (3, object): ['a', 'b', 'd']",
      "In [45]: df = pd.read_csv(StringIO(data), dtype=\"category\")\n\nIn [46]: df.dtypes\nOut[46]: \ncol1    category\ncol2    category\ncol3    category\ndtype: object\n\nIn [47]: df[\"col3\"]\nOut[47]: \n0    1\n1    2\n2    3\nName: col3, dtype: category\nCategories (3, object): ['1', '2', '3']\n\nIn [48]: new_categories = pd.to_numeric(df[\"col3\"].cat.categories)\n\nIn [49]: df[\"col3\"] = df[\"col3\"].cat.rename_categories(new_categories)\n\nIn [50]: df[\"col3\"]\nOut[50]: \n0    1\n1    2\n2    3\nName: col3, dtype: category\nCategories (3, int64): [1, 2, 3]",
      "In [51]: data = \"a,b,c\\n1,2,3\\n4,5,6\\n7,8,9\"\n\nIn [52]: print(data)\na,b,c\n1,2,3\n4,5,6\n7,8,9\n\nIn [53]: pd.read_csv(StringIO(data))\nOut[53]: \n   a  b  c\n0  1  2  3\n1  4  5  6\n2  7  8  9",
      "In [54]: print(data)\na,b,c\n1,2,3\n4,5,6\n7,8,9\n\nIn [55]: pd.read_csv(StringIO(data), names=[\"foo\", \"bar\", \"baz\"], header=0)\nOut[55]: \n   foo  bar  baz\n0    1    2    3\n1    4    5    6\n2    7    8    9\n\nIn [56]: pd.read_csv(StringIO(data), names=[\"foo\", \"bar\", \"baz\"], header=None)\nOut[56]: \n  foo bar baz\n0   a   b   c\n1   1   2   3\n2   4   5   6\n3   7   8   9",
      "In [57]: data = \"skip this skip it\\na,b,c\\n1,2,3\\n4,5,6\\n7,8,9\"\n\nIn [58]: pd.read_csv(StringIO(data), header=1)\nOut[58]: \n   a  b  c\n0  1  2  3\n1  4  5  6\n2  7  8  9",
      "In [59]: data = \"a,b,a\\n0,1,2\\n3,4,5\"\n\nIn [60]: pd.read_csv(StringIO(data))\nOut[60]: \n   a  b  a.1\n0  0  1    2\n1  3  4    5",
      "In [61]: data = \"a,b,c,d\\n1,2,3,foo\\n4,5,6,bar\\n7,8,9,baz\"\n\nIn [62]: pd.read_csv(StringIO(data))\nOut[62]: \n   a  b  c    d\n0  1  2  3  foo\n1  4  5  6  bar\n2  7  8  9  baz\n\nIn [63]: pd.read_csv(StringIO(data), usecols=[\"b\", \"d\"])\nOut[63]: \n   b    d\n0  2  foo\n1  5  bar\n2  8  baz\n\nIn [64]: pd.read_csv(StringIO(data), usecols=[0, 2, 3])\nOut[64]: \n   a  c    d\n0  1  3  foo\n1  4  6  bar\n2  7  9  baz\n\nIn [65]: pd.read_csv(StringIO(data), usecols=lambda x: x.upper() in [\"A\", \"C\"])\nOut[65]: \n   a  c\n0  1  3\n1  4  6\n2  7  9",
      "In [66]: pd.read_csv(StringIO(data), usecols=lambda x: x not in [\"a\", \"c\"])\nOut[66]: \n   b    d\n0  2  foo\n1  5  bar\n2  8  baz",
      "In [67]: data = \"\\na,b,c\\n  \\n# commented line\\n1,2,3\\n\\n4,5,6\"\n\nIn [68]: print(data)\n\na,b,c\n  \n# commented line\n1,2,3\n\n4,5,6\n\nIn [69]: pd.read_csv(StringIO(data), comment=\"#\")\nOut[69]: \n   a  b  c\n0  1  2  3\n1  4  5  6",
      "In [70]: data = \"a,b,c\\n\\n1,2,3\\n\\n\\n4,5,6\"\n\nIn [71]: pd.read_csv(StringIO(data), skip_blank_lines=False)\nOut[71]: \n     a    b    c\n0  NaN  NaN  NaN\n1  1.0  2.0  3.0\n2  NaN  NaN  NaN\n3  NaN  NaN  NaN\n4  4.0  5.0  6.0",
      "In [72]: data = \"#comment\\na,b,c\\nA,B,C\\n1,2,3\"\n\nIn [73]: pd.read_csv(StringIO(data), comment=\"#\", header=1)\nOut[73]: \n   A  B  C\n0  1  2  3\n\nIn [74]: data = \"A,B,C\\n#comment\\na,b,c\\n1,2,3\"\n\nIn [75]: pd.read_csv(StringIO(data), comment=\"#\", skiprows=2)\nOut[75]: \n   a  b  c\n0  1  2  3",
      "In [76]: data = (\n   ....:     \"# empty\\n\"\n   ....:     \"# second empty line\\n\"\n   ....:     \"# third emptyline\\n\"\n   ....:     \"X,Y,Z\\n\"\n   ....:     \"1,2,3\\n\"\n   ....:     \"A,B,C\\n\"\n   ....:     \"1,2.,4.\\n\"\n   ....:     \"5.,NaN,10.0\\n\"\n   ....: )\n   ....: \n\nIn [77]: print(data)\n# empty\n# second empty line\n# third emptyline\nX,Y,Z\n1,2,3\nA,B,C\n1,2.,4.\n5.,NaN,10.0\n\n\nIn [78]: pd.read_csv(StringIO(data), comment=\"#\", skiprows=4, header=1)\nOut[78]: \n     A    B     C\n0  1.0  2.0   4.0\n1  5.0  NaN  10.0",
      "In [82]: df = pd.read_csv(\"tmp.csv\")\n\nIn [83]: df\nOut[83]: \n         ID    level                        category\n0  Patient1   123000           x # really unpleasant\n1  Patient2    23000  y # wouldn't take his medicine\n2  Patient3  1234018                     z # awesome",
      "In [84]: df = pd.read_csv(\"tmp.csv\", comment=\"#\")\n\nIn [85]: df\nOut[85]: \n         ID    level category\n0  Patient1   123000       x \n1  Patient2    23000       y \n2  Patient3  1234018       z",
      "In [86]: from io import BytesIO\n\nIn [87]: data = b\"word,length\\n\" b\"Tr\\xc3\\xa4umen,7\\n\" b\"Gr\\xc3\\xbc\\xc3\\x9fe,5\"\n\nIn [88]: data = data.decode(\"utf8\").encode(\"latin-1\")\n\nIn [89]: df = pd.read_csv(BytesIO(data), encoding=\"latin-1\")\n\nIn [90]: df\nOut[90]: \n      word  length\n0  Tr\u00e4umen       7\n1    Gr\u00fc\u00dfe       5\n\nIn [91]: df[\"word\"][1]\nOut[91]: 'Gr\u00fc\u00dfe'",
      "In [92]: data = \"a,b,c\\n4,apple,bat,5.7\\n8,orange,cow,10\"\n\nIn [93]: pd.read_csv(StringIO(data))\nOut[93]: \n        a    b     c\n4   apple  bat   5.7\n8  orange  cow  10.0",
      "In [94]: data = \"index,a,b,c\\n4,apple,bat,5.7\\n8,orange,cow,10\"\n\nIn [95]: pd.read_csv(StringIO(data), index_col=0)\nOut[95]: \n            a    b     c\nindex                   \n4       apple  bat   5.7\n8      orange  cow  10.0",
      "In [96]: data = \"a,b,c\\n4,apple,bat,\\n8,orange,cow,\"\n\nIn [97]: print(data)\na,b,c\n4,apple,bat,\n8,orange,cow,\n\nIn [98]: pd.read_csv(StringIO(data))\nOut[98]: \n        a    b   c\n4   apple  bat NaN\n8  orange  cow NaN\n\nIn [99]: pd.read_csv(StringIO(data), index_col=False)\nOut[99]: \n   a       b    c\n0  4   apple  bat\n1  8  orange  cow",
      "In [100]: data = \"a,b,c\\n4,apple,bat,\\n8,orange,cow,\"\n\nIn [101]: print(data)\na,b,c\n4,apple,bat,\n8,orange,cow,\n\nIn [102]: pd.read_csv(StringIO(data), usecols=[\"b\", \"c\"])\nOut[102]: \n     b   c\n4  bat NaN\n8  cow NaN\n\nIn [103]: pd.read_csv(StringIO(data), usecols=[\"b\", \"c\"], index_col=0)\nOut[103]: \n     b   c\n4  bat NaN\n8  cow NaN",
      "In [104]: with open(\"foo.csv\", mode=\"w\") as f:\n   .....:     f.write(\"date,A,B,C\\n20090101,a,1,2\\n20090102,b,3,4\\n20090103,c,4,5\")\n   .....: \n\n# Use a column as an index, and parse it as dates.\nIn [105]: df = pd.read_csv(\"foo.csv\", index_col=0, parse_dates=True)\n\nIn [106]: df\nOut[106]: \n            A  B  C\ndate               \n2009-01-01  a  1  2\n2009-01-02  b  3  4\n2009-01-03  c  4  5\n\n# These are Python datetime objects\nIn [107]: df.index\nOut[107]: DatetimeIndex(['2009-01-01', '2009-01-02', '2009-01-03'], dtype='datetime64[ns]', name='date', freq=None)",
      "In [108]: data = (\n   .....:     \"KORD,19990127, 19:00:00, 18:56:00, 0.8100\\n\"\n   .....:     \"KORD,19990127, 20:00:00, 19:56:00, 0.0100\\n\"\n   .....:     \"KORD,19990127, 21:00:00, 20:56:00, -0.5900\\n\"\n   .....:     \"KORD,19990127, 21:00:00, 21:18:00, -0.9900\\n\"\n   .....:     \"KORD,19990127, 22:00:00, 21:56:00, -0.5900\\n\"\n   .....:     \"KORD,19990127, 23:00:00, 22:56:00, -0.5900\"\n   .....: )\n   .....: \n\nIn [109]: with open(\"tmp.csv\", \"w\") as fh:\n   .....:     fh.write(data)\n   .....: \n\nIn [110]: df = pd.read_csv(\"tmp.csv\", header=None, parse_dates=[[1, 2], [1, 3]])\n\nIn [111]: df\nOut[111]: \n                  1_2                 1_3     0     4\n0 1999-01-27 19:00:00 1999-01-27 18:56:00  KORD  0.81\n1 1999-01-27 20:00:00 1999-01-27 19:56:00  KORD  0.01\n2 1999-01-27 21:00:00 1999-01-27 20:56:00  KORD -0.59\n3 1999-01-27 21:00:00 1999-01-27 21:18:00  KORD -0.99\n4 1999-01-27 22:00:00 1999-01-27 21:56:00  KORD -0.59\n5 1999-01-27 23:00:00 1999-01-27 22:56:00  KORD -0.59",
      "In [112]: df = pd.read_csv(\n   .....:     \"tmp.csv\", header=None, parse_dates=[[1, 2], [1, 3]], keep_date_col=True\n   .....: )\n   .....: \n\nIn [113]: df\nOut[113]: \n                  1_2                 1_3     0  ...          2          3     4\n0 1999-01-27 19:00:00 1999-01-27 18:56:00  KORD  ...   19:00:00   18:56:00  0.81\n1 1999-01-27 20:00:00 1999-01-27 19:56:00  KORD  ...   20:00:00   19:56:00  0.01\n2 1999-01-27 21:00:00 1999-01-27 20:56:00  KORD  ...   21:00:00   20:56:00 -0.59\n3 1999-01-27 21:00:00 1999-01-27 21:18:00  KORD  ...   21:00:00   21:18:00 -0.99\n4 1999-01-27 22:00:00 1999-01-27 21:56:00  KORD  ...   22:00:00   21:56:00 -0.59\n5 1999-01-27 23:00:00 1999-01-27 22:56:00  KORD  ...   23:00:00   22:56:00 -0.59\n\n[6 rows x 7 columns]",
      "In [114]: date_spec = {\"nominal\": [1, 2], \"actual\": [1, 3]}\n\nIn [115]: df = pd.read_csv(\"tmp.csv\", header=None, parse_dates=date_spec)\n\nIn [116]: df\nOut[116]: \n              nominal              actual     0     4\n0 1999-01-27 19:00:00 1999-01-27 18:56:00  KORD  0.81\n1 1999-01-27 20:00:00 1999-01-27 19:56:00  KORD  0.01\n2 1999-01-27 21:00:00 1999-01-27 20:56:00  KORD -0.59\n3 1999-01-27 21:00:00 1999-01-27 21:18:00  KORD -0.99\n4 1999-01-27 22:00:00 1999-01-27 21:56:00  KORD -0.59\n5 1999-01-27 23:00:00 1999-01-27 22:56:00  KORD -0.59",
      "In [117]: date_spec = {\"nominal\": [1, 2], \"actual\": [1, 3]}\n\nIn [118]: df = pd.read_csv(\n   .....:     \"tmp.csv\", header=None, parse_dates=date_spec, index_col=0\n   .....: )  # index is the nominal column\n   .....: \n\nIn [119]: df\nOut[119]: \n                                 actual     0     4\nnominal                                            \n1999-01-27 19:00:00 1999-01-27 18:56:00  KORD  0.81\n1999-01-27 20:00:00 1999-01-27 19:56:00  KORD  0.01\n1999-01-27 21:00:00 1999-01-27 20:56:00  KORD -0.59\n1999-01-27 21:00:00 1999-01-27 21:18:00  KORD -0.99\n1999-01-27 22:00:00 1999-01-27 21:56:00  KORD -0.59\n1999-01-27 23:00:00 1999-01-27 22:56:00  KORD -0.59",
      "In [120]: content = \"\"\"\\\n   .....: a\n   .....: 2000-01-01T00:00:00+05:00\n   .....: 2000-01-01T00:00:00+06:00\"\"\"\n   .....: \n\nIn [121]: df = pd.read_csv(StringIO(content))\n\nIn [122]: df[\"a\"] = pd.to_datetime(df[\"a\"], utc=True)\n\nIn [123]: df[\"a\"]\nOut[123]: \n0   1999-12-31 19:00:00+00:00\n1   1999-12-31 18:00:00+00:00\nName: a, dtype: datetime64[ns, UTC]",
      "In [124]: df = pd.read_csv(\n   .....:     \"foo.csv\",\n   .....:     index_col=0,\n   .....:     parse_dates=True,\n   .....: )\n   .....: \n\nIn [125]: df\nOut[125]: \n            A  B  C\ndate               \n2009-01-01  a  1  2\n2009-01-02  b  3  4\n2009-01-03  c  4  5",
      "In [126]: data = StringIO(\"date\\n12 Jan 2000\\n2000-01-13\\n\")\n\nIn [127]: df = pd.read_csv(data)\n\nIn [128]: df['date'] = pd.to_datetime(df['date'], format='mixed')\n\nIn [129]: df\nOut[129]: \n        date\n0 2000-01-12\n1 2000-01-13",
      "In [130]: data = StringIO(\"date\\n2020-01-01\\n2020-01-01 03:00\\n\")\n\nIn [131]: df = pd.read_csv(data)\n\nIn [132]: df['date'] = pd.to_datetime(df['date'], format='ISO8601')\n\nIn [133]: df\nOut[133]: \n                 date\n0 2020-01-01 00:00:00\n1 2020-01-01 03:00:00",
      "In [134]: data = \"date,value,cat\\n1/6/2000,5,a\\n2/6/2000,10,b\\n3/6/2000,15,c\"\n\nIn [135]: print(data)\ndate,value,cat\n1/6/2000,5,a\n2/6/2000,10,b\n3/6/2000,15,c\n\nIn [136]: with open(\"tmp.csv\", \"w\") as fh:\n   .....:     fh.write(data)\n   .....: \n\nIn [137]: pd.read_csv(\"tmp.csv\", parse_dates=[0])\nOut[137]: \n        date  value cat\n0 2000-01-06      5   a\n1 2000-02-06     10   b\n2 2000-03-06     15   c\n\nIn [138]: pd.read_csv(\"tmp.csv\", dayfirst=True, parse_dates=[0])\nOut[138]: \n        date  value cat\n0 2000-06-01      5   a\n1 2000-06-02     10   b\n2 2000-06-03     15   c",
      "In [139]: import io\n\nIn [140]: data = pd.DataFrame([0, 1, 2])\n\nIn [141]: buffer = io.BytesIO()\n\nIn [142]: data.to_csv(buffer, encoding=\"utf-8\", compression=\"gzip\")",
      "In [143]: val = \"0.3066101993807095471566981359501369297504425048828125\"\n\nIn [144]: data = \"a,b,c\\n1,2,{0}\".format(val)\n\nIn [145]: abs(\n   .....:     pd.read_csv(\n   .....:         StringIO(data),\n   .....:         engine=\"c\",\n   .....:         float_precision=None,\n   .....:     )[\"c\"][0] - float(val)\n   .....: )\n   .....: \nOut[145]: 5.551115123125783e-17\n\nIn [146]: abs(\n   .....:     pd.read_csv(\n   .....:         StringIO(data),\n   .....:         engine=\"c\",\n   .....:         float_precision=\"high\",\n   .....:     )[\"c\"][0] - float(val)\n   .....: )\n   .....: \nOut[146]: 5.551115123125783e-17\n\nIn [147]: abs(\n   .....:     pd.read_csv(StringIO(data), engine=\"c\", float_precision=\"round_trip\")[\"c\"][0]\n   .....:     - float(val)\n   .....: )\n   .....: \nOut[147]: 0.0",
      "In [148]: data = (\n   .....:     \"ID|level|category\\n\"\n   .....:     \"Patient1|123,000|x\\n\"\n   .....:     \"Patient2|23,000|y\\n\"\n   .....:     \"Patient3|1,234,018|z\"\n   .....: )\n   .....: \n\nIn [149]: with open(\"tmp.csv\", \"w\") as fh:\n   .....:     fh.write(data)\n   .....: \n\nIn [150]: df = pd.read_csv(\"tmp.csv\", sep=\"|\")\n\nIn [151]: df\nOut[151]: \n         ID      level category\n0  Patient1    123,000        x\n1  Patient2     23,000        y\n2  Patient3  1,234,018        z\n\nIn [152]: df.level.dtype\nOut[152]: dtype('O')",
      "In [153]: df = pd.read_csv(\"tmp.csv\", sep=\"|\", thousands=\",\")\n\nIn [154]: df\nOut[154]: \n         ID    level category\n0  Patient1   123000        x\n1  Patient2    23000        y\n2  Patient3  1234018        z\n\nIn [155]: df.level.dtype\nOut[155]: dtype('int64')",
      "pd.read_csv(\"path_to_file.csv\", na_values=[5])",
      "pd.read_csv(\"path_to_file.csv\", keep_default_na=False, na_values=[\"\"])",
      "pd.read_csv(\"path_to_file.csv\", keep_default_na=False, na_values=[\"NA\", \"0\"])",
      "pd.read_csv(\"path_to_file.csv\", na_values=[\"Nope\"])",
      "In [156]: data = \"a,b,c\\n1,Yes,2\\n3,No,4\"\n\nIn [157]: print(data)\na,b,c\n1,Yes,2\n3,No,4\n\nIn [158]: pd.read_csv(StringIO(data))\nOut[158]: \n   a    b  c\n0  1  Yes  2\n1  3   No  4\n\nIn [159]: pd.read_csv(StringIO(data), true_values=[\"Yes\"], false_values=[\"No\"])\nOut[159]: \n   a      b  c\n0  1   True  2\n1  3  False  4",
      "In [160]: data = \"a,b,c\\n1,2,3\\n4,5,6,7\\n8,9,10\"\n\nIn [161]: pd.read_csv(StringIO(data))\n---------------------------------------------------------------------------\nParserError                               Traceback (most recent call last)\nCell In[161], line 1\n----> 1 pd.read_csv(StringIO(data))\n\nFile ~/work/pandas/pandas/pandas/io/parsers/readers.py:1026, in read_csv(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\n   1013 kwds_defaults = _refine_defaults_read(\n   1014     dialect,\n   1015     delimiter,\n   (...)\n   1022     dtype_backend=dtype_backend,\n   1023 )\n   1024 kwds.update(kwds_defaults)\n-> 1026 return _read(filepath_or_buffer, kwds)\n\nFile ~/work/pandas/pandas/pandas/io/parsers/readers.py:626, in _read(filepath_or_buffer, kwds)\n    623     return parser\n    625 with parser:\n--> 626     return parser.read(nrows)\n\nFile ~/work/pandas/pandas/pandas/io/parsers/readers.py:1923, in TextFileReader.read(self, nrows)\n   1916 nrows = validate_integer(\"nrows\", nrows)\n   1917 try:\n   1918     # error: \"ParserBase\" has no attribute \"read\"\n   1919     (\n   1920         index,\n   1921         columns,\n   1922         col_dict,\n-> 1923     ) = self._engine.read(  # type: ignore[attr-defined]\n   1924         nrows\n   1925     )\n   1926 except Exception:\n   1927     self.close()\n\nFile ~/work/pandas/pandas/pandas/io/parsers/c_parser_wrapper.py:234, in CParserWrapper.read(self, nrows)\n    232 try:\n    233     if self.low_memory:\n--> 234         chunks = self._reader.read_low_memory(nrows)\n    235         # destructive to chunks\n    236         data = _concatenate_chunks(chunks)\n\nFile parsers.pyx:838, in pandas._libs.parsers.TextReader.read_low_memory()\n\nFile parsers.pyx:905, in pandas._libs.parsers.TextReader._read_rows()\n\nFile parsers.pyx:874, in pandas._libs.parsers.TextReader._tokenize_rows()\n\nFile parsers.pyx:891, in pandas._libs.parsers.TextReader._check_tokenize_status()\n\nFile parsers.pyx:2061, in pandas._libs.parsers.raise_parser_error()\n\nParserError: Error tokenizing data. C error: Expected 3 fields in line 3, saw 4",
      "In [162]: data = \"a,b,c\\n1,2,3\\n4,5,6,7\\n8,9,10\"\n\nIn [163]: pd.read_csv(StringIO(data), on_bad_lines=\"skip\")\nOut[163]: \n   a  b   c\n0  1  2   3\n1  8  9  10",
      "In [164]: external_list = []\n\nIn [165]: def bad_lines_func(line):\n   .....:     external_list.append(line)\n   .....:     return line[-3:]\n   .....: \n\nIn [166]: external_list\nOut[166]: []",
      "In [167]: bad_lines_func = lambda line: print(line)\n\nIn [168]: data = 'name,type\\nname a,a is of type a\\nname b,\"b\\\" is of type b\"'\n\nIn [169]: data\nOut[169]: 'name,type\\nname a,a is of type a\\nname b,\"b\" is of type b\"'\n\nIn [170]: pd.read_csv(StringIO(data), on_bad_lines=bad_lines_func, engine=\"python\")\nOut[170]: \n     name            type\n0  name a  a is of type a",
      "In [171]: pd.read_csv(StringIO(data), usecols=[0, 1, 2])\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nCell In[171], line 1\n----> 1 pd.read_csv(StringIO(data), usecols=[0, 1, 2])\n\nFile ~/work/pandas/pandas/pandas/io/parsers/readers.py:1026, in read_csv(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\n   1013 kwds_defaults = _refine_defaults_read(\n   1014     dialect,\n   1015     delimiter,\n   (...)\n   1022     dtype_backend=dtype_backend,\n   1023 )\n   1024 kwds.update(kwds_defaults)\n-> 1026 return _read(filepath_or_buffer, kwds)\n\nFile ~/work/pandas/pandas/pandas/io/parsers/readers.py:620, in _read(filepath_or_buffer, kwds)\n    617 _validate_names(kwds.get(\"names\", None))\n    619 # Create the parser.\n--> 620 parser = TextFileReader(filepath_or_buffer, **kwds)\n    622 if chunksize or iterator:\n    623     return parser\n\nFile ~/work/pandas/pandas/pandas/io/parsers/readers.py:1620, in TextFileReader.__init__(self, f, engine, **kwds)\n   1617     self.options[\"has_index_names\"] = kwds[\"has_index_names\"]\n   1619 self.handles: IOHandles | None = None\n-> 1620 self._engine = self._make_engine(f, self.engine)\n\nFile ~/work/pandas/pandas/pandas/io/parsers/readers.py:1898, in TextFileReader._make_engine(self, f, engine)\n   1895     raise ValueError(msg)\n   1897 try:\n-> 1898     return mapping[engine](f, **self.options)\n   1899 except Exception:\n   1900     if self.handles is not None:\n\nFile ~/work/pandas/pandas/pandas/io/parsers/c_parser_wrapper.py:155, in CParserWrapper.__init__(self, src, **kwds)\n    152     # error: Cannot determine type of 'names'\n    153     if len(self.names) < len(usecols):  # type: ignore[has-type]\n    154         # error: Cannot determine type of 'names'\n--> 155         self._validate_usecols_names(\n    156             usecols,\n    157             self.names,  # type: ignore[has-type]\n    158         )\n    160 # error: Cannot determine type of 'names'\n    161 self._validate_parse_dates_presence(self.names)  # type: ignore[has-type]\n\nFile ~/work/pandas/pandas/pandas/io/parsers/base_parser.py:979, in ParserBase._validate_usecols_names(self, usecols, names)\n    977 missing = [c for c in usecols if c not in names]\n    978 if len(missing) > 0:\n--> 979     raise ValueError(\n    980         f\"Usecols do not match columns, columns expected but not found: \"\n    981         f\"{missing}\"\n    982     )\n    984 return usecols\n\nValueError: Usecols do not match columns, columns expected but not found: [0, 1, 2]",
      "In [172]: pd.read_csv(StringIO(data), names=['a', 'b', 'c', 'd'])\nOut[172]: \n        a                b   c   d\n0    name             type NaN NaN\n1  name a   a is of type a NaN NaN\n2  name b  b is of type b\" NaN NaN",
      "In [175]: import csv\n\nIn [176]: dia = csv.excel()\n\nIn [177]: dia.quoting = csv.QUOTE_NONE\n\nIn [178]: pd.read_csv(StringIO(data), dialect=dia)\nOut[178]: \n       label1 label2 label3\nindex1     \"a      c      e\nindex2      b      d      f",
      "In [179]: data = \"a,b,c~1,2,3~4,5,6\"\n\nIn [180]: pd.read_csv(StringIO(data), lineterminator=\"~\")\nOut[180]: \n   a  b  c\n0  1  2  3\n1  4  5  6",
      "In [181]: data = \"a, b, c\\n1, 2, 3\\n4, 5, 6\"\n\nIn [182]: print(data)\na, b, c\n1, 2, 3\n4, 5, 6\n\nIn [183]: pd.read_csv(StringIO(data), skipinitialspace=True)\nOut[183]: \n   a  b  c\n0  1  2  3\n1  4  5  6",
      "In [184]: data = 'a,b\\n\"hello, \\\\\"Bob\\\\\", nice to see you\",5'\n\nIn [185]: print(data)\na,b\n\"hello, \\\"Bob\\\", nice to see you\",5\n\nIn [186]: pd.read_csv(StringIO(data), escapechar=\"\\\\\")\nOut[186]: \n                               a  b\n0  hello, \"Bob\", nice to see you  5",
      "# Column specifications are a list of half-intervals\nIn [189]: colspecs = [(0, 6), (8, 20), (21, 33), (34, 43)]\n\nIn [190]: df = pd.read_fwf(\"bar.csv\", colspecs=colspecs, header=None, index_col=0)\n\nIn [191]: df\nOut[191]: \n                 1           2        3\n0                                      \nid8141  360.242940  149.910199  11950.7\nid1594  444.953632  166.985655  11788.4\nid1849  364.136849  183.628767  11806.2\nid1230  413.836124  184.375703  11916.8\nid1948  502.953953  173.237159  12468.3",
      "# Widths are a list of integers\nIn [192]: widths = [6, 14, 13, 10]\n\nIn [193]: df = pd.read_fwf(\"bar.csv\", widths=widths, header=None)\n\nIn [194]: df\nOut[194]: \n        0           1           2        3\n0  id8141  360.242940  149.910199  11950.7\n1  id1594  444.953632  166.985655  11788.4\n2  id1849  364.136849  183.628767  11806.2\n3  id1230  413.836124  184.375703  11916.8\n4  id1948  502.953953  173.237159  12468.3",
      "In [195]: df = pd.read_fwf(\"bar.csv\", header=None, index_col=0)\n\nIn [196]: df\nOut[196]: \n                 1           2        3\n0                                      \nid8141  360.242940  149.910199  11950.7\nid1594  444.953632  166.985655  11788.4\nid1849  364.136849  183.628767  11806.2\nid1230  413.836124  184.375703  11916.8\nid1948  502.953953  173.237159  12468.3",
      "In [197]: pd.read_fwf(\"bar.csv\", header=None, index_col=0).dtypes\nOut[197]: \n1    float64\n2    float64\n3    float64\ndtype: object\n\nIn [198]: pd.read_fwf(\"bar.csv\", header=None, dtype={2: \"object\"}).dtypes\nOut[198]: \n0     object\n1    float64\n2     object\n3    float64\ndtype: object",
      "In [202]: pd.read_csv(\"foo.csv\")\nOut[202]: \n          A  B  C\n20090101  a  1  2\n20090102  b  3  4\n20090103  c  4  5",
      "In [203]: df = pd.read_csv(\"foo.csv\", parse_dates=True)\n\nIn [204]: df.index\nOut[204]: DatetimeIndex(['2009-01-01', '2009-01-02', '2009-01-03'], dtype='datetime64[ns]', freq=None)",
      "In [208]: df = pd.read_csv(\"mindex_ex.csv\", index_col=[0, 1])\n\nIn [209]: df\nOut[209]: \n            zit  xit\nyear indiv          \n1977 A      1.2  0.6\n     B      1.5  0.5\n\nIn [210]: df.loc[1977]\nOut[210]: \n       zit  xit\nindiv          \nA      1.2  0.6\nB      1.5  0.5",
      "In [211]: mi_idx = pd.MultiIndex.from_arrays([[1, 2, 3, 4], list(\"abcd\")], names=list(\"ab\"))\n\nIn [212]: mi_col = pd.MultiIndex.from_arrays([[1, 2], list(\"ab\")], names=list(\"cd\"))\n\nIn [213]: df = pd.DataFrame(np.ones((4, 2)), index=mi_idx, columns=mi_col)\n\nIn [214]: df.to_csv(\"mi.csv\")\n\nIn [215]: print(open(\"mi.csv\").read())\nc,,1,2\nd,,a,b\na,b,,\n1,a,1.0,1.0\n2,b,1.0,1.0\n3,c,1.0,1.0\n4,d,1.0,1.0\n\n\nIn [216]: pd.read_csv(\"mi.csv\", header=[0, 1, 2, 3], index_col=[0, 1])\nOut[216]: \nc                    1                  2\nd                    a                  b\na   Unnamed: 2_level_2 Unnamed: 3_level_2\n1                  1.0                1.0\n2 b                1.0                1.0\n3 c                1.0                1.0\n4 d                1.0                1.0",
      "In [217]: data = \",a,a,a,b,c,c\\n,q,r,s,t,u,v\\none,1,2,3,4,5,6\\ntwo,7,8,9,10,11,12\"\n\nIn [218]: print(data)\n,a,a,a,b,c,c\n,q,r,s,t,u,v\none,1,2,3,4,5,6\ntwo,7,8,9,10,11,12\n\nIn [219]: with open(\"mi2.csv\", \"w\") as fh:\n   .....:     fh.write(data)\n   .....: \n\nIn [220]: pd.read_csv(\"mi2.csv\", header=[0, 1], index_col=0)\nOut[220]: \n     a         b   c    \n     q  r  s   t   u   v\none  1  2  3   4   5   6\ntwo  7  8  9  10  11  12",
      "In [221]: df = pd.DataFrame(np.random.randn(10, 4))\n\nIn [222]: df.to_csv(\"tmp2.csv\", sep=\":\", index=False)\n\nIn [223]: pd.read_csv(\"tmp2.csv\", sep=None, engine=\"python\")\nOut[223]: \n          0         1         2         3\n0  0.469112 -0.282863 -1.509059 -1.135632\n1  1.212112 -0.173215  0.119209 -1.044236\n2 -0.861849 -2.104569 -0.494929  1.071804\n3  0.721555 -0.706771 -1.039575  0.271860\n4 -0.424972  0.567020  0.276232 -1.087401\n5 -0.673690  0.113648 -1.478427  0.524988\n6  0.404705  0.577046 -1.715002 -1.039268\n7 -0.370647 -1.157892 -1.344312  0.844885\n8  1.075770 -0.109050  1.643563 -1.469388\n9  0.357021 -0.674600 -1.776904 -0.968914",
      "In [224]: df = pd.DataFrame(np.random.randn(10, 4))\n\nIn [225]: df.to_csv(\"tmp.csv\", index=False)\n\nIn [226]: table = pd.read_csv(\"tmp.csv\")\n\nIn [227]: table\nOut[227]: \n          0         1         2         3\n0 -1.294524  0.413738  0.276662 -0.472035\n1 -0.013960 -0.362543 -0.006154 -0.923061\n2  0.895717  0.805244 -1.206412  2.565646\n3  1.431256  1.340309 -1.170299 -0.226169\n4  0.410835  0.813850  0.132003 -0.827317\n5 -0.076467 -1.187678  1.130127 -1.436737\n6 -1.413681  1.607920  1.024180  0.569605\n7  0.875906 -2.211372  0.974466 -2.006747\n8 -0.410001 -0.078638  0.545952 -1.219217\n9 -1.226825  0.769804 -1.281247 -0.727707",
      "In [228]: with pd.read_csv(\"tmp.csv\", chunksize=4) as reader:\n   .....:     print(reader)\n   .....:     for chunk in reader:\n   .....:         print(chunk)\n   .....: \n<pandas.io.parsers.readers.TextFileReader object at 0x7fe8deebc0d0>\n          0         1         2         3\n0 -1.294524  0.413738  0.276662 -0.472035\n1 -0.013960 -0.362543 -0.006154 -0.923061\n2  0.895717  0.805244 -1.206412  2.565646\n3  1.431256  1.340309 -1.170299 -0.226169\n          0         1         2         3\n4  0.410835  0.813850  0.132003 -0.827317\n5 -0.076467 -1.187678  1.130127 -1.436737\n6 -1.413681  1.607920  1.024180  0.569605\n7  0.875906 -2.211372  0.974466 -2.006747\n          0         1         2         3\n8 -0.410001 -0.078638  0.545952 -1.219217\n9 -1.226825  0.769804 -1.281247 -0.727707",
      "In [229]: with pd.read_csv(\"tmp.csv\", iterator=True) as reader:\n   .....:     print(reader.get_chunk(5))\n   .....: \n          0         1         2         3\n0 -1.294524  0.413738  0.276662 -0.472035\n1 -0.013960 -0.362543 -0.006154 -0.923061\n2  0.895717  0.805244 -1.206412  2.565646\n3  1.431256  1.340309 -1.170299 -0.226169\n4  0.410835  0.813850  0.132003 -0.827317",
      "df = pd.read_csv(\"https://download.bls.gov/pub/time.series/cu/cu.item\", sep=\"\\t\")",
      "headers = {\"User-Agent\": \"pandas\"}\ndf = pd.read_csv(\n    \"https://download.bls.gov/pub/time.series/cu/cu.item\",\n    sep=\"\\t\",\n    storage_options=headers\n)",
      "df = pd.read_json(\"s3://pandas-test/adatafile.json\")",
      "storage_options = {\"client_kwargs\": {\"endpoint_url\": \"http://127.0.0.1:5555\"}}}\ndf = pd.read_json(\"s3://pandas-test/test-1\", storage_options=storage_options)",
      "pd.read_csv(\n    \"s3://ncei-wcsd-archive/data/processed/SH1305/18kHz/SaKe2013\"\n    \"-D20130523-T080854_to_SaKe2013-D20130523-T085643.csv\",\n    storage_options={\"anon\": True},\n)",
      "pd.read_csv(\n    \"simplecache::s3://ncei-wcsd-archive/data/processed/SH1305/18kHz/\"\n    \"SaKe2013-D20130523-T080854_to_SaKe2013-D20130523-T085643.csv\",\n    storage_options={\"s3\": {\"anon\": True}},\n)",
      "In [230]: dfj = pd.DataFrame(np.random.randn(5, 2), columns=list(\"AB\"))\n\nIn [231]: json = dfj.to_json()\n\nIn [232]: json\nOut[232]: '{\"A\":{\"0\":-0.1213062281,\"1\":0.6957746499,\"2\":0.9597255933,\"3\":-0.6199759194,\"4\":-0.7323393705},\"B\":{\"0\":-0.0978826728,\"1\":0.3417343559,\"2\":-1.1103361029,\"3\":0.1497483186,\"4\":0.6877383895}}'",
      "In [233]: dfjo = pd.DataFrame(\n   .....:     dict(A=range(1, 4), B=range(4, 7), C=range(7, 10)),\n   .....:     columns=list(\"ABC\"),\n   .....:     index=list(\"xyz\"),\n   .....: )\n   .....: \n\nIn [234]: dfjo\nOut[234]: \n   A  B  C\nx  1  4  7\ny  2  5  8\nz  3  6  9\n\nIn [235]: sjo = pd.Series(dict(x=15, y=16, z=17), name=\"D\")\n\nIn [236]: sjo\nOut[236]: \nx    15\ny    16\nz    17\nName: D, dtype: int64",
      "In [245]: dfd = pd.DataFrame(np.random.randn(5, 2), columns=list(\"AB\"))\n\nIn [246]: dfd[\"date\"] = pd.Timestamp(\"20130101\")\n\nIn [247]: dfd = dfd.sort_index(axis=1, ascending=False)\n\nIn [248]: json = dfd.to_json(date_format=\"iso\")\n\nIn [249]: json\nOut[249]: '{\"date\":{\"0\":\"2013-01-01T00:00:00.000\",\"1\":\"2013-01-01T00:00:00.000\",\"2\":\"2013-01-01T00:00:00.000\",\"3\":\"2013-01-01T00:00:00.000\",\"4\":\"2013-01-01T00:00:00.000\"},\"B\":{\"0\":0.403309524,\"1\":0.3016244523,\"2\":-1.3698493577,\"3\":1.4626960492,\"4\":-0.8265909164},\"A\":{\"0\":0.1764443426,\"1\":-0.1549507744,\"2\":-2.1798606054,\"3\":-0.9542078401,\"4\":-1.7431609117}}'",
      "In [254]: dfj2 = dfj.copy()\n\nIn [255]: dfj2[\"date\"] = pd.Timestamp(\"20130101\")\n\nIn [256]: dfj2[\"ints\"] = list(range(5))\n\nIn [257]: dfj2[\"bools\"] = True\n\nIn [258]: dfj2.index = pd.date_range(\"20130101\", periods=5)\n\nIn [259]: dfj2.to_json(\"test.json\")\n\nIn [260]: with open(\"test.json\") as fh:\n   .....:     print(fh.read())\n   .....: \n{\"A\":{\"1356998400000\":-0.1213062281,\"1357084800000\":0.6957746499,\"1357171200000\":0.9597255933,\"1357257600000\":-0.6199759194,\"1357344000000\":-0.7323393705},\"B\":{\"1356998400000\":-0.0978826728,\"1357084800000\":0.3417343559,\"1357171200000\":-1.1103361029,\"1357257600000\":0.1497483186,\"1357344000000\":0.6877383895},\"date\":{\"1356998400000\":1356,\"1357084800000\":1356,\"1357171200000\":1356,\"1357257600000\":1356,\"1357344000000\":1356},\"ints\":{\"1356998400000\":0,\"1357084800000\":1,\"1357171200000\":2,\"1357257600000\":3,\"1357344000000\":4},\"bools\":{\"1356998400000\":true,\"1357084800000\":true,\"1357171200000\":true,\"1357257600000\":true,\"1357344000000\":true}}",
      "In [261]: pd.DataFrame([1.0, 2.0, complex(1.0, 2.0)]).to_json(default_handler=str)\nOut[261]: '{\"0\":{\"0\":\"(1+0j)\",\"1\":\"(2+0j)\",\"2\":\"(1+2j)\"}}'",
      "In [262]: from io import StringIO\n\nIn [263]: pd.read_json(StringIO(json))\nOut[263]: \n   date         B         A\n0     1  0.403310  0.176444\n1     1  0.301624 -0.154951\n2     1 -1.369849 -2.179861\n3     1  1.462696 -0.954208\n4     1 -0.826591 -1.743161",
      "In [264]: pd.read_json(\"test.json\")\nOut[264]: \n                   A         B  date  ints  bools\n2013-01-01 -0.121306 -0.097883  1356     0   True\n2013-01-02  0.695775  0.341734  1356     1   True\n2013-01-03  0.959726 -1.110336  1356     2   True\n2013-01-04 -0.619976  0.149748  1356     3   True\n2013-01-05 -0.732339  0.687738  1356     4   True",
      "In [265]: pd.read_json(\"test.json\", dtype=object).dtypes\nOut[265]: \nA        object\nB        object\ndate     object\nints     object\nbools    object\ndtype: object",
      "In [266]: pd.read_json(\"test.json\", dtype={\"A\": \"float32\", \"bools\": \"int8\"}).dtypes\nOut[266]: \nA        float32\nB        float64\ndate       int64\nints       int64\nbools       int8\ndtype: object",
      "In [267]: from io import StringIO\n\nIn [268]: si = pd.DataFrame(\n   .....:     np.zeros((4, 4)), columns=list(range(4)), index=[str(i) for i in range(4)]\n   .....: )\n   .....: \n\nIn [269]: si\nOut[269]: \n     0    1    2    3\n0  0.0  0.0  0.0  0.0\n1  0.0  0.0  0.0  0.0\n2  0.0  0.0  0.0  0.0\n3  0.0  0.0  0.0  0.0\n\nIn [270]: si.index\nOut[270]: Index(['0', '1', '2', '3'], dtype='object')\n\nIn [271]: si.columns\nOut[271]: Index([0, 1, 2, 3], dtype='int64')\n\nIn [272]: json = si.to_json()\n\nIn [273]: sij = pd.read_json(StringIO(json), convert_axes=False)\n\nIn [274]: sij\nOut[274]: \n   0  1  2  3\n0  0  0  0  0\n1  0  0  0  0\n2  0  0  0  0\n3  0  0  0  0\n\nIn [275]: sij.index\nOut[275]: Index(['0', '1', '2', '3'], dtype='object')\n\nIn [276]: sij.columns\nOut[276]: Index(['0', '1', '2', '3'], dtype='object')",
      "In [277]: from io import StringIO\n\nIn [278]: json = dfj2.to_json(date_unit=\"ns\")\n\n# Try to parse timestamps as milliseconds -> Won't Work\nIn [279]: dfju = pd.read_json(StringIO(json), date_unit=\"ms\")\n\nIn [280]: dfju\nOut[280]: \n                            A         B        date  ints  bools\n1356998400000000000 -0.121306 -0.097883  1356998400     0   True\n1357084800000000000  0.695775  0.341734  1356998400     1   True\n1357171200000000000  0.959726 -1.110336  1356998400     2   True\n1357257600000000000 -0.619976  0.149748  1356998400     3   True\n1357344000000000000 -0.732339  0.687738  1356998400     4   True\n\n# Let pandas detect the correct precision\nIn [281]: dfju = pd.read_json(StringIO(json))\n\nIn [282]: dfju\nOut[282]: \n                   A         B       date  ints  bools\n2013-01-01 -0.121306 -0.097883 2013-01-01     0   True\n2013-01-02  0.695775  0.341734 2013-01-01     1   True\n2013-01-03  0.959726 -1.110336 2013-01-01     2   True\n2013-01-04 -0.619976  0.149748 2013-01-01     3   True\n2013-01-05 -0.732339  0.687738 2013-01-01     4   True\n\n# Or specify that all timestamps are in nanoseconds\nIn [283]: dfju = pd.read_json(StringIO(json), date_unit=\"ns\")\n\nIn [284]: dfju\nOut[284]: \n                   A         B        date  ints  bools\n2013-01-01 -0.121306 -0.097883  1356998400     0   True\n2013-01-02  0.695775  0.341734  1356998400     1   True\n2013-01-03  0.959726 -1.110336  1356998400     2   True\n2013-01-04 -0.619976  0.149748  1356998400     3   True\n2013-01-05 -0.732339  0.687738  1356998400     4   True",
      "In [285]: data = (\n   .....:  '{\"a\":{\"0\":1,\"1\":3},\"b\":{\"0\":2.5,\"1\":4.5},\"c\":{\"0\":true,\"1\":false},\"d\":{\"0\":\"a\",\"1\":\"b\"},'\n   .....:  '\"e\":{\"0\":null,\"1\":6.0},\"f\":{\"0\":null,\"1\":7.5},\"g\":{\"0\":null,\"1\":true},\"h\":{\"0\":null,\"1\":\"a\"},'\n   .....:  '\"i\":{\"0\":\"12-31-2019\",\"1\":\"12-31-2019\"},\"j\":{\"0\":null,\"1\":null}}'\n   .....: )\n   .....: \n\nIn [286]: df = pd.read_json(StringIO(data), dtype_backend=\"pyarrow\")\n\nIn [287]: df\nOut[287]: \n   a    b      c  d     e     f     g     h           i     j\n0  1  2.5   True  a  <NA>  <NA>  <NA>  <NA>  12-31-2019  None\n1  3  4.5  False  b     6   7.5  True     a  12-31-2019  None\n\nIn [288]: df.dtypes\nOut[288]: \na     int64[pyarrow]\nb    double[pyarrow]\nc      bool[pyarrow]\nd    string[pyarrow]\ne     int64[pyarrow]\nf    double[pyarrow]\ng      bool[pyarrow]\nh    string[pyarrow]\ni    string[pyarrow]\nj      null[pyarrow]\ndtype: object",
      "In [289]: data = [\n   .....:     {\"id\": 1, \"name\": {\"first\": \"Coleen\", \"last\": \"Volk\"}},\n   .....:     {\"name\": {\"given\": \"Mark\", \"family\": \"Regner\"}},\n   .....:     {\"id\": 2, \"name\": \"Faye Raker\"},\n   .....: ]\n   .....: \n\nIn [290]: pd.json_normalize(data)\nOut[290]: \n    id name.first name.last name.given name.family        name\n0  1.0     Coleen      Volk        NaN         NaN         NaN\n1  NaN        NaN       NaN       Mark      Regner         NaN\n2  2.0        NaN       NaN        NaN         NaN  Faye Raker",
      "In [291]: data = [\n   .....:     {\n   .....:         \"state\": \"Florida\",\n   .....:         \"shortname\": \"FL\",\n   .....:         \"info\": {\"governor\": \"Rick Scott\"},\n   .....:         \"county\": [\n   .....:             {\"name\": \"Dade\", \"population\": 12345},\n   .....:             {\"name\": \"Broward\", \"population\": 40000},\n   .....:             {\"name\": \"Palm Beach\", \"population\": 60000},\n   .....:         ],\n   .....:     },\n   .....:     {\n   .....:         \"state\": \"Ohio\",\n   .....:         \"shortname\": \"OH\",\n   .....:         \"info\": {\"governor\": \"John Kasich\"},\n   .....:         \"county\": [\n   .....:             {\"name\": \"Summit\", \"population\": 1234},\n   .....:             {\"name\": \"Cuyahoga\", \"population\": 1337},\n   .....:         ],\n   .....:     },\n   .....: ]\n   .....: \n\nIn [292]: pd.json_normalize(data, \"county\", [\"state\", \"shortname\", [\"info\", \"governor\"]])\nOut[292]: \n         name  population    state shortname info.governor\n0        Dade       12345  Florida        FL    Rick Scott\n1     Broward       40000  Florida        FL    Rick Scott\n2  Palm Beach       60000  Florida        FL    Rick Scott\n3      Summit        1234     Ohio        OH   John Kasich\n4    Cuyahoga        1337     Ohio        OH   John Kasich",
      "In [293]: data = [\n   .....:     {\n   .....:         \"CreatedBy\": {\"Name\": \"User001\"},\n   .....:         \"Lookup\": {\n   .....:             \"TextField\": \"Some text\",\n   .....:             \"UserField\": {\"Id\": \"ID001\", \"Name\": \"Name001\"},\n   .....:         },\n   .....:         \"Image\": {\"a\": \"b\"},\n   .....:     }\n   .....: ]\n   .....: \n\nIn [294]: pd.json_normalize(data, max_level=1)\nOut[294]: \n  CreatedBy.Name Lookup.TextField                    Lookup.UserField Image.a\n0        User001        Some text  {'Id': 'ID001', 'Name': 'Name001'}       b",
      "In [295]: from io import StringIO\n\nIn [296]: jsonl = \"\"\"\n   .....:     {\"a\": 1, \"b\": 2}\n   .....:     {\"a\": 3, \"b\": 4}\n   .....: \"\"\"\n   .....: \n\nIn [297]: df = pd.read_json(StringIO(jsonl), lines=True)\n\nIn [298]: df\nOut[298]: \n   a  b\n0  1  2\n1  3  4\n\nIn [299]: df.to_json(orient=\"records\", lines=True)\nOut[299]: '{\"a\":1,\"b\":2}\\n{\"a\":3,\"b\":4}\\n'\n\n# reader is an iterator that returns ``chunksize`` lines each iteration\nIn [300]: with pd.read_json(StringIO(jsonl), lines=True, chunksize=1) as reader:\n   .....:     reader\n   .....:     for chunk in reader:\n   .....:         print(chunk)\n   .....: \nEmpty DataFrame\nColumns: []\nIndex: []\n   a  b\n0  1  2\n   a  b\n1  3  4",
      "In [301]: from io import BytesIO\n\nIn [302]: df = pd.read_json(BytesIO(jsonl.encode()), lines=True, engine=\"pyarrow\")\n\nIn [303]: df\nOut[303]: \n   a  b\n0  1  2\n1  3  4",
      "In [304]: df = pd.DataFrame(\n   .....:     {\n   .....:         \"A\": [1, 2, 3],\n   .....:         \"B\": [\"a\", \"b\", \"c\"],\n   .....:         \"C\": pd.date_range(\"2016-01-01\", freq=\"d\", periods=3),\n   .....:     },\n   .....:     index=pd.Index(range(3), name=\"idx\"),\n   .....: )\n   .....: \n\nIn [305]: df\nOut[305]: \n     A  B          C\nidx                 \n0    1  a 2016-01-01\n1    2  b 2016-01-02\n2    3  c 2016-01-03\n\nIn [306]: df.to_json(orient=\"table\", date_format=\"iso\")\nOut[306]: '{\"schema\":{\"fields\":[{\"name\":\"idx\",\"type\":\"integer\"},{\"name\":\"A\",\"type\":\"integer\"},{\"name\":\"B\",\"type\":\"string\"},{\"name\":\"C\",\"type\":\"datetime\"}],\"primaryKey\":[\"idx\"],\"pandas_version\":\"1.4.0\"},\"data\":[{\"idx\":0,\"A\":1,\"B\":\"a\",\"C\":\"2016-01-01T00:00:00.000\"},{\"idx\":1,\"A\":2,\"B\":\"b\",\"C\":\"2016-01-02T00:00:00.000\"},{\"idx\":2,\"A\":3,\"B\":\"c\",\"C\":\"2016-01-03T00:00:00.000\"}]}'",
      "In [307]: from pandas.io.json import build_table_schema\n\nIn [308]: s = pd.Series(pd.date_range(\"2016\", periods=4))\n\nIn [309]: build_table_schema(s)\nOut[309]: \n{'fields': [{'name': 'index', 'type': 'integer'},\n  {'name': 'values', 'type': 'datetime'}],\n 'primaryKey': ['index'],\n 'pandas_version': '1.4.0'}",
      "In [310]: s_tz = pd.Series(pd.date_range(\"2016\", periods=12, tz=\"US/Central\"))\n\nIn [311]: build_table_schema(s_tz)\nOut[311]: \n{'fields': [{'name': 'index', 'type': 'integer'},\n  {'name': 'values', 'type': 'datetime', 'tz': 'US/Central'}],\n 'primaryKey': ['index'],\n 'pandas_version': '1.4.0'}",
      "In [312]: s_per = pd.Series(1, index=pd.period_range(\"2016\", freq=\"Y-DEC\", periods=4))\n\nIn [313]: build_table_schema(s_per)\nOut[313]: \n{'fields': [{'name': 'index', 'type': 'datetime', 'freq': 'YE-DEC'},\n  {'name': 'values', 'type': 'integer'}],\n 'primaryKey': ['index'],\n 'pandas_version': '1.4.0'}",
      "In [314]: s_cat = pd.Series(pd.Categorical([\"a\", \"b\", \"a\"]))\n\nIn [315]: build_table_schema(s_cat)\nOut[315]: \n{'fields': [{'name': 'index', 'type': 'integer'},\n  {'name': 'values',\n   'type': 'any',\n   'constraints': {'enum': ['a', 'b']},\n   'ordered': False}],\n 'primaryKey': ['index'],\n 'pandas_version': '1.4.0'}",
      "In [316]: s_dupe = pd.Series([1, 2], index=[1, 1])\n\nIn [317]: build_table_schema(s_dupe)\nOut[317]: \n{'fields': [{'name': 'index', 'type': 'integer'},\n  {'name': 'values', 'type': 'integer'}],\n 'pandas_version': '1.4.0'}",
      "In [318]: s_multi = pd.Series(1, index=pd.MultiIndex.from_product([(\"a\", \"b\"), (0, 1)]))\n\nIn [319]: build_table_schema(s_multi)\nOut[319]: \n{'fields': [{'name': 'level_0', 'type': 'string'},\n  {'name': 'level_1', 'type': 'integer'},\n  {'name': 'values', 'type': 'integer'}],\n 'primaryKey': FrozenList(['level_0', 'level_1']),\n 'pandas_version': '1.4.0'}",
      "In [320]: df = pd.DataFrame(\n   .....:     {\n   .....:         \"foo\": [1, 2, 3, 4],\n   .....:         \"bar\": [\"a\", \"b\", \"c\", \"d\"],\n   .....:         \"baz\": pd.date_range(\"2018-01-01\", freq=\"d\", periods=4),\n   .....:         \"qux\": pd.Categorical([\"a\", \"b\", \"c\", \"c\"]),\n   .....:     },\n   .....:     index=pd.Index(range(4), name=\"idx\"),\n   .....: )\n   .....: \n\nIn [321]: df\nOut[321]: \n     foo bar        baz qux\nidx                        \n0      1   a 2018-01-01   a\n1      2   b 2018-01-02   b\n2      3   c 2018-01-03   c\n3      4   d 2018-01-04   c\n\nIn [322]: df.dtypes\nOut[322]: \nfoo             int64\nbar            object\nbaz    datetime64[ns]\nqux          category\ndtype: object\n\nIn [323]: df.to_json(\"test.json\", orient=\"table\")\n\nIn [324]: new_df = pd.read_json(\"test.json\", orient=\"table\")\n\nIn [325]: new_df\nOut[325]: \n     foo bar        baz qux\nidx                        \n0      1   a 2018-01-01   a\n1      2   b 2018-01-02   b\n2      3   c 2018-01-03   c\n3      4   d 2018-01-04   c\n\nIn [326]: new_df.dtypes\nOut[326]: \nfoo             int64\nbar            object\nbaz    datetime64[ns]\nqux          category\ndtype: object",
      "In [327]: df.index.name = \"index\"\n\nIn [328]: df.to_json(\"test.json\", orient=\"table\")\n\nIn [329]: new_df = pd.read_json(\"test.json\", orient=\"table\")\n\nIn [330]: print(new_df.index.name)\nNone",
      "In [320]: url = \"https://www.fdic.gov/resources/resolutions/bank-failures/failed-bank-list\"\nIn [321]: pd.read_html(url)\nOut[321]:\n[                         Bank NameBank           CityCity StateSt  ...              Acquiring InstitutionAI Closing DateClosing FundFund\n 0                    Almena State Bank             Almena      KS  ...                          Equity Bank    October 23, 2020    10538\n 1           First City Bank of Florida  Fort Walton Beach      FL  ...            United Fidelity Bank, fsb    October 16, 2020    10537\n 2                 The First State Bank      Barboursville      WV  ...                       MVB Bank, Inc.       April 3, 2020    10536\n 3                   Ericson State Bank            Ericson      NE  ...           Farmers and Merchants Bank   February 14, 2020    10535\n 4     City National Bank of New Jersey             Newark      NJ  ...                      Industrial Bank    November 1, 2019    10534\n ..                                 ...                ...     ...  ...                                  ...                 ...      ...\n 558                 Superior Bank, FSB           Hinsdale      IL  ...                Superior Federal, FSB       July 27, 2001     6004\n 559                Malta National Bank              Malta      OH  ...                    North Valley Bank         May 3, 2001     4648\n 560    First Alliance Bank & Trust Co.         Manchester      NH  ...  Southern New Hampshire Bank & Trust    February 2, 2001     4647\n 561  National State Bank of Metropolis         Metropolis      IL  ...              Banterra Bank of Marion   December 14, 2000     4646\n 562                   Bank of Honolulu           Honolulu      HI  ...                   Bank of the Orient    October 13, 2000     4645\n\n [563 rows x 7 columns]]",
      "In [322]: url = 'https://www.sump.org/notes/request/' # HTTP request reflector\nIn [323]: pd.read_html(url)\nOut[323]:\n[                   0                    1\n 0     Remote Socket:  51.15.105.256:51760\n 1  Protocol Version:             HTTP/1.1\n 2    Request Method:                  GET\n 3       Request URI:      /notes/request/\n 4     Request Query:                  NaN,\n 0   Accept-Encoding:             identity\n 1              Host:         www.sump.org\n 2        User-Agent:    Python-urllib/3.8\n 3        Connection:                close]\nIn [324]: headers = {\nIn [325]:    'User-Agent':'Mozilla Firefox v14.0',\nIn [326]:    'Accept':'application/json',\nIn [327]:    'Connection':'keep-alive',\nIn [328]:    'Auth':'Bearer 2*/f3+fe68df*4'\nIn [329]: }\nIn [340]: pd.read_html(url, storage_options=headers)\nOut[340]:\n[                   0                    1\n 0     Remote Socket:  51.15.105.256:51760\n 1  Protocol Version:             HTTP/1.1\n 2    Request Method:                  GET\n 3       Request URI:      /notes/request/\n 4     Request Query:                  NaN,\n 0        User-Agent: Mozilla Firefox v14.0\n 1    AcceptEncoding:   gzip,  deflate,  br\n 2            Accept:      application/json\n 3        Connection:             keep-alive\n 4              Auth:  Bearer 2*/f3+fe68df*4]",
      "In [331]: html_str = \"\"\"\n   .....:          <table>\n   .....:              <tr>\n   .....:                  <th>A</th>\n   .....:                  <th colspan=\"1\">B</th>\n   .....:                  <th rowspan=\"1\">C</th>\n   .....:              </tr>\n   .....:              <tr>\n   .....:                  <td>a</td>\n   .....:                  <td>b</td>\n   .....:                  <td>c</td>\n   .....:              </tr>\n   .....:          </table>\n   .....:      \"\"\"\n   .....: \n\nIn [332]: with open(\"tmp.html\", \"w\") as f:\n   .....:     f.write(html_str)\n   .....: \n\nIn [333]: df = pd.read_html(\"tmp.html\")\n\nIn [334]: df[0]\nOut[334]: \n   A  B  C\n0  a  b  c",
      "In [335]: dfs = pd.read_html(StringIO(html_str))\n\nIn [336]: dfs[0]\nOut[336]: \n   A  B  C\n0  a  b  c",
      "match = \"Metcalf Bank\"\ndf_list = pd.read_html(url, match=match)",
      "dfs = pd.read_html(url, header=0)",
      "dfs = pd.read_html(url, index_col=0)",
      "dfs = pd.read_html(url, skiprows=0)",
      "dfs = pd.read_html(url, skiprows=range(2))",
      "dfs1 = pd.read_html(url, attrs={\"id\": \"table\"})\ndfs2 = pd.read_html(url, attrs={\"class\": \"sortable\"})\nprint(np.array_equal(dfs1[0], dfs2[0]))  # Should be True",
      "dfs = pd.read_html(url, na_values=[\"No Acquirer\"])",
      "dfs = pd.read_html(url, keep_default_na=False)",
      "url_mcc = \"https://en.wikipedia.org/wiki/Mobile_country_code?oldid=899173761\"\ndfs = pd.read_html(\n    url_mcc,\n    match=\"Telekom Albania\",\n    header=0,\n    converters={\"MNC\": str},\n)",
      "dfs = pd.read_html(url, match=\"Metcalf Bank\", index_col=0)",
      "df = pd.DataFrame(np.random.randn(2, 2))\ns = df.to_html(float_format=\"{0:.40g}\".format)\ndfin = pd.read_html(s, index_col=0)",
      "dfs = pd.read_html(url, \"Metcalf Bank\", index_col=0, flavor=[\"lxml\"])",
      "dfs = pd.read_html(url, \"Metcalf Bank\", index_col=0, flavor=\"lxml\")",
      "dfs = pd.read_html(url, \"Metcalf Bank\", index_col=0, flavor=[\"lxml\", \"bs4\"])",
      "In [337]: html_table = \"\"\"\n   .....: <table>\n   .....:   <tr>\n   .....:     <th>GitHub</th>\n   .....:   </tr>\n   .....:   <tr>\n   .....:     <td><a href=\"https://github.com/pandas-dev/pandas\">pandas</a></td>\n   .....:   </tr>\n   .....: </table>\n   .....: \"\"\"\n   .....: \n\nIn [338]: df = pd.read_html(\n   .....:     StringIO(html_table),\n   .....:     extract_links=\"all\"\n   .....: )[0]\n   .....: \n\nIn [339]: df\nOut[339]: \n                                   (GitHub, None)\n0  (pandas, https://github.com/pandas-dev/pandas)\n\nIn [340]: df[(\"GitHub\", None)]\nOut[340]: \n0    (pandas, https://github.com/pandas-dev/pandas)\nName: (GitHub, None), dtype: object\n\nIn [341]: df[(\"GitHub\", None)].str[1]\nOut[341]: \n0    https://github.com/pandas-dev/pandas\nName: (GitHub, None), dtype: object",
      "In [342]: from IPython.display import display, HTML\n\nIn [343]: df = pd.DataFrame(np.random.randn(2, 2))\n\nIn [344]: df\nOut[344]: \n          0         1\n0 -0.345352  1.314232\n1  0.690579  0.995761\n\nIn [345]: html = df.to_html()\n\nIn [346]: print(html)  # raw html\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-0.345352</td>\n      <td>1.314232</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.690579</td>\n      <td>0.995761</td>\n    </tr>\n  </tbody>\n</table>\n\nIn [347]: display(HTML(html))\n<IPython.core.display.HTML object>",
      "In [348]: html = df.to_html(columns=[0])\n\nIn [349]: print(html)\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-0.345352</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.690579</td>\n    </tr>\n  </tbody>\n</table>\n\nIn [350]: display(HTML(html))\n<IPython.core.display.HTML object>",
      "In [351]: html = df.to_html(float_format=\"{0:.10f}\".format)\n\nIn [352]: print(html)\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-0.3453521949</td>\n      <td>1.3142323796</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.6905793352</td>\n      <td>0.9957609037</td>\n    </tr>\n  </tbody>\n</table>\n\nIn [353]: display(HTML(html))\n<IPython.core.display.HTML object>",
      "In [354]: html = df.to_html(bold_rows=False)\n\nIn [355]: print(html)\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>-0.345352</td>\n      <td>1.314232</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>0.690579</td>\n      <td>0.995761</td>\n    </tr>\n  </tbody>\n</table>\n\nIn [356]: display(HTML(html))\n<IPython.core.display.HTML object>",
      "In [357]: print(df.to_html(classes=[\"awesome_table_class\", \"even_more_awesome_class\"]))\n<table border=\"1\" class=\"dataframe awesome_table_class even_more_awesome_class\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-0.345352</td>\n      <td>1.314232</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.690579</td>\n      <td>0.995761</td>\n    </tr>\n  </tbody>\n</table>",
      "In [358]: url_df = pd.DataFrame(\n   .....:     {\n   .....:         \"name\": [\"Python\", \"pandas\"],\n   .....:         \"url\": [\"https://www.python.org/\", \"https://pandas.pydata.org\"],\n   .....:     }\n   .....: )\n   .....: \n\nIn [359]: html = url_df.to_html(render_links=True)\n\nIn [360]: print(html)\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>name</th>\n      <th>url</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Python</td>\n      <td><a href=\"https://www.python.org/\" target=\"_blank\">https://www.python.org/</a></td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>pandas</td>\n      <td><a href=\"https://pandas.pydata.org\" target=\"_blank\">https://pandas.pydata.org</a></td>\n    </tr>\n  </tbody>\n</table>\n\nIn [361]: display(HTML(html))\n<IPython.core.display.HTML object>",
      "In [362]: df = pd.DataFrame({\"a\": list(\"&<>\"), \"b\": np.random.randn(3)})",
      "In [363]: html = df.to_html()\n\nIn [364]: print(html)\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>a</th>\n      <th>b</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>&amp;</td>\n      <td>2.396780</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>&lt;</td>\n      <td>0.014871</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>&gt;</td>\n      <td>3.357427</td>\n    </tr>\n  </tbody>\n</table>\n\nIn [365]: display(HTML(html))\n<IPython.core.display.HTML object>",
      "In [366]: html = df.to_html(escape=False)\n\nIn [367]: print(html)\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>a</th>\n      <th>b</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>&</td>\n      <td>2.396780</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td><</td>\n      <td>0.014871</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>></td>\n      <td>3.357427</td>\n    </tr>\n  </tbody>\n</table>\n\nIn [368]: display(HTML(html))\n<IPython.core.display.HTML object>",
      "In [369]: df = pd.DataFrame([[1, 2], [3, 4]], index=[\"a\", \"b\"], columns=[\"c\", \"d\"])\n\nIn [370]: print(df.style.to_latex())\n\\begin{tabular}{lrr}\n & c & d \\\\\na & 1 & 2 \\\\\nb & 3 & 4 \\\\\n\\end{tabular}",
      "In [372]: from io import StringIO\n\nIn [373]: xml = \"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n   .....: <bookstore>\n   .....:   <book category=\"cooking\">\n   .....:     <title lang=\"en\">Everyday Italian</title>\n   .....:     <author>Giada De Laurentiis</author>\n   .....:     <year>2005</year>\n   .....:     <price>30.00</price>\n   .....:   </book>\n   .....:   <book category=\"children\">\n   .....:     <title lang=\"en\">Harry Potter</title>\n   .....:     <author>J K. Rowling</author>\n   .....:     <year>2005</year>\n   .....:     <price>29.99</price>\n   .....:   </book>\n   .....:   <book category=\"web\">\n   .....:     <title lang=\"en\">Learning XML</title>\n   .....:     <author>Erik T. Ray</author>\n   .....:     <year>2003</year>\n   .....:     <price>39.95</price>\n   .....:   </book>\n   .....: </bookstore>\"\"\"\n   .....: \n\nIn [374]: df = pd.read_xml(StringIO(xml))\n\nIn [375]: df\nOut[375]: \n   category             title               author  year  price\n0   cooking  Everyday Italian  Giada De Laurentiis  2005  30.00\n1  children      Harry Potter         J K. Rowling  2005  29.99\n2       web      Learning XML          Erik T. Ray  2003  39.95",
      "In [376]: df = pd.read_xml(\"https://www.w3schools.com/xml/books.xml\")\n\nIn [377]: df\nOut[377]: \n   category              title                  author  year  price      cover\n0   cooking   Everyday Italian     Giada De Laurentiis  2005  30.00       None\n1  children       Harry Potter            J K. Rowling  2005  29.99       None\n2       web  XQuery Kick Start  Vaidyanathan Nagarajan  2003  49.99       None\n3       web       Learning XML             Erik T. Ray  2003  39.95  paperback",
      "In [378]: file_path = \"books.xml\"\n\nIn [379]: with open(file_path, \"w\") as f:\n   .....:     f.write(xml)\n   .....: \n\nIn [380]: with open(file_path, \"r\") as f:\n   .....:     df = pd.read_xml(StringIO(f.read()))\n   .....: \n\nIn [381]: df\nOut[381]: \n   category             title               author  year  price\n0   cooking  Everyday Italian  Giada De Laurentiis  2005  30.00\n1  children      Harry Potter         J K. Rowling  2005  29.99\n2       web      Learning XML          Erik T. Ray  2003  39.95",
      "In [382]: with open(file_path, \"r\") as f:\n   .....:     sio = StringIO(f.read())\n   .....: \n\nIn [383]: df = pd.read_xml(sio)\n\nIn [384]: df\nOut[384]: \n   category             title               author  year  price\n0   cooking  Everyday Italian  Giada De Laurentiis  2005  30.00\n1  children      Harry Potter         J K. Rowling  2005  29.99\n2       web      Learning XML          Erik T. Ray  2003  39.95",
      "In [385]: with open(file_path, \"rb\") as f:\n   .....:     bio = BytesIO(f.read())\n   .....: \n\nIn [386]: df = pd.read_xml(bio)\n\nIn [387]: df\nOut[387]: \n   category             title               author  year  price\n0   cooking  Everyday Italian  Giada De Laurentiis  2005  30.00\n1  children      Harry Potter         J K. Rowling  2005  29.99\n2       web      Learning XML          Erik T. Ray  2003  39.95",
      "In [388]: df = pd.read_xml(\n   .....:     \"s3://pmc-oa-opendata/oa_comm/xml/all/PMC1236943.xml\",\n   .....:     xpath=\".//journal-meta\",\n   .....: )\n   .....: \n\nIn [389]: df\nOut[389]: \n              journal-id              journal-title       issn  publisher\n0  Cardiovasc Ultrasound  Cardiovascular Ultrasound  1476-7120        NaN",
      "In [390]: df = pd.read_xml(file_path, xpath=\"//book[year=2005]\")\n\nIn [391]: df\nOut[391]: \n   category             title               author  year  price\n0   cooking  Everyday Italian  Giada De Laurentiis  2005  30.00\n1  children      Harry Potter         J K. Rowling  2005  29.99",
      "In [392]: df = pd.read_xml(file_path, elems_only=True)\n\nIn [393]: df\nOut[393]: \n              title               author  year  price\n0  Everyday Italian  Giada De Laurentiis  2005  30.00\n1      Harry Potter         J K. Rowling  2005  29.99\n2      Learning XML          Erik T. Ray  2003  39.95",
      "In [394]: df = pd.read_xml(file_path, attrs_only=True)\n\nIn [395]: df\nOut[395]: \n   category\n0   cooking\n1  children\n2       web",
      "In [396]: xml = \"\"\"<?xml version='1.0' encoding='utf-8'?>\n   .....: <doc:data xmlns:doc=\"https://example.com\">\n   .....:   <doc:row>\n   .....:     <doc:shape>square</doc:shape>\n   .....:     <doc:degrees>360</doc:degrees>\n   .....:     <doc:sides>4.0</doc:sides>\n   .....:   </doc:row>\n   .....:   <doc:row>\n   .....:     <doc:shape>circle</doc:shape>\n   .....:     <doc:degrees>360</doc:degrees>\n   .....:     <doc:sides/>\n   .....:   </doc:row>\n   .....:   <doc:row>\n   .....:     <doc:shape>triangle</doc:shape>\n   .....:     <doc:degrees>180</doc:degrees>\n   .....:     <doc:sides>3.0</doc:sides>\n   .....:   </doc:row>\n   .....: </doc:data>\"\"\"\n   .....: \n\nIn [397]: df = pd.read_xml(StringIO(xml),\n   .....:                  xpath=\"//doc:row\",\n   .....:                  namespaces={\"doc\": \"https://example.com\"})\n   .....: \n\nIn [398]: df\nOut[398]: \n      shape  degrees  sides\n0    square      360    4.0\n1    circle      360    NaN\n2  triangle      180    3.0",
      "In [399]: xml = \"\"\"<?xml version='1.0' encoding='utf-8'?>\n   .....: <data xmlns=\"https://example.com\">\n   .....:  <row>\n   .....:    <shape>square</shape>\n   .....:    <degrees>360</degrees>\n   .....:    <sides>4.0</sides>\n   .....:  </row>\n   .....:  <row>\n   .....:    <shape>circle</shape>\n   .....:    <degrees>360</degrees>\n   .....:    <sides/>\n   .....:  </row>\n   .....:  <row>\n   .....:    <shape>triangle</shape>\n   .....:    <degrees>180</degrees>\n   .....:    <sides>3.0</sides>\n   .....:  </row>\n   .....: </data>\"\"\"\n   .....: \n\nIn [400]: df = pd.read_xml(StringIO(xml),\n   .....:                  xpath=\"//pandas:row\",\n   .....:                  namespaces={\"pandas\": \"https://example.com\"})\n   .....: \n\nIn [401]: df\nOut[401]: \n      shape  degrees  sides\n0    square      360    4.0\n1    circle      360    NaN\n2  triangle      180    3.0",
      "In [402]: xml = \"\"\"\n   .....: <data>\n   .....:   <row>\n   .....:     <shape sides=\"4\">square</shape>\n   .....:     <degrees>360</degrees>\n   .....:   </row>\n   .....:   <row>\n   .....:     <shape sides=\"0\">circle</shape>\n   .....:     <degrees>360</degrees>\n   .....:   </row>\n   .....:   <row>\n   .....:     <shape sides=\"3\">triangle</shape>\n   .....:     <degrees>180</degrees>\n   .....:   </row>\n   .....: </data>\"\"\"\n   .....: \n\nIn [403]: df = pd.read_xml(StringIO(xml), xpath=\"./row\")\n\nIn [404]: df\nOut[404]: \n      shape  degrees\n0    square      360\n1    circle      360\n2  triangle      180",
      "In [405]: xml = \"\"\"<?xml version='1.0' encoding='utf-8'?>\n   .....:  <response>\n   .....:   <row>\n   .....:     <station id=\"40850\" name=\"Library\"/>\n   .....:     <month>2020-09-01T00:00:00</month>\n   .....:     <rides>\n   .....:       <avg_weekday_rides>864.2</avg_weekday_rides>\n   .....:       <avg_saturday_rides>534</avg_saturday_rides>\n   .....:       <avg_sunday_holiday_rides>417.2</avg_sunday_holiday_rides>\n   .....:     </rides>\n   .....:   </row>\n   .....:   <row>\n   .....:     <station id=\"41700\" name=\"Washington/Wabash\"/>\n   .....:     <month>2020-09-01T00:00:00</month>\n   .....:     <rides>\n   .....:       <avg_weekday_rides>2707.4</avg_weekday_rides>\n   .....:       <avg_saturday_rides>1909.8</avg_saturday_rides>\n   .....:       <avg_sunday_holiday_rides>1438.6</avg_sunday_holiday_rides>\n   .....:     </rides>\n   .....:   </row>\n   .....:   <row>\n   .....:     <station id=\"40380\" name=\"Clark/Lake\"/>\n   .....:     <month>2020-09-01T00:00:00</month>\n   .....:     <rides>\n   .....:       <avg_weekday_rides>2949.6</avg_weekday_rides>\n   .....:       <avg_saturday_rides>1657</avg_saturday_rides>\n   .....:       <avg_sunday_holiday_rides>1453.8</avg_sunday_holiday_rides>\n   .....:     </rides>\n   .....:   </row>\n   .....:  </response>\"\"\"\n   .....: \n\nIn [406]: xsl = \"\"\"<xsl:stylesheet version=\"1.0\" xmlns:xsl=\"http://www.w3.org/1999/XSL/Transform\">\n   .....:    <xsl:output method=\"xml\" omit-xml-declaration=\"no\" indent=\"yes\"/>\n   .....:    <xsl:strip-space elements=\"*\"/>\n   .....:    <xsl:template match=\"/response\">\n   .....:       <xsl:copy>\n   .....:         <xsl:apply-templates select=\"row\"/>\n   .....:       </xsl:copy>\n   .....:    </xsl:template>\n   .....:    <xsl:template match=\"row\">\n   .....:       <xsl:copy>\n   .....:         <station_id><xsl:value-of select=\"station/@id\"/></station_id>\n   .....:         <station_name><xsl:value-of select=\"station/@name\"/></station_name>\n   .....:         <xsl:copy-of select=\"month|rides/*\"/>\n   .....:       </xsl:copy>\n   .....:    </xsl:template>\n   .....:  </xsl:stylesheet>\"\"\"\n   .....: \n\nIn [407]: output = \"\"\"<?xml version='1.0' encoding='utf-8'?>\n   .....:  <response>\n   .....:    <row>\n   .....:       <station_id>40850</station_id>\n   .....:       <station_name>Library</station_name>\n   .....:       <month>2020-09-01T00:00:00</month>\n   .....:       <avg_weekday_rides>864.2</avg_weekday_rides>\n   .....:       <avg_saturday_rides>534</avg_saturday_rides>\n   .....:       <avg_sunday_holiday_rides>417.2</avg_sunday_holiday_rides>\n   .....:    </row>\n   .....:    <row>\n   .....:       <station_id>41700</station_id>\n   .....:       <station_name>Washington/Wabash</station_name>\n   .....:       <month>2020-09-01T00:00:00</month>\n   .....:       <avg_weekday_rides>2707.4</avg_weekday_rides>\n   .....:       <avg_saturday_rides>1909.8</avg_saturday_rides>\n   .....:       <avg_sunday_holiday_rides>1438.6</avg_sunday_holiday_rides>\n   .....:    </row>\n   .....:    <row>\n   .....:       <station_id>40380</station_id>\n   .....:       <station_name>Clark/Lake</station_name>\n   .....:       <month>2020-09-01T00:00:00</month>\n   .....:       <avg_weekday_rides>2949.6</avg_weekday_rides>\n   .....:       <avg_saturday_rides>1657</avg_saturday_rides>\n   .....:       <avg_sunday_holiday_rides>1453.8</avg_sunday_holiday_rides>\n   .....:    </row>\n   .....:  </response>\"\"\"\n   .....: \n\nIn [408]: df = pd.read_xml(StringIO(xml), stylesheet=xsl)\n\nIn [409]: df\nOut[409]: \n   station_id       station_name  ... avg_saturday_rides  avg_sunday_holiday_rides\n0       40850            Library  ...              534.0                     417.2\n1       41700  Washington/Wabash  ...             1909.8                    1438.6\n2       40380         Clark/Lake  ...             1657.0                    1453.8\n\n[3 rows x 6 columns]",
      "In [1]: df = pd.read_xml(\n...         \"/path/to/downloaded/enwikisource-latest-pages-articles.xml\",\n...         iterparse = {\"page\": [\"title\", \"ns\", \"id\"]}\n...     )\n...     df\nOut[2]:\n                                                     title   ns        id\n0                                       Gettysburg Address    0     21450\n1                                                Main Page    0     42950\n2                            Declaration by United Nations    0      8435\n3             Constitution of the United States of America    0      8435\n4                     Declaration of Independence (Israel)    0     17858\n...                                                    ...  ...       ...\n3578760               Page:Black cat 1897 07 v2 n10.pdf/17  104    219649\n3578761               Page:Black cat 1897 07 v2 n10.pdf/43  104    219649\n3578762               Page:Black cat 1897 07 v2 n10.pdf/44  104    219649\n3578763      The History of Tom Jones, a Foundling/Book IX    0  12084291\n3578764  Page:Shakespeare of Stratford (1926) Yale.djvu/91  104     21450\n\n[3578765 rows x 3 columns]",
      "In [410]: geom_df = pd.DataFrame(\n   .....:     {\n   .....:         \"shape\": [\"square\", \"circle\", \"triangle\"],\n   .....:         \"degrees\": [360, 360, 180],\n   .....:         \"sides\": [4, np.nan, 3],\n   .....:     }\n   .....: )\n   .....: \n\nIn [411]: print(geom_df.to_xml())\n<?xml version='1.0' encoding='utf-8'?>\n<data>\n  <row>\n    <index>0</index>\n    <shape>square</shape>\n    <degrees>360</degrees>\n    <sides>4.0</sides>\n  </row>\n  <row>\n    <index>1</index>\n    <shape>circle</shape>\n    <degrees>360</degrees>\n    <sides/>\n  </row>\n  <row>\n    <index>2</index>\n    <shape>triangle</shape>\n    <degrees>180</degrees>\n    <sides>3.0</sides>\n  </row>\n</data>",
      "In [415]: ext_geom_df = pd.DataFrame(\n   .....:     {\n   .....:         \"type\": [\"polygon\", \"other\", \"polygon\"],\n   .....:         \"shape\": [\"square\", \"circle\", \"triangle\"],\n   .....:         \"degrees\": [360, 360, 180],\n   .....:         \"sides\": [4, np.nan, 3],\n   .....:     }\n   .....: )\n   .....: \n\nIn [416]: pvt_df = ext_geom_df.pivot_table(index='shape',\n   .....:                                  columns='type',\n   .....:                                  values=['degrees', 'sides'],\n   .....:                                  aggfunc='sum')\n   .....: \n\nIn [417]: pvt_df\nOut[417]: \n         degrees         sides        \ntype       other polygon other polygon\nshape                                 \ncircle     360.0     NaN   0.0     NaN\nsquare       NaN   360.0   NaN     4.0\ntriangle     NaN   180.0   NaN     3.0\n\nIn [418]: print(pvt_df.to_xml())\n<?xml version='1.0' encoding='utf-8'?>\n<data>\n  <row>\n    <shape>circle</shape>\n    <degrees_other>360.0</degrees_other>\n    <degrees_polygon/>\n    <sides_other>0.0</sides_other>\n    <sides_polygon/>\n  </row>\n  <row>\n    <shape>square</shape>\n    <degrees_other/>\n    <degrees_polygon>360.0</degrees_polygon>\n    <sides_other/>\n    <sides_polygon>4.0</sides_polygon>\n  </row>\n  <row>\n    <shape>triangle</shape>\n    <degrees_other/>\n    <degrees_polygon>180.0</degrees_polygon>\n    <sides_other/>\n    <sides_polygon>3.0</sides_polygon>\n  </row>\n</data>",
      "# Returns a DataFrame\npd.read_excel(\"path_to_file.xls\", sheet_name=\"Sheet1\")",
      "xlsx = pd.ExcelFile(\"path_to_file.xls\")\ndf = pd.read_excel(xlsx, \"Sheet1\")",
      "with pd.ExcelFile(\"path_to_file.xls\") as xls:\n    df1 = pd.read_excel(xls, \"Sheet1\")\n    df2 = pd.read_excel(xls, \"Sheet2\")",
      "data = {}\n# For when Sheet1's format differs from Sheet2\nwith pd.ExcelFile(\"path_to_file.xls\") as xls:\n    data[\"Sheet1\"] = pd.read_excel(xls, \"Sheet1\", index_col=None, na_values=[\"NA\"])\n    data[\"Sheet2\"] = pd.read_excel(xls, \"Sheet2\", index_col=1)",
      "# using the ExcelFile class\ndata = {}\nwith pd.ExcelFile(\"path_to_file.xls\") as xls:\n    data[\"Sheet1\"] = pd.read_excel(xls, \"Sheet1\", index_col=None, na_values=[\"NA\"])\n    data[\"Sheet2\"] = pd.read_excel(xls, \"Sheet2\", index_col=None, na_values=[\"NA\"])\n\n# equivalent using the read_excel function\ndata = pd.read_excel(\n    \"path_to_file.xls\", [\"Sheet1\", \"Sheet2\"], index_col=None, na_values=[\"NA\"]\n)",
      "import xlrd\n\nxlrd_book = xlrd.open_workbook(\"path_to_file.xls\", on_demand=True)\nwith pd.ExcelFile(xlrd_book) as xls:\n    df1 = pd.read_excel(xls, \"Sheet1\")\n    df2 = pd.read_excel(xls, \"Sheet2\")",
      "# Returns a DataFrame\npd.read_excel(\"path_to_file.xls\", \"Sheet1\", index_col=None, na_values=[\"NA\"])",
      "# Returns a DataFrame\npd.read_excel(\"path_to_file.xls\", 0, index_col=None, na_values=[\"NA\"])",
      "# Returns a DataFrame\npd.read_excel(\"path_to_file.xls\")",
      "# Returns a dictionary of DataFrames\npd.read_excel(\"path_to_file.xls\", sheet_name=None)",
      "# Returns the 1st and 4th sheet, as a dictionary of DataFrames.\npd.read_excel(\"path_to_file.xls\", sheet_name=[\"Sheet1\", 3])",
      "In [424]: df = pd.DataFrame(\n   .....:     {\"a\": [1, 2, 3, 4], \"b\": [5, 6, 7, 8]},\n   .....:     index=pd.MultiIndex.from_product([[\"a\", \"b\"], [\"c\", \"d\"]]),\n   .....: )\n   .....: \n\nIn [425]: df.to_excel(\"path_to_file.xlsx\")\n\nIn [426]: df = pd.read_excel(\"path_to_file.xlsx\", index_col=[0, 1])\n\nIn [427]: df\nOut[427]: \n     a  b\na c  1  5\n  d  2  6\nb c  3  7\n  d  4  8",
      "In [428]: df.index = df.index.set_names([\"lvl1\", \"lvl2\"])\n\nIn [429]: df.to_excel(\"path_to_file.xlsx\")\n\nIn [430]: df = pd.read_excel(\"path_to_file.xlsx\", index_col=[0, 1])\n\nIn [431]: df\nOut[431]: \n           a  b\nlvl1 lvl2      \na    c     1  5\n     d     2  6\nb    c     3  7\n     d     4  8",
      "In [432]: df.columns = pd.MultiIndex.from_product([[\"a\"], [\"b\", \"d\"]], names=[\"c1\", \"c2\"])\n\nIn [433]: df.to_excel(\"path_to_file.xlsx\")\n\nIn [434]: df = pd.read_excel(\"path_to_file.xlsx\", index_col=[0, 1], header=[0, 1])\n\nIn [435]: df\nOut[435]: \nc1         a   \nc2         b  d\nlvl1 lvl2      \na    c     1  5\n     d     2  6\nb    c     3  7\n     d     4  8",
      "pd.read_excel(\"path_to_file.xls\", \"Sheet1\", usecols=\"A,C:E\")",
      "pd.read_excel(\"path_to_file.xls\", \"Sheet1\", usecols=[0, 2, 3])",
      "pd.read_excel(\"path_to_file.xls\", \"Sheet1\", usecols=[\"foo\", \"bar\"])",
      "pd.read_excel(\"path_to_file.xls\", \"Sheet1\", usecols=lambda x: x.isalpha())",
      "pd.read_excel(\"path_to_file.xls\", \"Sheet1\", parse_dates=[\"date_strings\"])",
      "pd.read_excel(\"path_to_file.xls\", \"Sheet1\", converters={\"MyBools\": bool})",
      "def cfun(x):\n    return int(x) if x else -1\n\n\npd.read_excel(\"path_to_file.xls\", \"Sheet1\", converters={\"MyInts\": cfun})",
      "pd.read_excel(\"path_to_file.xls\", dtype={\"MyInts\": \"int64\", \"MyText\": str})",
      "with pd.ExcelWriter(\"path_to_file.xlsx\") as writer:\n    df1.to_excel(writer, sheet_name=\"Sheet1\")\n    df2.to_excel(writer, sheet_name=\"Sheet2\")",
      "from io import BytesIO\n\nbio = BytesIO()\n\n# By setting the 'engine' in the ExcelWriter constructor.\nwriter = pd.ExcelWriter(bio, engine=\"xlsxwriter\")\ndf.to_excel(writer, sheet_name=\"Sheet1\")\n\n# Save the workbook\nwriter.save()\n\n# Seek to the beginning and read to copy the workbook to a variable in memory\nbio.seek(0)\nworkbook = bio.read()",
      "# By setting the 'engine' in the DataFrame 'to_excel()' methods.\ndf.to_excel(\"path_to_file.xlsx\", sheet_name=\"Sheet1\", engine=\"xlsxwriter\")\n\n# By setting the 'engine' in the ExcelWriter constructor.\nwriter = pd.ExcelWriter(\"path_to_file.xlsx\", engine=\"xlsxwriter\")\n\n# Or via pandas configuration.\nfrom pandas import options  # noqa: E402\n\noptions.io.excel.xlsx.writer = \"xlsxwriter\"\n\ndf.to_excel(\"path_to_file.xlsx\", sheet_name=\"Sheet1\")",
      "# Returns a DataFrame\npd.read_excel(\"path_to_file.ods\", engine=\"odf\")",
      "# Returns a DataFrame\npd.read_excel(\"path_to_file.xlsb\", engine=\"pyxlsb\")",
      "# Returns a DataFrame\npd.read_excel(\"path_to_file.xlsb\", engine=\"calamine\")",
      ">>> clipdf = pd.read_clipboard()\n>>> clipdf\n  A B C\nx 1 4 p\ny 2 5 q\nz 3 6 r",
      ">>> df = pd.DataFrame(\n...     {\"A\": [1, 2, 3], \"B\": [4, 5, 6], \"C\": [\"p\", \"q\", \"r\"]}, index=[\"x\", \"y\", \"z\"]\n... )\n\n>>> df\n  A B C\nx 1 4 p\ny 2 5 q\nz 3 6 r\n>>> df.to_clipboard()\n>>> pd.read_clipboard()\n  A B C\nx 1 4 p\ny 2 5 q\nz 3 6 r",
      "In [438]: pd.read_pickle(\"foo.pkl\")\nOut[438]: \nc1         a   \nc2         b  d\nlvl1 lvl2      \na    c     1  5\n     d     2  6\nb    c     3  7\n     d     4  8",
      "In [439]: df = pd.DataFrame(\n   .....:     {\n   .....:         \"A\": np.random.randn(1000),\n   .....:         \"B\": \"foo\",\n   .....:         \"C\": pd.date_range(\"20130101\", periods=1000, freq=\"s\"),\n   .....:     }\n   .....: )\n   .....: \n\nIn [440]: df\nOut[440]: \n            A    B                   C\n0   -0.317441  foo 2013-01-01 00:00:00\n1   -1.236269  foo 2013-01-01 00:00:01\n2    0.896171  foo 2013-01-01 00:00:02\n3   -0.487602  foo 2013-01-01 00:00:03\n4   -0.082240  foo 2013-01-01 00:00:04\n..        ...  ...                 ...\n995 -0.171092  foo 2013-01-01 00:16:35\n996  1.786173  foo 2013-01-01 00:16:36\n997 -0.575189  foo 2013-01-01 00:16:37\n998  0.820750  foo 2013-01-01 00:16:38\n999 -1.256530  foo 2013-01-01 00:16:39\n\n[1000 rows x 3 columns]",
      "In [441]: df.to_pickle(\"data.pkl.compress\", compression=\"gzip\")\n\nIn [442]: rt = pd.read_pickle(\"data.pkl.compress\", compression=\"gzip\")\n\nIn [443]: rt\nOut[443]: \n            A    B                   C\n0   -0.317441  foo 2013-01-01 00:00:00\n1   -1.236269  foo 2013-01-01 00:00:01\n2    0.896171  foo 2013-01-01 00:00:02\n3   -0.487602  foo 2013-01-01 00:00:03\n4   -0.082240  foo 2013-01-01 00:00:04\n..        ...  ...                 ...\n995 -0.171092  foo 2013-01-01 00:16:35\n996  1.786173  foo 2013-01-01 00:16:36\n997 -0.575189  foo 2013-01-01 00:16:37\n998  0.820750  foo 2013-01-01 00:16:38\n999 -1.256530  foo 2013-01-01 00:16:39\n\n[1000 rows x 3 columns]",
      "In [444]: df.to_pickle(\"data.pkl.xz\", compression=\"infer\")\n\nIn [445]: rt = pd.read_pickle(\"data.pkl.xz\", compression=\"infer\")\n\nIn [446]: rt\nOut[446]: \n            A    B                   C\n0   -0.317441  foo 2013-01-01 00:00:00\n1   -1.236269  foo 2013-01-01 00:00:01\n2    0.896171  foo 2013-01-01 00:00:02\n3   -0.487602  foo 2013-01-01 00:00:03\n4   -0.082240  foo 2013-01-01 00:00:04\n..        ...  ...                 ...\n995 -0.171092  foo 2013-01-01 00:16:35\n996  1.786173  foo 2013-01-01 00:16:36\n997 -0.575189  foo 2013-01-01 00:16:37\n998  0.820750  foo 2013-01-01 00:16:38\n999 -1.256530  foo 2013-01-01 00:16:39\n\n[1000 rows x 3 columns]",
      "In [447]: df.to_pickle(\"data.pkl.gz\")\n\nIn [448]: rt = pd.read_pickle(\"data.pkl.gz\")\n\nIn [449]: rt\nOut[449]: \n            A    B                   C\n0   -0.317441  foo 2013-01-01 00:00:00\n1   -1.236269  foo 2013-01-01 00:00:01\n2    0.896171  foo 2013-01-01 00:00:02\n3   -0.487602  foo 2013-01-01 00:00:03\n4   -0.082240  foo 2013-01-01 00:00:04\n..        ...  ...                 ...\n995 -0.171092  foo 2013-01-01 00:16:35\n996  1.786173  foo 2013-01-01 00:16:36\n997 -0.575189  foo 2013-01-01 00:16:37\n998  0.820750  foo 2013-01-01 00:16:38\n999 -1.256530  foo 2013-01-01 00:16:39\n\n[1000 rows x 3 columns]\n\nIn [450]: df[\"A\"].to_pickle(\"s1.pkl.bz2\")\n\nIn [451]: rt = pd.read_pickle(\"s1.pkl.bz2\")\n\nIn [452]: rt\nOut[452]: \n0     -0.317441\n1     -1.236269\n2      0.896171\n3     -0.487602\n4     -0.082240\n         ...   \n995   -0.171092\n996    1.786173\n997   -0.575189\n998    0.820750\n999   -1.256530\nName: A, Length: 1000, dtype: float64",
      "In [454]: store = pd.HDFStore(\"store.h5\")\n\nIn [455]: print(store)\n<class 'pandas.io.pytables.HDFStore'>\nFile path: store.h5",
      "In [456]: index = pd.date_range(\"1/1/2000\", periods=8)\n\nIn [457]: s = pd.Series(np.random.randn(5), index=[\"a\", \"b\", \"c\", \"d\", \"e\"])\n\nIn [458]: df = pd.DataFrame(np.random.randn(8, 3), index=index, columns=[\"A\", \"B\", \"C\"])\n\n# store.put('s', s) is an equivalent method\nIn [459]: store[\"s\"] = s\n\nIn [460]: store[\"df\"] = df\n\nIn [461]: store\nOut[461]: \n<class 'pandas.io.pytables.HDFStore'>\nFile path: store.h5",
      "# store.remove('df') is an equivalent method\nIn [464]: del store[\"df\"]\n\nIn [465]: store\nOut[465]: \n<class 'pandas.io.pytables.HDFStore'>\nFile path: store.h5",
      "In [466]: store.close()\n\nIn [467]: store\nOut[467]: \n<class 'pandas.io.pytables.HDFStore'>\nFile path: store.h5\n\nIn [468]: store.is_open\nOut[468]: False\n\n# Working with, and automatically closing the store using a context manager\nIn [469]: with pd.HDFStore(\"store.h5\") as store:\n   .....:     store.keys()\n   .....:",
      "In [470]: df_tl = pd.DataFrame({\"A\": list(range(5)), \"B\": list(range(5))})\n\nIn [471]: df_tl.to_hdf(\"store_tl.h5\", key=\"table\", append=True)\n\nIn [472]: pd.read_hdf(\"store_tl.h5\", \"table\", where=[\"index>2\"])\nOut[472]: \n   A  B\n3  3  3\n4  4  4",
      "In [473]: df_with_missing = pd.DataFrame(\n   .....:     {\n   .....:         \"col1\": [0, np.nan, 2],\n   .....:         \"col2\": [1, np.nan, np.nan],\n   .....:     }\n   .....: )\n   .....: \n\nIn [474]: df_with_missing\nOut[474]: \n   col1  col2\n0   0.0   1.0\n1   NaN   NaN\n2   2.0   NaN\n\nIn [475]: df_with_missing.to_hdf(\"file.h5\", key=\"df_with_missing\", format=\"table\", mode=\"w\")\n\nIn [476]: pd.read_hdf(\"file.h5\", \"df_with_missing\")\nOut[476]: \n   col1  col2\n0   0.0   1.0\n1   NaN   NaN\n2   2.0   NaN\n\nIn [477]: df_with_missing.to_hdf(\n   .....:     \"file.h5\", key=\"df_with_missing\", format=\"table\", mode=\"w\", dropna=True\n   .....: )\n   .....: \n\nIn [478]: pd.read_hdf(\"file.h5\", \"df_with_missing\")\nOut[478]: \n   col1  col2\n0   0.0   1.0\n2   2.0   NaN",
      "In [479]: pd.DataFrame(np.random.randn(10, 2)).to_hdf(\"test_fixed.h5\", key=\"df\")\n\nIn [480]: pd.read_hdf(\"test_fixed.h5\", \"df\", where=\"index>5\")\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[480], line 1\n----> 1 pd.read_hdf(\"test_fixed.h5\", \"df\", where=\"index>5\")\n\nFile ~/work/pandas/pandas/pandas/io/pytables.py:452, in read_hdf(path_or_buf, key, mode, errors, where, start, stop, columns, iterator, chunksize, **kwargs)\n    447                 raise ValueError(\n    448                     \"key must be provided when HDF5 \"\n    449                     \"file contains multiple datasets.\"\n    450                 )\n    451         key = candidate_only_group._v_pathname\n--> 452     return store.select(\n    453         key,\n    454         where=where,\n    455         start=start,\n    456         stop=stop,\n    457         columns=columns,\n    458         iterator=iterator,\n    459         chunksize=chunksize,\n    460         auto_close=auto_close,\n    461     )\n    462 except (ValueError, TypeError, LookupError):\n    463     if not isinstance(path_or_buf, HDFStore):\n    464         # if there is an error, close the store if we opened it.\n\nFile ~/work/pandas/pandas/pandas/io/pytables.py:906, in HDFStore.select(self, key, where, start, stop, columns, iterator, chunksize, auto_close)\n    892 # create the iterator\n    893 it = TableIterator(\n    894     self,\n    895     s,\n   (...)\n    903     auto_close=auto_close,\n    904 )\n--> 906 return it.get_result()\n\nFile ~/work/pandas/pandas/pandas/io/pytables.py:2029, in TableIterator.get_result(self, coordinates)\n   2026     where = self.where\n   2028 # directly return the result\n-> 2029 results = self.func(self.start, self.stop, where)\n   2030 self.close()\n   2031 return results\n\nFile ~/work/pandas/pandas/pandas/io/pytables.py:890, in HDFStore.select.<locals>.func(_start, _stop, _where)\n    889 def func(_start, _stop, _where):\n--> 890     return s.read(start=_start, stop=_stop, where=_where, columns=columns)\n\nFile ~/work/pandas/pandas/pandas/io/pytables.py:3278, in BlockManagerFixed.read(self, where, columns, start, stop)\n   3270 def read(\n   3271     self,\n   3272     where=None,\n   (...)\n   3276 ) -> DataFrame:\n   3277     # start, stop applied to rows, so 0th axis only\n-> 3278     self.validate_read(columns, where)\n   3279     select_axis = self.obj_type()._get_block_manager_axis(0)\n   3281     axes = []\n\nFile ~/work/pandas/pandas/pandas/io/pytables.py:2922, in GenericFixed.validate_read(self, columns, where)\n   2917     raise TypeError(\n   2918         \"cannot pass a column specification when reading \"\n   2919         \"a Fixed format store. this store must be selected in its entirety\"\n   2920     )\n   2921 if where is not None:\n-> 2922     raise TypeError(\n   2923         \"cannot pass a where specification when reading \"\n   2924         \"from a Fixed format store. this store must be selected in its entirety\"\n   2925     )\n\nTypeError: cannot pass a where specification when reading from a Fixed format store. this store must be selected in its entirety",
      "In [481]: store = pd.HDFStore(\"store.h5\")\n\nIn [482]: df1 = df[0:4]\n\nIn [483]: df2 = df[4:]\n\n# append data (creates a table automatically)\nIn [484]: store.append(\"df\", df1)\n\nIn [485]: store.append(\"df\", df2)\n\nIn [486]: store\nOut[486]: \n<class 'pandas.io.pytables.HDFStore'>\nFile path: store.h5\n\n# select the entire object\nIn [487]: store.select(\"df\")\nOut[487]: \n                   A         B         C\n2000-01-01  0.858644 -0.851236  1.058006\n2000-01-02 -0.080372 -1.268121  1.561967\n2000-01-03  0.816983  1.965656 -1.169408\n2000-01-04  0.712795 -0.062433  0.736755\n2000-01-05 -0.298721 -1.988045  1.475308\n2000-01-06  1.103675  1.382242 -0.650762\n2000-01-07 -0.729161 -0.142928 -1.063038\n2000-01-08 -1.005977  0.465222 -0.094517\n\n# the type of stored data\nIn [488]: store.root.df._v_attrs.pandas_type\nOut[488]: 'frame_table'",
      "In [489]: store.put(\"foo/bar/bah\", df)\n\nIn [490]: store.append(\"food/orange\", df)\n\nIn [491]: store.append(\"food/apple\", df)\n\nIn [492]: store\nOut[492]: \n<class 'pandas.io.pytables.HDFStore'>\nFile path: store.h5\n\n# a list of keys are returned\nIn [493]: store.keys()\nOut[493]: ['/df', '/food/apple', '/food/orange', '/foo/bar/bah']\n\n# remove all nodes under this level\nIn [494]: store.remove(\"food\")\n\nIn [495]: store\nOut[495]: \n<class 'pandas.io.pytables.HDFStore'>\nFile path: store.h5",
      "In [497]: store.foo.bar.bah\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[497], line 1\n----> 1 store.foo.bar.bah\n\nFile ~/work/pandas/pandas/pandas/io/pytables.py:613, in HDFStore.__getattr__(self, name)\n    611 \"\"\"allow attribute access to get stores\"\"\"\n    612 try:\n--> 613     return self.get(name)\n    614 except (KeyError, ClosedFileError):\n    615     pass\n\nFile ~/work/pandas/pandas/pandas/io/pytables.py:813, in HDFStore.get(self, key)\n    811 if group is None:\n    812     raise KeyError(f\"No object named {key} in the file\")\n--> 813 return self._read_group(group)\n\nFile ~/work/pandas/pandas/pandas/io/pytables.py:1878, in HDFStore._read_group(self, group)\n   1877 def _read_group(self, group: Node):\n-> 1878     s = self._create_storer(group)\n   1879     s.infer_axes()\n   1880     return s.read()\n\nFile ~/work/pandas/pandas/pandas/io/pytables.py:1752, in HDFStore._create_storer(self, group, format, value, encoding, errors)\n   1750         tt = \"generic_table\"\n   1751     else:\n-> 1752         raise TypeError(\n   1753             \"cannot create a storer if the object is not existing \"\n   1754             \"nor a value are passed\"\n   1755         )\n   1756 else:\n   1757     if isinstance(value, Series):\n\nTypeError: cannot create a storer if the object is not existing nor a value are passed",
      "In [500]: df_mixed = pd.DataFrame(\n   .....:     {\n   .....:         \"A\": np.random.randn(8),\n   .....:         \"B\": np.random.randn(8),\n   .....:         \"C\": np.array(np.random.randn(8), dtype=\"float32\"),\n   .....:         \"string\": \"string\",\n   .....:         \"int\": 1,\n   .....:         \"bool\": True,\n   .....:         \"datetime64\": pd.Timestamp(\"20010102\"),\n   .....:     },\n   .....:     index=list(range(8)),\n   .....: )\n   .....: \n\nIn [501]: df_mixed.loc[df_mixed.index[3:5], [\"A\", \"B\", \"string\", \"datetime64\"]] = np.nan\n\nIn [502]: store.append(\"df_mixed\", df_mixed, min_itemsize={\"values\": 50})\n\nIn [503]: df_mixed1 = store.select(\"df_mixed\")\n\nIn [504]: df_mixed1\nOut[504]: \n          A         B         C  ... int  bool                    datetime64\n0  0.013747 -1.166078 -1.292080  ...   1  True 1970-01-01 00:00:00.978393600\n1 -0.712009  0.247572  1.526911  ...   1  True 1970-01-01 00:00:00.978393600\n2 -0.645096  1.687406  0.288504  ...   1  True 1970-01-01 00:00:00.978393600\n3       NaN       NaN  0.097771  ...   1  True                           NaT\n4       NaN       NaN  1.536408  ...   1  True                           NaT\n5 -0.023202  0.043702  0.926790  ...   1  True 1970-01-01 00:00:00.978393600\n6  2.359782  0.088224 -0.676448  ...   1  True 1970-01-01 00:00:00.978393600\n7 -0.143428 -0.813360 -0.179724  ...   1  True 1970-01-01 00:00:00.978393600\n\n[8 rows x 7 columns]\n\nIn [505]: df_mixed1.dtypes.value_counts()\nOut[505]: \nfloat64           2\nfloat32           1\nobject            1\nint64             1\nbool              1\ndatetime64[ns]    1\nName: count, dtype: int64\n\n# we have provided a minimum string column size\nIn [506]: store.root.df_mixed.table\nOut[506]: \n/df_mixed/table (Table(8,)) ''\n  description := {\n  \"index\": Int64Col(shape=(), dflt=0, pos=0),\n  \"values_block_0\": Float64Col(shape=(2,), dflt=0.0, pos=1),\n  \"values_block_1\": Float32Col(shape=(1,), dflt=0.0, pos=2),\n  \"values_block_2\": StringCol(itemsize=50, shape=(1,), dflt=b'', pos=3),\n  \"values_block_3\": Int64Col(shape=(1,), dflt=0, pos=4),\n  \"values_block_4\": BoolCol(shape=(1,), dflt=False, pos=5),\n  \"values_block_5\": Int64Col(shape=(1,), dflt=0, pos=6)}\n  byteorder := 'little'\n  chunkshape := (689,)\n  autoindex := True\n  colindexes := {\n    \"index\": Index(6, mediumshuffle, zlib(1)).is_csi=False}",
      "In [507]: index = pd.MultiIndex(\n   .....:    levels=[[\"foo\", \"bar\", \"baz\", \"qux\"], [\"one\", \"two\", \"three\"]],\n   .....:    codes=[[0, 0, 0, 1, 1, 2, 2, 3, 3, 3], [0, 1, 2, 0, 1, 1, 2, 0, 1, 2]],\n   .....:    names=[\"foo\", \"bar\"],\n   .....: )\n   .....: \n\nIn [508]: df_mi = pd.DataFrame(np.random.randn(10, 3), index=index, columns=[\"A\", \"B\", \"C\"])\n\nIn [509]: df_mi\nOut[509]: \n                  A         B         C\nfoo bar                                \nfoo one   -1.303456 -0.642994 -0.649456\n    two    1.012694  0.414147  1.950460\n    three  1.094544 -0.802899 -0.583343\nbar one    0.410395  0.618321  0.560398\n    two    1.434027 -0.033270  0.343197\nbaz two   -1.646063 -0.695847 -0.429156\n    three -0.244688 -1.428229 -0.138691\nqux one    1.866184 -1.446617  0.036660\n    two   -1.660522  0.929553 -1.298649\n    three  3.565769  0.682402  1.041927\n\nIn [510]: store.append(\"df_mi\", df_mi)\n\nIn [511]: store.select(\"df_mi\")\nOut[511]: \n                  A         B         C\nfoo bar                                \nfoo one   -1.303456 -0.642994 -0.649456\n    two    1.012694  0.414147  1.950460\n    three  1.094544 -0.802899 -0.583343\nbar one    0.410395  0.618321  0.560398\n    two    1.434027 -0.033270  0.343197\nbaz two   -1.646063 -0.695847 -0.429156\n    three -0.244688 -1.428229 -0.138691\nqux one    1.866184 -1.446617  0.036660\n    two   -1.660522  0.929553 -1.298649\n    three  3.565769  0.682402  1.041927\n\n# the levels are automatically included as data columns\nIn [512]: store.select(\"df_mi\", \"foo=bar\")\nOut[512]: \n                A         B         C\nfoo bar                              \nbar one  0.410395  0.618321  0.560398\n    two  1.434027 -0.033270  0.343197",
      "In [513]: dfq = pd.DataFrame(\n   .....:     np.random.randn(10, 4),\n   .....:     columns=list(\"ABCD\"),\n   .....:     index=pd.date_range(\"20130101\", periods=10),\n   .....: )\n   .....: \n\nIn [514]: store.append(\"dfq\", dfq, format=\"table\", data_columns=True)",
      "In [515]: store.select(\"dfq\", \"index>pd.Timestamp('20130104') & columns=['A', 'B']\")\nOut[515]: \n                   A         B\n2013-01-05 -0.830545 -0.457071\n2013-01-06  0.431186  1.049421\n2013-01-07  0.617509 -0.811230\n2013-01-08  0.947422 -0.671233\n2013-01-09 -0.183798 -1.211230\n2013-01-10  0.361428  0.887304",
      "In [518]: from datetime import timedelta\n\nIn [519]: dftd = pd.DataFrame(\n   .....:     {\n   .....:         \"A\": pd.Timestamp(\"20130101\"),\n   .....:         \"B\": [\n   .....:             pd.Timestamp(\"20130101\") + timedelta(days=i, seconds=10)\n   .....:             for i in range(10)\n   .....:         ],\n   .....:     }\n   .....: )\n   .....: \n\nIn [520]: dftd[\"C\"] = dftd[\"A\"] - dftd[\"B\"]\n\nIn [521]: dftd\nOut[521]: \n           A                   B                  C\n0 2013-01-01 2013-01-01 00:00:10  -1 days +23:59:50\n1 2013-01-01 2013-01-02 00:00:10  -2 days +23:59:50\n2 2013-01-01 2013-01-03 00:00:10  -3 days +23:59:50\n3 2013-01-01 2013-01-04 00:00:10  -4 days +23:59:50\n4 2013-01-01 2013-01-05 00:00:10  -5 days +23:59:50\n5 2013-01-01 2013-01-06 00:00:10  -6 days +23:59:50\n6 2013-01-01 2013-01-07 00:00:10  -7 days +23:59:50\n7 2013-01-01 2013-01-08 00:00:10  -8 days +23:59:50\n8 2013-01-01 2013-01-09 00:00:10  -9 days +23:59:50\n9 2013-01-01 2013-01-10 00:00:10 -10 days +23:59:50\n\nIn [522]: store.append(\"dftd\", dftd, data_columns=True)\n\nIn [523]: store.select(\"dftd\", \"C<'-3.5D'\")\nOut[523]: \n                              A                   B                  C\n4 1970-01-01 00:00:01.356998400 2013-01-05 00:00:10  -5 days +23:59:50\n5 1970-01-01 00:00:01.356998400 2013-01-06 00:00:10  -6 days +23:59:50\n6 1970-01-01 00:00:01.356998400 2013-01-07 00:00:10  -7 days +23:59:50\n7 1970-01-01 00:00:01.356998400 2013-01-08 00:00:10  -8 days +23:59:50\n8 1970-01-01 00:00:01.356998400 2013-01-09 00:00:10  -9 days +23:59:50\n9 1970-01-01 00:00:01.356998400 2013-01-10 00:00:10 -10 days +23:59:50",
      "In [526]: index = pd.MultiIndex(\n   .....:     levels=[[\"foo\", \"bar\", \"baz\", \"qux\"], [\"one\", \"two\", \"three\"]],\n   .....:     codes=[[0, 0, 0, 1, 1, 2, 2, 3, 3, 3], [0, 1, 2, 0, 1, 1, 2, 0, 1, 2]],\n   .....: )\n   .....: \n\nIn [527]: df_mi_2 = pd.DataFrame(np.random.randn(10, 3), index=index, columns=[\"A\", \"B\", \"C\"])\n\nIn [528]: df_mi_2\nOut[528]: \n                  A         B         C\nfoo one   -0.219582  1.186860 -1.437189\n    two    0.053768  1.872644 -1.469813\n    three -0.564201  0.876341  0.407749\nbar one   -0.232583  0.179812  0.922152\n    two   -1.820952 -0.641360  2.133239\nbaz two   -0.941248 -0.136307 -1.271305\n    three -0.099774 -0.061438 -0.845172\nqux one    0.465793  0.756995 -0.541690\n    two   -0.802241  0.877657 -2.553831\n    three  0.094899 -2.319519  0.293601\n\nIn [529]: store.append(\"df_mi_2\", df_mi_2)\n\n# the levels are automatically included as data columns with keyword level_n\nIn [530]: store.select(\"df_mi_2\", \"level_0=foo and level_1=two\")\nOut[530]: \n                A         B         C\nfoo two  0.053768  1.872644 -1.469813",
      "In [536]: df_1 = pd.DataFrame(np.random.randn(10, 2), columns=list(\"AB\"))\n\nIn [537]: df_2 = pd.DataFrame(np.random.randn(10, 2), columns=list(\"AB\"))\n\nIn [538]: st = pd.HDFStore(\"appends.h5\", mode=\"w\")\n\nIn [539]: st.append(\"df\", df_1, data_columns=[\"B\"], index=False)\n\nIn [540]: st.append(\"df\", df_2, data_columns=[\"B\"], index=False)\n\nIn [541]: st.get_storer(\"df\").table\nOut[541]: \n/df/table (Table(20,)) ''\n  description := {\n  \"index\": Int64Col(shape=(), dflt=0, pos=0),\n  \"values_block_0\": Float64Col(shape=(1,), dflt=0.0, pos=1),\n  \"B\": Float64Col(shape=(), dflt=0.0, pos=2)}\n  byteorder := 'little'\n  chunkshape := (2730,)",
      "for df in pd.read_hdf(\"store.h5\", \"df\", chunksize=3):\n    print(df)",
      "In [558]: dfeq = pd.DataFrame({\"number\": np.arange(1, 11)})\n\nIn [559]: dfeq\nOut[559]: \n   number\n0       1\n1       2\n2       3\n3       4\n4       5\n5       6\n6       7\n7       8\n8       9\n9      10\n\nIn [560]: store.append(\"dfeq\", dfeq, data_columns=[\"number\"])\n\nIn [561]: def chunks(l, n):\n   .....:     return [l[i: i + n] for i in range(0, len(l), n)]\n   .....: \n\nIn [562]: evens = [2, 4, 6, 8, 10]\n\nIn [563]: coordinates = store.select_as_coordinates(\"dfeq\", \"number=evens\")\n\nIn [564]: for c in chunks(coordinates, 2):\n   .....:     print(store.select(\"dfeq\", where=c))\n   .....: \n   number\n1       2\n3       4\n   number\n5       6\n7       8\n   number\n9      10",
      "In [567]: df_coord = pd.DataFrame(\n   .....:     np.random.randn(1000, 2), index=pd.date_range(\"20000101\", periods=1000)\n   .....: )\n   .....: \n\nIn [568]: store.append(\"df_coord\", df_coord)\n\nIn [569]: c = store.select_as_coordinates(\"df_coord\", \"index > 20020101\")\n\nIn [570]: c\nOut[570]: \nIndex([732, 733, 734, 735, 736, 737, 738, 739, 740, 741,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', length=268)\n\nIn [571]: store.select(\"df_coord\", where=c)\nOut[571]: \n                   0         1\n2002-01-02  0.007717  1.168386\n2002-01-03  0.759328 -0.638934\n2002-01-04 -1.154018 -0.324071\n2002-01-05 -0.804551 -1.280593\n2002-01-06 -0.047208  1.260503\n...              ...       ...\n2002-09-22 -1.139583  0.344316\n2002-09-23 -0.760643 -1.306704\n2002-09-24  0.059018  1.775482\n2002-09-25  1.242255 -0.055457\n2002-09-26  0.410317  2.194489\n\n[268 rows x 2 columns]",
      "In [572]: df_mask = pd.DataFrame(\n   .....:     np.random.randn(1000, 2), index=pd.date_range(\"20000101\", periods=1000)\n   .....: )\n   .....: \n\nIn [573]: store.append(\"df_mask\", df_mask)\n\nIn [574]: c = store.select_column(\"df_mask\", \"index\")\n\nIn [575]: where = c[pd.DatetimeIndex(c).month == 5].index\n\nIn [576]: store.select(\"df_mask\", where=where)\nOut[576]: \n                   0         1\n2000-05-01  1.479511  0.516433\n2000-05-02 -0.334984 -1.493537\n2000-05-03  0.900321  0.049695\n2000-05-04  0.614266 -1.077151\n2000-05-05  0.233881  0.493246\n...              ...       ...\n2002-05-27  0.294122  0.457407\n2002-05-28 -1.102535  1.215650\n2002-05-29 -0.432911  0.753606\n2002-05-30 -1.105212  2.311877\n2002-05-31  2.567296  2.610691\n\n[93 rows x 2 columns]",
      "In [578]: df_mt = pd.DataFrame(\n   .....:     np.random.randn(8, 6),\n   .....:     index=pd.date_range(\"1/1/2000\", periods=8),\n   .....:     columns=[\"A\", \"B\", \"C\", \"D\", \"E\", \"F\"],\n   .....: )\n   .....: \n\nIn [579]: df_mt[\"foo\"] = \"bar\"\n\nIn [580]: df_mt.loc[df_mt.index[1], (\"A\", \"B\")] = np.nan\n\n# you can also create the tables individually\nIn [581]: store.append_to_multiple(\n   .....:     {\"df1_mt\": [\"A\", \"B\"], \"df2_mt\": None}, df_mt, selector=\"df1_mt\"\n   .....: )\n   .....: \n\nIn [582]: store\nOut[582]: \n<class 'pandas.io.pytables.HDFStore'>\nFile path: store.h5\n\n# individual tables were created\nIn [583]: store.select(\"df1_mt\")\nOut[583]: \n                   A         B\n2000-01-01  0.162291 -0.430489\n2000-01-02       NaN       NaN\n2000-01-03  0.429207 -1.099274\n2000-01-04  1.869081 -1.466039\n2000-01-05  0.092130 -1.726280\n2000-01-06  0.266901 -0.036854\n2000-01-07 -0.517871 -0.990317\n2000-01-08 -0.231342  0.557402\n\nIn [584]: store.select(\"df2_mt\")\nOut[584]: \n                   C         D         E         F  foo\n2000-01-01 -2.502042  0.668149  0.460708  1.834518  bar\n2000-01-02  0.130441 -0.608465  0.439872  0.506364  bar\n2000-01-03 -1.069546  1.236277  0.116634 -1.772519  bar\n2000-01-04  0.137462  0.313939  0.748471 -0.943009  bar\n2000-01-05  0.836517  2.049798  0.562167  0.189952  bar\n2000-01-06  1.112750 -0.151596  1.503311  0.939470  bar\n2000-01-07 -0.294348  0.335844 -0.794159  1.495614  bar\n2000-01-08  0.860312 -0.538674 -0.541986 -1.759606  bar\n\n# as a multiple\nIn [585]: store.select_as_multiple(\n   .....:     [\"df1_mt\", \"df2_mt\"],\n   .....:     where=[\"A>0\", \"B>0\"],\n   .....:     selector=\"df1_mt\",\n   .....: )\n   .....: \nOut[585]: \nEmpty DataFrame\nColumns: [A, B, C, D, E, F, foo]\nIndex: []",
      "store_compressed = pd.HDFStore(\n    \"store_compressed.h5\", complevel=9, complib=\"blosc:blosclz\"\n)",
      "In [586]: dfcat = pd.DataFrame(\n   .....:     {\"A\": pd.Series(list(\"aabbcdba\")).astype(\"category\"), \"B\": np.random.randn(8)}\n   .....: )\n   .....: \n\nIn [587]: dfcat\nOut[587]: \n   A         B\n0  a -1.520478\n1  a -1.069391\n2  b -0.551981\n3  b  0.452407\n4  c  0.409257\n5  d  0.301911\n6  b -0.640843\n7  a -2.253022\n\nIn [588]: dfcat.dtypes\nOut[588]: \nA    category\nB     float64\ndtype: object\n\nIn [589]: cstore = pd.HDFStore(\"cats.h5\", mode=\"w\")\n\nIn [590]: cstore.append(\"dfcat\", dfcat, format=\"table\", data_columns=[\"A\"])\n\nIn [591]: result = cstore.select(\"dfcat\", where=\"A in ['b', 'c']\")\n\nIn [592]: result\nOut[592]: \n   A         B\n2  b -0.551981\n3  b  0.452407\n4  c  0.409257\n6  b -0.640843\n\nIn [593]: result.dtypes\nOut[593]: \nA    category\nB     float64\ndtype: object",
      "In [594]: dfs = pd.DataFrame({\"A\": \"foo\", \"B\": \"bar\"}, index=list(range(5)))\n\nIn [595]: dfs\nOut[595]: \n     A    B\n0  foo  bar\n1  foo  bar\n2  foo  bar\n3  foo  bar\n4  foo  bar\n\n# A and B have a size of 30\nIn [596]: store.append(\"dfs\", dfs, min_itemsize=30)\n\nIn [597]: store.get_storer(\"dfs\").table\nOut[597]: \n/dfs/table (Table(5,)) ''\n  description := {\n  \"index\": Int64Col(shape=(), dflt=0, pos=0),\n  \"values_block_0\": StringCol(itemsize=30, shape=(2,), dflt=b'', pos=1)}\n  byteorder := 'little'\n  chunkshape := (963,)\n  autoindex := True\n  colindexes := {\n    \"index\": Index(6, mediumshuffle, zlib(1)).is_csi=False}\n\n# A is created as a data_column with a size of 30\n# B is size is calculated\nIn [598]: store.append(\"dfs2\", dfs, min_itemsize={\"A\": 30})\n\nIn [599]: store.get_storer(\"dfs2\").table\nOut[599]: \n/dfs2/table (Table(5,)) ''\n  description := {\n  \"index\": Int64Col(shape=(), dflt=0, pos=0),\n  \"values_block_0\": StringCol(itemsize=3, shape=(1,), dflt=b'', pos=1),\n  \"A\": StringCol(itemsize=30, shape=(), dflt=b'', pos=2)}\n  byteorder := 'little'\n  chunkshape := (1598,)\n  autoindex := True\n  colindexes := {\n    \"index\": Index(6, mediumshuffle, zlib(1)).is_csi=False,\n    \"A\": Index(6, mediumshuffle, zlib(1)).is_csi=False}",
      "In [600]: dfss = pd.DataFrame({\"A\": [\"foo\", \"bar\", \"nan\"]})\n\nIn [601]: dfss\nOut[601]: \n     A\n0  foo\n1  bar\n2  nan\n\nIn [602]: store.append(\"dfss\", dfss)\n\nIn [603]: store.select(\"dfss\")\nOut[603]: \n     A\n0  foo\n1  bar\n2  NaN\n\n# here you need to specify a different nan rep\nIn [604]: store.append(\"dfss2\", dfss, nan_rep=\"_nan_\")\n\nIn [605]: store.select(\"dfss2\")\nOut[605]: \n     A\n0  foo\n1  bar\n2  nan",
      "In [606]: df = pd.DataFrame(\n   .....:     {\n   .....:         \"a\": list(\"abc\"),\n   .....:         \"b\": list(range(1, 4)),\n   .....:         \"c\": np.arange(3, 6).astype(\"u1\"),\n   .....:         \"d\": np.arange(4.0, 7.0, dtype=\"float64\"),\n   .....:         \"e\": [True, False, True],\n   .....:         \"f\": pd.Categorical(list(\"abc\")),\n   .....:         \"g\": pd.date_range(\"20130101\", periods=3),\n   .....:         \"h\": pd.date_range(\"20130101\", periods=3, tz=\"US/Eastern\"),\n   .....:         \"i\": pd.date_range(\"20130101\", periods=3, freq=\"ns\"),\n   .....:     }\n   .....: )\n   .....: \n\nIn [607]: df\nOut[607]: \n   a  b  c  ...          g                         h                             i\n0  a  1  3  ... 2013-01-01 2013-01-01 00:00:00-05:00 2013-01-01 00:00:00.000000000\n1  b  2  4  ... 2013-01-02 2013-01-02 00:00:00-05:00 2013-01-01 00:00:00.000000001\n2  c  3  5  ... 2013-01-03 2013-01-03 00:00:00-05:00 2013-01-01 00:00:00.000000002\n\n[3 rows x 9 columns]\n\nIn [608]: df.dtypes\nOut[608]: \na                        object\nb                         int64\nc                         uint8\nd                       float64\ne                          bool\nf                      category\ng                datetime64[ns]\nh    datetime64[ns, US/Eastern]\ni                datetime64[ns]\ndtype: object",
      "In [610]: result = pd.read_feather(\"example.feather\")\n\nIn [611]: result\nOut[611]: \n   a  b  c  ...          g                         h                             i\n0  a  1  3  ... 2013-01-01 2013-01-01 00:00:00-05:00 2013-01-01 00:00:00.000000000\n1  b  2  4  ... 2013-01-02 2013-01-02 00:00:00-05:00 2013-01-01 00:00:00.000000001\n2  c  3  5  ... 2013-01-03 2013-01-03 00:00:00-05:00 2013-01-01 00:00:00.000000002\n\n[3 rows x 9 columns]\n\n# we preserve dtypes\nIn [612]: result.dtypes\nOut[612]: \na                        object\nb                         int64\nc                         uint8\nd                       float64\ne                          bool\nf                      category\ng                datetime64[ns]\nh    datetime64[ns, US/Eastern]\ni                datetime64[ns]\ndtype: object",
      "In [613]: df = pd.DataFrame(\n   .....:     {\n   .....:         \"a\": list(\"abc\"),\n   .....:         \"b\": list(range(1, 4)),\n   .....:         \"c\": np.arange(3, 6).astype(\"u1\"),\n   .....:         \"d\": np.arange(4.0, 7.0, dtype=\"float64\"),\n   .....:         \"e\": [True, False, True],\n   .....:         \"f\": pd.date_range(\"20130101\", periods=3),\n   .....:         \"g\": pd.date_range(\"20130101\", periods=3, tz=\"US/Eastern\"),\n   .....:         \"h\": pd.Categorical(list(\"abc\")),\n   .....:         \"i\": pd.Categorical(list(\"abc\"), ordered=True),\n   .....:     }\n   .....: )\n   .....: \n\nIn [614]: df\nOut[614]: \n   a  b  c    d      e          f                         g  h  i\n0  a  1  3  4.0   True 2013-01-01 2013-01-01 00:00:00-05:00  a  a\n1  b  2  4  5.0  False 2013-01-02 2013-01-02 00:00:00-05:00  b  b\n2  c  3  5  6.0   True 2013-01-03 2013-01-03 00:00:00-05:00  c  c\n\nIn [615]: df.dtypes\nOut[615]: \na                        object\nb                         int64\nc                         uint8\nd                       float64\ne                          bool\nf                datetime64[ns]\ng    datetime64[ns, US/Eastern]\nh                      category\ni                      category\ndtype: object",
      "In [618]: result = pd.read_parquet(\"example_fp.parquet\", engine=\"fastparquet\")\n\nIn [619]: result = pd.read_parquet(\"example_pa.parquet\", engine=\"pyarrow\")\n\nIn [620]: result.dtypes\nOut[620]: \na                        object\nb                         int64\nc                         uint8\nd                       float64\ne                          bool\nf                datetime64[ns]\ng    datetime64[ns, US/Eastern]\nh                      category\ni                      category\ndtype: object",
      "In [621]: result = pd.read_parquet(\"example_pa.parquet\", engine=\"pyarrow\", dtype_backend=\"pyarrow\")\n\nIn [622]: result.dtypes\nOut[622]: \na                                      string[pyarrow]\nb                                       int64[pyarrow]\nc                                       uint8[pyarrow]\nd                                      double[pyarrow]\ne                                        bool[pyarrow]\nf                               timestamp[ns][pyarrow]\ng                timestamp[ns, tz=US/Eastern][pyarrow]\nh    dictionary<values=string, indices=int32, order...\ni    dictionary<values=string, indices=int32, order...\ndtype: object",
      "In [623]: result = pd.read_parquet(\n   .....:     \"example_fp.parquet\",\n   .....:     engine=\"fastparquet\",\n   .....:     columns=[\"a\", \"b\"],\n   .....: )\n   .....: \n\nIn [624]: result = pd.read_parquet(\n   .....:     \"example_pa.parquet\",\n   .....:     engine=\"pyarrow\",\n   .....:     columns=[\"a\", \"b\"],\n   .....: )\n   .....: \n\nIn [625]: result.dtypes\nOut[625]: \na    object\nb     int64\ndtype: object",
      "In [626]: df = pd.DataFrame({\"a\": [1, 2], \"b\": [3, 4]})\n\nIn [627]: df.to_parquet(\"test.parquet\", engine=\"pyarrow\")",
      "In [629]: df = pd.DataFrame({\"a\": [0, 0, 1, 1], \"b\": [0, 1, 0, 1]})\n\nIn [630]: df.to_parquet(path=\"test\", engine=\"pyarrow\", partition_cols=[\"a\"], compression=None)",
      "In [631]: df = pd.DataFrame(\n   .....:     {\n   .....:         \"a\": list(\"abc\"),\n   .....:         \"b\": list(range(1, 4)),\n   .....:         \"c\": np.arange(4.0, 7.0, dtype=\"float64\"),\n   .....:         \"d\": [True, False, True],\n   .....:         \"e\": pd.date_range(\"20130101\", periods=3),\n   .....:     }\n   .....: )\n   .....: \n\nIn [632]: df\nOut[632]: \n   a  b    c      d          e\n0  a  1  4.0   True 2013-01-01\n1  b  2  5.0  False 2013-01-02\n2  c  3  6.0   True 2013-01-03\n\nIn [633]: df.dtypes\nOut[633]: \na            object\nb             int64\nc           float64\nd              bool\ne    datetime64[ns]\ndtype: object",
      "In [635]: result = pd.read_orc(\"example_pa.orc\")\n\nIn [636]: result.dtypes\nOut[636]: \na            object\nb             int64\nc           float64\nd              bool\ne    datetime64[ns]\ndtype: object",
      "In [637]: result = pd.read_orc(\n   .....:     \"example_pa.orc\",\n   .....:     columns=[\"a\", \"b\"],\n   .....: )\n   .....: \n\nIn [638]: result.dtypes\nOut[638]: \na    object\nb     int64\ndtype: object",
      "import adbc_driver_sqlite.dbapi as sqlite_dbapi\n\n# Create the connection\nwith sqlite_dbapi.connect(\"sqlite:///:memory:\") as conn:\n     df = pd.read_sql_table(\"data\", conn)",
      "In [639]: from sqlalchemy import create_engine\n\n# Create your engine.\nIn [640]: engine = create_engine(\"sqlite:///:memory:\")",
      "with engine.connect() as conn, conn.begin():\n    data = pd.read_sql_table(\"data\", conn)",
      "In [641]: import datetime\n\nIn [642]: c = [\"id\", \"Date\", \"Col_1\", \"Col_2\", \"Col_3\"]\n\nIn [643]: d = [\n   .....:     (26, datetime.datetime(2010, 10, 18), \"X\", 27.5, True),\n   .....:     (42, datetime.datetime(2010, 10, 19), \"Y\", -12.5, False),\n   .....:     (63, datetime.datetime(2010, 10, 20), \"Z\", 5.73, True),\n   .....: ]\n   .....: \n\nIn [644]: data = pd.DataFrame(d, columns=c)\n\nIn [645]: data\nOut[645]: \n   id       Date Col_1  Col_2  Col_3\n0  26 2010-10-18     X  27.50   True\n1  42 2010-10-19     Y -12.50  False\n2  63 2010-10-20     Z   5.73   True\n\nIn [646]: data.to_sql(\"data\", con=engine)\nOut[646]: 3",
      "# for roundtripping\nwith pg_dbapi.connect(uri) as conn:\n    df2 = pd.read_sql(\"pandas_table\", conn, dtype_backend=\"pyarrow\")",
      "In [648]: from sqlalchemy.types import String\n\nIn [649]: data.to_sql(\"data_dtype\", con=engine, dtype={\"Col_1\": String})\nOut[649]: 3",
      "# Alternative to_sql() *method* for DBs that support COPY FROM\nimport csv\nfrom io import StringIO\n\ndef psql_insert_copy(table, conn, keys, data_iter):\n    \"\"\"\n    Execute SQL statement inserting data\n\n    Parameters\n    ----------\n    table : pandas.io.sql.SQLTable\n    conn : sqlalchemy.engine.Engine or sqlalchemy.engine.Connection\n    keys : list of str\n        Column names\n    data_iter : Iterable that iterates the values to be inserted\n    \"\"\"\n    # gets a DBAPI connection that can provide a cursor\n    dbapi_conn = conn.connection\n    with dbapi_conn.cursor() as cur:\n        s_buf = StringIO()\n        writer = csv.writer(s_buf)\n        writer.writerows(data_iter)\n        s_buf.seek(0)\n\n        columns = ', '.join(['\"{}\"'.format(k) for k in keys])\n        if table.schema:\n            table_name = '{}.{}'.format(table.schema, table.name)\n        else:\n            table_name = table.name\n\n        sql = 'COPY {} ({}) FROM STDIN WITH CSV'.format(\n            table_name, columns)\n        cur.copy_expert(sql=sql, file=s_buf)",
      "In [650]: pd.read_sql_table(\"data\", engine)\nOut[650]: \n   index  id       Date Col_1  Col_2  Col_3\n0      0  26 2010-10-18     X  27.50   True\n1      1  42 2010-10-19     Y -12.50  False\n2      2  63 2010-10-20     Z   5.73   True",
      "In [651]: pd.read_sql_table(\"data\", engine, index_col=\"id\")\nOut[651]: \n    index       Date Col_1  Col_2  Col_3\nid                                      \n26      0 2010-10-18     X  27.50   True\n42      1 2010-10-19     Y -12.50  False\n63      2 2010-10-20     Z   5.73   True\n\nIn [652]: pd.read_sql_table(\"data\", engine, columns=[\"Col_1\", \"Col_2\"])\nOut[652]: \n  Col_1  Col_2\n0     X  27.50\n1     Y -12.50\n2     Z   5.73",
      "In [653]: pd.read_sql_table(\"data\", engine, parse_dates=[\"Date\"])\nOut[653]: \n   index  id       Date Col_1  Col_2  Col_3\n0      0  26 2010-10-18     X  27.50   True\n1      1  42 2010-10-19     Y -12.50  False\n2      2  63 2010-10-20     Z   5.73   True",
      "pd.read_sql_table(\"data\", engine, parse_dates={\"Date\": \"%Y-%m-%d\"})\npd.read_sql_table(\n    \"data\",\n    engine,\n    parse_dates={\"Date\": {\"format\": \"%Y-%m-%d %H:%M:%S\"}},\n)",
      "df.to_sql(name=\"table\", con=engine, schema=\"other_schema\")\npd.read_sql_table(\"table\", engine, schema=\"other_schema\")",
      "In [654]: pd.read_sql_query(\"SELECT * FROM data\", engine)\nOut[654]: \n   index  id                        Date Col_1  Col_2  Col_3\n0      0  26  2010-10-18 00:00:00.000000     X  27.50      1\n1      1  42  2010-10-19 00:00:00.000000     Y -12.50      0\n2      2  63  2010-10-20 00:00:00.000000     Z   5.73      1",
      "In [655]: pd.read_sql_query(\"SELECT id, Col_1, Col_2 FROM data WHERE id = 42;\", engine)\nOut[655]: \n   id Col_1  Col_2\n0  42     Y  -12.5",
      "In [656]: df = pd.DataFrame(np.random.randn(20, 3), columns=list(\"abc\"))\n\nIn [657]: df.to_sql(name=\"data_chunks\", con=engine, index=False)\nOut[657]: 20",
      "In [658]: for chunk in pd.read_sql_query(\"SELECT * FROM data_chunks\", engine, chunksize=5):\n   .....:     print(chunk)\n   .....: \n          a         b         c\n0 -0.395347 -0.822726 -0.363777\n1  1.676124 -0.908102 -1.391346\n2 -1.094269  0.278380  1.205899\n3  1.503443  0.932171 -0.709459\n4 -0.645944 -1.351389  0.132023\n          a         b         c\n0  0.210427  0.192202  0.661949\n1  1.690629 -1.046044  0.618697\n2 -0.013863  1.314289  1.951611\n3 -1.485026  0.304662  1.194757\n4 -0.446717  0.528496 -0.657575\n          a         b         c\n0 -0.876654  0.336252  0.172668\n1  0.337684 -0.411202 -0.828394\n2 -0.244413  1.094948  0.087183\n3  1.125934 -1.480095  1.205944\n4 -0.451849  0.452214 -2.208192\n          a         b         c\n0 -2.061019  0.044184 -0.017118\n1  1.248959 -0.675595 -1.908296\n2 -0.125934  1.491974  0.648726\n3  0.391214  0.438609  1.634248\n4  1.208707 -1.535740  1.620399",
      "from sqlalchemy import create_engine\n\nengine = create_engine(\"postgresql://scott:tiger@localhost:5432/mydatabase\")\n\nengine = create_engine(\"mysql+mysqldb://scott:tiger@localhost/foo\")\n\nengine = create_engine(\"oracle://scott:[email\u00a0protected]:1521/sidname\")\n\nengine = create_engine(\"mssql+pyodbc://mydsn\")\n\n# sqlite://<nohostname>/<path>\n# where <path> is relative:\nengine = create_engine(\"sqlite:///foo.db\")\n\n# or absolute, starting with a slash:\nengine = create_engine(\"sqlite:////absolute/path/to/foo.db\")",
      "In [659]: import sqlalchemy as sa\n\nIn [660]: pd.read_sql(\n   .....:     sa.text(\"SELECT * FROM data where Col_1=:col1\"), engine, params={\"col1\": \"X\"}\n   .....: )\n   .....: \nOut[660]: \n   index  id                        Date Col_1  Col_2  Col_3\n0      0  26  2010-10-18 00:00:00.000000     X   27.5      1",
      "In [661]: metadata = sa.MetaData()\n\nIn [662]: data_table = sa.Table(\n   .....:     \"data\",\n   .....:     metadata,\n   .....:     sa.Column(\"index\", sa.Integer),\n   .....:     sa.Column(\"Date\", sa.DateTime),\n   .....:     sa.Column(\"Col_1\", sa.String),\n   .....:     sa.Column(\"Col_2\", sa.Float),\n   .....:     sa.Column(\"Col_3\", sa.Boolean),\n   .....: )\n   .....: \n\nIn [663]: pd.read_sql(sa.select(data_table).where(data_table.c.Col_3 is True), engine)\nOut[663]: \nEmpty DataFrame\nColumns: [index, Date, Col_1, Col_2, Col_3]\nIndex: []",
      "In [664]: import datetime as dt\n\nIn [665]: expr = sa.select(data_table).where(data_table.c.Date > sa.bindparam(\"date\"))\n\nIn [666]: pd.read_sql(expr, engine, params={\"date\": dt.datetime(2010, 10, 18)})\nOut[666]: \n   index       Date Col_1  Col_2  Col_3\n0      1 2010-10-19     Y -12.50  False\n1      2 2010-10-20     Z   5.73   True",
      "import sqlite3\n\ncon = sqlite3.connect(\":memory:\")",
      "data.to_sql(\"data\", con)\npd.read_sql_query(\"SELECT * FROM data\", con)",
      "In [667]: df = pd.DataFrame(np.random.randn(10, 2), columns=list(\"AB\"))\n\nIn [668]: df.to_stata(\"stata.dta\")",
      "In [669]: pd.read_stata(\"stata.dta\")\nOut[669]: \n   index         A         B\n0      0 -0.165614  0.490482\n1      1 -0.637829  0.067091\n2      2 -0.242577  1.348038\n3      3  0.647699 -0.644937\n4      4  0.625771  0.918376\n5      5  0.401781 -1.488919\n6      6 -0.981845 -0.046882\n7      7 -0.306796  0.877025\n8      8 -0.336606  0.624747\n9      9 -1.582600  0.806340",
      "In [670]: with pd.read_stata(\"stata.dta\", chunksize=3) as reader:\n   .....:     for df in reader:\n   .....:         print(df.shape)\n   .....: \n(3, 3)\n(3, 3)\n(3, 3)\n(1, 3)",
      "In [671]: with pd.read_stata(\"stata.dta\", iterator=True) as reader:\n   .....:     chunk1 = reader.read(5)\n   .....:     chunk2 = reader.read(5)\n   .....:",
      "df = pd.read_sas(\"sas_data.sas7bdat\")",
      "def do_something(chunk):\n    pass\n\n\nwith pd.read_sas(\"sas_xport.xpt\", chunk=100000) as rdr:\n    for chunk in rdr:\n        do_something(chunk)",
      "df = pd.read_spss(\"spss_data.sav\")",
      "df = pd.read_spss(\n    \"spss_data.sav\",\n    usecols=[\"foo\", \"bar\"],\n    convert_categoricals=False,\n)",
      "In [1]: sz = 1000000\nIn [2]: df = pd.DataFrame({'A': np.random.randn(sz), 'B': [1] * sz})\n\nIn [3]: df.info()\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 1000000 entries, 0 to 999999\nData columns (total 2 columns):\nA    1000000 non-null float64\nB    1000000 non-null int64\ndtypes: float64(1), int64(1)\nmemory usage: 15.3 MB",
      "import numpy as np\n\nimport os\n\nsz = 1000000\ndf = pd.DataFrame({\"A\": np.random.randn(sz), \"B\": [1] * sz})\n\nsz = 1000000\nnp.random.seed(42)\ndf = pd.DataFrame({\"A\": np.random.randn(sz), \"B\": [1] * sz})\n\n\ndef test_sql_write(df):\n    if os.path.exists(\"test.sql\"):\n        os.remove(\"test.sql\")\n    sql_db = sqlite3.connect(\"test.sql\")\n    df.to_sql(name=\"test_table\", con=sql_db)\n    sql_db.close()\n\n\ndef test_sql_read():\n    sql_db = sqlite3.connect(\"test.sql\")\n    pd.read_sql_query(\"select * from test_table\", sql_db)\n    sql_db.close()\n\n\ndef test_hdf_fixed_write(df):\n    df.to_hdf(\"test_fixed.hdf\", key=\"test\", mode=\"w\")\n\n\ndef test_hdf_fixed_read():\n    pd.read_hdf(\"test_fixed.hdf\", \"test\")\n\n\ndef test_hdf_fixed_write_compress(df):\n    df.to_hdf(\"test_fixed_compress.hdf\", key=\"test\", mode=\"w\", complib=\"blosc\")\n\n\ndef test_hdf_fixed_read_compress():\n    pd.read_hdf(\"test_fixed_compress.hdf\", \"test\")\n\n\ndef test_hdf_table_write(df):\n    df.to_hdf(\"test_table.hdf\", key=\"test\", mode=\"w\", format=\"table\")\n\n\ndef test_hdf_table_read():\n    pd.read_hdf(\"test_table.hdf\", \"test\")\n\n\ndef test_hdf_table_write_compress(df):\n    df.to_hdf(\n        \"test_table_compress.hdf\", key=\"test\", mode=\"w\", complib=\"blosc\", format=\"table\"\n    )\n\n\ndef test_hdf_table_read_compress():\n    pd.read_hdf(\"test_table_compress.hdf\", \"test\")\n\n\ndef test_csv_write(df):\n    df.to_csv(\"test.csv\", mode=\"w\")\n\n\ndef test_csv_read():\n    pd.read_csv(\"test.csv\", index_col=0)\n\n\ndef test_feather_write(df):\n    df.to_feather(\"test.feather\")\n\n\ndef test_feather_read():\n    pd.read_feather(\"test.feather\")\n\n\ndef test_pickle_write(df):\n    df.to_pickle(\"test.pkl\")\n\n\ndef test_pickle_read():\n    pd.read_pickle(\"test.pkl\")\n\n\ndef test_pickle_write_compress(df):\n    df.to_pickle(\"test.pkl.compress\", compression=\"xz\")\n\n\ndef test_pickle_read_compress():\n    pd.read_pickle(\"test.pkl.compress\", compression=\"xz\")\n\n\ndef test_parquet_write(df):\n    df.to_parquet(\"test.parquet\")\n\n\ndef test_parquet_read():\n    pd.read_parquet(\"test.parquet\")"
    ]
  },
  {
    "url": "https://pandas.pydata.org/docs/user_guide/text.html",
    "title": "Working with text data#",
    "code_snippets": [
      "In [1]: pd.Series([\"a\", \"b\", \"c\"])\nOut[1]: \n0    a\n1    b\n2    c\ndtype: object",
      "In [2]: pd.Series([\"a\", \"b\", \"c\"], dtype=\"string\")\nOut[2]: \n0    a\n1    b\n2    c\ndtype: string\n\nIn [3]: pd.Series([\"a\", \"b\", \"c\"], dtype=pd.StringDtype())\nOut[3]: \n0    a\n1    b\n2    c\ndtype: string",
      "In [4]: s = pd.Series([\"a\", \"b\", \"c\"])\n\nIn [5]: s\nOut[5]: \n0    a\n1    b\n2    c\ndtype: object\n\nIn [6]: s.astype(\"string\")\nOut[6]: \n0    a\n1    b\n2    c\ndtype: string",
      "In [7]: s = pd.Series([\"a\", 2, np.nan], dtype=\"string\")\n\nIn [8]: s\nOut[8]: \n0       a\n1       2\n2    <NA>\ndtype: string\n\nIn [9]: type(s[1])\nOut[9]: str",
      "In [10]: s1 = pd.Series([1, 2, np.nan], dtype=\"Int64\")\n\nIn [11]: s1\nOut[11]: \n0       1\n1       2\n2    <NA>\ndtype: Int64\n\nIn [12]: s2 = s1.astype(\"string\")\n\nIn [13]: s2\nOut[13]: \n0       1\n1       2\n2    <NA>\ndtype: string\n\nIn [14]: type(s2[0])\nOut[14]: str",
      "In [15]: s = pd.Series([\"a\", None, \"b\"], dtype=\"string\")\n\nIn [16]: s\nOut[16]: \n0       a\n1    <NA>\n2       b\ndtype: string\n\nIn [17]: s.str.count(\"a\")\nOut[17]: \n0       1\n1    <NA>\n2       0\ndtype: Int64\n\nIn [18]: s.dropna().str.count(\"a\")\nOut[18]: \n0    1\n2    0\ndtype: Int64",
      "In [19]: s2 = pd.Series([\"a\", None, \"b\"], dtype=\"object\")\n\nIn [20]: s2.str.count(\"a\")\nOut[20]: \n0    1.0\n1    NaN\n2    0.0\ndtype: float64\n\nIn [21]: s2.dropna().str.count(\"a\")\nOut[21]: \n0    1\n2    0\ndtype: int64",
      "In [24]: s = pd.Series(\n   ....:     [\"A\", \"B\", \"C\", \"Aaba\", \"Baca\", np.nan, \"CABA\", \"dog\", \"cat\"], dtype=\"string\"\n   ....: )\n   ....: \n\nIn [25]: s.str.lower()\nOut[25]: \n0       a\n1       b\n2       c\n3    aaba\n4    baca\n5    <NA>\n6    caba\n7     dog\n8     cat\ndtype: string\n\nIn [26]: s.str.upper()\nOut[26]: \n0       A\n1       B\n2       C\n3    AABA\n4    BACA\n5    <NA>\n6    CABA\n7     DOG\n8     CAT\ndtype: string\n\nIn [27]: s.str.len()\nOut[27]: \n0       1\n1       1\n2       1\n3       4\n4       4\n5    <NA>\n6       4\n7       3\n8       3\ndtype: Int64",
      "In [28]: idx = pd.Index([\" jack\", \"jill \", \" jesse \", \"frank\"])\n\nIn [29]: idx.str.strip()\nOut[29]: Index(['jack', 'jill', 'jesse', 'frank'], dtype='object')\n\nIn [30]: idx.str.lstrip()\nOut[30]: Index(['jack', 'jill ', 'jesse ', 'frank'], dtype='object')\n\nIn [31]: idx.str.rstrip()\nOut[31]: Index([' jack', 'jill', ' jesse', 'frank'], dtype='object')",
      "In [32]: df = pd.DataFrame(\n   ....:     np.random.randn(3, 2), columns=[\" Column A \", \" Column B \"], index=range(3)\n   ....: )\n   ....: \n\nIn [33]: df\nOut[33]: \n   Column A   Column B \n0   0.469112  -0.282863\n1  -1.509059  -1.135632\n2   1.212112  -0.173215",
      "In [38]: s2 = pd.Series([\"a_b_c\", \"c_d_e\", np.nan, \"f_g_h\"], dtype=\"string\")\n\nIn [39]: s2.str.split(\"_\")\nOut[39]: \n0    [a, b, c]\n1    [c, d, e]\n2         <NA>\n3    [f, g, h]\ndtype: object",
      "In [45]: s3 = pd.Series(\n   ....:     [\"A\", \"B\", \"C\", \"Aaba\", \"Baca\", \"\", np.nan, \"CABA\", \"dog\", \"cat\"],\n   ....:     dtype=\"string\",\n   ....: )\n   ....: \n\nIn [46]: s3\nOut[46]: \n0       A\n1       B\n2       C\n3    Aaba\n4    Baca\n5        \n6    <NA>\n7    CABA\n8     dog\n9     cat\ndtype: string\n\nIn [47]: s3.str.replace(\"^.a|dog\", \"XX-XX \", case=False, regex=True)\nOut[47]: \n0           A\n1           B\n2           C\n3    XX-XX ba\n4    XX-XX ca\n5            \n6        <NA>\n7    XX-XX BA\n8      XX-XX \n9     XX-XX t\ndtype: string",
      "In [48]: s4 = pd.Series([\"a.b\", \".\", \"b\", np.nan, \"\"], dtype=\"string\")\n\nIn [49]: s4\nOut[49]: \n0     a.b\n1       .\n2       b\n3    <NA>\n4        \ndtype: string\n\nIn [50]: s4.str.replace(\".\", \"a\", regex=True)\nOut[50]: \n0     aaa\n1       a\n2       a\n3    <NA>\n4        \ndtype: string",
      "In [51]: dollars = pd.Series([\"12\", \"-$10\", \"$10,000\"], dtype=\"string\")\n\n# These lines are equivalent\nIn [52]: dollars.str.replace(r\"-\\$\", \"-\", regex=True)\nOut[52]: \n0         12\n1        -10\n2    $10,000\ndtype: string\n\nIn [53]: dollars.str.replace(\"-$\", \"-\", regex=False)\nOut[53]: \n0         12\n1        -10\n2    $10,000\ndtype: string",
      "# Reverse every lowercase alphabetic word\nIn [54]: pat = r\"[a-z]+\"\n\nIn [55]: def repl(m):\n   ....:     return m.group(0)[::-1]\n   ....: \n\nIn [56]: pd.Series([\"foo 123\", \"bar baz\", np.nan], dtype=\"string\").str.replace(\n   ....:     pat, repl, regex=True\n   ....: )\n   ....: \nOut[56]: \n0    oof 123\n1    rab zab\n2       <NA>\ndtype: string\n\n# Using regex groups\nIn [57]: pat = r\"(?P<one>\\w+) (?P<two>\\w+) (?P<three>\\w+)\"\n\nIn [58]: def repl(m):\n   ....:     return m.group(\"two\").swapcase()\n   ....: \n\nIn [59]: pd.Series([\"Foo Bar Baz\", np.nan], dtype=\"string\").str.replace(\n   ....:     pat, repl, regex=True\n   ....: )\n   ....: \nOut[59]: \n0     bAR\n1    <NA>\ndtype: string",
      "In [60]: import re\n\nIn [61]: regex_pat = re.compile(r\"^.a|dog\", flags=re.IGNORECASE)\n\nIn [62]: s3.str.replace(regex_pat, \"XX-XX \", regex=True)\nOut[62]: \n0           A\n1           B\n2           C\n3    XX-XX ba\n4    XX-XX ca\n5            \n6        <NA>\n7    XX-XX BA\n8      XX-XX \n9     XX-XX t\ndtype: string",
      "In [64]: s = pd.Series([\"str_foo\", \"str_bar\", \"no_prefix\"])\n\nIn [65]: s.str.removeprefix(\"str_\")\nOut[65]: \n0          foo\n1          bar\n2    no_prefix\ndtype: object\n\nIn [66]: s = pd.Series([\"foo_str\", \"bar_str\", \"no_suffix\"])\n\nIn [67]: s.str.removesuffix(\"_str\")\nOut[67]: \n0          foo\n1          bar\n2    no_suffix\ndtype: object",
      "In [68]: s = pd.Series([\"a\", \"b\", \"c\", \"d\"], dtype=\"string\")\n\nIn [69]: s.str.cat(sep=\",\")\nOut[69]: 'a,b,c,d'",
      "In [71]: t = pd.Series([\"a\", \"b\", np.nan, \"d\"], dtype=\"string\")\n\nIn [72]: t.str.cat(sep=\",\")\nOut[72]: 'a,b,d'\n\nIn [73]: t.str.cat(sep=\",\", na_rep=\"-\")\nOut[73]: 'a,b,-,d'",
      "In [77]: d = pd.concat([t, s], axis=1)\n\nIn [78]: s\nOut[78]: \n0    a\n1    b\n2    c\n3    d\ndtype: string\n\nIn [79]: d\nOut[79]: \n      0  1\n0     a  a\n1     b  b\n2  <NA>  c\n3     d  d\n\nIn [80]: s.str.cat(d, na_rep=\"-\")\nOut[80]: \n0    aaa\n1    bbb\n2    c-c\n3    ddd\ndtype: string",
      "In [81]: u = pd.Series([\"b\", \"d\", \"a\", \"c\"], index=[1, 3, 0, 2], dtype=\"string\")\n\nIn [82]: s\nOut[82]: \n0    a\n1    b\n2    c\n3    d\ndtype: string\n\nIn [83]: u\nOut[83]: \n1    b\n3    d\n0    a\n2    c\ndtype: string\n\nIn [84]: s.str.cat(u)\nOut[84]: \n0    aa\n1    bb\n2    cc\n3    dd\ndtype: string\n\nIn [85]: s.str.cat(u, join=\"left\")\nOut[85]: \n0    aa\n1    bb\n2    cc\n3    dd\ndtype: string",
      "In [86]: v = pd.Series([\"z\", \"a\", \"b\", \"d\", \"e\"], index=[-1, 0, 1, 3, 4], dtype=\"string\")\n\nIn [87]: s\nOut[87]: \n0    a\n1    b\n2    c\n3    d\ndtype: string\n\nIn [88]: v\nOut[88]: \n-1    z\n 0    a\n 1    b\n 3    d\n 4    e\ndtype: string\n\nIn [89]: s.str.cat(v, join=\"left\", na_rep=\"-\")\nOut[89]: \n0    aa\n1    bb\n2    c-\n3    dd\ndtype: string\n\nIn [90]: s.str.cat(v, join=\"outer\", na_rep=\"-\")\nOut[90]: \n-1    -z\n 0    aa\n 1    bb\n 2    c-\n 3    dd\n 4    -e\ndtype: string",
      "In [103]: s = pd.Series(\n   .....:     [\"A\", \"B\", \"C\", \"Aaba\", \"Baca\", np.nan, \"CABA\", \"dog\", \"cat\"], dtype=\"string\"\n   .....: )\n   .....: \n\nIn [104]: s.str[0]\nOut[104]: \n0       A\n1       B\n2       C\n3       A\n4       B\n5    <NA>\n6       C\n7       d\n8       c\ndtype: string\n\nIn [105]: s.str[1]\nOut[105]: \n0    <NA>\n1    <NA>\n2    <NA>\n3       a\n4       a\n5    <NA>\n6       A\n7       o\n8       a\ndtype: string",
      "In [106]: pd.Series(\n   .....:     [\"a1\", \"b2\", \"c3\"],\n   .....:     dtype=\"string\",\n   .....: ).str.extract(r\"([ab])(\\d)\", expand=False)\n   .....: \nOut[106]: \n      0     1\n0     a     1\n1     b     2\n2  <NA>  <NA>",
      "In [107]: pd.Series([\"a1\", \"b2\", \"c3\"], dtype=\"string\").str.extract(\n   .....:     r\"(?P<letter>[ab])(?P<digit>\\d)\", expand=False\n   .....: )\n   .....: \nOut[107]: \n  letter digit\n0      a     1\n1      b     2\n2   <NA>  <NA>",
      "In [108]: pd.Series(\n   .....:     [\"a1\", \"b2\", \"3\"],\n   .....:     dtype=\"string\",\n   .....: ).str.extract(r\"([ab])?(\\d)\", expand=False)\n   .....: \nOut[108]: \n      0  1\n0     a  1\n1     b  2\n2  <NA>  3",
      "In [109]: pd.Series([\"a1\", \"b2\", \"c3\"], dtype=\"string\").str.extract(r\"[ab](\\d)\", expand=True)\nOut[109]: \n      0\n0     1\n1     2\n2  <NA>",
      "In [110]: pd.Series([\"a1\", \"b2\", \"c3\"], dtype=\"string\").str.extract(r\"[ab](\\d)\", expand=False)\nOut[110]: \n0       1\n1       2\n2    <NA>\ndtype: string",
      "In [111]: s = pd.Series([\"a1\", \"b2\", \"c3\"], [\"A11\", \"B22\", \"C33\"], dtype=\"string\")\n\nIn [112]: s\nOut[112]: \nA11    a1\nB22    b2\nC33    c3\ndtype: string\n\nIn [113]: s.index.str.extract(\"(?P<letter>[a-zA-Z])\", expand=True)\nOut[113]: \n  letter\n0      A\n1      B\n2      C",
      "In [117]: s = pd.Series([\"a1a2\", \"b1\", \"c1\"], index=[\"A\", \"B\", \"C\"], dtype=\"string\")\n\nIn [118]: s\nOut[118]: \nA    a1a2\nB      b1\nC      c1\ndtype: string\n\nIn [119]: two_groups = \"(?P<letter>[a-z])(?P<digit>[0-9])\"\n\nIn [120]: s.str.extract(two_groups, expand=True)\nOut[120]: \n  letter digit\nA      a     1\nB      b     1\nC      c     1",
      "In [122]: s = pd.Series([\"a3\", \"b3\", \"c2\"], dtype=\"string\")\n\nIn [123]: s\nOut[123]: \n0    a3\n1    b3\n2    c2\ndtype: string",
      "In [129]: pd.Index([\"a1a2\", \"b1\", \"c1\"]).str.extractall(two_groups)\nOut[129]: \n        letter digit\n  match             \n0 0          a     1\n  1          a     2\n1 0          b     1\n2 0          c     1\n\nIn [130]: pd.Series([\"a1a2\", \"b1\", \"c1\"], dtype=\"string\").str.extractall(two_groups)\nOut[130]: \n        letter digit\n  match             \n0 0          a     1\n  1          a     2\n1 0          b     1\n2 0          c     1",
      "In [131]: pattern = r\"[0-9][a-z]\"\n\nIn [132]: pd.Series(\n   .....:     [\"1\", \"2\", \"3a\", \"3b\", \"03c\", \"4dx\"],\n   .....:     dtype=\"string\",\n   .....: ).str.contains(pattern)\n   .....: \nOut[132]: \n0    False\n1    False\n2     True\n3     True\n4     True\n5     True\ndtype: boolean",
      "In [133]: pd.Series(\n   .....:     [\"1\", \"2\", \"3a\", \"3b\", \"03c\", \"4dx\"],\n   .....:     dtype=\"string\",\n   .....: ).str.match(pattern)\n   .....: \nOut[133]: \n0    False\n1    False\n2     True\n3     True\n4    False\n5     True\ndtype: boolean",
      "In [134]: pd.Series(\n   .....:     [\"1\", \"2\", \"3a\", \"3b\", \"03c\", \"4dx\"],\n   .....:     dtype=\"string\",\n   .....: ).str.fullmatch(pattern)\n   .....: \nOut[134]: \n0    False\n1    False\n2     True\n3     True\n4    False\n5    False\ndtype: boolean",
      "In [135]: s4 = pd.Series(\n   .....:     [\"A\", \"B\", \"C\", \"Aaba\", \"Baca\", np.nan, \"CABA\", \"dog\", \"cat\"], dtype=\"string\"\n   .....: )\n   .....: \n\nIn [136]: s4.str.contains(\"A\", na=False)\nOut[136]: \n0     True\n1    False\n2    False\n3     True\n4    False\n5    False\n6     True\n7    False\n8    False\ndtype: boolean",
      "In [137]: s = pd.Series([\"a\", \"a|b\", np.nan, \"a|c\"], dtype=\"string\")\n\nIn [138]: s.str.get_dummies(sep=\"|\")\nOut[138]: \n   a  b  c\n0  1  0  0\n1  1  1  0\n2  0  0  0\n3  1  0  1",
      "In [139]: idx = pd.Index([\"a\", \"a|b\", np.nan, \"a|c\"])\n\nIn [140]: idx.str.get_dummies(sep=\"|\")\nOut[140]: \nMultiIndex([(1, 0, 0),\n            (1, 1, 0),\n            (0, 0, 0),\n            (1, 0, 1)],\n           names=['a', 'b', 'c'])"
    ]
  },
  {
    "url": "https://pandas.pydata.org/docs/user_guide/visualization.html",
    "title": "Chart visualization#",
    "code_snippets": [
      "In [1]: import matplotlib.pyplot as plt\n\nIn [2]: plt.close(\"all\")",
      "In [3]: np.random.seed(123456)\n\nIn [4]: ts = pd.Series(np.random.randn(1000), index=pd.date_range(\"1/1/2000\", periods=1000))\n\nIn [5]: ts = ts.cumsum()\n\nIn [6]: ts.plot();",
      "In [7]: df = pd.DataFrame(np.random.randn(1000, 4), index=ts.index, columns=list(\"ABCD\"))\n\nIn [8]: df = df.cumsum()\n\nIn [9]: plt.figure();\n\nIn [10]: df.plot();",
      "In [11]: df3 = pd.DataFrame(np.random.randn(1000, 2), columns=[\"B\", \"C\"]).cumsum()\n\nIn [12]: df3[\"A\"] = pd.Series(list(range(len(df))))\n\nIn [13]: df3.plot(x=\"A\", y=\"B\");",
      "In [16]: df = pd.DataFrame()\n\nIn [17]: df.plot.<TAB>  # noqa: E225, E999\ndf.plot.area     df.plot.barh     df.plot.density  df.plot.hist     df.plot.line     df.plot.scatter\ndf.plot.bar      df.plot.box      df.plot.hexbin   df.plot.kde      df.plot.pie",
      "In [21]: df2 = pd.DataFrame(np.random.rand(10, 4), columns=[\"a\", \"b\", \"c\", \"d\"])\n\nIn [22]: df2.plot.bar();",
      "In [25]: df4 = pd.DataFrame(\n   ....:     {\n   ....:         \"a\": np.random.randn(1000) + 1,\n   ....:         \"b\": np.random.randn(1000),\n   ....:         \"c\": np.random.randn(1000) - 1,\n   ....:     },\n   ....:     columns=[\"a\", \"b\", \"c\"],\n   ....: )\n   ....: \n\nIn [26]: plt.figure();\n\nIn [27]: df4.plot.hist(alpha=0.5);",
      "In [36]: data = pd.Series(np.random.randn(1000))\n\nIn [37]: data.hist(by=np.random.randint(0, 4, 1000), figsize=(6, 4));",
      "In [38]: data = pd.DataFrame(\n   ....:     {\n   ....:         \"a\": np.random.choice([\"x\", \"y\", \"z\"], 1000),\n   ....:         \"b\": np.random.choice([\"e\", \"f\", \"g\"], 1000),\n   ....:         \"c\": np.random.randn(1000),\n   ....:         \"d\": np.random.randn(1000) - 1,\n   ....:     },\n   ....: )\n   ....: \n\nIn [39]: data.plot.hist(by=[\"a\", \"b\"], figsize=(10, 5));",
      "In [40]: df = pd.DataFrame(np.random.rand(10, 5), columns=[\"A\", \"B\", \"C\", \"D\", \"E\"])\n\nIn [41]: df.plot.box();",
      "In [45]: df = pd.DataFrame(np.random.rand(10, 5))\n\nIn [46]: plt.figure();\n\nIn [47]: bp = df.boxplot()",
      "In [48]: df = pd.DataFrame(np.random.rand(10, 2), columns=[\"Col1\", \"Col2\"])\n\nIn [49]: df[\"X\"] = pd.Series([\"A\", \"A\", \"A\", \"A\", \"A\", \"B\", \"B\", \"B\", \"B\", \"B\"])\n\nIn [50]: plt.figure();\n\nIn [51]: bp = df.boxplot(by=\"X\")",
      "In [52]: df = pd.DataFrame(np.random.rand(10, 3), columns=[\"Col1\", \"Col2\", \"Col3\"])\n\nIn [53]: df[\"X\"] = pd.Series([\"A\", \"A\", \"A\", \"A\", \"A\", \"B\", \"B\", \"B\", \"B\", \"B\"])\n\nIn [54]: df[\"Y\"] = pd.Series([\"A\", \"B\", \"A\", \"B\", \"A\", \"B\", \"A\", \"B\", \"A\", \"B\"])\n\nIn [55]: plt.figure();\n\nIn [56]: bp = df.boxplot(column=[\"Col1\", \"Col2\"], by=[\"X\", \"Y\"])",
      "In [57]: df = pd.DataFrame(np.random.rand(10, 3), columns=[\"Col1\", \"Col2\", \"Col3\"])\n\nIn [58]: df[\"X\"] = pd.Series([\"A\", \"A\", \"A\", \"A\", \"A\", \"B\", \"B\", \"B\", \"B\", \"B\"])\n\nIn [59]: plt.figure();\n\nIn [60]: bp = df.plot.box(column=[\"Col1\", \"Col2\"], by=\"X\")",
      "In [61]: np.random.seed(1234)\n\nIn [62]: df_box = pd.DataFrame(np.random.randn(50, 2))\n\nIn [63]: df_box[\"g\"] = np.random.choice([\"A\", \"B\"], size=50)\n\nIn [64]: df_box.loc[df_box[\"g\"] == \"B\", 1] += 3\n\nIn [65]: bp = df_box.boxplot(by=\"g\")",
      "In [67]: df = pd.DataFrame(np.random.rand(10, 4), columns=[\"a\", \"b\", \"c\", \"d\"])\n\nIn [68]: df.plot.area();",
      "In [70]: df = pd.DataFrame(np.random.rand(50, 4), columns=[\"a\", \"b\", \"c\", \"d\"])\n\nIn [71]: df[\"species\"] = pd.Categorical(\n   ....:     [\"setosa\"] * 20 + [\"versicolor\"] * 20 + [\"virginica\"] * 10\n   ....: )\n   ....: \n\nIn [72]: df.plot.scatter(x=\"a\", y=\"b\");",
      "In [78]: df = pd.DataFrame(np.random.randn(1000, 2), columns=[\"a\", \"b\"])\n\nIn [79]: df[\"b\"] = df[\"b\"] + np.arange(1000)\n\nIn [80]: df.plot.hexbin(x=\"a\", y=\"b\", gridsize=25);",
      "In [81]: df = pd.DataFrame(np.random.randn(1000, 2), columns=[\"a\", \"b\"])\n\nIn [82]: df[\"b\"] = df[\"b\"] + np.arange(1000)\n\nIn [83]: df[\"z\"] = np.random.uniform(0, 3, 1000)\n\nIn [84]: df.plot.hexbin(x=\"a\", y=\"b\", C=\"z\", reduce_C_function=np.max, gridsize=25);",
      "In [85]: series = pd.Series(3 * np.random.rand(4), index=[\"a\", \"b\", \"c\", \"d\"], name=\"series\")\n\nIn [86]: series.plot.pie(figsize=(6, 6));",
      "In [87]: df = pd.DataFrame(\n   ....:     3 * np.random.rand(4, 2), index=[\"a\", \"b\", \"c\", \"d\"], columns=[\"x\", \"y\"]\n   ....: )\n   ....: \n\nIn [88]: df.plot.pie(subplots=True, figsize=(8, 4));",
      "In [90]: series = pd.Series([0.1] * 4, index=[\"a\", \"b\", \"c\", \"d\"], name=\"series2\")\n\nIn [91]: series.plot.pie(figsize=(6, 6));",
      "In [92]: from pandas.plotting import scatter_matrix\n\nIn [93]: df = pd.DataFrame(np.random.randn(1000, 4), columns=[\"a\", \"b\", \"c\", \"d\"])\n\nIn [94]: scatter_matrix(df, alpha=0.2, figsize=(6, 6), diagonal=\"kde\");",
      "In [95]: ser = pd.Series(np.random.randn(1000))\n\nIn [96]: ser.plot.kde();",
      "In [97]: from pandas.plotting import andrews_curves\n\nIn [98]: data = pd.read_csv(\"data/iris.data\")\n\nIn [99]: plt.figure();\n\nIn [100]: andrews_curves(data, \"Name\");",
      "In [101]: from pandas.plotting import parallel_coordinates\n\nIn [102]: data = pd.read_csv(\"data/iris.data\")\n\nIn [103]: plt.figure();\n\nIn [104]: parallel_coordinates(data, \"Name\");",
      "In [105]: from pandas.plotting import lag_plot\n\nIn [106]: plt.figure();\n\nIn [107]: spacing = np.linspace(-99 * np.pi, 99 * np.pi, num=1000)\n\nIn [108]: data = pd.Series(0.1 * np.random.rand(1000) + 0.9 * np.sin(spacing))\n\nIn [109]: lag_plot(data);",
      "In [110]: from pandas.plotting import autocorrelation_plot\n\nIn [111]: plt.figure();\n\nIn [112]: spacing = np.linspace(-9 * np.pi, 9 * np.pi, num=1000)\n\nIn [113]: data = pd.Series(0.7 * np.random.rand(1000) + 0.3 * np.sin(spacing))\n\nIn [114]: autocorrelation_plot(data);",
      "In [115]: from pandas.plotting import bootstrap_plot\n\nIn [116]: data = pd.Series(np.random.rand(1000))\n\nIn [117]: bootstrap_plot(data, size=50, samples=500, color=\"grey\");",
      "In [118]: from pandas.plotting import radviz\n\nIn [119]: data = pd.read_csv(\"data/iris.data\")\n\nIn [120]: plt.figure();\n\nIn [121]: radviz(data, \"Name\");",
      "In [124]: df = pd.DataFrame(np.random.randn(1000, 4), index=ts.index, columns=list(\"ABCD\"))\n\nIn [125]: df = df.cumsum()\n\nIn [126]: df.plot(legend=False);",
      "In [129]: ts = pd.Series(np.random.randn(1000), index=pd.date_range(\"1/1/2000\", periods=1000))\n\nIn [130]: ts = np.exp(ts.cumsum())\n\nIn [131]: ts.plot(logy=True);",
      "In [144]: plt.figure();\n\nIn [145]: with pd.plotting.plot_params.use(\"x_compat\", True):\n   .....:     df[\"A\"].plot(color=\"r\")\n   .....:     df[\"B\"].plot(color=\"g\")\n   .....:     df[\"C\"].plot(color=\"b\")\n   .....:",
      "In [155]: np.random.seed(123456)\n\nIn [156]: ts = pd.Series(np.random.randn(1000), index=pd.date_range(\"1/1/2000\", periods=1000))\n\nIn [157]: ts = ts.cumsum()\n\nIn [158]: df = pd.DataFrame(np.random.randn(1000, 4), index=ts.index, columns=list(\"ABCD\"))\n\nIn [159]: df = df.cumsum()",
      "# Generate the data\nIn [170]: ix3 = pd.MultiIndex.from_arrays(\n   .....:     [\n   .....:         [\"a\", \"a\", \"a\", \"a\", \"a\", \"b\", \"b\", \"b\", \"b\", \"b\"],\n   .....:         [\"foo\", \"foo\", \"foo\", \"bar\", \"bar\", \"foo\", \"foo\", \"bar\", \"bar\", \"bar\"],\n   .....:     ],\n   .....:     names=[\"letter\", \"word\"],\n   .....: )\n   .....: \n\nIn [171]: df3 = pd.DataFrame(\n   .....:     {\n   .....:         \"data1\": [9, 3, 2, 4, 3, 2, 4, 6, 3, 2],\n   .....:         \"data2\": [9, 6, 5, 7, 5, 4, 5, 6, 5, 1],\n   .....:     },\n   .....:     index=ix3,\n   .....: )\n   .....: \n\n# Group by index labels and take the means and standard deviations\n# for each group\nIn [172]: gp3 = df3.groupby(level=(\"letter\", \"word\"))\n\nIn [173]: means = gp3.mean()\n\nIn [174]: errors = gp3.std()\n\nIn [175]: means\nOut[175]: \n                data1     data2\nletter word                    \na      bar   3.500000  6.000000\n       foo   4.666667  6.666667\nb      bar   3.666667  4.000000\n       foo   3.000000  4.500000\n\nIn [176]: errors\nOut[176]: \n                data1     data2\nletter word                    \na      bar   0.707107  1.414214\n       foo   3.785939  2.081666\nb      bar   2.081666  2.645751\n       foo   1.414214  0.707107\n\n# Plot\nIn [177]: fig, ax = plt.subplots()\n\nIn [178]: means.plot.bar(yerr=errors, ax=ax, capsize=4, rot=0);",
      "In [179]: mins = gp3.min()\n\nIn [180]: maxs = gp3.max()\n\n# errors should be positive, and defined in the order of lower, upper\nIn [181]: errors = [[means[c] - mins[c], maxs[c] - means[c]] for c in df3.columns]\n\n# Plot\nIn [182]: fig, ax = plt.subplots()\n\nIn [183]: means.plot.bar(yerr=errors, ax=ax, capsize=4, rot=0);",
      "In [184]: np.random.seed(123456)\n\nIn [185]: fig, ax = plt.subplots(1, 1, figsize=(7, 6.5))\n\nIn [186]: df = pd.DataFrame(np.random.rand(5, 3), columns=[\"a\", \"b\", \"c\"])\n\nIn [187]: ax.xaxis.tick_top()  # Display x-axis ticks on top.\n\nIn [188]: df.plot(table=True, ax=ax);",
      "In [192]: from pandas.plotting import table\n\nIn [193]: fig, ax = plt.subplots(1, 1)\n\nIn [194]: table(ax, np.round(df.describe(), 2), loc=\"upper right\", colWidths=[0.2, 0.2, 0.2]);\n\nIn [195]: df.plot(ax=ax, ylim=(0, 2), legend=None);",
      "In [196]: np.random.seed(123456)\n\nIn [197]: df = pd.DataFrame(np.random.randn(1000, 10), index=ts.index)\n\nIn [198]: df = df.cumsum()\n\nIn [199]: plt.figure();\n\nIn [200]: df.plot(colormap=\"cubehelix\");",
      "In [201]: from matplotlib import cm\n\nIn [202]: plt.figure();\n\nIn [203]: df.plot(colormap=cm.cubehelix);",
      "In [204]: np.random.seed(123456)\n\nIn [205]: dd = pd.DataFrame(np.random.randn(10, 10)).map(abs)\n\nIn [206]: dd = dd.cumsum()\n\nIn [207]: plt.figure();\n\nIn [208]: dd.plot.bar(colormap=\"Greens\");",
      "In [213]: np.random.seed(123456)\n\nIn [214]: price = pd.Series(\n   .....:     np.random.randn(150).cumsum(),\n   .....:     index=pd.date_range(\"2000-1-1\", periods=150, freq=\"B\"),\n   .....: )\n   .....: \n\nIn [215]: ma = price.rolling(20).mean()\n\nIn [216]: mstd = price.rolling(20).std()\n\nIn [217]: plt.figure();\n\nIn [218]: plt.plot(price.index, price, \"k\");\n\nIn [219]: plt.plot(ma.index, ma, \"b\");\n\nIn [220]: plt.fill_between(mstd.index, ma - 2 * mstd, ma + 2 * mstd, color=\"b\", alpha=0.2);",
      ">>> pd.set_option(\"plotting.backend\", \"backend.module\")\n>>> pd.Series([1, 2, 3]).plot()",
      ">>> pd.options.plotting.backend = \"backend.module\"\n>>> pd.Series([1, 2, 3]).plot()",
      ">>> import backend.module\n>>> backend.module.plot(pd.Series([1, 2, 3]))"
    ]
  },
  {
    "url": "https://pandas.pydata.org/docs/user_guide/cookbook.html",
    "title": "Cookbook#",
    "code_snippets": [
      "In [1]: df = pd.DataFrame(\n   ...:     {\"AAA\": [4, 5, 6, 7], \"BBB\": [10, 20, 30, 40], \"CCC\": [100, 50, -30, -50]}\n   ...: )\n   ...: \n\nIn [2]: df\nOut[2]: \n   AAA  BBB  CCC\n0    4   10  100\n1    5   20   50\n2    6   30  -30\n3    7   40  -50",
      "In [9]: df_mask = pd.DataFrame(\n   ...:     {\"AAA\": [True] * 4, \"BBB\": [False] * 4, \"CCC\": [True, False] * 2}\n   ...: )\n   ...: \n\nIn [10]: df.where(df_mask, -1000)\nOut[10]: \n   AAA   BBB   CCC\n0    4 -1000  2000\n1    5 -1000 -1000\n2    6 -1000   555\n3    7 -1000 -1000",
      "In [11]: df = pd.DataFrame(\n   ....:     {\"AAA\": [4, 5, 6, 7], \"BBB\": [10, 20, 30, 40], \"CCC\": [100, 50, -30, -50]}\n   ....: )\n   ....: \n\nIn [12]: df\nOut[12]: \n   AAA  BBB  CCC\n0    4   10  100\n1    5   20   50\n2    6   30  -30\n3    7   40  -50\n\nIn [13]: df[\"logic\"] = np.where(df[\"AAA\"] > 5, \"high\", \"low\")\n\nIn [14]: df\nOut[14]: \n   AAA  BBB  CCC logic\n0    4   10  100   low\n1    5   20   50   low\n2    6   30  -30  high\n3    7   40  -50  high",
      "In [15]: df = pd.DataFrame(\n   ....:     {\"AAA\": [4, 5, 6, 7], \"BBB\": [10, 20, 30, 40], \"CCC\": [100, 50, -30, -50]}\n   ....: )\n   ....: \n\nIn [16]: df\nOut[16]: \n   AAA  BBB  CCC\n0    4   10  100\n1    5   20   50\n2    6   30  -30\n3    7   40  -50\n\nIn [17]: df[df.AAA <= 5]\nOut[17]: \n   AAA  BBB  CCC\n0    4   10  100\n1    5   20   50\n\nIn [18]: df[df.AAA > 5]\nOut[18]: \n   AAA  BBB  CCC\n2    6   30  -30\n3    7   40  -50",
      "In [19]: df = pd.DataFrame(\n   ....:     {\"AAA\": [4, 5, 6, 7], \"BBB\": [10, 20, 30, 40], \"CCC\": [100, 50, -30, -50]}\n   ....: )\n   ....: \n\nIn [20]: df\nOut[20]: \n   AAA  BBB  CCC\n0    4   10  100\n1    5   20   50\n2    6   30  -30\n3    7   40  -50",
      "In [25]: df = pd.DataFrame(\n   ....:     {\"AAA\": [4, 5, 6, 7], \"BBB\": [10, 20, 30, 40], \"CCC\": [100, 50, -30, -50]}\n   ....: )\n   ....: \n\nIn [26]: df\nOut[26]: \n   AAA  BBB  CCC\n0    4   10  100\n1    5   20   50\n2    6   30  -30\n3    7   40  -50\n\nIn [27]: aValue = 43.0\n\nIn [28]: df.loc[(df.CCC - aValue).abs().argsort()]\nOut[28]: \n   AAA  BBB  CCC\n1    5   20   50\n0    4   10  100\n2    6   30  -30\n3    7   40  -50",
      "In [29]: df = pd.DataFrame(\n   ....:     {\"AAA\": [4, 5, 6, 7], \"BBB\": [10, 20, 30, 40], \"CCC\": [100, 50, -30, -50]}\n   ....: )\n   ....: \n\nIn [30]: df\nOut[30]: \n   AAA  BBB  CCC\n0    4   10  100\n1    5   20   50\n2    6   30  -30\n3    7   40  -50\n\nIn [31]: Crit1 = df.AAA <= 5.5\n\nIn [32]: Crit2 = df.BBB == 10.0\n\nIn [33]: Crit3 = df.CCC > -40.0",
      "In [35]: import functools\n\nIn [36]: CritList = [Crit1, Crit2, Crit3]\n\nIn [37]: AllCrit = functools.reduce(lambda x, y: x & y, CritList)\n\nIn [38]: df[AllCrit]\nOut[38]: \n   AAA  BBB  CCC\n0    4   10  100",
      "In [39]: df = pd.DataFrame(\n   ....:     {\"AAA\": [4, 5, 6, 7], \"BBB\": [10, 20, 30, 40], \"CCC\": [100, 50, -30, -50]}\n   ....: )\n   ....: \n\nIn [40]: df\nOut[40]: \n   AAA  BBB  CCC\n0    4   10  100\n1    5   20   50\n2    6   30  -30\n3    7   40  -50\n\nIn [41]: df[(df.AAA <= 6) & (df.index.isin([0, 2, 4]))]\nOut[41]: \n   AAA  BBB  CCC\n0    4   10  100\n2    6   30  -30",
      "In [42]: df = pd.DataFrame(\n   ....:     {\"AAA\": [4, 5, 6, 7], \"BBB\": [10, 20, 30, 40], \"CCC\": [100, 50, -30, -50]},\n   ....:     index=[\"foo\", \"bar\", \"boo\", \"kar\"],\n   ....: )\n   ....:",
      "In [46]: data = {\"AAA\": [4, 5, 6, 7], \"BBB\": [10, 20, 30, 40], \"CCC\": [100, 50, -30, -50]}\n\nIn [47]: df2 = pd.DataFrame(data=data, index=[1, 2, 3, 4])  # Note index starts at 1.\n\nIn [48]: df2.iloc[1:3]  # Position-oriented\nOut[48]: \n   AAA  BBB  CCC\n2    5   20   50\n3    6   30  -30\n\nIn [49]: df2.loc[1:3]  # Label-oriented\nOut[49]: \n   AAA  BBB  CCC\n1    4   10  100\n2    5   20   50\n3    6   30  -30",
      "In [50]: df = pd.DataFrame(\n   ....:     {\"AAA\": [4, 5, 6, 7], \"BBB\": [10, 20, 30, 40], \"CCC\": [100, 50, -30, -50]}\n   ....: )\n   ....: \n\nIn [51]: df\nOut[51]: \n   AAA  BBB  CCC\n0    4   10  100\n1    5   20   50\n2    6   30  -30\n3    7   40  -50\n\nIn [52]: df[~((df.AAA <= 6) & (df.index.isin([0, 2, 4])))]\nOut[52]: \n   AAA  BBB  CCC\n1    5   20   50\n3    7   40  -50",
      "In [53]: df = pd.DataFrame({\"AAA\": [1, 2, 1, 3], \"BBB\": [1, 1, 2, 2], \"CCC\": [2, 1, 3, 1]})\n\nIn [54]: df\nOut[54]: \n   AAA  BBB  CCC\n0    1    1    2\n1    2    1    1\n2    1    2    3\n3    3    2    1\n\nIn [55]: source_cols = df.columns  # Or some subset would work too\n\nIn [56]: new_cols = [str(x) + \"_cat\" for x in source_cols]\n\nIn [57]: categories = {1: \"Alpha\", 2: \"Beta\", 3: \"Charlie\"}\n\nIn [58]: df[new_cols] = df[source_cols].map(categories.get)\n\nIn [59]: df\nOut[59]: \n   AAA  BBB  CCC  AAA_cat BBB_cat  CCC_cat\n0    1    1    2    Alpha   Alpha     Beta\n1    2    1    1     Beta   Alpha    Alpha\n2    1    2    3    Alpha    Beta  Charlie\n3    3    2    1  Charlie    Beta    Alpha",
      "In [60]: df = pd.DataFrame(\n   ....:     {\"AAA\": [1, 1, 1, 2, 2, 2, 3, 3], \"BBB\": [2, 1, 3, 4, 5, 1, 2, 3]}\n   ....: )\n   ....: \n\nIn [61]: df\nOut[61]: \n   AAA  BBB\n0    1    2\n1    1    1\n2    1    3\n3    2    4\n4    2    5\n5    2    1\n6    3    2\n7    3    3",
      "In [64]: df = pd.DataFrame(\n   ....:     {\n   ....:         \"row\": [0, 1, 2],\n   ....:         \"One_X\": [1.1, 1.1, 1.1],\n   ....:         \"One_Y\": [1.2, 1.2, 1.2],\n   ....:         \"Two_X\": [1.11, 1.11, 1.11],\n   ....:         \"Two_Y\": [1.22, 1.22, 1.22],\n   ....:     }\n   ....: )\n   ....: \n\nIn [65]: df\nOut[65]: \n   row  One_X  One_Y  Two_X  Two_Y\n0    0    1.1    1.2   1.11   1.22\n1    1    1.1    1.2   1.11   1.22\n2    2    1.1    1.2   1.11   1.22\n\n# As Labelled Index\nIn [66]: df = df.set_index(\"row\")\n\nIn [67]: df\nOut[67]: \n     One_X  One_Y  Two_X  Two_Y\nrow                            \n0      1.1    1.2   1.11   1.22\n1      1.1    1.2   1.11   1.22\n2      1.1    1.2   1.11   1.22\n\n# With Hierarchical Columns\nIn [68]: df.columns = pd.MultiIndex.from_tuples([tuple(c.split(\"_\")) for c in df.columns])\n\nIn [69]: df\nOut[69]: \n     One        Two      \n       X    Y     X     Y\nrow                      \n0    1.1  1.2  1.11  1.22\n1    1.1  1.2  1.11  1.22\n2    1.1  1.2  1.11  1.22\n\n# Now stack & Reset\nIn [70]: df = df.stack(0, future_stack=True).reset_index(1)\n\nIn [71]: df\nOut[71]: \n    level_1     X     Y\nrow                    \n0       One  1.10  1.20\n0       Two  1.11  1.22\n1       One  1.10  1.20\n1       Two  1.11  1.22\n2       One  1.10  1.20\n2       Two  1.11  1.22\n\n# And fix the labels (Notice the label 'level_1' got added automatically)\nIn [72]: df.columns = [\"Sample\", \"All_X\", \"All_Y\"]\n\nIn [73]: df\nOut[73]: \n    Sample  All_X  All_Y\nrow                     \n0      One   1.10   1.20\n0      Two   1.11   1.22\n1      One   1.10   1.20\n1      Two   1.11   1.22\n2      One   1.10   1.20\n2      Two   1.11   1.22",
      "In [74]: cols = pd.MultiIndex.from_tuples(\n   ....:     [(x, y) for x in [\"A\", \"B\", \"C\"] for y in [\"O\", \"I\"]]\n   ....: )\n   ....: \n\nIn [75]: df = pd.DataFrame(np.random.randn(2, 6), index=[\"n\", \"m\"], columns=cols)\n\nIn [76]: df\nOut[76]: \n          A                   B                   C          \n          O         I         O         I         O         I\nn  0.469112 -0.282863 -1.509059 -1.135632  1.212112 -0.173215\nm  0.119209 -1.044236 -0.861849 -2.104569 -0.494929  1.071804\n\nIn [77]: df = df.div(df[\"C\"], level=1)\n\nIn [78]: df\nOut[78]: \n          A                   B              C     \n          O         I         O         I    O    I\nn  0.387021  1.633022 -1.244983  6.556214  1.0  1.0\nm -0.240860 -0.974279  1.741358 -1.963577  1.0  1.0",
      "In [79]: coords = [(\"AA\", \"one\"), (\"AA\", \"six\"), (\"BB\", \"one\"), (\"BB\", \"two\"), (\"BB\", \"six\")]\n\nIn [80]: index = pd.MultiIndex.from_tuples(coords)\n\nIn [81]: df = pd.DataFrame([11, 22, 33, 44, 55], index, [\"MyData\"])\n\nIn [82]: df\nOut[82]: \n        MyData\nAA one      11\n   six      22\nBB one      33\n   two      44\n   six      55",
      "# Note : level and axis are optional, and default to zero\nIn [83]: df.xs(\"BB\", level=0, axis=0)\nOut[83]: \n     MyData\none      33\ntwo      44\nsix      55",
      "In [85]: import itertools\n\nIn [86]: index = list(itertools.product([\"Ada\", \"Quinn\", \"Violet\"], [\"Comp\", \"Math\", \"Sci\"]))\n\nIn [87]: headr = list(itertools.product([\"Exams\", \"Labs\"], [\"I\", \"II\"]))\n\nIn [88]: indx = pd.MultiIndex.from_tuples(index, names=[\"Student\", \"Course\"])\n\nIn [89]: cols = pd.MultiIndex.from_tuples(headr)  # Notice these are un-named\n\nIn [90]: data = [[70 + x + y + (x * y) % 3 for x in range(4)] for y in range(9)]\n\nIn [91]: df = pd.DataFrame(data, indx, cols)\n\nIn [92]: df\nOut[92]: \n               Exams     Labs    \n                   I  II    I  II\nStudent Course                   \nAda     Comp      70  71   72  73\n        Math      71  73   75  74\n        Sci       72  75   75  75\nQuinn   Comp      73  74   75  76\n        Math      74  76   78  77\n        Sci       75  78   78  78\nViolet  Comp      76  77   78  79\n        Math      77  79   81  80\n        Sci       78  81   81  81\n\nIn [93]: All = slice(None)\n\nIn [94]: df.loc[\"Violet\"]\nOut[94]: \n       Exams     Labs    \n           I  II    I  II\nCourse                   \nComp      76  77   78  79\nMath      77  79   81  80\nSci       78  81   81  81\n\nIn [95]: df.loc[(All, \"Math\"), All]\nOut[95]: \n               Exams     Labs    \n                   I  II    I  II\nStudent Course                   \nAda     Math      71  73   75  74\nQuinn   Math      74  76   78  77\nViolet  Math      77  79   81  80\n\nIn [96]: df.loc[(slice(\"Ada\", \"Quinn\"), \"Math\"), All]\nOut[96]: \n               Exams     Labs    \n                   I  II    I  II\nStudent Course                   \nAda     Math      71  73   75  74\nQuinn   Math      74  76   78  77\n\nIn [97]: df.loc[(All, \"Math\"), (\"Exams\")]\nOut[97]: \n                 I  II\nStudent Course        \nAda     Math    71  73\nQuinn   Math    74  76\nViolet  Math    77  79\n\nIn [98]: df.loc[(All, \"Math\"), (All, \"II\")]\nOut[98]: \n               Exams Labs\n                  II   II\nStudent Course           \nAda     Math      73   74\nQuinn   Math      76   77\nViolet  Math      79   80",
      "In [100]: df = pd.DataFrame(\n   .....:     np.random.randn(6, 1),\n   .....:     index=pd.date_range(\"2013-08-01\", periods=6, freq=\"B\"),\n   .....:     columns=list(\"A\"),\n   .....: )\n   .....: \n\nIn [101]: df.loc[df.index[3], \"A\"] = np.nan\n\nIn [102]: df\nOut[102]: \n                   A\n2013-08-01  0.721555\n2013-08-02 -0.706771\n2013-08-05 -1.039575\n2013-08-06       NaN\n2013-08-07 -0.424972\n2013-08-08  0.567020\n\nIn [103]: df.bfill()\nOut[103]: \n                   A\n2013-08-01  0.721555\n2013-08-02 -0.706771\n2013-08-05 -1.039575\n2013-08-06 -0.424972\n2013-08-07 -0.424972\n2013-08-08  0.567020",
      "In [104]: df = pd.DataFrame(\n   .....:     {\n   .....:         \"animal\": \"cat dog cat fish dog cat cat\".split(),\n   .....:         \"size\": list(\"SSMMMLL\"),\n   .....:         \"weight\": [8, 10, 11, 1, 20, 12, 12],\n   .....:         \"adult\": [False] * 5 + [True] * 2,\n   .....:     }\n   .....: )\n   .....: \n\nIn [105]: df\nOut[105]: \n  animal size  weight  adult\n0    cat    S       8  False\n1    dog    S      10  False\n2    cat    M      11  False\n3   fish    M       1  False\n4    dog    M      20  False\n5    cat    L      12   True\n6    cat    L      12   True\n\n# List the size of the animals with the highest weight.\nIn [106]: df.groupby(\"animal\").apply(lambda subf: subf[\"size\"][subf[\"weight\"].idxmax()], include_groups=False)\nOut[106]: \nanimal\ncat     L\ndog     M\nfish    M\ndtype: object",
      "In [109]: def GrowUp(x):\n   .....:     avg_weight = sum(x[x[\"size\"] == \"S\"].weight * 1.5)\n   .....:     avg_weight += sum(x[x[\"size\"] == \"M\"].weight * 1.25)\n   .....:     avg_weight += sum(x[x[\"size\"] == \"L\"].weight)\n   .....:     avg_weight /= len(x)\n   .....:     return pd.Series([\"L\", avg_weight, True], index=[\"size\", \"weight\", \"adult\"])\n   .....: \n\nIn [110]: expected_df = gb.apply(GrowUp, include_groups=False)\n\nIn [111]: expected_df\nOut[111]: \n       size   weight  adult\nanimal                     \ncat       L  12.4375   True\ndog       L  20.0000   True\nfish      L   1.2500   True",
      "In [112]: S = pd.Series([i / 100.0 for i in range(1, 11)])\n\nIn [113]: def cum_ret(x, y):\n   .....:     return x * (1 + y)\n   .....: \n\nIn [114]: def red(x):\n   .....:     return functools.reduce(cum_ret, x, 1.0)\n   .....: \n\nIn [115]: S.expanding().apply(red, raw=True)\nOut[115]: \n0    1.010000\n1    1.030200\n2    1.061106\n3    1.103550\n4    1.158728\n5    1.228251\n6    1.314229\n7    1.419367\n8    1.547110\n9    1.701821\ndtype: float64",
      "In [116]: df = pd.DataFrame({\"A\": [1, 1, 2, 2], \"B\": [1, -1, 1, 2]})\n\nIn [117]: gb = df.groupby(\"A\")\n\nIn [118]: def replace(g):\n   .....:     mask = g < 0\n   .....:     return g.where(~mask, g[~mask].mean())\n   .....: \n\nIn [119]: gb.transform(replace)\nOut[119]: \n   B\n0  1\n1  1\n2  1\n3  2",
      "In [120]: df = pd.DataFrame(\n   .....:     {\n   .....:         \"code\": [\"foo\", \"bar\", \"baz\"] * 2,\n   .....:         \"data\": [0.16, -0.21, 0.33, 0.45, -0.59, 0.62],\n   .....:         \"flag\": [False, True] * 3,\n   .....:     }\n   .....: )\n   .....: \n\nIn [121]: code_groups = df.groupby(\"code\")\n\nIn [122]: agg_n_sort_order = code_groups[[\"data\"]].transform(\"sum\").sort_values(by=\"data\")\n\nIn [123]: sorted_df = df.loc[agg_n_sort_order.index]\n\nIn [124]: sorted_df\nOut[124]: \n  code  data   flag\n1  bar -0.21   True\n4  bar -0.59  False\n0  foo  0.16  False\n3  foo  0.45   True\n2  baz  0.33  False\n5  baz  0.62   True",
      "In [125]: rng = pd.date_range(start=\"2014-10-07\", periods=10, freq=\"2min\")\n\nIn [126]: ts = pd.Series(data=list(range(10)), index=rng)\n\nIn [127]: def MyCust(x):\n   .....:     if len(x) > 2:\n   .....:         return x.iloc[1] * 1.234\n   .....:     return pd.NaT\n   .....: \n\nIn [128]: mhc = {\"Mean\": \"mean\", \"Max\": \"max\", \"Custom\": MyCust}\n\nIn [129]: ts.resample(\"5min\").apply(mhc)\nOut[129]: \n                     Mean  Max Custom\n2014-10-07 00:00:00   1.0    2  1.234\n2014-10-07 00:05:00   3.5    4    NaT\n2014-10-07 00:10:00   6.0    7  7.404\n2014-10-07 00:15:00   8.5    9    NaT\n\nIn [130]: ts\nOut[130]: \n2014-10-07 00:00:00    0\n2014-10-07 00:02:00    1\n2014-10-07 00:04:00    2\n2014-10-07 00:06:00    3\n2014-10-07 00:08:00    4\n2014-10-07 00:10:00    5\n2014-10-07 00:12:00    6\n2014-10-07 00:14:00    7\n2014-10-07 00:16:00    8\n2014-10-07 00:18:00    9\nFreq: 2min, dtype: int64",
      "In [131]: df = pd.DataFrame(\n   .....:     {\"Color\": \"Red Red Red Blue\".split(), \"Value\": [100, 150, 50, 50]}\n   .....: )\n   .....: \n\nIn [132]: df\nOut[132]: \n  Color  Value\n0   Red    100\n1   Red    150\n2   Red     50\n3  Blue     50\n\nIn [133]: df[\"Counts\"] = df.groupby([\"Color\"]).transform(len)\n\nIn [134]: df\nOut[134]: \n  Color  Value  Counts\n0   Red    100       3\n1   Red    150       3\n2   Red     50       3\n3  Blue     50       1",
      "In [135]: df = pd.DataFrame(\n   .....:     {\"line_race\": [10, 10, 8, 10, 10, 8], \"beyer\": [99, 102, 103, 103, 88, 100]},\n   .....:     index=[\n   .....:         \"Last Gunfighter\",\n   .....:         \"Last Gunfighter\",\n   .....:         \"Last Gunfighter\",\n   .....:         \"Paynter\",\n   .....:         \"Paynter\",\n   .....:         \"Paynter\",\n   .....:     ],\n   .....: )\n   .....: \n\nIn [136]: df\nOut[136]: \n                 line_race  beyer\nLast Gunfighter         10     99\nLast Gunfighter         10    102\nLast Gunfighter          8    103\nPaynter                 10    103\nPaynter                 10     88\nPaynter                  8    100\n\nIn [137]: df[\"beyer_shifted\"] = df.groupby(level=0)[\"beyer\"].shift(1)\n\nIn [138]: df\nOut[138]: \n                 line_race  beyer  beyer_shifted\nLast Gunfighter         10     99            NaN\nLast Gunfighter         10    102           99.0\nLast Gunfighter          8    103          102.0\nPaynter                 10    103            NaN\nPaynter                 10     88          103.0\nPaynter                  8    100           88.0",
      "In [139]: df = pd.DataFrame(\n   .....:     {\n   .....:         \"host\": [\"other\", \"other\", \"that\", \"this\", \"this\"],\n   .....:         \"service\": [\"mail\", \"web\", \"mail\", \"mail\", \"web\"],\n   .....:         \"no\": [1, 2, 1, 2, 1],\n   .....:     }\n   .....: ).set_index([\"host\", \"service\"])\n   .....: \n\nIn [140]: mask = df.groupby(level=0).agg(\"idxmax\")\n\nIn [141]: df_count = df.loc[mask[\"no\"]].reset_index()\n\nIn [142]: df_count\nOut[142]: \n    host service  no\n0  other     web   2\n1   that    mail   1\n2   this    mail   2",
      "In [143]: df = pd.DataFrame([0, 1, 0, 1, 1, 1, 0, 1, 1], columns=[\"A\"])\n\nIn [144]: df[\"A\"].groupby((df[\"A\"] != df[\"A\"].shift()).cumsum()).groups\nOut[144]: {1: [0], 2: [1], 3: [2], 4: [3, 4, 5], 5: [6], 6: [7, 8]}\n\nIn [145]: df[\"A\"].groupby((df[\"A\"] != df[\"A\"].shift()).cumsum()).cumsum()\nOut[145]: \n0    0\n1    1\n2    0\n3    1\n4    2\n5    3\n6    0\n7    1\n8    2\nName: A, dtype: int64",
      "In [146]: df = pd.DataFrame(\n   .....:     data={\n   .....:         \"Case\": [\"A\", \"A\", \"A\", \"B\", \"A\", \"A\", \"B\", \"A\", \"A\"],\n   .....:         \"Data\": np.random.randn(9),\n   .....:     }\n   .....: )\n   .....: \n\nIn [147]: dfs = list(\n   .....:     zip(\n   .....:         *df.groupby(\n   .....:             (1 * (df[\"Case\"] == \"B\"))\n   .....:             .cumsum()\n   .....:             .rolling(window=3, min_periods=1)\n   .....:             .median()\n   .....:         )\n   .....:     )\n   .....: )[-1]\n   .....: \n\nIn [148]: dfs[0]\nOut[148]: \n  Case      Data\n0    A  0.276232\n1    A -1.087401\n2    A -0.673690\n3    B  0.113648\n\nIn [149]: dfs[1]\nOut[149]: \n  Case      Data\n4    A -1.478427\n5    A  0.524988\n6    B  0.404705\n\nIn [150]: dfs[2]\nOut[150]: \n  Case      Data\n7    A  0.577046\n8    A -1.715002",
      "In [151]: df = pd.DataFrame(\n   .....:     data={\n   .....:         \"Province\": [\"ON\", \"QC\", \"BC\", \"AL\", \"AL\", \"MN\", \"ON\"],\n   .....:         \"City\": [\n   .....:             \"Toronto\",\n   .....:             \"Montreal\",\n   .....:             \"Vancouver\",\n   .....:             \"Calgary\",\n   .....:             \"Edmonton\",\n   .....:             \"Winnipeg\",\n   .....:             \"Windsor\",\n   .....:         ],\n   .....:         \"Sales\": [13, 6, 16, 8, 4, 3, 1],\n   .....:     }\n   .....: )\n   .....: \n\nIn [152]: table = pd.pivot_table(\n   .....:     df,\n   .....:     values=[\"Sales\"],\n   .....:     index=[\"Province\"],\n   .....:     columns=[\"City\"],\n   .....:     aggfunc=\"sum\",\n   .....:     margins=True,\n   .....: )\n   .....: \n\nIn [153]: table.stack(\"City\", future_stack=True)\nOut[153]: \n                    Sales\nProvince City            \nAL       Calgary      8.0\n         Edmonton     4.0\n         Montreal     NaN\n         Toronto      NaN\n         Vancouver    NaN\n...                   ...\nAll      Toronto     13.0\n         Vancouver   16.0\n         Windsor      1.0\n         Winnipeg     3.0\n         All         51.0\n\n[48 rows x 1 columns]",
      "In [154]: grades = [48, 99, 75, 80, 42, 80, 72, 68, 36, 78]\n\nIn [155]: df = pd.DataFrame(\n   .....:     {\n   .....:         \"ID\": [\"x%d\" % r for r in range(10)],\n   .....:         \"Gender\": [\"F\", \"M\", \"F\", \"M\", \"F\", \"M\", \"F\", \"M\", \"M\", \"M\"],\n   .....:         \"ExamYear\": [\n   .....:             \"2007\",\n   .....:             \"2007\",\n   .....:             \"2007\",\n   .....:             \"2008\",\n   .....:             \"2008\",\n   .....:             \"2008\",\n   .....:             \"2008\",\n   .....:             \"2009\",\n   .....:             \"2009\",\n   .....:             \"2009\",\n   .....:         ],\n   .....:         \"Class\": [\n   .....:             \"algebra\",\n   .....:             \"stats\",\n   .....:             \"bio\",\n   .....:             \"algebra\",\n   .....:             \"algebra\",\n   .....:             \"stats\",\n   .....:             \"stats\",\n   .....:             \"algebra\",\n   .....:             \"bio\",\n   .....:             \"bio\",\n   .....:         ],\n   .....:         \"Participated\": [\n   .....:             \"yes\",\n   .....:             \"yes\",\n   .....:             \"yes\",\n   .....:             \"yes\",\n   .....:             \"no\",\n   .....:             \"yes\",\n   .....:             \"yes\",\n   .....:             \"yes\",\n   .....:             \"yes\",\n   .....:             \"yes\",\n   .....:         ],\n   .....:         \"Passed\": [\"yes\" if x > 50 else \"no\" for x in grades],\n   .....:         \"Employed\": [\n   .....:             True,\n   .....:             True,\n   .....:             True,\n   .....:             False,\n   .....:             False,\n   .....:             False,\n   .....:             False,\n   .....:             True,\n   .....:             True,\n   .....:             False,\n   .....:         ],\n   .....:         \"Grade\": grades,\n   .....:     }\n   .....: )\n   .....: \n\nIn [156]: df.groupby(\"ExamYear\").agg(\n   .....:     {\n   .....:         \"Participated\": lambda x: x.value_counts()[\"yes\"],\n   .....:         \"Passed\": lambda x: sum(x == \"yes\"),\n   .....:         \"Employed\": lambda x: sum(x),\n   .....:         \"Grade\": lambda x: sum(x) / len(x),\n   .....:     }\n   .....: )\n   .....: \nOut[156]: \n          Participated  Passed  Employed      Grade\nExamYear                                           \n2007                 3       2         3  74.000000\n2008                 3       3         0  68.500000\n2009                 3       2         2  60.666667",
      "In [157]: df = pd.DataFrame(\n   .....:     {\"value\": np.random.randn(36)},\n   .....:     index=pd.date_range(\"2011-01-01\", freq=\"ME\", periods=36),\n   .....: )\n   .....: \n\nIn [158]: pd.pivot_table(\n   .....:     df, index=df.index.month, columns=df.index.year, values=\"value\", aggfunc=\"sum\"\n   .....: )\n   .....: \nOut[158]: \n        2011      2012      2013\n1  -1.039268 -0.968914  2.565646\n2  -0.370647 -1.294524  1.431256\n3  -1.157892  0.413738  1.340309\n4  -1.344312  0.276662 -1.170299\n5   0.844885 -0.472035 -0.226169\n6   1.075770 -0.013960  0.410835\n7  -0.109050 -0.362543  0.813850\n8   1.643563 -0.006154  0.132003\n9  -1.469388 -0.923061 -0.827317\n10  0.357021  0.895717 -0.076467\n11 -0.674600  0.805244 -1.187678\n12 -1.776904 -1.206412  1.130127",
      "In [159]: df = pd.DataFrame(\n   .....:     data={\n   .....:         \"A\": [[2, 4, 8, 16], [100, 200], [10, 20, 30]],\n   .....:         \"B\": [[\"a\", \"b\", \"c\"], [\"jj\", \"kk\"], [\"ccc\"]],\n   .....:     },\n   .....:     index=[\"I\", \"II\", \"III\"],\n   .....: )\n   .....: \n\nIn [160]: def SeriesFromSubList(aList):\n   .....:     return pd.Series(aList)\n   .....: \n\nIn [161]: df_orgz = pd.concat(\n   .....:     {ind: row.apply(SeriesFromSubList) for ind, row in df.iterrows()}\n   .....: )\n   .....: \n\nIn [162]: df_orgz\nOut[162]: \n         0     1     2     3\nI   A    2     4     8  16.0\n    B    a     b     c   NaN\nII  A  100   200   NaN   NaN\n    B   jj    kk   NaN   NaN\nIII A   10  20.0  30.0   NaN\n    B  ccc   NaN   NaN   NaN",
      "In [163]: df = pd.DataFrame(\n   .....:     data=np.random.randn(2000, 2) / 10000,\n   .....:     index=pd.date_range(\"2001-01-01\", periods=2000),\n   .....:     columns=[\"A\", \"B\"],\n   .....: )\n   .....: \n\nIn [164]: df\nOut[164]: \n                   A         B\n2001-01-01 -0.000144 -0.000141\n2001-01-02  0.000161  0.000102\n2001-01-03  0.000057  0.000088\n2001-01-04 -0.000221  0.000097\n2001-01-05 -0.000201 -0.000041\n...              ...       ...\n2006-06-19  0.000040 -0.000235\n2006-06-20 -0.000123 -0.000021\n2006-06-21 -0.000113  0.000114\n2006-06-22  0.000136  0.000109\n2006-06-23  0.000027  0.000030\n\n[2000 rows x 2 columns]\n\nIn [165]: def gm(df, const):\n   .....:     v = ((((df[\"A\"] + df[\"B\"]) + 1).cumprod()) - 1) * const\n   .....:     return v.iloc[-1]\n   .....: \n\nIn [166]: s = pd.Series(\n   .....:     {\n   .....:         df.index[i]: gm(df.iloc[i: min(i + 51, len(df) - 1)], 5)\n   .....:         for i in range(len(df) - 50)\n   .....:     }\n   .....: )\n   .....: \n\nIn [167]: s\nOut[167]: \n2001-01-01    0.000930\n2001-01-02    0.002615\n2001-01-03    0.001281\n2001-01-04    0.001117\n2001-01-05    0.002772\n                ...   \n2006-04-30    0.003296\n2006-05-01    0.002629\n2006-05-02    0.002081\n2006-05-03    0.004247\n2006-05-04    0.003928\nLength: 1950, dtype: float64",
      "In [168]: rng = pd.date_range(start=\"2014-01-01\", periods=100)\n\nIn [169]: df = pd.DataFrame(\n   .....:     {\n   .....:         \"Open\": np.random.randn(len(rng)),\n   .....:         \"Close\": np.random.randn(len(rng)),\n   .....:         \"Volume\": np.random.randint(100, 2000, len(rng)),\n   .....:     },\n   .....:     index=rng,\n   .....: )\n   .....: \n\nIn [170]: df\nOut[170]: \n                Open     Close  Volume\n2014-01-01 -1.611353 -0.492885    1219\n2014-01-02 -3.000951  0.445794    1054\n2014-01-03 -0.138359 -0.076081    1381\n2014-01-04  0.301568  1.198259    1253\n2014-01-05  0.276381 -0.669831    1728\n...              ...       ...     ...\n2014-04-06 -0.040338  0.937843    1188\n2014-04-07  0.359661 -0.285908    1864\n2014-04-08  0.060978  1.714814     941\n2014-04-09  1.759055 -0.455942    1065\n2014-04-10  0.138185 -1.147008    1453\n\n[100 rows x 3 columns]\n\nIn [171]: def vwap(bars):\n   .....:     return (bars.Close * bars.Volume).sum() / bars.Volume.sum()\n   .....: \n\nIn [172]: window = 5\n\nIn [173]: s = pd.concat(\n   .....:     [\n   .....:         (pd.Series(vwap(df.iloc[i: i + window]), index=[df.index[i + window]]))\n   .....:         for i in range(len(df) - window)\n   .....:     ]\n   .....: )\n   .....: \n\nIn [174]: s.round(2)\nOut[174]: \n2014-01-06    0.02\n2014-01-07    0.11\n2014-01-08    0.10\n2014-01-09    0.07\n2014-01-10   -0.29\n              ... \n2014-04-06   -0.63\n2014-04-07   -0.02\n2014-04-08   -0.03\n2014-04-09    0.34\n2014-04-10    0.29\nLength: 95, dtype: float64",
      "In [175]: dates = pd.date_range(\"2000-01-01\", periods=5)\n\nIn [176]: dates.to_period(freq=\"M\").to_timestamp()\nOut[176]: \nDatetimeIndex(['2000-01-01', '2000-01-01', '2000-01-01', '2000-01-01',\n               '2000-01-01'],\n              dtype='datetime64[ns]', freq=None)",
      "In [177]: rng = pd.date_range(\"2000-01-01\", periods=6)\n\nIn [178]: df1 = pd.DataFrame(np.random.randn(6, 3), index=rng, columns=[\"A\", \"B\", \"C\"])\n\nIn [179]: df2 = df1.copy()",
      "In [180]: df = pd.concat([df1, df2], ignore_index=True)\n\nIn [181]: df\nOut[181]: \n           A         B         C\n0  -0.870117 -0.479265 -0.790855\n1   0.144817  1.726395 -0.464535\n2  -0.821906  1.597605  0.187307\n3  -0.128342 -1.511638 -0.289858\n4   0.399194 -1.430030 -0.639760\n5   1.115116 -2.012600  1.810662\n6  -0.870117 -0.479265 -0.790855\n7   0.144817  1.726395 -0.464535\n8  -0.821906  1.597605  0.187307\n9  -0.128342 -1.511638 -0.289858\n10  0.399194 -1.430030 -0.639760\n11  1.115116 -2.012600  1.810662",
      "In [182]: df = pd.DataFrame(\n   .....:     data={\n   .....:         \"Area\": [\"A\"] * 5 + [\"C\"] * 2,\n   .....:         \"Bins\": [110] * 2 + [160] * 3 + [40] * 2,\n   .....:         \"Test_0\": [0, 1, 0, 1, 2, 0, 1],\n   .....:         \"Data\": np.random.randn(7),\n   .....:     }\n   .....: )\n   .....: \n\nIn [183]: df\nOut[183]: \n  Area  Bins  Test_0      Data\n0    A   110       0 -0.433937\n1    A   110       1 -0.160552\n2    A   160       0  0.744434\n3    A   160       1  1.754213\n4    A   160       2  0.000850\n5    C    40       0  0.342243\n6    C    40       1  1.070599\n\nIn [184]: df[\"Test_1\"] = df[\"Test_0\"] - 1\n\nIn [185]: pd.merge(\n   .....:     df,\n   .....:     df,\n   .....:     left_on=[\"Bins\", \"Area\", \"Test_0\"],\n   .....:     right_on=[\"Bins\", \"Area\", \"Test_1\"],\n   .....:     suffixes=(\"_L\", \"_R\"),\n   .....: )\n   .....: \nOut[185]: \n  Area  Bins  Test_0_L    Data_L  Test_1_L  Test_0_R    Data_R  Test_1_R\n0    A   110         0 -0.433937        -1         1 -0.160552         0\n1    A   160         0  0.744434        -1         1  1.754213         0\n2    A   160         1  1.754213         0         2  0.000850         1\n3    C    40         0  0.342243        -1         1  1.070599         0",
      "In [186]: df = pd.DataFrame(\n   .....:     {\n   .....:         \"stratifying_var\": np.random.uniform(0, 100, 20),\n   .....:         \"price\": np.random.normal(100, 5, 20),\n   .....:     }\n   .....: )\n   .....: \n\nIn [187]: df[\"quartiles\"] = pd.qcut(\n   .....:     df[\"stratifying_var\"], 4, labels=[\"0-25%\", \"25-50%\", \"50-75%\", \"75-100%\"]\n   .....: )\n   .....: \n\nIn [188]: df.boxplot(column=\"price\", by=\"quartiles\")\nOut[188]: <Axes: title={'center': 'price'}, xlabel='quartiles'>",
      "In [189]: for i in range(3):\n   .....:     data = pd.DataFrame(np.random.randn(10, 4))\n   .....:     data.to_csv(\"file_{}.csv\".format(i))\n   .....: \n\nIn [190]: files = [\"file_0.csv\", \"file_1.csv\", \"file_2.csv\"]\n\nIn [191]: result = pd.concat([pd.read_csv(f) for f in files], ignore_index=True)",
      "In [192]: import glob\n\nIn [193]: import os\n\nIn [194]: files = glob.glob(\"file_*.csv\")\n\nIn [195]: result = pd.concat([pd.read_csv(f) for f in files], ignore_index=True)",
      "In [196]: i = pd.date_range(\"20000101\", periods=10000)\n\nIn [197]: df = pd.DataFrame({\"year\": i.year, \"month\": i.month, \"day\": i.day})\n\nIn [198]: df.head()\nOut[198]: \n   year  month  day\n0  2000      1    1\n1  2000      1    2\n2  2000      1    3\n3  2000      1    4\n4  2000      1    5\n\nIn [199]: %timeit pd.to_datetime(df.year * 10000 + df.month * 100 + df.day, format='%Y%m%d')\n   .....: ds = df.apply(lambda x: \"%04d%02d%02d\" % (x[\"year\"], x[\"month\"], x[\"day\"]), axis=1)\n   .....: ds.head()\n   .....: %timeit pd.to_datetime(ds)\n   .....: \n2.7 ms +- 240 us per loop (mean +- std. dev. of 7 runs, 100 loops each)\n1.09 ms +- 5.62 us per loop (mean +- std. dev. of 7 runs, 1,000 loops each)",
      "In [201]: from io import StringIO\n\nIn [202]: pd.read_csv(\n   .....:     StringIO(data),\n   .....:     sep=\";\",\n   .....:     skiprows=[11, 12],\n   .....:     index_col=0,\n   .....:     parse_dates=True,\n   .....:     header=10,\n   .....: )\n   .....: \nOut[202]: \n                     Param1  Param2  Param4  Param5\ndate                                               \n1990-01-01 00:00:00       1       1       2       3\n1990-01-01 01:00:00       5       3       4       5\n1990-01-01 02:00:00       9       5       6       7\n1990-01-01 03:00:00      13       7       8       9\n1990-01-01 04:00:00      17       9      10      11\n1990-01-01 05:00:00      21      11      12      13",
      "In [203]: pd.read_csv(StringIO(data), sep=\";\", header=10, nrows=10).columns\nOut[203]: Index(['date', 'Param1', 'Param2', 'Param4', 'Param5'], dtype='object')\n\nIn [204]: columns = pd.read_csv(StringIO(data), sep=\";\", header=10, nrows=10).columns\n\nIn [205]: pd.read_csv(\n   .....:     StringIO(data), sep=\";\", index_col=0, header=12, parse_dates=True, names=columns\n   .....: )\n   .....: \nOut[205]: \n                     Param1  Param2  Param4  Param5\ndate                                               \n1990-01-01 00:00:00       1       1       2       3\n1990-01-01 01:00:00       5       3       4       5\n1990-01-01 02:00:00       9       5       6       7\n1990-01-01 03:00:00      13       7       8       9\n1990-01-01 04:00:00      17       9      10      11\n1990-01-01 05:00:00      21      11      12      13",
      "In [206]: df = pd.DataFrame(np.random.randn(8, 3))\n\nIn [207]: store = pd.HDFStore(\"test.h5\")\n\nIn [208]: store.put(\"df\", df)\n\n# you can store an arbitrary Python object via pickle\nIn [209]: store.get_storer(\"df\").attrs.my_attribute = {\"A\": 10}\n\nIn [210]: store.get_storer(\"df\").attrs.my_attribute\nOut[210]: {'A': 10}",
      "In [211]: store = pd.HDFStore(\"test.h5\", \"w\", driver=\"H5FD_CORE\")\n\nIn [212]: df = pd.DataFrame(np.random.randn(8, 3))\n\nIn [213]: store[\"test\"] = df\n\n# only after closing the store, data is written to disk:\nIn [214]: store.close()",
      "#include <stdio.h>\n#include <stdint.h>\n\ntypedef struct _Data\n{\n    int32_t count;\n    double avg;\n    float scale;\n} Data;\n\nint main(int argc, const char *argv[])\n{\n    size_t n = 10;\n    Data d[n];\n\n    for (int i = 0; i < n; ++i)\n    {\n        d[i].count = i;\n        d[i].avg = i + 1.0;\n        d[i].scale = (float) i + 2.0f;\n    }\n\n    FILE *file = fopen(\"binary.dat\", \"wb\");\n    fwrite(&d, sizeof(Data), n, file);\n    fclose(file);\n\n    return 0;\n}",
      "names = \"count\", \"avg\", \"scale\"\n\n# note that the offsets are larger than the size of the type because of\n# struct padding\noffsets = 0, 8, 16\nformats = \"i4\", \"f8\", \"f4\"\ndt = np.dtype({\"names\": names, \"offsets\": offsets, \"formats\": formats}, align=True)\ndf = pd.DataFrame(np.fromfile(\"binary.dat\", dt))",
      "In [215]: df = pd.DataFrame(np.random.random(size=(100, 5)))\n\nIn [216]: corr_mat = df.corr()\n\nIn [217]: mask = np.tril(np.ones_like(corr_mat, dtype=np.bool_), k=-1)\n\nIn [218]: corr_mat.where(mask)\nOut[218]: \n          0         1         2        3   4\n0       NaN       NaN       NaN      NaN NaN\n1 -0.079861       NaN       NaN      NaN NaN\n2 -0.236573  0.183801       NaN      NaN NaN\n3 -0.013795 -0.051975  0.037235      NaN NaN\n4 -0.031974  0.118342 -0.073499 -0.02063 NaN",
      "In [219]: def distcorr(x, y):\n   .....:     n = len(x)\n   .....:     a = np.zeros(shape=(n, n))\n   .....:     b = np.zeros(shape=(n, n))\n   .....:     for i in range(n):\n   .....:         for j in range(i + 1, n):\n   .....:             a[i, j] = abs(x[i] - x[j])\n   .....:             b[i, j] = abs(y[i] - y[j])\n   .....:     a += a.T\n   .....:     b += b.T\n   .....:     a_bar = np.vstack([np.nanmean(a, axis=0)] * n)\n   .....:     b_bar = np.vstack([np.nanmean(b, axis=0)] * n)\n   .....:     A = a - a_bar - a_bar.T + np.full(shape=(n, n), fill_value=a_bar.mean())\n   .....:     B = b - b_bar - b_bar.T + np.full(shape=(n, n), fill_value=b_bar.mean())\n   .....:     cov_ab = np.sqrt(np.nansum(A * B)) / n\n   .....:     std_a = np.sqrt(np.sqrt(np.nansum(A ** 2)) / n)\n   .....:     std_b = np.sqrt(np.sqrt(np.nansum(B ** 2)) / n)\n   .....:     return cov_ab / std_a / std_b\n   .....: \n\nIn [220]: df = pd.DataFrame(np.random.normal(size=(100, 3)))\n\nIn [221]: df.corr(method=distcorr)\nOut[221]: \n          0         1         2\n0  1.000000  0.197613  0.216328\n1  0.197613  1.000000  0.208749\n2  0.216328  0.208749  1.000000",
      "In [222]: import datetime\n\nIn [223]: s = pd.Series(pd.date_range(\"2012-1-1\", periods=3, freq=\"D\"))\n\nIn [224]: s - s.max()\nOut[224]: \n0   -2 days\n1   -1 days\n2    0 days\ndtype: timedelta64[ns]\n\nIn [225]: s.max() - s\nOut[225]: \n0   2 days\n1   1 days\n2   0 days\ndtype: timedelta64[ns]\n\nIn [226]: s - datetime.datetime(2011, 1, 1, 3, 5)\nOut[226]: \n0   364 days 20:55:00\n1   365 days 20:55:00\n2   366 days 20:55:00\ndtype: timedelta64[ns]\n\nIn [227]: s + datetime.timedelta(minutes=5)\nOut[227]: \n0   2012-01-01 00:05:00\n1   2012-01-02 00:05:00\n2   2012-01-03 00:05:00\ndtype: datetime64[ns]\n\nIn [228]: datetime.datetime(2011, 1, 1, 3, 5) - s\nOut[228]: \n0   -365 days +03:05:00\n1   -366 days +03:05:00\n2   -367 days +03:05:00\ndtype: timedelta64[ns]\n\nIn [229]: datetime.timedelta(minutes=5) + s\nOut[229]: \n0   2012-01-01 00:05:00\n1   2012-01-02 00:05:00\n2   2012-01-03 00:05:00\ndtype: datetime64[ns]",
      "In [230]: deltas = pd.Series([datetime.timedelta(days=i) for i in range(3)])\n\nIn [231]: df = pd.DataFrame({\"A\": s, \"B\": deltas})\n\nIn [232]: df\nOut[232]: \n           A      B\n0 2012-01-01 0 days\n1 2012-01-02 1 days\n2 2012-01-03 2 days\n\nIn [233]: df[\"New Dates\"] = df[\"A\"] + df[\"B\"]\n\nIn [234]: df[\"Delta\"] = df[\"A\"] - df[\"New Dates\"]\n\nIn [235]: df\nOut[235]: \n           A      B  New Dates   Delta\n0 2012-01-01 0 days 2012-01-01  0 days\n1 2012-01-02 1 days 2012-01-03 -1 days\n2 2012-01-03 2 days 2012-01-05 -2 days\n\nIn [236]: df.dtypes\nOut[236]: \nA             datetime64[ns]\nB            timedelta64[ns]\nNew Dates     datetime64[ns]\nDelta        timedelta64[ns]\ndtype: object",
      "In [241]: def expand_grid(data_dict):\n   .....:     rows = itertools.product(*data_dict.values())\n   .....:     return pd.DataFrame.from_records(rows, columns=data_dict.keys())\n   .....: \n\nIn [242]: df = expand_grid(\n   .....:     {\"height\": [60, 70], \"weight\": [100, 140, 180], \"sex\": [\"Male\", \"Female\"]}\n   .....: )\n   .....: \n\nIn [243]: df\nOut[243]: \n    height  weight     sex\n0       60     100    Male\n1       60     100  Female\n2       60     140    Male\n3       60     140  Female\n4       60     180    Male\n5       60     180  Female\n6       70     100    Male\n7       70     100  Female\n8       70     140    Male\n9       70     140  Female\n10      70     180    Male\n11      70     180  Female",
      "In [248]: v = s.to_numpy()\n\nIn [249]: is_constant = v.shape[0] == 0 or (s[0] == s).all() or not pd.notna(v).any()"
    ]
  }
]