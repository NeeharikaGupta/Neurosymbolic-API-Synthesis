[
  {
    "url": "https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct",
    "title": "meta-llama\n\n/\nLlama-3.2-3B-Instruct\n\n\n\nlike\n1.48k\nFollow\n\nMeta Llama\n45.1k",
    "code_snippets": [
      "import torch\nfrom transformers import pipeline\n\nmodel_id = \"meta-llama/Llama-3.2-3B-Instruct\"\npipe = pipeline(\n    \"text-generation\",\n    model=model_id,\n    torch_dtype=torch.bfloat16,\n    device_map=\"auto\",\n)\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n    {\"role\": \"user\", \"content\": \"Who are you?\"},\n]\noutputs = pipe(\n    messages,\n    max_new_tokens=256,\n)\nprint(outputs[0][\"generated_text\"][-1])"
    ]
  },
  {
    "url": "https://huggingface.co/google/gemma-3-27b-it",
    "title": "google\n\n/\ngemma-3-27b-it\n\n\n\nlike\n1.38k\nFollow\n\nGoogle\n14.2k",
    "code_snippets": [
      "from transformers import pipeline\nimport torch\n\npipe = pipeline(\n    \"image-text-to-text\",\n    model=\"google/gemma-3-27b-it\",\n    device=\"cuda\",\n    torch_dtype=torch.bfloat16\n)",
      "# pip install accelerate\n\nfrom transformers import AutoProcessor, Gemma3ForConditionalGeneration\nfrom PIL import Image\nimport requests\nimport torch\n\nmodel_id = \"google/gemma-3-27b-it\"\n\nmodel = Gemma3ForConditionalGeneration.from_pretrained(\n    model_id, device_map=\"auto\"\n).eval()\n\nprocessor = AutoProcessor.from_pretrained(model_id)\n\nmessages = [\n    {\n        \"role\": \"system\",\n        \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}]\n    },\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"image\", \"image\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee.jpg\"},\n            {\"type\": \"text\", \"text\": \"Describe this image in detail.\"}\n        ]\n    }\n]\n\ninputs = processor.apply_chat_template(\n    messages, add_generation_prompt=True, tokenize=True,\n    return_dict=True, return_tensors=\"pt\"\n).to(model.device, dtype=torch.bfloat16)\n\ninput_len = inputs[\"input_ids\"].shape[-1]\n\nwith torch.inference_mode():\n    generation = model.generate(**inputs, max_new_tokens=100, do_sample=False)\n    generation = generation[0][input_len:]\n\ndecoded = processor.decode(generation, skip_special_tokens=True)\nprint(decoded)\n\n# **Overall Impression:** The image is a close-up shot of a vibrant garden scene, \n# focusing on a cluster of pink cosmos flowers and a busy bumblebee. \n# It has a slightly soft, natural feel, likely captured in daylight."
    ]
  },
  {
    "url": "https://huggingface.co/google/gemma-3-1b-it",
    "title": "google\n\n/\ngemma-3-1b-it\n\n\n\nlike\n430\nFollow\n\nGoogle\n14.2k",
    "code_snippets": [
      "from transformers import pipeline\nimport torch\n\npipe = pipeline(\"text-generation\", model=\"google/gemma-3-1b-it\", device=\"cuda\", torch_dtype=torch.bfloat16)\n\nmessages = [\n    [\n        {\n            \"role\": \"system\",\n            \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"},]\n        },\n        {\n            \"role\": \"user\",\n            \"content\": [{\"type\": \"text\", \"text\": \"Write a poem on Hugging Face, the company\"},]\n        },\n    ],\n]\n\noutput = pipe(messages, max_new_tokens=50)",
      "from transformers import AutoTokenizer, BitsAndBytesConfig, Gemma3ForCausalLM\nimport torch\n\nmodel_id = \"google/gemma-3-1b-it\"\n\nquantization_config = BitsAndBytesConfig(load_in_8bit=True)\n\nmodel = Gemma3ForCausalLM.from_pretrained(\n    model_id, quantization_config=quantization_config\n).eval()\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\nmessages = [\n    [\n        {\n            \"role\": \"system\",\n            \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"},]\n        },\n        {\n            \"role\": \"user\",\n            \"content\": [{\"type\": \"text\", \"text\": \"Write a poem on Hugging Face, the company\"},]\n        },\n    ],\n]\ninputs = tokenizer.apply_chat_template(\n    messages,\n    add_generation_prompt=True,\n    tokenize=True,\n    return_dict=True,\n    return_tensors=\"pt\",\n).to(model.device).to(torch.bfloat16)\n\n\nwith torch.inference_mode():\n    outputs = model.generate(**inputs, max_new_tokens=64)\n\noutputs = tokenizer.batch_decode(outputs)"
    ]
  },
  {
    "url": "https://huggingface.co/Qwen/Qwen3-0.6B",
    "title": "Qwen\n\n/\nQwen3-0.6B\n\n\n\nlike\n292\nFollow\n\nQwen\n32.3k",
    "code_snippets": [
      "from transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"Qwen/Qwen3-0.6B\"\n\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\n\n# prepare the model input\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n    enable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\n# conduct text completion\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=32768\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n\n# parsing thinking content\ntry:\n    # rindex finding 151668 (</think>)\n    index = len(output_ids) - output_ids[::-1].index(151668)\nexcept ValueError:\n    index = 0\n\nthinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\ncontent = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n\nprint(\"thinking content:\", thinking_content)\nprint(\"content:\", content)",
      "from transformers import AutoModelForCausalLM, AutoTokenizer\n\nclass QwenChatbot:\n    def __init__(self, model_name=\"Qwen/Qwen3-0.6B\"):\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.model = AutoModelForCausalLM.from_pretrained(model_name)\n        self.history = []\n\n    def generate_response(self, user_input):\n        messages = self.history + [{\"role\": \"user\", \"content\": user_input}]\n\n        text = self.tokenizer.apply_chat_template(\n            messages,\n            tokenize=False,\n            add_generation_prompt=True\n        )\n\n        inputs = self.tokenizer(text, return_tensors=\"pt\")\n        response_ids = self.model.generate(**inputs, max_new_tokens=32768)[0][len(inputs.input_ids[0]):].tolist()\n        response = self.tokenizer.decode(response_ids, skip_special_tokens=True)\n\n        # Update history\n        self.history.append({\"role\": \"user\", \"content\": user_input})\n        self.history.append({\"role\": \"assistant\", \"content\": response})\n\n        return response\n\n# Example Usage\nif __name__ == \"__main__\":\n    chatbot = QwenChatbot()\n\n    # First input (without /think or /no_think tags, thinking mode is enabled by default)\n    user_input_1 = \"How many r's in strawberries?\"\n    print(f\"User: {user_input_1}\")\n    response_1 = chatbot.generate_response(user_input_1)\n    print(f\"Bot: {response_1}\")\n    print(\"----------------------\")\n\n    # Second input with /no_think\n    user_input_2 = \"Then, how many r's in blueberries? /no_think\"\n    print(f\"User: {user_input_2}\")\n    response_2 = chatbot.generate_response(user_input_2)\n    print(f\"Bot: {response_2}\") \n    print(\"----------------------\")\n\n    # Third input with /think\n    user_input_3 = \"Really? /think\"\n    print(f\"User: {user_input_3}\")\n    response_3 = chatbot.generate_response(user_input_3)\n    print(f\"Bot: {response_3}\")"
    ]
  },
  {
    "url": "https://huggingface.co/ds4sd/SmolDocling-256M-preview",
    "title": "ds4sd\n\n/\nSmolDocling-256M-preview\n\n\n\nlike\n1.39k\nFollow\n\nDocling\n676",
    "code_snippets": [
      "# Prerequisites:\n# pip install torch\n# pip install docling_core\n# pip install transformers\n\nimport torch\nfrom docling_core.types.doc import DoclingDocument\nfrom docling_core.types.doc.document import DocTagsDocument\nfrom transformers import AutoProcessor, AutoModelForVision2Seq\nfrom transformers.image_utils import load_image\nfrom pathlib import Path\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Load images\nimage = load_image(\"https://upload.wikimedia.org/wikipedia/commons/7/76/GazettedeFrance.jpg\")\n\n# Initialize processor and model\nprocessor = AutoProcessor.from_pretrained(\"ds4sd/SmolDocling-256M-preview\")\nmodel = AutoModelForVision2Seq.from_pretrained(\n    \"ds4sd/SmolDocling-256M-preview\",\n    torch_dtype=torch.bfloat16,\n    _attn_implementation=\"flash_attention_2\" if DEVICE == \"cuda\" else \"eager\",\n).to(DEVICE)\n\n# Create input messages\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"image\"},\n            {\"type\": \"text\", \"text\": \"Convert this page to docling.\"}\n        ]\n    },\n]\n\n# Prepare inputs\nprompt = processor.apply_chat_template(messages, add_generation_prompt=True)\ninputs = processor(text=prompt, images=[image], return_tensors=\"pt\")\ninputs = inputs.to(DEVICE)\n\n# Generate outputs\ngenerated_ids = model.generate(**inputs, max_new_tokens=8192)\nprompt_length = inputs.input_ids.shape[1]\ntrimmed_generated_ids = generated_ids[:, prompt_length:]\ndoctags = processor.batch_decode(\n    trimmed_generated_ids,\n    skip_special_tokens=False,\n)[0].lstrip()\n\n# Populate document\ndoctags_doc = DocTagsDocument.from_doctags_and_image_pairs([doctags], [image])\nprint(doctags)\n# create a docling document\ndoc = DoclingDocument.load_from_doctags(doctags_doc, document_name=\"Document\")\n\n# export as any format\n# HTML\n# Path(\"Out/\").mkdir(parents=True, exist_ok=True)\n# output_path_html = Path(\"Out/\") / \"example.html\"\n# doc.save_as_html(output_path_html)\n# MD\nprint(doc.export_to_markdown())",
      "# Prerequisites:\n# pip install onnxruntime\n# pip install onnxruntime-gpu\nfrom transformers import AutoConfig, AutoProcessor\nfrom transformers.image_utils import load_image\nimport onnxruntime\nimport numpy as np\nimport os\nfrom docling_core.types.doc import DoclingDocument\nfrom docling_core.types.doc.document import DocTagsDocument\n\nos.environ[\"OMP_NUM_THREADS\"] = \"1\"\n# cuda\nos.environ[\"ORT_CUDA_USE_MAX_WORKSPACE\"] = \"1\"\n\n# 1. Load models\n## Load config and processor\nmodel_id = \"ds4sd/SmolDocling-256M-preview\"\nconfig = AutoConfig.from_pretrained(model_id)\nprocessor = AutoProcessor.from_pretrained(model_id)\n\n## Load sessions\n# !wget https://huggingface.co/ds4sd/SmolDocling-256M-preview/resolve/main/onnx/vision_encoder.onnx\n# !wget https://huggingface.co/ds4sd/SmolDocling-256M-preview/resolve/main/onnx/embed_tokens.onnx\n# !wget https://huggingface.co/ds4sd/SmolDocling-256M-preview/resolve/main/onnx/decoder_model_merged.onnx\n# cpu\n# vision_session = onnxruntime.InferenceSession(\"vision_encoder.onnx\")\n# embed_session = onnxruntime.InferenceSession(\"embed_tokens.onnx\")\n# decoder_session = onnxruntime.InferenceSession(\"decoder_model_merged.onnx\"\n\n# cuda\nvision_session = onnxruntime.InferenceSession(\"vision_encoder.onnx\", providers=[\"CUDAExecutionProvider\"])\nembed_session = onnxruntime.InferenceSession(\"embed_tokens.onnx\", providers=[\"CUDAExecutionProvider\"])\ndecoder_session = onnxruntime.InferenceSession(\"decoder_model_merged.onnx\", providers=[\"CUDAExecutionProvider\"])\n\n## Set config values\nnum_key_value_heads = config.text_config.num_key_value_heads\nhead_dim = config.text_config.head_dim\nnum_hidden_layers = config.text_config.num_hidden_layers\neos_token_id = config.text_config.eos_token_id\nimage_token_id = config.image_token_id\nend_of_utterance_id = processor.tokenizer.convert_tokens_to_ids(\"<end_of_utterance>\")\n\n# 2. Prepare inputs\n## Create input messages\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"image\"},\n            {\"type\": \"text\", \"text\": \"Convert this page to docling.\"}\n        ]\n    },\n]\n\n## Load image and apply processor\nimage = load_image(\"https://ibm.biz/docling-page-with-table\")\nprompt = processor.apply_chat_template(messages, add_generation_prompt=True)\ninputs = processor(text=prompt, images=[image], return_tensors=\"np\")\n\n## Prepare decoder inputs\nbatch_size = inputs['input_ids'].shape[0]\npast_key_values = {\n    f'past_key_values.{layer}.{kv}': np.zeros([batch_size, num_key_value_heads, 0, head_dim], dtype=np.float32)\n    for layer in range(num_hidden_layers)\n    for kv in ('key', 'value')\n}\nimage_features = None\ninput_ids = inputs['input_ids']\nattention_mask = inputs['attention_mask']\nposition_ids = np.cumsum(inputs['attention_mask'], axis=-1)\n\n\n# 3. Generation loop\nmax_new_tokens = 8192\ngenerated_tokens = np.array([[]], dtype=np.int64)\nfor i in range(max_new_tokens):\n  inputs_embeds = embed_session.run(None, {'input_ids': input_ids})[0]\n\n  if image_features is None:\n    ## Only compute vision features if not already computed\n    image_features = vision_session.run(\n        ['image_features'],  # List of output names or indices\n        {\n            'pixel_values': inputs['pixel_values'],\n            'pixel_attention_mask': inputs['pixel_attention_mask'].astype(np.bool_)\n        }\n    )[0]\n    \n    ## Merge text and vision embeddings\n    inputs_embeds[inputs['input_ids'] == image_token_id] = image_features.reshape(-1, image_features.shape[-1])\n\n  logits, *present_key_values = decoder_session.run(None, dict(\n      inputs_embeds=inputs_embeds,\n      attention_mask=attention_mask,\n      position_ids=position_ids,\n      **past_key_values,\n  ))\n\n  ## Update values for next generation loop\n  input_ids = logits[:, -1].argmax(-1, keepdims=True)\n  attention_mask = np.ones_like(input_ids)\n  position_ids = position_ids[:, -1:] + 1\n  for j, key in enumerate(past_key_values):\n    past_key_values[key] = present_key_values[j]\n\n  generated_tokens = np.concatenate([generated_tokens, input_ids], axis=-1)\n  if (input_ids == eos_token_id).all() or (input_ids == end_of_utterance_id).all():\n    break  # Stop predicting\n\ndoctags = processor.batch_decode(\n    generated_tokens,\n    skip_special_tokens=False,\n)[0].lstrip()\n\nprint(doctags)\n\ndoctags_doc = DocTagsDocument.from_doctags_and_image_pairs([doctags], [image])\nprint(doctags)\n# create a docling document\ndoc = DoclingDocument.load_from_doctags(doctags_doc, document_name=\"Document\")\n\nprint(doc.export_to_markdown())"
    ]
  },
  {
    "url": "https://huggingface.co/tiiuae/Falcon-H1-34B-Instruct",
    "title": "tiiuae\n\n/\nFalcon-H1-34B-Instruct\n\n\n\nlike\n27\nFollow\n\nTechnology Innovation Institute\n1.4k",
    "code_snippets": [
      "import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_id = \"tiiuae/Falcon-H1-1B-Base\"\n\nmodel = AutoModelForCausalLM.from_pretrained(\n  model_id,\n  torch_dtype=torch.bfloat16,\n  device_map=\"auto\"\n)\n\n# Perform text generation"
    ]
  },
  {
    "url": "https://huggingface.co/a-m-team/AM-Thinking-v1",
    "title": "a-m-team\n\n/\nAM-Thinking-v1\n\n\n\nlike\n176\nFollow\n\nam team\n156",
    "code_snippets": [
      "from transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"a-m-team/AM-Thinking-v1\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\n\nprompt = \"How can I find inner peace?\"\nmessages = [\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=49152\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n\nresponse = tokenizer.decode(output_ids, skip_special_tokens=True)\nthink_content = response.split(\"<think>\")[1].split(\"</think>\")[0]\nanswer_content = response.split(\"<answer>\")[1].split(\"</answer>\")[0]\n\nprint (f\"user prompt: {prompt}\")\nprint (f\"model thinking: {think_content}\")\nprint (f\"model answer: {answer_content}\")"
    ]
  },
  {
    "url": "https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3",
    "title": "mistralai\n\n/\nMistral-7B-Instruct-v0.3\n\n\n\nlike\n1.74k\nFollow\n\nMistral AI_\n8.74k",
    "code_snippets": [
      "from transformers import pipeline\n\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n    {\"role\": \"user\", \"content\": \"Who are you?\"},\n]\nchatbot = pipeline(\"text-generation\", model=\"mistralai/Mistral-7B-Instruct-v0.3\")\nchatbot(messages)",
      "from transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\n\nmodel_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\ndef get_current_weather(location: str, format: str):\n    \"\"\"\n    Get the current weather\n\n    Args:\n        location: The city and state, e.g. San Francisco, CA\n        format: The temperature unit to use. Infer this from the users location. (choices: [\"celsius\", \"fahrenheit\"])\n    \"\"\"\n    pass\n\nconversation = [{\"role\": \"user\", \"content\": \"What's the weather like in Paris?\"}]\ntools = [get_current_weather]\n\n\n# format and tokenize the tool use prompt \ninputs = tokenizer.apply_chat_template(\n            conversation,\n            tools=tools,\n            add_generation_prompt=True,\n            return_dict=True,\n            return_tensors=\"pt\",\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"auto\")\n\ninputs.to(model.device)\noutputs = model.generate(**inputs, max_new_tokens=1000)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))"
    ]
  },
  {
    "url": "https://huggingface.co/stabilityai/stable-diffusion-3.5-large",
    "title": "stabilityai\n\n/\nstable-diffusion-3.5-large\n\n\n\nlike\n2.82k\nFollow\n\nStability AI\n23.8k",
    "code_snippets": [
      "from diffusers import BitsAndBytesConfig, SD3Transformer2DModel\nfrom diffusers import StableDiffusion3Pipeline\nimport torch\n\nmodel_id = \"stabilityai/stable-diffusion-3.5-large\"\n\nnf4_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16\n)\nmodel_nf4 = SD3Transformer2DModel.from_pretrained(\n    model_id,\n    subfolder=\"transformer\",\n    quantization_config=nf4_config,\n    torch_dtype=torch.bfloat16\n)\n\npipeline = StableDiffusion3Pipeline.from_pretrained(\n    model_id, \n    transformer=model_nf4,\n    torch_dtype=torch.bfloat16\n)\npipeline.enable_model_cpu_offload()\n\nprompt = \"A whimsical and creative image depicting a hybrid creature that is a mix of a waffle and a hippopotamus, basking in a river of melted butter amidst a breakfast-themed landscape. It features the distinctive, bulky body shape of a hippo. However, instead of the usual grey skin, the creature's body resembles a golden-brown, crispy waffle fresh off the griddle. The skin is textured with the familiar grid pattern of a waffle, each square filled with a glistening sheen of syrup. The environment combines the natural habitat of a hippo with elements of a breakfast table setting, a river of warm, melted butter, with oversized utensils or plates peeking out from the lush, pancake-like foliage in the background, a towering pepper mill standing in for a tree.  As the sun rises in this fantastical world, it casts a warm, buttery glow over the scene. The creature, content in its butter river, lets out a yawn. Nearby, a flock of birds take flight\"\n\nimage = pipeline(\n    prompt=prompt,\n    num_inference_steps=28,\n    guidance_scale=4.5,\n    max_sequence_length=512,\n).images[0]\nimage.save(\"whimsical.png\")"
    ]
  },
  {
    "url": "https://huggingface.co/aleksa-codes/flux-ghibsky-illustration",
    "title": "aleksa-codes\n\n/\nflux-ghibsky-illustration\n\n\n\nlike\n275",
    "code_snippets": [
      "from diffusers import AutoPipelineForText2Image\nimport torch\npipeline = AutoPipelineForText2Image.from_pretrained(\"black-forest-labs/FLUX.1-dev\", torch_dtype=torch.bfloat16).to('cuda')\npipeline.load_lora_weights('aleksa-codes/flux-ghibsky-illustration', weight_name='lora.safetensors')\nimage = pipeline('GHIBSKY style, a serene lakeside village with colorful houses and towering mountains under a dreamy sky').images[0]"
    ]
  },
  {
    "url": "https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2",
    "title": "mistralai\n\n/\nMistral-7B-Instruct-v0.2\n\n\n\nlike\n2.78k\nFollow\n\nMistral AI_\n8.74k",
    "code_snippets": [
      "from transformers import AutoModelForCausalLM\n \nmodel = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\nmodel.to(\"cuda\")\n \ngenerated_ids = model.generate(tokens, max_new_tokens=1000, do_sample=True)\n\n# decode with mistral tokenizer\nresult = tokenizer.decode(generated_ids[0].tolist())\nprint(result)",
      "from transformers import AutoModelForCausalLM, AutoTokenizer\n\ndevice = \"cuda\" # the device to load the model onto\n\nmodel = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\ntokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\n\nmessages = [\n    {\"role\": \"user\", \"content\": \"What is your favourite condiment?\"},\n    {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n    {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}\n]\n\nencodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n\nmodel_inputs = encodeds.to(device)\nmodel.to(device)\n\ngenerated_ids = model.generate(model_inputs, max_new_tokens=1000, do_sample=True)\ndecoded = tokenizer.batch_decode(generated_ids)\nprint(decoded[0])"
    ]
  },
  {
    "url": "https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0",
    "title": "stabilityai\n\n/\nstable-diffusion-xl-base-1.0\n\n\n\nlike\n6.62k\nFollow\n\nStability AI\n23.8k",
    "code_snippets": [
      "- from diffusers import StableDiffusionXLPipeline\n+ from optimum.intel import OVStableDiffusionXLPipeline\n\nmodel_id = \"stabilityai/stable-diffusion-xl-base-1.0\"\n- pipeline = StableDiffusionXLPipeline.from_pretrained(model_id)\n+ pipeline = OVStableDiffusionXLPipeline.from_pretrained(model_id)\nprompt = \"A majestic lion jumping from a big stone at night\"\nimage = pipeline(prompt).images[0]",
      "- from diffusers import StableDiffusionXLPipeline\n+ from optimum.onnxruntime import ORTStableDiffusionXLPipeline\n\nmodel_id = \"stabilityai/stable-diffusion-xl-base-1.0\"\n- pipeline = StableDiffusionXLPipeline.from_pretrained(model_id)\n+ pipeline = ORTStableDiffusionXLPipeline.from_pretrained(model_id)\nprompt = \"A majestic lion jumping from a big stone at night\"\nimage = pipeline(prompt).images[0]"
    ]
  },
  {
    "url": "https://huggingface.co/Wan-AI/Wan2.1-VACE-1.3B",
    "title": "Wan-AI\n\n/\nWan2.1-VACE-1.3B\n\n\n\nlike\n70\nFollow\n\nWan-AI\n2.59k",
    "code_snippets": [
      "import torch\nimport numpy as np\nfrom diffusers import AutoencoderKLWan, WanImageToVideoPipeline\nfrom diffusers.utils import export_to_video, load_image\nfrom transformers import CLIPVisionModel\n\n# Available models: Wan-AI/Wan2.1-I2V-14B-480P-Diffusers, Wan-AI/Wan2.1-I2V-14B-720P-Diffusers\nmodel_id = \"Wan-AI/Wan2.1-I2V-14B-720P-Diffusers\"\nimage_encoder = CLIPVisionModel.from_pretrained(model_id, subfolder=\"image_encoder\", torch_dtype=torch.float32)\nvae = AutoencoderKLWan.from_pretrained(model_id, subfolder=\"vae\", torch_dtype=torch.float32)\npipe = WanImageToVideoPipeline.from_pretrained(model_id, vae=vae, image_encoder=image_encoder, torch_dtype=torch.bfloat16)\npipe.to(\"cuda\")\n\nimage = load_image(\n    \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/astronaut.jpg\"\n)\nmax_area = 720 * 1280\naspect_ratio = image.height / image.width\nmod_value = pipe.vae_scale_factor_spatial * pipe.transformer.config.patch_size[1]\nheight = round(np.sqrt(max_area * aspect_ratio)) // mod_value * mod_value\nwidth = round(np.sqrt(max_area / aspect_ratio)) // mod_value * mod_value\nimage = image.resize((width, height))\nprompt = (\n    \"An astronaut hatching from an egg, on the surface of the moon, the darkness and depth of space realised in \"\n    \"the background. High quality, ultrarealistic detail and breath-taking movie-like camera shot.\"\n)\nnegative_prompt = \"Bright tones, overexposed, static, blurred details, subtitles, style, works, paintings, images, static, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, misshapen limbs, fused fingers, still picture, messy background, three legs, many people in the background, walking backwards\"\n\noutput = pipe(\n    image=image,\n    prompt=prompt,\n    negative_prompt=negative_prompt,\n    height=height, width=width,\n    num_frames=81,\n    guidance_scale=5.0\n).frames[0]\nexport_to_video(output, \"output.mp4\", fps=16)"
    ]
  },
  {
    "url": "https://huggingface.co/Qwen/Qwen3-4B",
    "title": "Qwen\n\n/\nQwen3-4B\n\n\n\nlike\n220\nFollow\n\nQwen\n32.3k",
    "code_snippets": [
      "from transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"Qwen/Qwen3-4B\"\n\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\n\n# prepare the model input\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n    enable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\n# conduct text completion\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=32768\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n\n# parsing thinking content\ntry:\n    # rindex finding 151668 (</think>)\n    index = len(output_ids) - output_ids[::-1].index(151668)\nexcept ValueError:\n    index = 0\n\nthinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\ncontent = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n\nprint(\"thinking content:\", thinking_content)\nprint(\"content:\", content)",
      "from transformers import AutoModelForCausalLM, AutoTokenizer\n\nclass QwenChatbot:\n    def __init__(self, model_name=\"Qwen/Qwen3-4B\"):\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.model = AutoModelForCausalLM.from_pretrained(model_name)\n        self.history = []\n\n    def generate_response(self, user_input):\n        messages = self.history + [{\"role\": \"user\", \"content\": user_input}]\n\n        text = self.tokenizer.apply_chat_template(\n            messages,\n            tokenize=False,\n            add_generation_prompt=True\n        )\n\n        inputs = self.tokenizer(text, return_tensors=\"pt\")\n        response_ids = self.model.generate(**inputs, max_new_tokens=32768)[0][len(inputs.input_ids[0]):].tolist()\n        response = self.tokenizer.decode(response_ids, skip_special_tokens=True)\n\n        # Update history\n        self.history.append({\"role\": \"user\", \"content\": user_input})\n        self.history.append({\"role\": \"assistant\", \"content\": response})\n\n        return response\n\n# Example Usage\nif __name__ == \"__main__\":\n    chatbot = QwenChatbot()\n\n    # First input (without /think or /no_think tags, thinking mode is enabled by default)\n    user_input_1 = \"How many r's in strawberries?\"\n    print(f\"User: {user_input_1}\")\n    response_1 = chatbot.generate_response(user_input_1)\n    print(f\"Bot: {response_1}\")\n    print(\"----------------------\")\n\n    # Second input with /no_think\n    user_input_2 = \"Then, how many r's in blueberries? /no_think\"\n    print(f\"User: {user_input_2}\")\n    response_2 = chatbot.generate_response(user_input_2)\n    print(f\"Bot: {response_2}\") \n    print(\"----------------------\")\n\n    # Third input with /think\n    user_input_3 = \"Really? /think\"\n    print(f\"User: {user_input_3}\")\n    response_3 = chatbot.generate_response(user_input_3)\n    print(f\"Bot: {response_3}\")"
    ]
  },
  {
    "url": "https://huggingface.co/Tongyi-Zhiwen/QwenLong-L1-32B",
    "title": "Tongyi-Zhiwen\n\n/\nQwenLong-L1-32B\n\n\n\nlike\n35\nFollow\n\nTongyi-Zhiwen\n22",
    "code_snippets": [
      "from transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"Tongyi-Zhiwen/QwenLong-L1-32B\"\n\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\n\n# prepare the model input\ntemplate = \"\"\"Please read the following text and answer the question below.\n\n<text>\n$DOC$\n</text>\n\n$Q$\n\nFormat your response as follows: \"Therefore, the answer is (insert answer here)\".\"\"\"\ncontext = \"<YOUR_CONTEXT_HERE>\" \nquestion = \"<YOUR_QUESTION_HERE>\"\nprompt = template.replace('$DOC$', context.strip()).replace('$Q$', question.strip())\nmessages = [\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\n# conduct text completion\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=10000,\n    temperature=0.7,\n    top_p=0.95\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n\n# parsing thinking content\ntry:\n    # rindex finding 151649 (</think>)\n    index = len(output_ids) - output_ids[::-1].index(151649)\nexcept ValueError:\n    index = 0\n\nthinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\ncontent = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n\nprint(\"thinking content:\", thinking_content)\nprint(\"content:\", content)"
    ]
  },
  {
    "url": "https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct",
    "title": "meta-llama\n\n/\nLlama-3.1-8B-Instruct\n\n\n\nlike\n4.01k\nFollow\n\nMeta Llama\n45.1k",
    "code_snippets": [
      "import transformers\nimport torch\n\nmodel_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n\npipeline = transformers.pipeline(\n    \"text-generation\",\n    model=model_id,\n    model_kwargs={\"torch_dtype\": torch.bfloat16},\n    device_map=\"auto\",\n)\n\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n    {\"role\": \"user\", \"content\": \"Who are you?\"},\n]\n\noutputs = pipeline(\n    messages,\n    max_new_tokens=256,\n)\nprint(outputs[0][\"generated_text\"][-1])"
    ]
  },
  {
    "url": "https://huggingface.co/Qwen/Qwen3-30B-A3B",
    "title": "Qwen\n\n/\nQwen3-30B-A3B\n\n\n\nlike\n601\nFollow\n\nQwen\n32.3k",
    "code_snippets": [
      "from transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"Qwen/Qwen3-30B-A3B\"\n\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\n\n# prepare the model input\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n    enable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\n# conduct text completion\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=32768\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n\n# parsing thinking content\ntry:\n    # rindex finding 151668 (</think>)\n    index = len(output_ids) - output_ids[::-1].index(151668)\nexcept ValueError:\n    index = 0\n\nthinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\ncontent = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n\nprint(\"thinking content:\", thinking_content)\nprint(\"content:\", content)",
      "from transformers import AutoModelForCausalLM, AutoTokenizer\n\nclass QwenChatbot:\n    def __init__(self, model_name=\"Qwen/Qwen3-30B-A3B\"):\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.model = AutoModelForCausalLM.from_pretrained(model_name)\n        self.history = []\n\n    def generate_response(self, user_input):\n        messages = self.history + [{\"role\": \"user\", \"content\": user_input}]\n\n        text = self.tokenizer.apply_chat_template(\n            messages,\n            tokenize=False,\n            add_generation_prompt=True\n        )\n\n        inputs = self.tokenizer(text, return_tensors=\"pt\")\n        response_ids = self.model.generate(**inputs, max_new_tokens=32768)[0][len(inputs.input_ids[0]):].tolist()\n        response = self.tokenizer.decode(response_ids, skip_special_tokens=True)\n\n        # Update history\n        self.history.append({\"role\": \"user\", \"content\": user_input})\n        self.history.append({\"role\": \"assistant\", \"content\": response})\n\n        return response\n\n# Example Usage\nif __name__ == \"__main__\":\n    chatbot = QwenChatbot()\n\n    # First input (without /think or /no_think tags, thinking mode is enabled by default)\n    user_input_1 = \"How many r's in strawberries?\"\n    print(f\"User: {user_input_1}\")\n    response_1 = chatbot.generate_response(user_input_1)\n    print(f\"Bot: {response_1}\")\n    print(\"----------------------\")\n\n    # Second input with /no_think\n    user_input_2 = \"Then, how many r's in blueberries? /no_think\"\n    print(f\"User: {user_input_2}\")\n    response_2 = chatbot.generate_response(user_input_2)\n    print(f\"Bot: {response_2}\") \n    print(\"----------------------\")\n\n    # Third input with /think\n    user_input_3 = \"Really? /think\"\n    print(f\"User: {user_input_3}\")\n    response_3 = chatbot.generate_response(user_input_3)\n    print(f\"Bot: {response_3}\")"
    ]
  },
  {
    "url": "https://huggingface.co/Qwen/Qwen3-8B",
    "title": "Qwen\n\n/\nQwen3-8B\n\n\n\nlike\n333\nFollow\n\nQwen\n32.3k",
    "code_snippets": [
      "from transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"Qwen/Qwen3-8B\"\n\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\n\n# prepare the model input\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n    enable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\n# conduct text completion\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=32768\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n\n# parsing thinking content\ntry:\n    # rindex finding 151668 (</think>)\n    index = len(output_ids) - output_ids[::-1].index(151668)\nexcept ValueError:\n    index = 0\n\nthinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\ncontent = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n\nprint(\"thinking content:\", thinking_content)\nprint(\"content:\", content)",
      "from transformers import AutoModelForCausalLM, AutoTokenizer\n\nclass QwenChatbot:\n    def __init__(self, model_name=\"Qwen/Qwen3-8B\"):\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.model = AutoModelForCausalLM.from_pretrained(model_name)\n        self.history = []\n\n    def generate_response(self, user_input):\n        messages = self.history + [{\"role\": \"user\", \"content\": user_input}]\n\n        text = self.tokenizer.apply_chat_template(\n            messages,\n            tokenize=False,\n            add_generation_prompt=True\n        )\n\n        inputs = self.tokenizer(text, return_tensors=\"pt\")\n        response_ids = self.model.generate(**inputs, max_new_tokens=32768)[0][len(inputs.input_ids[0]):].tolist()\n        response = self.tokenizer.decode(response_ids, skip_special_tokens=True)\n\n        # Update history\n        self.history.append({\"role\": \"user\", \"content\": user_input})\n        self.history.append({\"role\": \"assistant\", \"content\": response})\n\n        return response\n\n# Example Usage\nif __name__ == \"__main__\":\n    chatbot = QwenChatbot()\n\n    # First input (without /think or /no_think tags, thinking mode is enabled by default)\n    user_input_1 = \"How many r's in strawberries?\"\n    print(f\"User: {user_input_1}\")\n    response_1 = chatbot.generate_response(user_input_1)\n    print(f\"Bot: {response_1}\")\n    print(\"----------------------\")\n\n    # Second input with /no_think\n    user_input_2 = \"Then, how many r's in blueberries? /no_think\"\n    print(f\"User: {user_input_2}\")\n    response_2 = chatbot.generate_response(user_input_2)\n    print(f\"Bot: {response_2}\") \n    print(\"----------------------\")\n\n    # Third input with /think\n    user_input_3 = \"Really? /think\"\n    print(f\"User: {user_input_3}\")\n    response_3 = chatbot.generate_response(user_input_3)\n    print(f\"Bot: {response_3}\")"
    ]
  },
  {
    "url": "https://huggingface.co/google/gemma-3-4b-it",
    "title": "google\n\n/\ngemma-3-4b-it\n\n\n\nlike\n560\nFollow\n\nGoogle\n14.2k",
    "code_snippets": [
      "from transformers import pipeline\nimport torch\n\npipe = pipeline(\n    \"image-text-to-text\",\n    model=\"google/gemma-3-4b-it\",\n    device=\"cuda\",\n    torch_dtype=torch.bfloat16\n)",
      "# pip install accelerate\n\nfrom transformers import AutoProcessor, Gemma3ForConditionalGeneration\nfrom PIL import Image\nimport requests\nimport torch\n\nmodel_id = \"google/gemma-3-4b-it\"\n\nmodel = Gemma3ForConditionalGeneration.from_pretrained(\n    model_id, device_map=\"auto\"\n).eval()\n\nprocessor = AutoProcessor.from_pretrained(model_id)\n\nmessages = [\n    {\n        \"role\": \"system\",\n        \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}]\n    },\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"image\", \"image\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee.jpg\"},\n            {\"type\": \"text\", \"text\": \"Describe this image in detail.\"}\n        ]\n    }\n]\n\ninputs = processor.apply_chat_template(\n    messages, add_generation_prompt=True, tokenize=True,\n    return_dict=True, return_tensors=\"pt\"\n).to(model.device, dtype=torch.bfloat16)\n\ninput_len = inputs[\"input_ids\"].shape[-1]\n\nwith torch.inference_mode():\n    generation = model.generate(**inputs, max_new_tokens=100, do_sample=False)\n    generation = generation[0][input_len:]\n\ndecoded = processor.decode(generation, skip_special_tokens=True)\nprint(decoded)\n\n# **Overall Impression:** The image is a close-up shot of a vibrant garden scene, \n# focusing on a cluster of pink cosmos flowers and a busy bumblebee. \n# It has a slightly soft, natural feel, likely captured in daylight."
    ]
  },
  {
    "url": "https://huggingface.co/Qwen/Qwen3-32B",
    "title": "Qwen\n\n/\nQwen3-32B\n\n\n\nlike\n347\nFollow\n\nQwen\n32.3k",
    "code_snippets": [
      "from transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"Qwen/Qwen3-32B\"\n\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\n\n# prepare the model input\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n    enable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\n# conduct text completion\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=32768\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n\n# parsing thinking content\ntry:\n    # rindex finding 151668 (</think>)\n    index = len(output_ids) - output_ids[::-1].index(151668)\nexcept ValueError:\n    index = 0\n\nthinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\ncontent = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n\nprint(\"thinking content:\", thinking_content)\nprint(\"content:\", content)",
      "from transformers import AutoModelForCausalLM, AutoTokenizer\n\nclass QwenChatbot:\n    def __init__(self, model_name=\"Qwen/Qwen3-32B\"):\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.model = AutoModelForCausalLM.from_pretrained(model_name)\n        self.history = []\n\n    def generate_response(self, user_input):\n        messages = self.history + [{\"role\": \"user\", \"content\": user_input}]\n\n        text = self.tokenizer.apply_chat_template(\n            messages,\n            tokenize=False,\n            add_generation_prompt=True\n        )\n\n        inputs = self.tokenizer(text, return_tensors=\"pt\")\n        response_ids = self.model.generate(**inputs, max_new_tokens=32768)[0][len(inputs.input_ids[0]):].tolist()\n        response = self.tokenizer.decode(response_ids, skip_special_tokens=True)\n\n        # Update history\n        self.history.append({\"role\": \"user\", \"content\": user_input})\n        self.history.append({\"role\": \"assistant\", \"content\": response})\n\n        return response\n\n# Example Usage\nif __name__ == \"__main__\":\n    chatbot = QwenChatbot()\n\n    # First input (without /think or /no_think tags, thinking mode is enabled by default)\n    user_input_1 = \"How many r's in strawberries?\"\n    print(f\"User: {user_input_1}\")\n    response_1 = chatbot.generate_response(user_input_1)\n    print(f\"Bot: {response_1}\")\n    print(\"----------------------\")\n\n    # Second input with /no_think\n    user_input_2 = \"Then, how many r's in blueberries? /no_think\"\n    print(f\"User: {user_input_2}\")\n    response_2 = chatbot.generate_response(user_input_2)\n    print(f\"Bot: {response_2}\") \n    print(\"----------------------\")\n\n    # Third input with /think\n    user_input_3 = \"Really? /think\"\n    print(f\"User: {user_input_3}\")\n    response_3 = chatbot.generate_response(user_input_3)\n    print(f\"Bot: {response_3}\")"
    ]
  },
  {
    "url": "https://huggingface.co/jinaai/jina-embeddings-v3",
    "title": "jinaai\n\n/\njina-embeddings-v3\n\n\n\nlike\n980\nFollow\n\nJina AI\n1.02k",
    "code_snippets": [
      "import torch\nimport torch.nn.functional as F\nfrom transformers import AutoTokenizer, AutoModel\n\n\ndef mean_pooling(model_output, attention_mask):\n    token_embeddings = model_output[0]\n    input_mask_expanded = (\n        attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    )\n    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(\n        input_mask_expanded.sum(1), min=1e-9\n    )\n\n\nsentences = [\"How is the weather today?\", \"What is the current weather like today?\"]\n\ntokenizer = AutoTokenizer.from_pretrained(\"jinaai/jina-embeddings-v3\")\nmodel = AutoModel.from_pretrained(\"jinaai/jina-embeddings-v3\", trust_remote_code=True)\n\nencoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\ntask = 'retrieval.query'\ntask_id = model._adaptation_map[task]\nadapter_mask = torch.full((len(sentences),), task_id, dtype=torch.int32)\nwith torch.no_grad():\n    model_output = model(**encoded_input, adapter_mask=adapter_mask)\n\nembeddings = mean_pooling(model_output, encoded_input[\"attention_mask\"])\nembeddings = F.normalize(embeddings, p=2, dim=1)",
      "from transformers import AutoModel\n\n# Initialize the model\nmodel = AutoModel.from_pretrained(\"jinaai/jina-embeddings-v3\", trust_remote_code=True)\n\ntexts = [\n    \"Follow the white rabbit.\",  # English\n    \"Sigue al conejo blanco.\",  # Spanish\n    \"Suis le lapin blanc.\",  # French\n    \"\u8ddf\u7740\u767d\u5154\u8d70\u3002\",  # Chinese\n    \"\u0627\u062a\u0628\u0639 \u0627\u0644\u0623\u0631\u0646\u0628 \u0627\u0644\u0623\u0628\u064a\u0636.\",  # Arabic\n    \"Folge dem wei\u00dfen Kaninchen.\",  # German\n]\n\n# When calling the `encode` function, you can choose a `task` based on the use case:\n# 'retrieval.query', 'retrieval.passage', 'separation', 'classification', 'text-matching'\n# Alternatively, you can choose not to pass a `task`, and no specific LoRA adapter will be used.\nembeddings = model.encode(texts, task=\"text-matching\")\n\n# Compute similarities\nprint(embeddings[0] @ embeddings[1].T)",
      "import onnxruntime\nimport numpy as np\nfrom transformers import AutoTokenizer, PretrainedConfig\n\n# Mean pool function\ndef mean_pooling(model_output: np.ndarray, attention_mask: np.ndarray):\n    token_embeddings = model_output\n    input_mask_expanded = np.expand_dims(attention_mask, axis=-1)\n    input_mask_expanded = np.broadcast_to(input_mask_expanded, token_embeddings.shape)\n    sum_embeddings = np.sum(token_embeddings * input_mask_expanded, axis=1)\n    sum_mask = np.clip(np.sum(input_mask_expanded, axis=1), a_min=1e-9, a_max=None)\n    return sum_embeddings / sum_mask\n\n# Load tokenizer and model config\ntokenizer = AutoTokenizer.from_pretrained('jinaai/jina-embeddings-v3')\nconfig = PretrainedConfig.from_pretrained('jinaai/jina-embeddings-v3')\n\n# Tokenize input\ninput_text = tokenizer('sample text', return_tensors='np')\n\n# ONNX session\nmodel_path = 'jina-embeddings-v3/onnx/model.onnx'\nsession = onnxruntime.InferenceSession(model_path)\n\n# Prepare inputs for ONNX model\ntask_type = 'text-matching'\ntask_id = np.array(config.lora_adaptations.index(task_type), dtype=np.int64)\ninputs = {\n    'input_ids': input_text['input_ids'],\n    'attention_mask': input_text['attention_mask'],\n    'task_id': task_id\n}\n\n# Run model\noutputs = session.run(None, inputs)[0]\n\n# Apply mean pooling and normalization to the model outputs\nembeddings = mean_pooling(outputs, input_text[\"attention_mask\"])\nembeddings = embeddings / np.linalg.norm(embeddings, ord=2, axis=1, keepdims=True)"
    ]
  },
  {
    "url": "https://huggingface.co/openai/whisper-large-v3",
    "title": "openai\n\n/\nwhisper-large-v3\n\n\n\nlike\n4.42k\nFollow\n\nOpenAI\n8.14k",
    "code_snippets": [
      "import torch\nfrom transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\nfrom datasets import load_dataset\n\n\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\ntorch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n\nmodel_id = \"openai/whisper-large-v3\"\n\nmodel = AutoModelForSpeechSeq2Seq.from_pretrained(\n    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n)\nmodel.to(device)\n\nprocessor = AutoProcessor.from_pretrained(model_id)\n\npipe = pipeline(\n    \"automatic-speech-recognition\",\n    model=model,\n    tokenizer=processor.tokenizer,\n    feature_extractor=processor.feature_extractor,\n    torch_dtype=torch_dtype,\n    device=device,\n)\n\ndataset = load_dataset(\"distil-whisper/librispeech_long\", \"clean\", split=\"validation\")\nsample = dataset[0][\"audio\"]\n\nresult = pipe(sample)\nprint(result[\"text\"])",
      "import torch\nfrom transformers import AutoModelForSpeechSeq2Seq, AutoProcessor\nfrom datasets import Audio, load_dataset\n\n\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\ntorch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n\nmodel_id = \"openai/whisper-large-v3\"\n\nmodel = AutoModelForSpeechSeq2Seq.from_pretrained(\n    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True\n)\nmodel.to(device)\n\nprocessor = AutoProcessor.from_pretrained(model_id)\n\ndataset = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\ndataset = dataset.cast_column(\"audio\", Audio(processor.feature_extractor.sampling_rate))\nsample = dataset[0][\"audio\"]\n\ninputs = processor(\n    sample[\"array\"],\n    sampling_rate=sample[\"sampling_rate\"],\n    return_tensors=\"pt\",\n    truncation=False,\n    padding=\"longest\",\n    return_attention_mask=True,\n)\ninputs = inputs.to(device, dtype=torch_dtype)\n\ngen_kwargs = {\n    \"max_new_tokens\": 448,\n    \"num_beams\": 1,\n    \"condition_on_prev_tokens\": False,\n    \"compression_ratio_threshold\": 1.35,  # zlib compression ratio threshold (in token space)\n    \"temperature\": (0.0, 0.2, 0.4, 0.6, 0.8, 1.0),\n    \"logprob_threshold\": -1.0,\n    \"no_speech_threshold\": 0.6,\n    \"return_timestamps\": True,\n}\n\npred_ids = model.generate(**inputs, **gen_kwargs)\npred_text = processor.batch_decode(pred_ids, skip_special_tokens=True, decode_with_timestamps=False)\n\nprint(pred_text)",
      "import torch\nfrom transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\nfrom datasets import load_dataset\n\n\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\ntorch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n\nmodel_id = \"openai/whisper-large-v3\"\n\nmodel = AutoModelForSpeechSeq2Seq.from_pretrained(\n    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True\n)\nmodel.to(device)\n\nprocessor = AutoProcessor.from_pretrained(model_id)\n\npipe = pipeline(\n    \"automatic-speech-recognition\",\n    model=model,\n    tokenizer=processor.tokenizer,\n    feature_extractor=processor.feature_extractor,\n    chunk_length_s=30,\n    batch_size=16,  # batch size for inference - set based on your device\n    torch_dtype=torch_dtype,\n    device=device,\n)\n\ndataset = load_dataset(\"distil-whisper/librispeech_long\", \"clean\", split=\"validation\")\nsample = dataset[0][\"audio\"]\n\nresult = pipe(sample)\nprint(result[\"text\"])",
      "import torch\nfrom torch.nn.attention import SDPBackend, sdpa_kernel\nfrom transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\nfrom datasets import load_dataset\nfrom tqdm import tqdm\n\ntorch.set_float32_matmul_precision(\"high\")\n\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\ntorch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n\nmodel_id = \"openai/whisper-large-v3\"\n\nmodel = AutoModelForSpeechSeq2Seq.from_pretrained(\n    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True\n).to(device)\n\n# Enable static cache and compile the forward pass\nmodel.generation_config.cache_implementation = \"static\"\nmodel.generation_config.max_new_tokens = 256\nmodel.forward = torch.compile(model.forward, mode=\"reduce-overhead\", fullgraph=True)\n\nprocessor = AutoProcessor.from_pretrained(model_id)\n\npipe = pipeline(\n    \"automatic-speech-recognition\",\n    model=model,\n    tokenizer=processor.tokenizer,\n    feature_extractor=processor.feature_extractor,\n    torch_dtype=torch_dtype,\n    device=device,\n)\n\ndataset = load_dataset(\"distil-whisper/librispeech_long\", \"clean\", split=\"validation\")\nsample = dataset[0][\"audio\"]\n\n# 2 warmup steps\nfor _ in tqdm(range(2), desc=\"Warm-up step\"):\n    with sdpa_kernel(SDPBackend.MATH):\n        result = pipe(sample.copy(), generate_kwargs={\"min_new_tokens\": 256, \"max_new_tokens\": 256})\n\n# fast run\nwith sdpa_kernel(SDPBackend.MATH):\n    result = pipe(sample.copy())\n\nprint(result[\"text\"])",
      "model = AutoModelForSpeechSeq2Seq.from_pretrained(model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, attn_implementation=\"flash_attention_2\")",
      "from transformers.utils import is_torch_sdpa_available\n\nprint(is_torch_sdpa_available())",
      "model = AutoModelForSpeechSeq2Seq.from_pretrained(model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, attn_implementation=\"sdpa\")"
    ]
  },
  {
    "url": "https://huggingface.co/unsloth/medgemma-27b-text-it-GGUF",
    "title": "unsloth\n\n/\nmedgemma-27b-text-it-GGUF\n\n\n\nlike\n18\nFollow\n\nUnsloth AI\n5.8k",
    "code_snippets": [
      "from transformers import pipeline\nimport torch\n\npipe = pipeline(\n    \"text-generation\",\n    model=\"google/medgemma-27b-text-it\",\n    torch_dtype=torch.bfloat16,\n    device=\"cuda\",\n)\n\nmessages = [\n    {\n        \"role\": \"system\",\n        \"content\": \"You are a helpful medical assistant.\"\n    },\n    {\n        \"role\": \"user\",\n        \"content\": \"How do you differentiate bacterial from viral pneumonia?\"\n    }\n]\n\noutput = pipe(text=messages, max_new_tokens=200)\nprint(output[0][\"generated_text\"][-1][\"content\"])",
      "# pip install accelerate\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\n\nmodel_id = \"google/medgemma-27b-text-it\"\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    torch_dtype=torch.bfloat16,\n    device_map=\"auto\",\n)\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\nmessages = [\n    {\n        \"role\": \"system\",\n        \"content\": \"You are a helpful medical assistant.\"\n    },\n    {\n        \"role\": \"user\",\n        \"content\": \"How do you differentiate bacterial from viral pneumonia?\"\n    }\n]\n\ninputs = tokenizer.apply_chat_template(\n    messages,\n    add_generation_prompt=True,\n    tokenize=True,\n    return_dict=True,\n    return_tensors=\"pt\",\n).to(model.device)\n\ninput_len = inputs[\"input_ids\"].shape[-1]\n\nwith torch.inference_mode():\n    generation = model.generate(**inputs, max_new_tokens=200, do_sample=False)\n    generation = generation[0][input_len:]\n\ndecoded = tokenizer.decode(generation, skip_special_tokens=True)\nprint(decoded)"
    ]
  },
  {
    "url": "https://huggingface.co/fancyfeast/llama-joycaption-beta-one-hf-llava",
    "title": "fancyfeast\n\n/\nllama-joycaption-beta-one-hf-llava\n\n\n\nlike\n88",
    "code_snippets": [
      "import torch\nfrom PIL import Image\nfrom transformers import AutoProcessor, LlavaForConditionalGeneration\n\n\nIMAGE_PATH = \"image.jpg\"\nPROMPT = \"Write a long descriptive caption for this image in a formal tone.\"\nMODEL_NAME = \"fancyfeast/llama-joycaption-beta-one-hf-llava\"\n\n\n# Load JoyCaption\n# bfloat16 is the native dtype of the LLM used in JoyCaption (Llama 3.1)\n# device_map=0 loads the model into the first GPU\nprocessor = AutoProcessor.from_pretrained(MODEL_NAME)\nllava_model = LlavaForConditionalGeneration.from_pretrained(MODEL_NAME, torch_dtype=\"bfloat16\", device_map=0)\nllava_model.eval()\n\nwith torch.no_grad():\n    # Load image\n    image = Image.open(IMAGE_PATH)\n\n    # Build the conversation\n    convo = [\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a helpful image captioner.\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": PROMPT,\n        },\n    ]\n\n    # Format the conversation\n    # WARNING: HF's handling of chat's on Llava models is very fragile.  This specific combination of processor.apply_chat_template(), and processor() works\n    # but if using other combinations always inspect the final input_ids to ensure they are correct.  Often times you will end up with multiple <bos> tokens\n    # if not careful, which can make the model perform poorly.\n    convo_string = processor.apply_chat_template(convo, tokenize = False, add_generation_prompt = True)\n    assert isinstance(convo_string, str)\n\n    # Process the inputs\n    inputs = processor(text=[convo_string], images=[image], return_tensors=\"pt\").to('cuda')\n    inputs['pixel_values'] = inputs['pixel_values'].to(torch.bfloat16)\n\n    # Generate the captions\n    generate_ids = llava_model.generate(\n        **inputs,\n        max_new_tokens=512,\n        do_sample=True,\n        suppress_tokens=None,\n        use_cache=True,\n        temperature=0.6,\n        top_k=None,\n        top_p=0.9,\n    )[0]\n\n    # Trim off the prompt\n    generate_ids = generate_ids[inputs['input_ids'].shape[1]:]\n\n    # Decode the caption\n    caption = processor.tokenizer.decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n    caption = caption.strip()\n    print(caption)"
    ]
  },
  {
    "url": "https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2",
    "title": "sentence-transformers\n\n/\nall-MiniLM-L6-v2\n\n\n\nlike\n3.44k\nFollow\n\nSentence Transformers\n1.99k",
    "code_snippets": [
      "from transformers import AutoTokenizer, AutoModel\nimport torch\nimport torch.nn.functional as F\n\n#Mean Pooling - Take attention mask into account for correct averaging\ndef mean_pooling(model_output, attention_mask):\n    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n\n\n# Sentences we want sentence embeddings for\nsentences = ['This is an example sentence', 'Each sentence is converted']\n\n# Load model from HuggingFace Hub\ntokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\nmodel = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n\n# Tokenize sentences\nencoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n\n# Compute token embeddings\nwith torch.no_grad():\n    model_output = model(**encoded_input)\n\n# Perform pooling\nsentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n\n# Normalize embeddings\nsentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n\nprint(\"Sentence embeddings:\")\nprint(sentence_embeddings)"
    ]
  },
  {
    "url": "https://huggingface.co/pyannote/speaker-diarization-3.1",
    "title": "pyannote\n\n/\nspeaker-diarization-3.1\n\n\n\nlike\n885\nFollow\n\npyannote.audio\n896",
    "code_snippets": [
      "# instantiate the pipeline\nfrom pyannote.audio import Pipeline\npipeline = Pipeline.from_pretrained(\n  \"pyannote/speaker-diarization-3.1\",\n  use_auth_token=\"HUGGINGFACE_ACCESS_TOKEN_GOES_HERE\")\n\n# run the pipeline on an audio file\ndiarization = pipeline(\"audio.wav\")\n\n# dump the diarization output to disk using RTTM format\nwith open(\"audio.rttm\", \"w\") as rttm:\n    diarization.write_rttm(rttm)",
      "import torch\npipeline.to(torch.device(\"cuda\"))",
      "waveform, sample_rate = torchaudio.load(\"audio.wav\")\ndiarization = pipeline({\"waveform\": waveform, \"sample_rate\": sample_rate})",
      "from pyannote.audio.pipelines.utils.hook import ProgressHook\nwith ProgressHook() as hook:\n    diarization = pipeline(\"audio.wav\", hook=hook)",
      "diarization = pipeline(\"audio.wav\", num_speakers=2)",
      "diarization = pipeline(\"audio.wav\", min_speakers=2, max_speakers=5)",
      "@inproceedings{Bredin23,\n  author={Herv\u00e9 Bredin},\n  title={{pyannote.audio 2.1 speaker diarization pipeline: principle, benchmark, and recipe}},\n  year=2023,\n  booktitle={Proc. INTERSPEECH 2023},\n}"
    ]
  },
  {
    "url": "https://huggingface.co/moondream/moondream-2b-2025-04-14-4bit",
    "title": "moondream\n\n/\nmoondream-2b-2025-04-14-4bit\n\n\n\nlike\n31\nFollow\n\nmoondream\n83",
    "code_snippets": [
      "from transformers import AutoModelForCausalLM, AutoTokenizer\nfrom PIL import Image\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"moondream/moondream-2b-2025-04-14-4bit\",\n    trust_remote_code=True,\n    device_map={\"\": \"cuda\"}\n)\n\n# Optional, but recommended when running inference on a large number of\n# images since it has upfront compilation cost but significantly speeds\n# up inference:\nmodel.model.compile()\n\n# Captioning\nprint(\"Short caption:\")\nprint(model.caption(image, length=\"short\")[\"caption\"])\n\nprint(\"\\nNormal caption:\")\nfor t in model.caption(image, length=\"normal\", stream=True)[\"caption\"]:\n    # Streaming generation example, supported for caption() and detect()\n    print(t, end=\"\", flush=True)\nprint(model.caption(image, length=\"normal\"))\n\n# Visual Querying\nprint(\"\\nVisual query: 'How many people are in the image?'\")\nprint(model.query(image, \"How many people are in the image?\")[\"answer\"])\n\n# Object Detection\nprint(\"\\nObject detection: 'face'\")\nobjects = model.detect(image, \"face\")[\"objects\"]\nprint(f\"Found {len(objects)} face(s)\")\n\n# Pointing\nprint(\"\\nPointing: 'person'\")\npoints = model.point(image, \"person\")[\"points\"]\nprint(f\"Found {len(points)} person(s)\")"
    ]
  },
  {
    "url": "https://huggingface.co/stepfun-ai/Step1X-3D",
    "title": "stepfun-ai\n\n/\nStep1X-3D\n\n\n\nlike\n78\nFollow\n\nStepFun\n1.07k",
    "code_snippets": [
      "# Stage 1: 3D geometry generation\nfrom step1x3d_geometry.models.pipelines.pipeline import Step1X3DGeometryPipeline\n\n# define the pipeline\ngeometry_pipeline = Step1X3DGeometryPipeline.from_pretrained(\"stepfun-ai/Step1X-3D\", subfolder='Step1X-3D-Geometry-1300m'\n).to(\"cuda\")\n\n# input image\ninput_image_path = \"examples/test.png\"\n\n# run pipeline and obtain the untextured mesh \ngenerator = torch.Generator(device=geometry_pipeline.device).manual_seed(2025)\nout = geometry_pipeline(input_image_path\uff0cguidance_scale=7.5, num_inference_steps=50)\n\n# export untextured mesh as .glb format\nout.mesh[0].export(\"untexture_mesh.glb\")\n\n\n# Stage 2: 3D texure synthsis\nfrom step1x3d_texture.pipelines.step1x_3d_texture_synthesis_pipeline import (\n    Step1X3DTexturePipeline,\n)\nfrom step1x3d_geometry.models.pipelines.pipeline_utils import reduce_face, remove_degenerate_face\nimport trimesh\n\n# load untextured mesh\nuntexture_mesh = trimesh.load(\"untexture_mesh.glb\")\n\n# define texture_pipeline\ntexture_pipeline = Step1X3DTexturePipeline.from_pretrained(\"stepfun-ai/Step1X-3D\", subfolder=\"Step1X-3D-Texture\")\n\n# reduce face\nuntexture_mesh = remove_degenerate_face(untexture_mesh)\nuntexture_mesh = reduce_face(untexture_mesh)\n\n# texture mapping\ntextured_mesh = texture_pipeline(input_image_path, untexture_mesh)\n\n# export textured mesh as .glb format\ntextured_mesh.export(\"textured_mesh.glb\")"
    ]
  },
  {
    "url": "https://huggingface.co/tencent/Hunyuan3D-2",
    "title": "tencent\n\n/\nHunyuan3D-2\n\n\n\nlike\n1.46k\nFollow\n\nTencent\n3.5k",
    "code_snippets": [
      "from hy3dgen.shapegen import Hunyuan3DDiTFlowMatchingPipeline\n\npipeline = Hunyuan3DDiTFlowMatchingPipeline.from_pretrained('tencent/Hunyuan3D-2')\nmesh = pipeline(image='assets/demo.png')[0]",
      "from hy3dgen.texgen import Hunyuan3DPaintPipeline\nfrom hy3dgen.shapegen import Hunyuan3DDiTFlowMatchingPipeline\n\n# let's generate a mesh first\npipeline = Hunyuan3DDiTFlowMatchingPipeline.from_pretrained('tencent/Hunyuan3D-2')\nmesh = pipeline(image='assets/demo.png')[0]\n\npipeline = Hunyuan3DPaintPipeline.from_pretrained('tencent/Hunyuan3D-2')\nmesh = pipeline(mesh, image='assets/demo.png')"
    ]
  }
]