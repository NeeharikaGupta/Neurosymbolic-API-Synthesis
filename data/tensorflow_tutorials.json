[
  {
    "url": "https://www.tensorflow.org/tutorials/generative/pix2pix",
    "title": "pix2pix: Image-to-image translation with a conditional GAN\n      \n\n\n      \n      Stay organized with collections\n    \n\n      \n      Save and categorize content based on your preferences.",
    "code_snippets": [
      "import tensorflow as tf\n\nimport os\nimport pathlib\nimport time\nimport datetime\n\nfrom matplotlib import pyplot as plt\nfrom IPython import display",
      "_URL = f'http://efrosgans.eecs.berkeley.edu/pix2pix/datasets/{dataset_name}.tar.gz'\n\npath_to_zip = tf.keras.utils.get_file(\n    fname=f\"{dataset_name}.tar.gz\",\n    origin=_URL,\n    extract=True)\n\npath_to_zip  = pathlib.Path(path_to_zip)\n\nPATH = path_to_zip.parent/dataset_name",
      "sample_image = tf.io.read_file(str(PATH / 'train/1.jpg'))\nsample_image = tf.io.decode_jpeg(sample_image)\nprint(sample_image.shape)",
      "def load(image_file):\n  # Read and decode an image file to a uint8 tensor\n  image = tf.io.read_file(image_file)\n  image = tf.io.decode_jpeg(image)\n\n  # Split each image tensor into two tensors:\n  # - one with a real building facade image\n  # - one with an architecture label image \n  w = tf.shape(image)[1]\n  w = w // 2\n  input_image = image[:, w:, :]\n  real_image = image[:, :w, :]\n\n  # Convert both images to float32 tensors\n  input_image = tf.cast(input_image, tf.float32)\n  real_image = tf.cast(real_image, tf.float32)\n\n  return input_image, real_image",
      "def resize(input_image, real_image, height, width):\n  input_image = tf.image.resize(input_image, [height, width],\n                                method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n  real_image = tf.image.resize(real_image, [height, width],\n                               method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n\n  return input_image, real_image",
      "def random_crop(input_image, real_image):\n  stacked_image = tf.stack([input_image, real_image], axis=0)\n  cropped_image = tf.image.random_crop(\n      stacked_image, size=[2, IMG_HEIGHT, IMG_WIDTH, 3])\n\n  return cropped_image[0], cropped_image[1]",
      "@tf.function()\ndef random_jitter(input_image, real_image):\n  # Resizing to 286x286\n  input_image, real_image = resize(input_image, real_image, 286, 286)\n\n  # Random cropping back to 256x256\n  input_image, real_image = random_crop(input_image, real_image)\n\n  if tf.random.uniform(()) > 0.5:\n    # Random mirroring\n    input_image = tf.image.flip_left_right(input_image)\n    real_image = tf.image.flip_left_right(real_image)\n\n  return input_image, real_image",
      "def load_image_train(image_file):\n  input_image, real_image = load(image_file)\n  input_image, real_image = random_jitter(input_image, real_image)\n  input_image, real_image = normalize(input_image, real_image)\n\n  return input_image, real_image",
      "def load_image_test(image_file):\n  input_image, real_image = load(image_file)\n  input_image, real_image = resize(input_image, real_image,\n                                   IMG_HEIGHT, IMG_WIDTH)\n  input_image, real_image = normalize(input_image, real_image)\n\n  return input_image, real_image",
      "train_dataset = tf.data.Dataset.list_files(str(PATH / 'train/*.jpg'))\ntrain_dataset = train_dataset.map(load_image_train,\n                                  num_parallel_calls=tf.data.AUTOTUNE)\ntrain_dataset = train_dataset.shuffle(BUFFER_SIZE)\ntrain_dataset = train_dataset.batch(BATCH_SIZE)",
      "try:\n  test_dataset = tf.data.Dataset.list_files(str(PATH / 'test/*.jpg'))\nexcept tf.errors.InvalidArgumentError:\n  test_dataset = tf.data.Dataset.list_files(str(PATH / 'val/*.jpg'))\ntest_dataset = test_dataset.map(load_image_test)\ntest_dataset = test_dataset.batch(BATCH_SIZE)",
      "def downsample(filters, size, apply_batchnorm=True):\n  initializer = tf.random_normal_initializer(0., 0.02)\n\n  result = tf.keras.Sequential()\n  result.add(\n      tf.keras.layers.Conv2D(filters, size, strides=2, padding='same',\n                             kernel_initializer=initializer, use_bias=False))\n\n  if apply_batchnorm:\n    result.add(tf.keras.layers.BatchNormalization())\n\n  result.add(tf.keras.layers.LeakyReLU())\n\n  return result",
      "down_model = downsample(3, 4)\ndown_result = down_model(tf.expand_dims(inp, 0))\nprint (down_result.shape)",
      "def upsample(filters, size, apply_dropout=False):\n  initializer = tf.random_normal_initializer(0., 0.02)\n\n  result = tf.keras.Sequential()\n  result.add(\n    tf.keras.layers.Conv2DTranspose(filters, size, strides=2,\n                                    padding='same',\n                                    kernel_initializer=initializer,\n                                    use_bias=False))\n\n  result.add(tf.keras.layers.BatchNormalization())\n\n  if apply_dropout:\n      result.add(tf.keras.layers.Dropout(0.5))\n\n  result.add(tf.keras.layers.ReLU())\n\n  return result",
      "def Generator():\n  inputs = tf.keras.layers.Input(shape=[256, 256, 3])\n\n  down_stack = [\n    downsample(64, 4, apply_batchnorm=False),  # (batch_size, 128, 128, 64)\n    downsample(128, 4),  # (batch_size, 64, 64, 128)\n    downsample(256, 4),  # (batch_size, 32, 32, 256)\n    downsample(512, 4),  # (batch_size, 16, 16, 512)\n    downsample(512, 4),  # (batch_size, 8, 8, 512)\n    downsample(512, 4),  # (batch_size, 4, 4, 512)\n    downsample(512, 4),  # (batch_size, 2, 2, 512)\n    downsample(512, 4),  # (batch_size, 1, 1, 512)\n  ]\n\n  up_stack = [\n    upsample(512, 4, apply_dropout=True),  # (batch_size, 2, 2, 1024)\n    upsample(512, 4, apply_dropout=True),  # (batch_size, 4, 4, 1024)\n    upsample(512, 4, apply_dropout=True),  # (batch_size, 8, 8, 1024)\n    upsample(512, 4),  # (batch_size, 16, 16, 1024)\n    upsample(256, 4),  # (batch_size, 32, 32, 512)\n    upsample(128, 4),  # (batch_size, 64, 64, 256)\n    upsample(64, 4),  # (batch_size, 128, 128, 128)\n  ]\n\n  initializer = tf.random_normal_initializer(0., 0.02)\n  last = tf.keras.layers.Conv2DTranspose(OUTPUT_CHANNELS, 4,\n                                         strides=2,\n                                         padding='same',\n                                         kernel_initializer=initializer,\n                                         activation='tanh')  # (batch_size, 256, 256, 3)\n\n  x = inputs\n\n  # Downsampling through the model\n  skips = []\n  for down in down_stack:\n    x = down(x)\n    skips.append(x)\n\n  skips = reversed(skips[:-1])\n\n  # Upsampling and establishing the skip connections\n  for up, skip in zip(up_stack, skips):\n    x = up(x)\n    x = tf.keras.layers.Concatenate()([x, skip])\n\n  x = last(x)\n\n  return tf.keras.Model(inputs=inputs, outputs=x)",
      "generator = Generator()\ntf.keras.utils.plot_model(generator, show_shapes=True, dpi=64)",
      "gen_output = generator(inp[tf.newaxis, ...], training=False)\nplt.imshow(gen_output[0, ...])",
      "loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)",
      "def generator_loss(disc_generated_output, gen_output, target):\n  gan_loss = loss_object(tf.ones_like(disc_generated_output), disc_generated_output)\n\n  # Mean absolute error\n  l1_loss = tf.reduce_mean(tf.abs(target - gen_output))\n\n  total_gen_loss = gan_loss + (LAMBDA * l1_loss)\n\n  return total_gen_loss, gan_loss, l1_loss",
      "def Discriminator():\n  initializer = tf.random_normal_initializer(0., 0.02)\n\n  inp = tf.keras.layers.Input(shape=[256, 256, 3], name='input_image')\n  tar = tf.keras.layers.Input(shape=[256, 256, 3], name='target_image')\n\n  x = tf.keras.layers.concatenate([inp, tar])  # (batch_size, 256, 256, channels*2)\n\n  down1 = downsample(64, 4, False)(x)  # (batch_size, 128, 128, 64)\n  down2 = downsample(128, 4)(down1)  # (batch_size, 64, 64, 128)\n  down3 = downsample(256, 4)(down2)  # (batch_size, 32, 32, 256)\n\n  zero_pad1 = tf.keras.layers.ZeroPadding2D()(down3)  # (batch_size, 34, 34, 256)\n  conv = tf.keras.layers.Conv2D(512, 4, strides=1,\n                                kernel_initializer=initializer,\n                                use_bias=False)(zero_pad1)  # (batch_size, 31, 31, 512)\n\n  batchnorm1 = tf.keras.layers.BatchNormalization()(conv)\n\n  leaky_relu = tf.keras.layers.LeakyReLU()(batchnorm1)\n\n  zero_pad2 = tf.keras.layers.ZeroPadding2D()(leaky_relu)  # (batch_size, 33, 33, 512)\n\n  last = tf.keras.layers.Conv2D(1, 4, strides=1,\n                                kernel_initializer=initializer)(zero_pad2)  # (batch_size, 30, 30, 1)\n\n  return tf.keras.Model(inputs=[inp, tar], outputs=last)",
      "discriminator = Discriminator()\ntf.keras.utils.plot_model(discriminator, show_shapes=True, dpi=64)",
      "disc_out = discriminator([inp[tf.newaxis, ...], gen_output], training=False)\nplt.imshow(disc_out[0, ..., -1], vmin=-20, vmax=20, cmap='RdBu_r')\nplt.colorbar()",
      "def discriminator_loss(disc_real_output, disc_generated_output):\n  real_loss = loss_object(tf.ones_like(disc_real_output), disc_real_output)\n\n  generated_loss = loss_object(tf.zeros_like(disc_generated_output), disc_generated_output)\n\n  total_disc_loss = real_loss + generated_loss\n\n  return total_disc_loss",
      "generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\ndiscriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)",
      "checkpoint_dir = './training_checkpoints'\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\ncheckpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n                                 discriminator_optimizer=discriminator_optimizer,\n                                 generator=generator,\n                                 discriminator=discriminator)",
      "def generate_images(model, test_input, tar):\n  prediction = model(test_input, training=True)\n  plt.figure(figsize=(15, 15))\n\n  display_list = [test_input[0], tar[0], prediction[0]]\n  title = ['Input Image', 'Ground Truth', 'Predicted Image']\n\n  for i in range(3):\n    plt.subplot(1, 3, i+1)\n    plt.title(title[i])\n    # Getting the pixel values in the [0, 1] range to plot.\n    plt.imshow(display_list[i] * 0.5 + 0.5)\n    plt.axis('off')\n  plt.show()",
      "log_dir=\"logs/\"\n\nsummary_writer = tf.summary.create_file_writer(\n  log_dir + \"fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))",
      "@tf.function\ndef train_step(input_image, target, step):\n  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n    gen_output = generator(input_image, training=True)\n\n    disc_real_output = discriminator([input_image, target], training=True)\n    disc_generated_output = discriminator([input_image, gen_output], training=True)\n\n    gen_total_loss, gen_gan_loss, gen_l1_loss = generator_loss(disc_generated_output, gen_output, target)\n    disc_loss = discriminator_loss(disc_real_output, disc_generated_output)\n\n  generator_gradients = gen_tape.gradient(gen_total_loss,\n                                          generator.trainable_variables)\n  discriminator_gradients = disc_tape.gradient(disc_loss,\n                                               discriminator.trainable_variables)\n\n  generator_optimizer.apply_gradients(zip(generator_gradients,\n                                          generator.trainable_variables))\n  discriminator_optimizer.apply_gradients(zip(discriminator_gradients,\n                                              discriminator.trainable_variables))\n\n  with summary_writer.as_default():\n    tf.summary.scalar('gen_total_loss', gen_total_loss, step=step//1000)\n    tf.summary.scalar('gen_gan_loss', gen_gan_loss, step=step//1000)\n    tf.summary.scalar('gen_l1_loss', gen_l1_loss, step=step//1000)\n    tf.summary.scalar('disc_loss', disc_loss, step=step//1000)",
      "def fit(train_ds, test_ds, steps):\n  example_input, example_target = next(iter(test_ds.take(1)))\n  start = time.time()\n\n  for step, (input_image, target) in train_ds.repeat().take(steps).enumerate():\n    if (step) % 1000 == 0:\n      display.clear_output(wait=True)\n\n      if step != 0:\n        print(f'Time taken for 1000 steps: {time.time()-start:.2f} sec\\n')\n\n      start = time.time()\n\n      generate_images(generator, example_input, example_target)\n      print(f\"Step: {step//1000}k\")\n\n    train_step(input_image, target, step)\n\n    # Training step\n    if (step+1) % 10 == 0:\n      print('.', end='', flush=True)\n\n\n    # Save (checkpoint) the model every 5k steps\n    if (step + 1) % 5000 == 0:\n      checkpoint.save(file_prefix=checkpoint_prefix)",
      "# Restoring the latest checkpoint in checkpoint_dir\ncheckpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
    ]
  },
  {
    "url": "https://www.tensorflow.org/tutorials/keras/text_classification_with_hub",
    "title": "Text classification with TensorFlow Hub: Movie reviews\n      \n\n\n      \n      Stay organized with collections\n    \n\n      \n      Save and categorize content based on your preferences.",
    "code_snippets": [
      "import os\nimport numpy as np\n\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport tensorflow_datasets as tfds\n\nprint(\"Version: \", tf.__version__)\nprint(\"Eager mode: \", tf.executing_eagerly())\nprint(\"Hub version: \", hub.__version__)\nprint(\"GPU is\", \"available\" if tf.config.list_physical_devices(\"GPU\") else \"NOT AVAILABLE\")",
      "embedding = \"https://tfhub.dev/google/nnlm-en-dim50/2\"\nhub_layer = hub.KerasLayer(embedding, input_shape=[], \n                           dtype=tf.string, trainable=True)\nhub_layer(train_examples_batch[:3])",
      "model = tf.keras.Sequential()\nmodel.add(hub_layer)\nmodel.add(tf.keras.layers.Dense(16, activation='relu'))\nmodel.add(tf.keras.layers.Dense(1))\n\nmodel.summary()",
      "model.compile(optimizer='adam',\n              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n              metrics=['accuracy'])"
    ]
  },
  {
    "url": "https://www.tensorflow.org/tutorials/keras/classification",
    "title": "Basic classification: Classify images of clothing\n      \n\n\n      \n      Stay organized with collections\n    \n\n      \n      Save and categorize content based on your preferences.",
    "code_snippets": [
      "# TensorFlow and tf.keras\nimport tensorflow as tf\n\n# Helper libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nprint(tf.__version__)",
      "fashion_mnist = tf.keras.datasets.fashion_mnist\n\n(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()",
      "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']",
      "model = tf.keras.Sequential([\n    tf.keras.layers.Flatten(input_shape=(28, 28)),\n    tf.keras.layers.Dense(128, activation='relu'),\n    tf.keras.layers.Dense(10)\n])",
      "model.compile(optimizer='adam',\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])",
      "probability_model = tf.keras.Sequential([model, \n                                         tf.keras.layers.Softmax()])",
      "def plot_image(i, predictions_array, true_label, img):\n  true_label, img = true_label[i], img[i]\n  plt.grid(False)\n  plt.xticks([])\n  plt.yticks([])\n\n  plt.imshow(img, cmap=plt.cm.binary)\n\n  predicted_label = np.argmax(predictions_array)\n  if predicted_label == true_label:\n    color = 'blue'\n  else:\n    color = 'red'\n\n  plt.xlabel(\"{} {:2.0f}% ({})\".format(class_names[predicted_label],\n                                100*np.max(predictions_array),\n                                class_names[true_label]),\n                                color=color)\n\ndef plot_value_array(i, predictions_array, true_label):\n  true_label = true_label[i]\n  plt.grid(False)\n  plt.xticks(range(10))\n  plt.yticks([])\n  thisplot = plt.bar(range(10), predictions_array, color=\"#777777\")\n  plt.ylim([0, 1])\n  predicted_label = np.argmax(predictions_array)\n\n  thisplot[predicted_label].set_color('red')\n  thisplot[true_label].set_color('blue')"
    ]
  },
  {
    "url": "https://www.tensorflow.org/tutorials/load_data/text",
    "title": "Load text\n      \n\n\n      \n      Stay organized with collections\n    \n\n      \n      Save and categorize content based on your preferences.",
    "code_snippets": [
      "import collections\nimport pathlib\n\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\n\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import losses\nfrom tensorflow.keras import utils\nfrom tensorflow.keras.layers import TextVectorization\n\nimport tensorflow_datasets as tfds\nimport tensorflow_text as tf_text",
      "raw_train_ds = raw_train_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\nraw_val_ds = raw_val_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\nraw_test_ds = raw_test_ds.prefetch(buffer_size=tf.data.AUTOTUNE)",
      "Question: tf.Tensor(b'\"unit testing of setters and getters teacher wanted us to do a comprehensive unit test. for me, this will be the first time that i use junit. i am confused about testing set and get methods. do you think should i test them? if the answer is yes; is this code enough for testing?..  public void testsetandget(){.    int a = 10;.    class firstclass = new class();.    firstclass.setvalue(10);.    int value = firstclass.getvalue();.    assert.asserttrue(\"\"error\"\", value==a);.  }...in my code, i think if there is an error, we can\\'t know that the error is deriving because of setter or getter.\"\\n', shape=(), dtype=string)\nLabel: tf.Tensor(1, shape=(), dtype=int32)",
      "binary_model = tf.keras.Sequential([\n    binary_vectorize_layer,\n    layers.Dense(4)])\n\nbinary_model.compile(\n    loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n    optimizer='adam',\n    metrics=['accuracy'])\n\ntf.keras.utils.plot_model(binary_model, show_shapes=True)",
      "def create_model(vocab_size, num_labels, vectorizer=None):\n  my_layers =[]\n  if vectorizer is not None:\n    my_layers = [vectorizer]\n\n  my_layers.extend([\n      layers.Embedding(vocab_size, 64, mask_zero=True),\n      layers.Dropout(0.5),\n      layers.Conv1D(64, 5, padding=\"valid\", activation=\"relu\", strides=2),\n      layers.GlobalMaxPooling1D(),\n      layers.Dense(num_labels)\n  ])\n\n  model = tf.keras.Sequential(my_layers)\n  return model",
      "# `vocab_size` is `VOCAB_SIZE + 1` since `0` is used additionally for padding.\nint_model = create_model(vocab_size=VOCAB_SIZE + 1, num_labels=4, vectorizer=int_vectorize_layer)\n\ntf.keras.utils.plot_model(int_model, show_shapes=True)",
      "loaded = tf.saved_model.load('bin.tf')",
      "loaded.serve(tf.constant(['How do you sort a list?'])).numpy()",
      "def labeler(example, index):\n  return example, tf.cast(index, tf.int64)",
      "labeled_data_sets = []\n\nfor i, file_name in enumerate(FILE_NAMES):\n  lines_dataset = tf.data.TextLineDataset(str(parent_dir/file_name))\n  labeled_dataset = lines_dataset.map(lambda ex: labeler(ex, i))\n  labeled_data_sets.append(labeled_dataset)",
      "class MyTokenizer(tf.keras.layers.Layer):\n  def __init__(self):\n    super().__init__()\n    self.tokenizer = tf_text.UnicodeScriptTokenizer()\n\n  def call(self, text):\n    lower_case = tf_text.case_fold_utf8(text)\n    result = self.tokenizer.tokenize(lower_case)\n    # If you pass a batch of strings, it will return a RaggedTensor.\n    if isinstance(result, tf.RaggedTensor):\n      # Convert to dense 0-padded.\n      result = result.to_tensor()\n    return result",
      "<_MapDataset element_spec=(TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))>",
      "tf.Tensor(\n[b'and' b'proto' b',' b'pherusa' b'and' b'dynamene' b',' b'dexamene' b','\n b'amphinome' b'and' b'callianeira' b','], shape=(13,), dtype=string)\n\ntf.Tensor(2, shape=(), dtype=int64)",
      "tokenized_ds = tokenized_ds.cache().prefetch(tf.data.AUTOTUNE)\n\nvocab_count = collections.Counter()\nfor toks, labels in tokenized_ds.ragged_batch(1000):\n  toks = tf.reshape(toks, [-1])\n  for tok in toks.numpy():\n    vocab_count[tok] += 1\n\nvocab = [tok for tok, count in vocab_count.most_common(VOCAB_SIZE)]\n\nprint(\"First five vocab entries:\", vocab[:5])\nprint()",
      "class MyVocabTable(tf.keras.layers.Layer):\n  def __init__(self, vocab):\n    super().__init__()\n    self.keys = [''] + vocab\n    self.values = range(len(self.keys))\n\n    self.init = tf.lookup.KeyValueTensorInitializer(\n        self.keys, self.values, key_dtype=tf.string, value_dtype=tf.int64)\n\n    num_oov_buckets = 1\n\n    self.table = tf.lookup.StaticVocabularyTable(self.init, num_oov_buckets)\n\n  def call(self, x):\n    result = self.table.lookup(x)\n    return result",
      "vocab_table = MyVocabTable(['a','b','c'])\nvocab_table(tf.constant([''] + list('abcdefghi')))",
      "<tf.Tensor: shape=(10,), dtype=int64, numpy=array([0, 1, 2, 3, 4, 4, 4, 4, 4, 4])>",
      "preprocess_text = tf.keras.Sequential([\n    tokenizer,\n    vocab_table\n])",
      "Text batch shape:  (64, 18)\nLabel batch shape:  (64,)\nFirst text example:  tf.Tensor(\n[   3 7805    1 7806    3 7807    1 7808    1 7809    3 9790    1    0\n    0    0    0    0], shape=(18,), dtype=int64)\nFirst label example:  tf.Tensor(2, shape=(), dtype=int64)",
      "train_data = train_data.prefetch(tf.data.AUTOTUNE)\nvalidation_data = validation_data.prefetch(tf.data.AUTOTUNE)",
      "tf.keras.utils.plot_model(model, show_shapes=True)",
      "export_model = tf.keras.Sequential([\n    preprocess_text,\n    model\n])",
      "# Create a test dataset of raw strings.\ntest_ds = all_labeled_data.take(VALIDATION_SIZE).batch(BATCH_SIZE)\ntest_ds = test_ds.cache().prefetch(tf.data.AUTOTUNE)\ntest_ds",
      "<_PrefetchDataset element_spec=(TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None,), dtype=tf.int64, name=None))>",
      "tf.saved_model.save(export_model, 'export.tf')",
      "loaded = tf.saved_model.load('export.tf')",
      "export_model(tf.constant(['The field bristled with the long and deadly spears which they bore.'])).numpy()",
      "loaded(tf.constant(['The field bristled with the long and deadly spears which they bore.'])).numpy()",
      "inputs = [\n    \"Join'd to th' Ionians with their flowing robes,\",  # Label: 1\n    \"the allies, and his armour flashed about him so that he seemed to all\",  # Label: 2\n    \"And with loud clangor of his arms he fell.\",  # Label: 0\n]\n\npredicted_scores = export_model.predict(inputs)\npredicted_labels = tf.math.argmax(predicted_scores, axis=1)\n\nfor input, label in zip(inputs, predicted_labels):\n  print(\"Question: \", input)\n  print(\"Predicted label: \", label.numpy())",
      "def vectorize_text(text, label):\n  text = tf.expand_dims(text, -1)\n  return vectorize_layer(text), label",
      "# Configure datasets for performance as before.\ntrain_ds = train_ds.cache().prefetch(tf.data.AUTOTUNE)\nval_ds = val_ds.cache().prefetch(tf.data.AUTOTUNE)",
      "export_model = tf.keras.Sequential(\n    [vectorize_layer, model,\n     layers.Activation('sigmoid')])\n\nexport_model.compile(\n    loss=losses.SparseCategoricalCrossentropy(from_logits=False),\n    optimizer='adam',\n    metrics=['accuracy'])"
    ]
  },
  {
    "url": "https://www.tensorflow.org/tutorials/keras/overfit_and_underfit",
    "title": "Overfit and underfit\n      \n\n\n      \n      Stay organized with collections\n    \n\n      \n      Save and categorize content based on your preferences.",
    "code_snippets": [
      "import tensorflow as tf\n\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import regularizers\n\nprint(tf.__version__)",
      "gz = tf.keras.utils.get_file('HIGGS.csv.gz', 'http://mlphysics.ics.uci.edu/data/higgs/HIGGS.csv.gz')",
      "ds = tf.data.experimental.CsvDataset(gz,[float(),]*(FEATURES+1), compression_type=\"GZIP\")",
      "def pack_row(*row):\n  label = row[0]\n  features = tf.stack(row[1:],1)\n  return features, label",
      "lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n  0.001,\n  decay_steps=STEPS_PER_EPOCH*1000,\n  decay_rate=1,\n  staircase=False)\n\ndef get_optimizer():\n  return tf.keras.optimizers.Adam(lr_schedule)",
      "def get_callbacks(name):\n  return [\n    tfdocs.modeling.EpochDots(),\n    tf.keras.callbacks.EarlyStopping(monitor='val_binary_crossentropy', patience=200),\n    tf.keras.callbacks.TensorBoard(logdir/name),\n  ]",
      "def compile_and_fit(model, name, optimizer=None, max_epochs=10000):\n  if optimizer is None:\n    optimizer = get_optimizer()\n  model.compile(optimizer=optimizer,\n                loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n                metrics=[\n                  tf.keras.metrics.BinaryCrossentropy(\n                      from_logits=True, name='binary_crossentropy'),\n                  'accuracy'])\n\n  model.summary()\n\n  history = model.fit(\n    train_ds,\n    steps_per_epoch = STEPS_PER_EPOCH,\n    epochs=max_epochs,\n    validation_data=validate_ds,\n    callbacks=get_callbacks(name),\n    verbose=0)\n  return history",
      "tiny_model = tf.keras.Sequential([\n    layers.Dense(16, activation='elu', input_shape=(FEATURES,)),\n    layers.Dense(1)\n])",
      "small_model = tf.keras.Sequential([\n    # `input_shape` is only required here so that `.summary` works.\n    layers.Dense(16, activation='elu', input_shape=(FEATURES,)),\n    layers.Dense(16, activation='elu'),\n    layers.Dense(1)\n])",
      "medium_model = tf.keras.Sequential([\n    layers.Dense(64, activation='elu', input_shape=(FEATURES,)),\n    layers.Dense(64, activation='elu'),\n    layers.Dense(64, activation='elu'),\n    layers.Dense(1)\n])",
      "large_model = tf.keras.Sequential([\n    layers.Dense(512, activation='elu', input_shape=(FEATURES,)),\n    layers.Dense(512, activation='elu'),\n    layers.Dense(512, activation='elu'),\n    layers.Dense(512, activation='elu'),\n    layers.Dense(1)\n])",
      "l2_model = tf.keras.Sequential([\n    layers.Dense(512, activation='elu',\n                 kernel_regularizer=regularizers.l2(0.001),\n                 input_shape=(FEATURES,)),\n    layers.Dense(512, activation='elu',\n                 kernel_regularizer=regularizers.l2(0.001)),\n    layers.Dense(512, activation='elu',\n                 kernel_regularizer=regularizers.l2(0.001)),\n    layers.Dense(512, activation='elu',\n                 kernel_regularizer=regularizers.l2(0.001)),\n    layers.Dense(1)\n])\n\nregularizer_histories['l2'] = compile_and_fit(l2_model, \"regularizers/l2\")",
      "result = l2_model(features)\nregularization_loss=tf.add_n(l2_model.losses)",
      "dropout_model = tf.keras.Sequential([\n    layers.Dense(512, activation='elu', input_shape=(FEATURES,)),\n    layers.Dropout(0.5),\n    layers.Dense(512, activation='elu'),\n    layers.Dropout(0.5),\n    layers.Dense(512, activation='elu'),\n    layers.Dropout(0.5),\n    layers.Dense(512, activation='elu'),\n    layers.Dropout(0.5),\n    layers.Dense(1)\n])\n\nregularizer_histories['dropout'] = compile_and_fit(dropout_model, \"regularizers/dropout\")",
      "combined_model = tf.keras.Sequential([\n    layers.Dense(512, kernel_regularizer=regularizers.l2(0.0001),\n                 activation='elu', input_shape=(FEATURES,)),\n    layers.Dropout(0.5),\n    layers.Dense(512, kernel_regularizer=regularizers.l2(0.0001),\n                 activation='elu'),\n    layers.Dropout(0.5),\n    layers.Dense(512, kernel_regularizer=regularizers.l2(0.0001),\n                 activation='elu'),\n    layers.Dropout(0.5),\n    layers.Dense(512, kernel_regularizer=regularizers.l2(0.0001),\n                 activation='elu'),\n    layers.Dropout(0.5),\n    layers.Dense(1)\n])\n\nregularizer_histories['combined'] = compile_and_fit(combined_model, \"regularizers/combined\")"
    ]
  },
  {
    "url": "https://www.tensorflow.org/tutorials/distribute/save_and_load",
    "title": "Save and load a model using a distribution strategy\n      \n\n\n      \n      Stay organized with collections\n    \n\n      \n      Save and categorize content based on your preferences.",
    "code_snippets": [
      "import tensorflow_datasets as tfds\n\nimport tensorflow as tf",
      "mirrored_strategy = tf.distribute.MirroredStrategy()\n\ndef get_data():\n  datasets = tfds.load(name='mnist', as_supervised=True)\n  mnist_train, mnist_test = datasets['train'], datasets['test']\n\n  BUFFER_SIZE = 10000\n\n  BATCH_SIZE_PER_REPLICA = 64\n  BATCH_SIZE = BATCH_SIZE_PER_REPLICA * mirrored_strategy.num_replicas_in_sync\n\n  def scale(image, label):\n    image = tf.cast(image, tf.float32)\n    image /= 255\n\n    return image, label\n\n  train_dataset = mnist_train.map(scale).cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n  eval_dataset = mnist_test.map(scale).batch(BATCH_SIZE)\n\n  return train_dataset, eval_dataset\n\ndef get_model():\n  with mirrored_strategy.scope():\n    model = tf.keras.Sequential([\n        tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(28, 28, 1)),\n        tf.keras.layers.MaxPooling2D(),\n        tf.keras.layers.Flatten(),\n        tf.keras.layers.Dense(64, activation='relu'),\n        tf.keras.layers.Dense(10)\n    ])\n\n    model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n                  optimizer=tf.keras.optimizers.Adam(),\n                  metrics=[tf.metrics.SparseCategoricalAccuracy()])\n    return model",
      "restored_keras_model = tf.keras.models.load_model(keras_model_path)\nrestored_keras_model.fit(train_dataset, epochs=2)",
      "another_strategy = tf.distribute.OneDeviceStrategy('/cpu:0')\nwith another_strategy.scope():\n  restored_keras_model_ds = tf.keras.models.load_model(keras_model_path)\n  restored_keras_model_ds.fit(train_dataset, epochs=2)",
      "model = get_model()  # get a fresh model\nsaved_model_path = '/tmp/tf_save'\ntf.saved_model.save(model, saved_model_path)",
      "DEFAULT_FUNCTION_KEY = 'serving_default'\nloaded = tf.saved_model.load(saved_model_path)\ninference_func = loaded.signatures[DEFAULT_FUNCTION_KEY]",
      "another_strategy = tf.distribute.MirroredStrategy()\nwith another_strategy.scope():\n  loaded = tf.saved_model.load(saved_model_path)\n  inference_func = loaded.signatures[DEFAULT_FUNCTION_KEY]\n\n  dist_predict_dataset = another_strategy.experimental_distribute_dataset(\n      predict_dataset)\n\n  # Calling the function in a distributed manner\n  for batch in dist_predict_dataset:\n    result = another_strategy.run(inference_func, args=(batch,))\n    print(result)\n    break",
      "import tensorflow_hub as hub\n\ndef build_model(loaded):\n  x = tf.keras.layers.Input(shape=(28, 28, 1), name='input_x')\n  # Wrap what's loaded to a KerasLayer\n  keras_layer = hub.KerasLayer(loaded, trainable=True)(x)\n  model = tf.keras.Model(x, keras_layer)\n  return model\n\nanother_strategy = tf.distribute.MirroredStrategy()\nwith another_strategy.scope():\n  loaded = tf.saved_model.load(saved_model_path)\n  model = build_model(loaded)\n\n  model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n                optimizer=tf.keras.optimizers.Adam(),\n                metrics=[tf.metrics.SparseCategoricalAccuracy()])\n  model.fit(train_dataset, epochs=2)",
      "model = get_model()\n\n# Saving the model using Keras `Model.save`\nmodel.save(saved_model_path)\n\nanother_strategy = tf.distribute.MirroredStrategy()\n# Loading the model using the lower-level API\nwith another_strategy.scope():\n  loaded = tf.saved_model.load(saved_model_path)",
      "model = get_model()\n\n# Saving the model to a path on localhost.\nsaved_model_path = '/tmp/tf_save'\nsave_options = tf.saved_model.SaveOptions(experimental_io_device='/job:localhost')\nmodel.save(saved_model_path, options=save_options)\n\n# Loading the model from a path on localhost.\nanother_strategy = tf.distribute.MirroredStrategy()\nwith another_strategy.scope():\n  load_options = tf.saved_model.LoadOptions(experimental_io_device='/job:localhost')\n  loaded = tf.keras.models.load_model(saved_model_path, options=load_options)",
      "class SubclassedModel(tf.keras.Model):\n  \"\"\"Example model defined by subclassing `tf.keras.Model`.\"\"\"\n\n  output_name = 'output_layer'\n\n  def __init__(self):\n    super(SubclassedModel, self).__init__()\n    self._dense_layer = tf.keras.layers.Dense(\n        5, dtype=tf.dtypes.float32, name=self.output_name)\n\n  def call(self, inputs):\n    return self._dense_layer(inputs)\n\nmy_model = SubclassedModel()\ntry:\n  my_model.save(saved_model_path)\nexcept ValueError as e:\n  print(f'{type(e).__name__}: ', *e.args)",
      "tf.saved_model.save(my_model, saved_model_path)\nx = tf.saved_model.load(saved_model_path)\nx.signatures",
      "BATCH_SIZE_PER_REPLICA = 4\nBATCH_SIZE = BATCH_SIZE_PER_REPLICA * mirrored_strategy.num_replicas_in_sync\n\ndataset_size = 100\ndataset = tf.data.Dataset.from_tensors(\n    (tf.range(5, dtype=tf.float32), tf.range(5, dtype=tf.float32))\n    ).repeat(dataset_size).batch(BATCH_SIZE)\n\nmy_model.compile(optimizer='adam', loss='mean_squared_error')\nmy_model.fit(dataset, epochs=2)\n\nprint(my_model.save_spec() is None)\nmy_model.save(saved_model_path)"
    ]
  },
  {
    "url": "https://www.tensorflow.org/tutorials/video/transfer_learning_with_movinet",
    "title": "Transfer learning for video classification with MoViNet\n      \n\n\n      \n      Stay organized with collections\n    \n\n      \n      Save and categorize content based on your preferences.",
    "code_snippets": [
      "import tqdm\nimport random\nimport pathlib\nimport itertools\nimport collections\n\nimport cv2\nimport numpy as np\nimport remotezip as rz\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport keras\nimport tensorflow as tf\nimport tensorflow_hub as hub\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.losses import SparseCategoricalCrossentropy\n\n# Import the MoViNet model from TensorFlow Models (tf-models-official) for the MoViNet model\nfrom official.projects.movinet.modeling import movinet\nfrom official.projects.movinet.modeling import movinet_model",
      "def list_files_per_class(zip_url):\n  \"\"\"\n    List the files in each class of the dataset given the zip URL.\n\n    Args:\n      zip_url: URL from which the files can be unzipped. \n\n    Return:\n      files: List of files in each of the classes.\n  \"\"\"\n  files = []\n  with rz.RemoteZip(URL) as zip:\n    for zip_info in zip.infolist():\n      files.append(zip_info.filename)\n  return files\n\ndef get_class(fname):\n  \"\"\"\n    Retrieve the name of the class given a filename.\n\n    Args:\n      fname: Name of the file in the UCF101 dataset.\n\n    Return:\n      Class that the file belongs to.\n  \"\"\"\n  return fname.split('_')[-3]\n\ndef get_files_per_class(files):\n  \"\"\"\n    Retrieve the files that belong to each class. \n\n    Args:\n      files: List of files in the dataset.\n\n    Return:\n      Dictionary of class names (key) and files (values).\n  \"\"\"\n  files_for_class = collections.defaultdict(list)\n  for fname in files:\n    class_name = get_class(fname)\n    files_for_class[class_name].append(fname)\n  return files_for_class\n\ndef download_from_zip(zip_url, to_dir, file_names):\n  \"\"\"\n    Download the contents of the zip file from the zip URL.\n\n    Args:\n      zip_url: Zip URL containing data.\n      to_dir: Directory to download data to.\n      file_names: Names of files to download.\n  \"\"\"\n  with rz.RemoteZip(zip_url) as zip:\n    for fn in tqdm.tqdm(file_names):\n      class_name = get_class(fn)\n      zip.extract(fn, str(to_dir / class_name))\n      unzipped_file = to_dir / class_name / fn\n\n      fn = pathlib.Path(fn).parts[-1]\n      output_file = to_dir / class_name / fn\n      unzipped_file.rename(output_file,)\n\ndef split_class_lists(files_for_class, count):\n  \"\"\"\n    Returns the list of files belonging to a subset of data as well as the remainder of\n    files that need to be downloaded.\n\n    Args:\n      files_for_class: Files belonging to a particular class of data.\n      count: Number of files to download.\n\n    Return:\n      split_files: Files belonging to the subset of data.\n      remainder: Dictionary of the remainder of files that need to be downloaded.\n  \"\"\"\n  split_files = []\n  remainder = {}\n  for cls in files_for_class:\n    split_files.extend(files_for_class[cls][:count])\n    remainder[cls] = files_for_class[cls][count:]\n  return split_files, remainder\n\ndef download_ufc_101_subset(zip_url, num_classes, splits, download_dir):\n  \"\"\"\n    Download a subset of the UFC101 dataset and split them into various parts, such as\n    training, validation, and test. \n\n    Args:\n      zip_url: Zip URL containing data.\n      num_classes: Number of labels.\n      splits: Dictionary specifying the training, validation, test, etc. (key) division of data \n              (value is number of files per split).\n      download_dir: Directory to download data to.\n\n    Return:\n      dir: Posix path of the resulting directories containing the splits of data.\n  \"\"\"\n  files = list_files_per_class(zip_url)\n  for f in files:\n    tokens = f.split('/')\n    if len(tokens) <= 2:\n      files.remove(f) # Remove that item from the list if it does not have a filename\n\n  files_for_class = get_files_per_class(files)\n\n  classes = list(files_for_class.keys())[:num_classes]\n\n  for cls in classes:\n    new_files_for_class = files_for_class[cls]\n    random.shuffle(new_files_for_class)\n    files_for_class[cls] = new_files_for_class\n\n  # Only use the number of classes you want in the dictionary\n  files_for_class = {x: files_for_class[x] for x in list(files_for_class)[:num_classes]}\n\n  dirs = {}\n  for split_name, split_count in splits.items():\n    print(split_name, \":\")\n    split_dir = download_dir / split_name\n    split_files, files_for_class = split_class_lists(files_for_class, split_count)\n    download_from_zip(zip_url, split_dir, split_files)\n    dirs[split_name] = split_dir\n\n  return dirs\n\ndef format_frames(frame, output_size):\n  \"\"\"\n    Pad and resize an image from a video.\n\n    Args:\n      frame: Image that needs to resized and padded. \n      output_size: Pixel size of the output frame image.\n\n    Return:\n      Formatted frame with padding of specified output size.\n  \"\"\"\n  frame = tf.image.convert_image_dtype(frame, tf.float32)\n  frame = tf.image.resize_with_pad(frame, *output_size)\n  return frame\n\ndef frames_from_video_file(video_path, n_frames, output_size = (224,224), frame_step = 15):\n  \"\"\"\n    Creates frames from each video file present for each category.\n\n    Args:\n      video_path: File path to the video.\n      n_frames: Number of frames to be created per video file.\n      output_size: Pixel size of the output frame image.\n\n    Return:\n      An NumPy array of frames in the shape of (n_frames, height, width, channels).\n  \"\"\"\n  # Read each video frame by frame\n  result = []\n  src = cv2.VideoCapture(str(video_path))  \n\n  video_length = src.get(cv2.CAP_PROP_FRAME_COUNT)\n\n  need_length = 1 + (n_frames - 1) * frame_step\n\n  if need_length > video_length:\n    start = 0\n  else:\n    max_start = video_length - need_length\n    start = random.randint(0, max_start + 1)\n\n  src.set(cv2.CAP_PROP_POS_FRAMES, start)\n  # ret is a boolean indicating whether read was successful, frame is the image itself\n  ret, frame = src.read()\n  result.append(format_frames(frame, output_size))\n\n  for _ in range(n_frames - 1):\n    for _ in range(frame_step):\n      ret, frame = src.read()\n    if ret:\n      frame = format_frames(frame, output_size)\n      result.append(frame)\n    else:\n      result.append(np.zeros_like(result[0]))\n  src.release()\n  result = np.array(result)[..., [2, 1, 0]]\n\n  return result\n\nclass FrameGenerator:\n  def __init__(self, path, n_frames, training = False):\n    \"\"\" Returns a set of frames with their associated label. \n\n      Args:\n        path: Video file paths.\n        n_frames: Number of frames. \n        training: Boolean to determine if training dataset is being created.\n    \"\"\"\n    self.path = path\n    self.n_frames = n_frames\n    self.training = training\n    self.class_names = sorted(set(p.name for p in self.path.iterdir() if p.is_dir()))\n    self.class_ids_for_name = dict((name, idx) for idx, name in enumerate(self.class_names))\n\n  def get_files_and_class_names(self):\n    video_paths = list(self.path.glob('*/*.avi'))\n    classes = [p.parent.name for p in video_paths] \n    return video_paths, classes\n\n  def __call__(self):\n    video_paths, classes = self.get_files_and_class_names()\n\n    pairs = list(zip(video_paths, classes))\n\n    if self.training:\n      random.shuffle(pairs)\n\n    for path, name in pairs:\n      video_frames = frames_from_video_file(path, self.n_frames) \n      label = self.class_ids_for_name[name] # Encode labels\n      yield video_frames, label",
      "batch_size = 8\nnum_frames = 8\n\noutput_signature = (tf.TensorSpec(shape = (None, None, None, 3), dtype = tf.float32),\n                    tf.TensorSpec(shape = (), dtype = tf.int16))\n\ntrain_ds = tf.data.Dataset.from_generator(FrameGenerator(subset_paths['train'], num_frames, training = True),\n                                          output_signature = output_signature)\ntrain_ds = train_ds.batch(batch_size)\n\ntest_ds = tf.data.Dataset.from_generator(FrameGenerator(subset_paths['test'], num_frames),\n                                         output_signature = output_signature)\ntest_ds = test_ds.batch(batch_size)",
      "gru = layers.GRU(units=4, return_sequences=True, return_state=True)\n\ninputs = tf.random.normal(shape=[1, 10, 8]) # (batch, sequence, channels)\n\nresult, state = gru(inputs) # Run it all at once",
      "model_id = 'a0'\nresolution = 224\n\ntf.keras.backend.clear_session()\n\nbackbone = movinet.Movinet(model_id=model_id)\nbackbone.trainable = False\n\n# Set num_classes=600 to load the pre-trained weights from the original model\nmodel = movinet_model.MovinetClassifier(backbone=backbone, num_classes=600)\nmodel.build([None, None, None, None, 3])\n\n# Load pre-trained weights\n!wget https://storage.googleapis.com/tf_model_garden/vision/movinet/movinet_a0_base.tar.gz -O movinet_a0_base.tar.gz -q\n!tar -xvf movinet_a0_base.tar.gz\n\ncheckpoint_dir = f'movinet_{model_id}_base'\ncheckpoint_path = tf.train.latest_checkpoint(checkpoint_dir)\ncheckpoint = tf.train.Checkpoint(model=model)\nstatus = checkpoint.restore(checkpoint_path)\nstatus.assert_existing_objects_matched()",
      "def build_classifier(batch_size, num_frames, resolution, backbone, num_classes):\n  \"\"\"Builds a classifier on top of a backbone model.\"\"\"\n  model = movinet_model.MovinetClassifier(\n      backbone=backbone,\n      num_classes=num_classes)\n  model.build([batch_size, num_frames, resolution, resolution, 3])\n\n  return model",
      "num_epochs = 2\n\nloss_obj = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n\noptimizer = tf.keras.optimizers.Adam(learning_rate = 0.001)\n\nmodel.compile(loss=loss_obj, optimizer=optimizer, metrics=['accuracy'])",
      "def get_actual_predicted_labels(dataset):\n  \"\"\"\n    Create a list of actual ground truth values and the predictions from the model.\n\n    Args:\n      dataset: An iterable data structure, such as a TensorFlow Dataset, with features and labels.\n\n    Return:\n      Ground truth and predicted values for a particular dataset.\n  \"\"\"\n  actual = [labels for _, labels in dataset.unbatch()]\n  predicted = model.predict(dataset)\n\n  actual = tf.stack(actual, axis=0)\n  predicted = tf.concat(predicted, axis=0)\n  predicted = tf.argmax(predicted, axis=1)\n\n  return actual, predicted",
      "def plot_confusion_matrix(actual, predicted, labels, ds_type):\n  cm = tf.math.confusion_matrix(actual, predicted)\n  ax = sns.heatmap(cm, annot=True, fmt='g')\n  sns.set(rc={'figure.figsize':(12, 12)})\n  sns.set(font_scale=1.4)\n  ax.set_title('Confusion matrix of action recognition for ' + ds_type)\n  ax.set_xlabel('Predicted Action')\n  ax.set_ylabel('Actual Action')\n  plt.xticks(rotation=90)\n  plt.yticks(rotation=0)\n  ax.xaxis.set_ticklabels(labels)\n  ax.yaxis.set_ticklabels(labels)"
    ]
  },
  {
    "url": "https://www.tensorflow.org/tutorials/distribute/dtensor_keras_tutorial",
    "title": "Using DTensors with Keras\n      \n\n\n      \n      Stay organized with collections\n    \n\n      \n      Save and categorize content based on your preferences.",
    "code_snippets": [
      "import tensorflow as tf\nimport tensorflow_datasets as tfds\nfrom tensorflow.experimental import dtensor",
      "def configure_virtual_cpus(ncpu):\n  phy_devices = tf.config.list_physical_devices('CPU')\n  tf.config.set_logical_device_configuration(\n        phy_devices[0], \n        [tf.config.LogicalDeviceConfiguration()] * ncpu)\n\nconfigure_virtual_cpus(8)\ntf.config.list_logical_devices('CPU')\n\ndevices = [f'CPU:{i}' for i in range(8)]",
      "tf.keras.backend.experimental.enable_tf_random_generator()\ntf.keras.utils.set_random_seed(1337)",
      "model = tf.keras.models.Sequential([\n  tf.keras.layers.Flatten(input_shape=(28, 28)),\n  tf.keras.layers.Dense(128, \n                        activation='relu',\n                        name='d1',\n                        kernel_layout=unsharded_layout_2d, \n                        bias_layout=unsharded_layout_1d),\n  tf.keras.layers.Dense(10,\n                        name='d2',\n                        kernel_layout=unsharded_layout_2d, \n                        bias_layout=unsharded_layout_1d)\n])",
      "def normalize_img(image, label):\n  \"\"\"Normalizes images: `uint8` -> `float32`.\"\"\"\n  return tf.cast(image, tf.float32) / 255., label",
      "batch_size = 128\n\nds_train = ds_train.map(\n    normalize_img, num_parallel_calls=tf.data.AUTOTUNE)\nds_train = ds_train.cache()\nds_train = ds_train.shuffle(ds_info.splits['train'].num_examples)\nds_train = ds_train.batch(batch_size)\nds_train = ds_train.prefetch(tf.data.AUTOTUNE)",
      "ds_test = ds_test.map(\n    normalize_img, num_parallel_calls=tf.data.AUTOTUNE)\nds_test = ds_test.batch(batch_size)\nds_test = ds_test.cache()\nds_test = ds_test.prefetch(tf.data.AUTOTUNE)",
      "@tf.function\ndef train_step(model, x, y, optimizer, metrics):\n  with tf.GradientTape() as tape:\n    logits = model(x, training=True)\n    # tf.reduce_sum sums the batch sharded per-example loss to a replicated\n    # global loss (scalar).\n    loss = tf.reduce_sum(tf.keras.losses.sparse_categorical_crossentropy(\n        y, logits, from_logits=True))\n\n  gradients = tape.gradient(loss, model.trainable_variables)\n  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n\n  for metric in metrics.values():\n    metric.update_state(y_true=y, y_pred=logits)\n\n  loss_per_sample = loss / len(x)\n  results = {'loss': loss_per_sample}\n  return results",
      "@tf.function\ndef eval_step(model, x, y, metrics):\n  logits = model(x, training=False)\n  loss = tf.reduce_sum(tf.keras.losses.sparse_categorical_crossentropy(\n        y, logits, from_logits=True))\n\n  for metric in metrics.values():\n    metric.update_state(y_true=y, y_pred=logits)\n\n  loss_per_sample = loss / len(x)\n  results = {'eval_loss': loss_per_sample}\n  return results",
      "def pack_dtensor_inputs(images, labels, image_layout, label_layout):\n  num_local_devices = image_layout.mesh.num_local_devices()\n  images = tf.split(images, num_local_devices)\n  labels = tf.split(labels, num_local_devices)\n  images = dtensor.pack(images, image_layout)\n  labels = dtensor.pack(labels, label_layout)\n  return  images, labels",
      "optimizer = tf.keras.dtensor.experimental.optimizers.Adam(0.01, mesh=mesh)\nmetrics = {'accuracy': tf.keras.metrics.SparseCategoricalAccuracy(mesh=mesh)}\neval_metrics = {'eval_accuracy': tf.keras.metrics.SparseCategoricalAccuracy(mesh=mesh)}",
      "num_epochs = 3\n\nimage_layout = dtensor.Layout.batch_sharded(mesh, 'batch', rank=4)\nlabel_layout = dtensor.Layout.batch_sharded(mesh, 'batch', rank=1)\n\nfor epoch in range(num_epochs):\n  print(\"============================\") \n  print(\"Epoch: \", epoch)\n  for metric in metrics.values():\n    metric.reset_state()\n  step = 0\n  results = {}\n  pbar = tf.keras.utils.Progbar(target=None, stateful_metrics=[])\n  for input in ds_train:\n    images, labels = input[0], input[1]\n    images, labels = pack_dtensor_inputs(\n        images, labels, image_layout, label_layout)\n\n    results.update(train_step(model, images, labels, optimizer, metrics))\n    for metric_name, metric in metrics.items():\n      results[metric_name] = metric.result()\n\n    pbar.update(step, values=results.items(), finalize=False)\n    step += 1\n  pbar.update(step, values=results.items(), finalize=True)\n\n  for metric in eval_metrics.values():\n    metric.reset_state()\n  for input in ds_test:\n    images, labels = input[0], input[1]\n    images, labels = pack_dtensor_inputs(\n        images, labels, image_layout, label_layout)\n    results.update(eval_step(model, images, labels, eval_metrics))\n\n  for metric_name, metric in eval_metrics.items():\n    results[metric_name] = metric.result()\n\n  for metric_name, metric in results.items():\n    print(f\"{metric_name}: {metric.numpy()}\")",
      "class SubclassedModel(tf.keras.Model):\n\n  def __init__(self, name=None):\n    super().__init__(name=name)\n    self.feature = tf.keras.layers.Dense(16)\n    self.feature_2 = tf.keras.layers.Dense(24)\n    self.dropout = tf.keras.layers.Dropout(0.1)\n\n  def call(self, inputs, training=None):\n    x = self.feature(inputs)\n    x = self.dropout(x, training=training)\n    return self.feature_2(x)",
      "layout_map = tf.keras.dtensor.experimental.LayoutMap(mesh=mesh)\n\nlayout_map['feature.*kernel'] = dtensor.Layout.batch_sharded(mesh, 'batch', rank=2)\nlayout_map['feature.*bias'] = dtensor.Layout.batch_sharded(mesh, 'batch', rank=1)\n\nwith layout_map.scope():\n  subclassed_model = SubclassedModel()",
      "dtensor_input = dtensor.copy_to_mesh(tf.zeros((16, 16)), layout=unsharded_layout_2d)\n# Trigger the weights creation for subclass model\nsubclassed_model(dtensor_input)\n\nprint(subclassed_model.feature.kernel.layout)",
      "layout_map = tf.keras.dtensor.experimental.LayoutMap(mesh=mesh)\n\nlayout_map['feature.*kernel'] = dtensor.Layout.batch_sharded(mesh, 'batch', rank=2)\nlayout_map['feature.*bias'] = dtensor.Layout.batch_sharded(mesh, 'batch', rank=1)",
      "with layout_map.scope():\n  inputs = tf.keras.Input((16,), batch_size=16)\n  x = tf.keras.layers.Dense(16, name='feature')(inputs)\n  x = tf.keras.layers.Dropout(0.1)(x)\n  output = tf.keras.layers.Dense(32, name='feature_2')(x)\n  model = tf.keras.Model(inputs, output)\n\nprint(model.layers[1].kernel.layout)",
      "with layout_map.scope():\n  model = tf.keras.Sequential([\n      tf.keras.layers.Dense(16, name='feature', input_shape=(16,)),\n      tf.keras.layers.Dropout(0.1),\n      tf.keras.layers.Dense(32, name='feature_2')\n  ])\n\nprint(model.layers[2].kernel.layout)"
    ]
  },
  {
    "url": "https://www.tensorflow.org/tutorials/generative/cyclegan",
    "title": "CycleGAN\n      \n\n\n      \n      Stay organized with collections\n    \n\n      \n      Save and categorize content based on your preferences.",
    "code_snippets": [
      "import tensorflow as tf",
      "import tensorflow_datasets as tfds\nfrom tensorflow_examples.models.pix2pix import pix2pix\n\nimport os\nimport time\nimport matplotlib.pyplot as plt\nfrom IPython.display import clear_output\n\nAUTOTUNE = tf.data.AUTOTUNE",
      "def random_crop(image):\n  cropped_image = tf.image.random_crop(\n      image, size=[IMG_HEIGHT, IMG_WIDTH, 3])\n\n  return cropped_image",
      "# normalizing the images to [-1, 1]\ndef normalize(image):\n  image = tf.cast(image, tf.float32)\n  image = (image / 127.5) - 1\n  return image",
      "def random_jitter(image):\n  # resizing to 286 x 286 x 3\n  image = tf.image.resize(image, [286, 286],\n                          method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n\n  # randomly cropping to 256 x 256 x 3\n  image = random_crop(image)\n\n  # random mirroring\n  image = tf.image.random_flip_left_right(image)\n\n  return image",
      "def preprocess_image_train(image, label):\n  image = random_jitter(image)\n  image = normalize(image)\n  return image",
      "def preprocess_image_test(image, label):\n  image = normalize(image)\n  return image",
      "loss_obj = tf.keras.losses.BinaryCrossentropy(from_logits=True)",
      "def discriminator_loss(real, generated):\n  real_loss = loss_obj(tf.ones_like(real), real)\n\n  generated_loss = loss_obj(tf.zeros_like(generated), generated)\n\n  total_disc_loss = real_loss + generated_loss\n\n  return total_disc_loss * 0.5",
      "def generator_loss(generated):\n  return loss_obj(tf.ones_like(generated), generated)",
      "def calc_cycle_loss(real_image, cycled_image):\n  loss1 = tf.reduce_mean(tf.abs(real_image - cycled_image))\n\n  return LAMBDA * loss1",
      "def identity_loss(real_image, same_image):\n  loss = tf.reduce_mean(tf.abs(real_image - same_image))\n  return LAMBDA * 0.5 * loss",
      "generator_g_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\ngenerator_f_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n\ndiscriminator_x_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\ndiscriminator_y_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)",
      "checkpoint_path = \"./checkpoints/train\"\n\nckpt = tf.train.Checkpoint(generator_g=generator_g,\n                           generator_f=generator_f,\n                           discriminator_x=discriminator_x,\n                           discriminator_y=discriminator_y,\n                           generator_g_optimizer=generator_g_optimizer,\n                           generator_f_optimizer=generator_f_optimizer,\n                           discriminator_x_optimizer=discriminator_x_optimizer,\n                           discriminator_y_optimizer=discriminator_y_optimizer)\n\nckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n\n# if a checkpoint exists, restore the latest checkpoint.\nif ckpt_manager.latest_checkpoint:\n  ckpt.restore(ckpt_manager.latest_checkpoint)\n  print ('Latest checkpoint restored!!')",
      "def generate_images(model, test_input):\n  prediction = model(test_input)\n\n  plt.figure(figsize=(12, 12))\n\n  display_list = [test_input[0], prediction[0]]\n  title = ['Input Image', 'Predicted Image']\n\n  for i in range(2):\n    plt.subplot(1, 2, i+1)\n    plt.title(title[i])\n    # getting the pixel values between [0, 1] to plot it.\n    plt.imshow(display_list[i] * 0.5 + 0.5)\n    plt.axis('off')\n  plt.show()",
      "@tf.function\ndef train_step(real_x, real_y):\n  # persistent is set to True because the tape is used more than\n  # once to calculate the gradients.\n  with tf.GradientTape(persistent=True) as tape:\n    # Generator G translates X -> Y\n    # Generator F translates Y -> X.\n\n    fake_y = generator_g(real_x, training=True)\n    cycled_x = generator_f(fake_y, training=True)\n\n    fake_x = generator_f(real_y, training=True)\n    cycled_y = generator_g(fake_x, training=True)\n\n    # same_x and same_y are used for identity loss.\n    same_x = generator_f(real_x, training=True)\n    same_y = generator_g(real_y, training=True)\n\n    disc_real_x = discriminator_x(real_x, training=True)\n    disc_real_y = discriminator_y(real_y, training=True)\n\n    disc_fake_x = discriminator_x(fake_x, training=True)\n    disc_fake_y = discriminator_y(fake_y, training=True)\n\n    # calculate the loss\n    gen_g_loss = generator_loss(disc_fake_y)\n    gen_f_loss = generator_loss(disc_fake_x)\n\n    total_cycle_loss = calc_cycle_loss(real_x, cycled_x) + calc_cycle_loss(real_y, cycled_y)\n\n    # Total generator loss = adversarial loss + cycle loss\n    total_gen_g_loss = gen_g_loss + total_cycle_loss + identity_loss(real_y, same_y)\n    total_gen_f_loss = gen_f_loss + total_cycle_loss + identity_loss(real_x, same_x)\n\n    disc_x_loss = discriminator_loss(disc_real_x, disc_fake_x)\n    disc_y_loss = discriminator_loss(disc_real_y, disc_fake_y)\n\n  # Calculate the gradients for generator and discriminator\n  generator_g_gradients = tape.gradient(total_gen_g_loss, \n                                        generator_g.trainable_variables)\n  generator_f_gradients = tape.gradient(total_gen_f_loss, \n                                        generator_f.trainable_variables)\n\n  discriminator_x_gradients = tape.gradient(disc_x_loss, \n                                            discriminator_x.trainable_variables)\n  discriminator_y_gradients = tape.gradient(disc_y_loss, \n                                            discriminator_y.trainable_variables)\n\n  # Apply the gradients to the optimizer\n  generator_g_optimizer.apply_gradients(zip(generator_g_gradients, \n                                            generator_g.trainable_variables))\n\n  generator_f_optimizer.apply_gradients(zip(generator_f_gradients, \n                                            generator_f.trainable_variables))\n\n  discriminator_x_optimizer.apply_gradients(zip(discriminator_x_gradients,\n                                                discriminator_x.trainable_variables))\n\n  discriminator_y_optimizer.apply_gradients(zip(discriminator_y_gradients,\n                                                discriminator_y.trainable_variables))",
      "for epoch in range(EPOCHS):\n  start = time.time()\n\n  n = 0\n  for image_x, image_y in tf.data.Dataset.zip((train_horses, train_zebras)):\n    train_step(image_x, image_y)\n    if n % 10 == 0:\n      print ('.', end='')\n    n += 1\n\n  clear_output(wait=True)\n  # Using a consistent image (sample_horse) so that the progress of the model\n  # is clearly visible.\n  generate_images(generator_g, sample_horse)\n\n  if (epoch + 1) % 5 == 0:\n    ckpt_save_path = ckpt_manager.save()\n    print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n                                                         ckpt_save_path))\n\n  print ('Time taken for epoch {} is {} sec\\n'.format(epoch + 1,\n                                                      time.time()-start))"
    ]
  },
  {
    "url": "https://www.tensorflow.org/tutorials/generative/dcgan",
    "title": "Deep Convolutional Generative Adversarial Network\n      \n\n\n      \n      Stay organized with collections\n    \n\n      \n      Save and categorize content based on your preferences.",
    "code_snippets": [
      "import tensorflow as tf",
      "tf.__version__",
      "import glob\nimport imageio\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport PIL\nfrom tensorflow.keras import layers\nimport time\n\nfrom IPython import display",
      "(train_images, train_labels), (_, _) = tf.keras.datasets.mnist.load_data()",
      "# Batch and shuffle the data\ntrain_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)",
      "def make_generator_model():\n    model = tf.keras.Sequential()\n    model.add(layers.Dense(7*7*256, use_bias=False, input_shape=(100,)))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n\n    model.add(layers.Reshape((7, 7, 256)))\n    assert model.output_shape == (None, 7, 7, 256)  # Note: None is the batch size\n\n    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n    assert model.output_shape == (None, 7, 7, 128)\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n\n    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n    assert model.output_shape == (None, 14, 14, 64)\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n\n    model.add(layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n    assert model.output_shape == (None, 28, 28, 1)\n\n    return model",
      "generator = make_generator_model()\n\nnoise = tf.random.normal([1, 100])\ngenerated_image = generator(noise, training=False)\n\nplt.imshow(generated_image[0, :, :, 0], cmap='gray')",
      "def make_discriminator_model():\n    model = tf.keras.Sequential()\n    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same',\n                                     input_shape=[28, 28, 1]))\n    model.add(layers.LeakyReLU())\n    model.add(layers.Dropout(0.3))\n\n    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n    model.add(layers.LeakyReLU())\n    model.add(layers.Dropout(0.3))\n\n    model.add(layers.Flatten())\n    model.add(layers.Dense(1))\n\n    return model",
      "tf.Tensor([[0.00357757]], shape=(1, 1), dtype=float32)\n/tmpfs/src/tf_docs_env/lib/python3.9/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\nW0000 00:00:1723789977.415503  174689 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\nW0000 00:00:1723789977.422830  174689 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\nW0000 00:00:1723789977.426039  174689 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\nW0000 00:00:1723789977.427237  174689 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\nW0000 00:00:1723789977.428359  174689 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\nW0000 00:00:1723789977.431492  174689 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\nW0000 00:00:1723789977.432673  174689 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\nW0000 00:00:1723789977.433841  174689 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\nW0000 00:00:1723789977.435037  174689 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\nW0000 00:00:1723789977.436201  174689 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\nW0000 00:00:1723789977.454917  174689 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\nW0000 00:00:1723789977.456118  174689 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\nW0000 00:00:1723789977.457251  174689 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\nW0000 00:00:1723789977.458415  174689 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\nW0000 00:00:1723789977.468295  174689 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\nW0000 00:00:1723789977.469644  174689 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\nW0000 00:00:1723789977.471012  174689 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\nW0000 00:00:1723789977.472515  174689 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\nW0000 00:00:1723789977.474116  174689 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\nW0000 00:00:1723789977.475706  174689 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\nW0000 00:00:1723789977.477290  174689 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\nW0000 00:00:1723789977.478837  174689 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\nW0000 00:00:1723789977.480442  174689 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\nW0000 00:00:1723789977.482068  174689 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\nW0000 00:00:1723789977.483698  174689 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\nW0000 00:00:1723789977.485061  174689 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\nW0000 00:00:1723789977.486803  174689 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\nW0000 00:00:1723789977.488627  174689 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\nW0000 00:00:1723789977.489960  174689 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced",
      "# This method returns a helper function to compute cross entropy loss\ncross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)",
      "def discriminator_loss(real_output, fake_output):\n    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n    total_loss = real_loss + fake_loss\n    return total_loss",
      "def generator_loss(fake_output):\n    return cross_entropy(tf.ones_like(fake_output), fake_output)",
      "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\ndiscriminator_optimizer = tf.keras.optimizers.Adam(1e-4)",
      "checkpoint_dir = './training_checkpoints'\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\ncheckpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n                                 discriminator_optimizer=discriminator_optimizer,\n                                 generator=generator,\n                                 discriminator=discriminator)",
      "EPOCHS = 50\nnoise_dim = 100\nnum_examples_to_generate = 16\n\n# You will reuse this seed overtime (so it's easier)\n# to visualize progress in the animated GIF)\nseed = tf.random.normal([num_examples_to_generate, noise_dim])",
      "# Notice the use of `tf.function`\n# This annotation causes the function to be \"compiled\".\n@tf.function\ndef train_step(images):\n    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n\n    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n      generated_images = generator(noise, training=True)\n\n      real_output = discriminator(images, training=True)\n      fake_output = discriminator(generated_images, training=True)\n\n      gen_loss = generator_loss(fake_output)\n      disc_loss = discriminator_loss(real_output, fake_output)\n\n    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n\n    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))",
      "def train(dataset, epochs):\n  for epoch in range(epochs):\n    start = time.time()\n\n    for image_batch in dataset:\n      train_step(image_batch)\n\n    # Produce images for the GIF as you go\n    display.clear_output(wait=True)\n    generate_and_save_images(generator,\n                             epoch + 1,\n                             seed)\n\n    # Save the model every 15 epochs\n    if (epoch + 1) % 15 == 0:\n      checkpoint.save(file_prefix = checkpoint_prefix)\n\n    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n\n  # Generate after the final epoch\n  display.clear_output(wait=True)\n  generate_and_save_images(generator,\n                           epochs,\n                           seed)",
      "def generate_and_save_images(model, epoch, test_input):\n  # Notice `training` is set to False.\n  # This is so all layers run in inference mode (batchnorm).\n  predictions = model(test_input, training=False)\n\n  fig = plt.figure(figsize=(4, 4))\n\n  for i in range(predictions.shape[0]):\n      plt.subplot(4, 4, i+1)\n      plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n      plt.axis('off')\n\n  plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n  plt.show()",
      "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))",
      "import tensorflow_docs.vis.embed as embed\nembed.embed_file(anim_file)"
    ]
  },
  {
    "url": "https://www.tensorflow.org/tutorials/distribute/parameter_server_training",
    "title": "Parameter server training with ParameterServerStrategy\n      \n\n\n      \n      Stay organized with collections\n    \n\n      \n      Save and categorize content based on your preferences.",
    "code_snippets": [
      "import multiprocessing\nimport os\nimport random\nimport portpicker\nimport tensorflow as tf",
      "def create_in_process_cluster(num_workers, num_ps):\n  \"\"\"Creates and starts local servers and returns the cluster_resolver.\"\"\"\n  worker_ports = [portpicker.pick_unused_port() for _ in range(num_workers)]\n  ps_ports = [portpicker.pick_unused_port() for _ in range(num_ps)]\n\n  cluster_dict = {}\n  cluster_dict[\"worker\"] = [\"localhost:%s\" % port for port in worker_ports]\n  if num_ps > 0:\n    cluster_dict[\"ps\"] = [\"localhost:%s\" % port for port in ps_ports]\n\n  cluster_spec = tf.train.ClusterSpec(cluster_dict)\n\n  # Workers need some inter_ops threads to work properly.\n  worker_config = tf.compat.v1.ConfigProto()\n  if multiprocessing.cpu_count() < num_workers + 1:\n    worker_config.inter_op_parallelism_threads = num_workers + 1\n\n  for i in range(num_workers):\n    tf.distribute.Server(\n        cluster_spec,\n        job_name=\"worker\",\n        task_index=i,\n        config=worker_config,\n        protocol=\"grpc\")\n\n  for i in range(num_ps):\n    tf.distribute.Server(\n        cluster_spec,\n        job_name=\"ps\",\n        task_index=i,\n        protocol=\"grpc\")\n\n  cluster_resolver = tf.distribute.cluster_resolver.SimpleClusterResolver(\n      cluster_spec, rpc_layer=\"grpc\")\n  return cluster_resolver\n\n# Set the environment variable to allow reporting worker and ps failure to the\n# coordinator. This is a workaround and won't be necessary in the future.\nos.environ[\"GRPC_FAIL_FAST\"] = \"use_caller\"\n\nNUM_WORKERS = 3\nNUM_PS = 2\ncluster_resolver = create_in_process_cluster(NUM_WORKERS, NUM_PS)",
      "variable_partitioner = (\n    tf.distribute.experimental.partitioners.MinSizePartitioner(\n        min_shard_bytes=(256 << 10),\n        max_shards=NUM_PS))\n\nstrategy = tf.distribute.ParameterServerStrategy(\n    cluster_resolver,\n    variable_partitioner=variable_partitioner)",
      "global_batch_size = 64\n\nx = tf.random.uniform((10, 10))\ny = tf.random.uniform((10,))\n\ndataset = tf.data.Dataset.from_tensor_slices((x, y)).shuffle(10).repeat()\ndataset = dataset.batch(global_batch_size)\ndataset = dataset.prefetch(2)",
      "with strategy.scope():\n  model = tf.keras.models.Sequential([tf.keras.layers.Dense(10)])\n\n  model.compile(tf.keras.optimizers.legacy.SGD(), loss=\"mse\", steps_per_execution=10)",
      "working_dir = \"/tmp/my_working_dir\"\nlog_dir = os.path.join(working_dir, \"log\")\nckpt_filepath = os.path.join(working_dir, \"ckpt\")\nbackup_dir = os.path.join(working_dir, \"backup\")\n\ncallbacks = [\n    tf.keras.callbacks.TensorBoard(log_dir=log_dir),\n    tf.keras.callbacks.ModelCheckpoint(filepath=ckpt_filepath),\n    tf.keras.callbacks.BackupAndRestore(backup_dir=backup_dir),\n]\n\nmodel.fit(dataset, epochs=5, steps_per_epoch=20, callbacks=callbacks)",
      "feature_vocab = [\n    \"avenger\", \"ironman\", \"batman\", \"hulk\", \"spiderman\", \"kingkong\", \"wonder_woman\"\n]\nlabel_vocab = [\"yes\", \"no\"]\n\nwith strategy.scope():\n  feature_lookup_layer = tf.keras.layers.StringLookup(\n      vocabulary=feature_vocab,\n      mask_token=None)\n  label_lookup_layer = tf.keras.layers.StringLookup(\n      vocabulary=label_vocab,\n      num_oov_indices=0,\n      mask_token=None)\n\n  raw_feature_input = tf.keras.layers.Input(\n      shape=(3,),\n      dtype=tf.string,\n      name=\"feature\")\n  feature_id_input = feature_lookup_layer(raw_feature_input)\n  feature_preprocess_stage = tf.keras.Model(\n      {\"features\": raw_feature_input},\n      feature_id_input)\n\n  raw_label_input = tf.keras.layers.Input(\n      shape=(1,),\n      dtype=tf.string,\n      name=\"label\")\n  label_id_input = label_lookup_layer(raw_label_input)\n\n  label_preprocess_stage = tf.keras.Model(\n      {\"label\": raw_label_input},\n      label_id_input)",
      "def feature_and_label_gen(num_examples=200):\n  examples = {\"features\": [], \"label\": []}\n  for _ in range(num_examples):\n    features = random.sample(feature_vocab, 3)\n    label = [\"yes\"] if \"avenger\" in features else [\"no\"]\n    examples[\"features\"].append(features)\n    examples[\"label\"].append(label)\n  return examples\n\nexamples = feature_and_label_gen()",
      "def dataset_fn(_):\n  raw_dataset = tf.data.Dataset.from_tensor_slices(examples)\n\n  train_dataset = raw_dataset.map(\n      lambda x: (\n          {\"features\": feature_preprocess_stage(x[\"features\"])},\n          label_preprocess_stage(x[\"label\"])\n      )).shuffle(200).batch(32).repeat()\n  return train_dataset",
      "# These variables created under the `Strategy.scope` will be placed on parameter\n# servers in a round-robin fashion.\nwith strategy.scope():\n  # Create the model. The input needs to be compatible with Keras processing layers.\n  model_input = tf.keras.layers.Input(\n      shape=(3,), dtype=tf.int64, name=\"model_input\")\n\n  emb_layer = tf.keras.layers.Embedding(\n      input_dim=len(feature_lookup_layer.get_vocabulary()), output_dim=16384)\n  emb_output = tf.reduce_mean(emb_layer(model_input), axis=1)\n  dense_output = tf.keras.layers.Dense(\n      units=1, activation=\"sigmoid\",\n      kernel_regularizer=tf.keras.regularizers.L2(1e-4),\n  )(emb_output)\n  model = tf.keras.Model({\"features\": model_input}, dense_output)\n\n  optimizer = tf.keras.optimizers.legacy.RMSprop(learning_rate=0.1)\n  accuracy = tf.keras.metrics.Accuracy()",
      "@tf.function\ndef step_fn(iterator):\n\n  def replica_fn(batch_data, labels):\n    with tf.GradientTape() as tape:\n      pred = model(batch_data, training=True)\n      per_example_loss = tf.keras.losses.BinaryCrossentropy(\n          reduction=tf.keras.losses.Reduction.NONE)(labels, pred)\n      loss = tf.nn.compute_average_loss(per_example_loss)\n      model_losses = model.losses\n      if model_losses:\n        loss += tf.nn.scale_regularization_loss(tf.add_n(model_losses))\n    gradients = tape.gradient(loss, model.trainable_variables)\n\n    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n\n    actual_pred = tf.cast(tf.greater(pred, 0.5), tf.int64)\n    accuracy.update_state(labels, actual_pred)\n    return loss\n\n  batch_data, labels = next(iterator)\n  losses = strategy.run(replica_fn, args=(batch_data, labels))\n  return strategy.reduce(tf.distribute.ReduceOp.SUM, losses, axis=None)",
      "coordinator = tf.distribute.coordinator.ClusterCoordinator(strategy)",
      "@tf.function\ndef per_worker_dataset_fn():\n  return strategy.distribute_datasets_from_function(dataset_fn)\n\nper_worker_dataset = coordinator.create_per_worker_dataset(per_worker_dataset_fn)\nper_worker_iterator = iter(per_worker_dataset)",
      "eval_dataset = tf.data.Dataset.from_tensor_slices(\n    feature_and_label_gen(num_examples=16)).map(\n          lambda x: (\n              {\"features\": feature_preprocess_stage(x[\"features\"])},\n              label_preprocess_stage(x[\"label\"])\n          )).batch(8)\n\neval_accuracy = tf.keras.metrics.Accuracy()\n\nfor batch_data, labels in eval_dataset:\n  pred = model(batch_data, training=False)\n  actual_pred = tf.cast(tf.greater(pred, 0.5), tf.int64)\n  eval_accuracy.update_state(labels, actual_pred)\n\nprint(\"Evaluation accuracy: %f\" % eval_accuracy.result())",
      "with strategy.scope():\n  # Define the eval metric on parameter servers.\n  eval_accuracy = tf.keras.metrics.Accuracy()\n\n@tf.function\ndef eval_step(iterator):\n  def replica_fn(batch_data, labels):\n    pred = model(batch_data, training=False)\n    actual_pred = tf.cast(tf.greater(pred, 0.5), tf.int64)\n    eval_accuracy.update_state(labels, actual_pred)\n  batch_data, labels = next(iterator)\n  strategy.run(replica_fn, args=(batch_data, labels))\n\ndef eval_dataset_fn():\n  return tf.data.Dataset.from_tensor_slices(\n      feature_and_label_gen(num_examples=16)).map(\n          lambda x: (\n              {\"features\": feature_preprocess_stage(x[\"features\"])},\n              label_preprocess_stage(x[\"label\"])\n          )).shuffle(16).repeat().batch(8)\n\nper_worker_eval_dataset = coordinator.create_per_worker_dataset(eval_dataset_fn)\nper_worker_eval_iterator = iter(per_worker_eval_dataset)\n\neval_steps_per_epoch = 2\nfor _ in range(eval_steps_per_epoch):\n  coordinator.schedule(eval_step, args=(per_worker_eval_iterator,))\ncoordinator.join()\nprint(\"Evaluation accuracy: %f\" % eval_accuracy.result())",
      "checkpoint_dir = ...\neval_model = ...\neval_data = ...\ncheckpoint = tf.train.Checkpoint(model=eval_model)\n\nfor latest_checkpoint in tf.train.checkpoints_iterator(\n    checkpoint_dir):\n  try:\n    checkpoint.restore(latest_checkpoint).expect_partial()\n  except (tf.errors.OpError,) as e:\n    # checkpoint may be deleted by training when it is about to read it.\n    continue\n\n  # Optionally add callbacks to write summaries.\n  eval_model.evaluate(eval_data)\n\n  # Evaluation finishes when it has evaluated the last epoch.\n  if latest_checkpoint.endswith('-{}'.format(train_epochs)):\n    break",
      "cluster_resolver = tf.distribute.cluster_resolver.TFConfigClusterResolver()\nif cluster_resolver.task_type in (\"worker\", \"ps\"):\n  # Start a TensorFlow server and wait.\nelif cluster_resolver.task_type == \"evaluator\":\n  # Run sidecar evaluation\nelse:\n  # Run the coordinator.",
      "# Set the environment variable to allow reporting worker and ps failure to the\n# coordinator. This is a workaround and won't be necessary in the future.\nos.environ[\"GRPC_FAIL_FAST\"] = \"use_caller\"\n\nserver = tf.distribute.Server(\n    cluster_resolver.cluster_spec(),\n    job_name=cluster_resolver.task_type,\n    task_index=cluster_resolver.task_id,\n    protocol=cluster_resolver.rpc_layer or \"grpc\",\n    start=True)\nserver.join()",
      "checkpoint_manager = tf.train.CheckpointManager(\n    tf.train.Checkpoint(model=model, optimizer=optimizer),\n    checkpoint_dir,\n    max_to_keep=3)\nif checkpoint_manager.latest_checkpoint:\n  checkpoint = checkpoint_manager.checkpoint\n  checkpoint.restore(\n      checkpoint_manager.latest_checkpoint).assert_existing_objects_matched()\n\nglobal_steps = int(optimizer.iterations.numpy())\nstarting_epoch = global_steps // steps_per_epoch\n\nfor _ in range(starting_epoch, num_epochs):\n  for _ in range(steps_per_epoch):\n    coordinator.schedule(step_fn, args=(per_worker_iterator,))\n  coordinator.join()\n  checkpoint_manager.save()",
      "steps_per_invocation = 10\n\n@tf.function\ndef step_fn(iterator):\n  for _ in range(steps_per_invocation):\n    features, labels = next(iterator)\n    def replica_fn(features, labels):\n      ...\n\n    strategy.run(replica_fn, args=(features, labels))"
    ]
  },
  {
    "url": "https://www.tensorflow.org/tutorials/generative/autoencoder",
    "title": "Intro to Autoencoders\n      \n\n\n      \n      Stay organized with collections\n    \n\n      \n      Save and categorize content based on your preferences.",
    "code_snippets": [
      "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\n\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras import layers, losses\nfrom tensorflow.keras.datasets import fashion_mnist\nfrom tensorflow.keras.models import Model",
      "class Autoencoder(Model):\n  def __init__(self, latent_dim, shape):\n    super(Autoencoder, self).__init__()\n    self.latent_dim = latent_dim\n    self.shape = shape\n    self.encoder = tf.keras.Sequential([\n      layers.Flatten(),\n      layers.Dense(latent_dim, activation='relu'),\n    ])\n    self.decoder = tf.keras.Sequential([\n      layers.Dense(tf.math.reduce_prod(shape).numpy(), activation='sigmoid'),\n      layers.Reshape(shape)\n    ])\n\n  def call(self, x):\n    encoded = self.encoder(x)\n    decoded = self.decoder(encoded)\n    return decoded\n\n\nshape = x_test.shape[1:]\nlatent_dim = 64\nautoencoder = Autoencoder(latent_dim, shape)",
      "x_train = x_train.astype('float32') / 255.\nx_test = x_test.astype('float32') / 255.\n\nx_train = x_train[..., tf.newaxis]\nx_test = x_test[..., tf.newaxis]\n\nprint(x_train.shape)",
      "noise_factor = 0.2\nx_train_noisy = x_train + noise_factor * tf.random.normal(shape=x_train.shape)\nx_test_noisy = x_test + noise_factor * tf.random.normal(shape=x_test.shape)\n\nx_train_noisy = tf.clip_by_value(x_train_noisy, clip_value_min=0., clip_value_max=1.)\nx_test_noisy = tf.clip_by_value(x_test_noisy, clip_value_min=0., clip_value_max=1.)",
      "n = 10\nplt.figure(figsize=(20, 2))\nfor i in range(n):\n    ax = plt.subplot(1, n, i + 1)\n    plt.title(\"original + noise\")\n    plt.imshow(tf.squeeze(x_test_noisy[i]))\n    plt.gray()\nplt.show()",
      "class Denoise(Model):\n  def __init__(self):\n    super(Denoise, self).__init__()\n    self.encoder = tf.keras.Sequential([\n      layers.Input(shape=(28, 28, 1)),\n      layers.Conv2D(16, (3, 3), activation='relu', padding='same', strides=2),\n      layers.Conv2D(8, (3, 3), activation='relu', padding='same', strides=2)])\n\n    self.decoder = tf.keras.Sequential([\n      layers.Conv2DTranspose(8, kernel_size=3, strides=2, activation='relu', padding='same'),\n      layers.Conv2DTranspose(16, kernel_size=3, strides=2, activation='relu', padding='same'),\n      layers.Conv2D(1, kernel_size=(3, 3), activation='sigmoid', padding='same')])\n\n  def call(self, x):\n    encoded = self.encoder(x)\n    decoded = self.decoder(encoded)\n    return decoded\n\nautoencoder = Denoise()",
      "n = 10\nplt.figure(figsize=(20, 4))\nfor i in range(n):\n\n    # display original + noise\n    ax = plt.subplot(2, n, i + 1)\n    plt.title(\"original + noise\")\n    plt.imshow(tf.squeeze(x_test_noisy[i]))\n    plt.gray()\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\n\n    # display reconstruction\n    bx = plt.subplot(2, n, i + n + 1)\n    plt.title(\"reconstructed\")\n    plt.imshow(tf.squeeze(decoded_imgs[i]))\n    plt.gray()\n    bx.get_xaxis().set_visible(False)\n    bx.get_yaxis().set_visible(False)\nplt.show()",
      "min_val = tf.reduce_min(train_data)\nmax_val = tf.reduce_max(train_data)\n\ntrain_data = (train_data - min_val) / (max_val - min_val)\ntest_data = (test_data - min_val) / (max_val - min_val)\n\ntrain_data = tf.cast(train_data, tf.float32)\ntest_data = tf.cast(test_data, tf.float32)",
      "class AnomalyDetector(Model):\n  def __init__(self):\n    super(AnomalyDetector, self).__init__()\n    self.encoder = tf.keras.Sequential([\n      layers.Dense(32, activation=\"relu\"),\n      layers.Dense(16, activation=\"relu\"),\n      layers.Dense(8, activation=\"relu\")])\n\n    self.decoder = tf.keras.Sequential([\n      layers.Dense(16, activation=\"relu\"),\n      layers.Dense(32, activation=\"relu\"),\n      layers.Dense(140, activation=\"sigmoid\")])\n\n  def call(self, x):\n    encoded = self.encoder(x)\n    decoded = self.decoder(encoded)\n    return decoded\n\nautoencoder = AnomalyDetector()",
      "reconstructions = autoencoder.predict(normal_train_data)\ntrain_loss = tf.keras.losses.mae(reconstructions, normal_train_data)\n\nplt.hist(train_loss[None,:], bins=50)\nplt.xlabel(\"Train loss\")\nplt.ylabel(\"No of examples\")\nplt.show()",
      "reconstructions = autoencoder.predict(anomalous_test_data)\ntest_loss = tf.keras.losses.mae(reconstructions, anomalous_test_data)\n\nplt.hist(test_loss[None, :], bins=50)\nplt.xlabel(\"Test loss\")\nplt.ylabel(\"No of examples\")\nplt.show()",
      "def predict(model, data, threshold):\n  reconstructions = model(data)\n  loss = tf.keras.losses.mae(reconstructions, data)\n  return tf.math.less(loss, threshold)\n\ndef print_stats(predictions, labels):\n  print(\"Accuracy = {}\".format(accuracy_score(labels, predictions)))\n  print(\"Precision = {}\".format(precision_score(labels, predictions)))\n  print(\"Recall = {}\".format(recall_score(labels, predictions)))"
    ]
  },
  {
    "url": "https://www.tensorflow.org/tutorials/keras/text_classification",
    "title": "Basic text classification\n      \n\n\n      \n      Stay organized with collections\n    \n\n      \n      Save and categorize content based on your preferences.",
    "code_snippets": [
      "import matplotlib.pyplot as plt\nimport os\nimport re\nimport shutil\nimport string\nimport tensorflow as tf\n\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import losses",
      "print(tf.__version__)",
      "url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n\ndataset = tf.keras.utils.get_file(\"aclImdb_v1\", url,\n                                    untar=True, cache_dir='.',\n                                    cache_subdir='')\n\ndataset_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')",
      "batch_size = 32\nseed = 42\n\nraw_train_ds = tf.keras.utils.text_dataset_from_directory(\n    'aclImdb/train',\n    batch_size=batch_size,\n    validation_split=0.2,\n    subset='training',\n    seed=seed)",
      "raw_val_ds = tf.keras.utils.text_dataset_from_directory(\n    'aclImdb/train',\n    batch_size=batch_size,\n    validation_split=0.2,\n    subset='validation',\n    seed=seed)",
      "raw_test_ds = tf.keras.utils.text_dataset_from_directory(\n    'aclImdb/test',\n    batch_size=batch_size)",
      "def custom_standardization(input_data):\n  lowercase = tf.strings.lower(input_data)\n  stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n  return tf.strings.regex_replace(stripped_html,\n                                  '[%s]' % re.escape(string.punctuation),\n                                  '')",
      "def vectorize_text(text, label):\n  text = tf.expand_dims(text, -1)\n  return vectorize_layer(text), label",
      "Review tf.Tensor(b'Silent Night, Deadly Night 5 is the very last of the series, and like part 4, it\\'s unrelated to the first three except by title and the fact that it\\'s a Christmas-themed horror flick.<br /><br />Except to the oblivious, there\\'s some obvious things going on here...Mickey Rooney plays a toymaker named Joe Petto and his creepy son\\'s name is Pino. Ring a bell, anyone? Now, a little boy named Derek heard a knock at the door one evening, and opened it to find a present on the doorstep for him. Even though it said \"don\\'t open till Christmas\", he begins to open it anyway but is stopped by his dad, who scolds him and sends him to bed, and opens the gift himself. Inside is a little red ball that sprouts Santa arms and a head, and proceeds to kill dad. Oops, maybe he should have left well-enough alone. Of course Derek is then traumatized by the incident since he watched it from the stairs, but he doesn\\'t grow up to be some killer Santa, he just stops talking.<br /><br />There\\'s a mysterious stranger lurking around, who seems very interested in the toys that Joe Petto makes. We even see him buying a bunch when Derek\\'s mom takes him to the store to find a gift for him to bring him out of his trauma. And what exactly is this guy doing? Well, we\\'re not sure but he does seem to be taking these toys apart to see what makes them tick. He does keep his landlord from evicting him by promising him to pay him in cash the next day and presents him with a \"Larry the Larvae\" toy for his kid, but of course \"Larry\" is not a good toy and gets out of the box in the car and of course, well, things aren\\'t pretty.<br /><br />Anyway, eventually what\\'s going on with Joe Petto and Pino is of course revealed, and as with the old story, Pino is not a \"real boy\". Pino is probably even more agitated and naughty because he suffers from \"Kenitalia\" (a smooth plastic crotch) so that could account for his evil ways. And the identity of the lurking stranger is revealed too, and there\\'s even kind of a happy ending of sorts. Whee.<br /><br />A step up from part 4, but not much of one. Again, Brian Yuzna is involved, and Screaming Mad George, so some decent special effects, but not enough to make this great. A few leftovers from part 4 are hanging around too, like Clint Howard and Neith Hunter, but that doesn\\'t really make any difference. Anyway, I now have seeing the whole series out of my system. Now if I could get some of it out of my brain. 4 out of 5.', shape=(), dtype=string)\nLabel neg\nVectorized review (<tf.Tensor: shape=(1, 250), dtype=int64, numpy=\narray([[1287,  313, 2380,  313,  661,    7,    2,   52,  229,    5,    2,\n         200,    3,   38,  170,  669,   29, 5492,    6,    2,   83,  297,\n         549,   32,  410,    3,    2,  186,   12,   29,    4,    1,  191,\n         510,  549,    6,    2, 8229,  212,   46,  576,  175,  168,   20,\n           1, 5361,  290,    4,    1,  761,  969,    1,    3,   24,  935,\n        2271,  393,    7,    1, 1675,    4, 3747,  250,  148,    4,  112,\n         436,  761, 3529,  548,    4, 3633,   31,    2, 1331,   28, 2096,\n           3, 2912,    9,    6,  163,    4, 1006,   20,    2,    1,   15,\n          85,   53,  147,    9,  292,   89,  959, 2314,  984,   27,  762,\n           6,  959,    9,  564,   18,    7, 2140,   32,   24, 1254,   36,\n           1,   85,    3, 3298,   85,    6, 1410,    3, 1936,    2, 3408,\n         301,  965,    7,    4,  112,  740, 1977,   12,    1, 2014, 2772,\n           3,    4,  428,    3, 5177,    6,  512, 1254,    1,  278,   27,\n         139,   25,  308,    1,  579,    5,  259, 3529,    7,   92, 8981,\n          32,    2, 3842,  230,   27,  289,    9,   35,    2, 5712,   18,\n          27,  144, 2166,   56,    6,   26,   46,  466, 2014,   27,   40,\n        2745,  657,  212,    4, 1376, 3002, 7080,  183,   36,  180,   52,\n         920,    8,    2, 4028,   12,  969,    1,  158,   71,   53,   67,\n          85, 2754,    4,  734,   51,    1, 1611,  294,   85,    6,    2,\n        1164,    6,  163,    4, 3408,   15,   85,    6,  717,   85,   44,\n           5,   24, 7158,    3,   48,  604,    7,   11,  225,  384,   73,\n          65,   21,  242,   18,   27,  120,  295,    6,   26,  667,  129,\n        4028,  948,    6,   67,   48,  158,   93,    1]])>, <tf.Tensor: shape=(), dtype=int32, numpy=0>)",
      "AUTOTUNE = tf.data.AUTOTUNE\n\ntrain_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\nval_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\ntest_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)",
      "model = tf.keras.Sequential([\n  layers.Embedding(max_features, embedding_dim),\n  layers.Dropout(0.2),\n  layers.GlobalAveragePooling1D(),\n  layers.Dropout(0.2),\n  layers.Dense(1, activation='sigmoid')])\n\nmodel.summary()",
      "model.compile(loss=losses.BinaryCrossentropy(),\n              optimizer='adam',\n              metrics=[tf.metrics.BinaryAccuracy(threshold=0.5)])",
      "export_model = tf.keras.Sequential([\n  vectorize_layer,\n  model,\n  layers.Activation('sigmoid')\n])\n\nexport_model.compile(\n    loss=losses.BinaryCrossentropy(from_logits=False), optimizer=\"adam\", metrics=['accuracy']\n)\n\n# Test it with `raw_test_ds`, which yields raw strings\nmetrics = export_model.evaluate(raw_test_ds, return_dict=True)\nprint(metrics)",
      "examples = tf.constant([\n  \"The movie was great!\",\n  \"The movie was okay.\",\n  \"The movie was terrible...\"\n])\n\nexport_model.predict(examples)"
    ]
  },
  {
    "url": "https://www.tensorflow.org/tutorials/optimization/compression",
    "title": "Scalable model compression\n      \n\n\n      \n      Stay organized with collections\n    \n\n      \n      Save and categorize content based on your preferences.",
    "code_snippets": [
      "import matplotlib.pyplot as plt\nimport tensorflow as tf\nimport tensorflow_compression as tfc\nimport tensorflow_datasets as tfds",
      "class CustomDense(tf.keras.layers.Layer):\n\n  def __init__(self, filters, name=\"dense\"):\n    super().__init__(name=name)\n    self.filters = filters\n\n  @classmethod\n  def copy(cls, other, **kwargs):\n    \"\"\"Returns an instantiated and built layer, initialized from `other`.\"\"\"\n    self = cls(filters=other.filters, name=other.name, **kwargs)\n    self.build(None, other=other)\n    return self\n\n  def build(self, input_shape, other=None):\n    \"\"\"Instantiates weights, optionally initializing them from `other`.\"\"\"\n    if other is None:\n      kernel_shape = (input_shape[-1], self.filters)\n      kernel = tf.keras.initializers.GlorotUniform()(shape=kernel_shape)\n      bias = tf.keras.initializers.Zeros()(shape=(self.filters,))\n    else:\n      kernel, bias = other.kernel, other.bias\n    self.kernel = tf.Variable(\n        tf.cast(kernel, self.variable_dtype), name=\"kernel\")\n    self.bias = tf.Variable(\n        tf.cast(bias, self.variable_dtype), name=\"bias\")\n    self.built = True\n\n  def call(self, inputs):\n    outputs = tf.linalg.matvec(self.kernel, inputs, transpose_a=True)\n    outputs = tf.nn.bias_add(outputs, self.bias)\n    return tf.nn.leaky_relu(outputs)",
      "class CustomConv2D(tf.keras.layers.Layer):\n\n  def __init__(self, filters, kernel_size,\n               strides=1, padding=\"SAME\", name=\"conv2d\"):\n    super().__init__(name=name)\n    self.filters = filters\n    self.kernel_size = kernel_size\n    self.strides = strides\n    self.padding = padding\n\n  @classmethod\n  def copy(cls, other, **kwargs):\n    \"\"\"Returns an instantiated and built layer, initialized from `other`.\"\"\"\n    self = cls(filters=other.filters, kernel_size=other.kernel_size,\n               strides=other.strides, padding=other.padding, name=other.name,\n               **kwargs)\n    self.build(None, other=other)\n    return self\n\n  def build(self, input_shape, other=None):\n    \"\"\"Instantiates weights, optionally initializing them from `other`.\"\"\"\n    if other is None:\n      kernel_shape = 2 * (self.kernel_size,) + (input_shape[-1], self.filters)\n      kernel = tf.keras.initializers.GlorotUniform()(shape=kernel_shape)\n      bias = tf.keras.initializers.Zeros()(shape=(self.filters,))\n    else:\n      kernel, bias = other.kernel, other.bias\n    self.kernel = tf.Variable(\n        tf.cast(kernel, self.variable_dtype), name=\"kernel\")\n    self.bias = tf.Variable(\n        tf.cast(bias, self.variable_dtype), name=\"bias\")\n    self.built = True\n\n  def call(self, inputs):\n    outputs = tf.nn.convolution(\n        inputs, self.kernel, strides=self.strides, padding=self.padding)\n    outputs = tf.nn.bias_add(outputs, self.bias)\n    return tf.nn.leaky_relu(outputs)",
      "classifier = tf.keras.Sequential([\n    CustomConv2D(20, 5, strides=2, name=\"conv_1\"),\n    CustomConv2D(50, 5, strides=2, name=\"conv_2\"),\n    tf.keras.layers.Flatten(),\n    CustomDense(500, name=\"fc_1\"),\n    CustomDense(10, name=\"fc_2\"),\n], name=\"classifier\")",
      "def normalize_img(image, label):\n  \"\"\"Normalizes images: `uint8` -> `float32`.\"\"\"\n  return tf.cast(image, tf.float32) / 255., label\n\ntraining_dataset, validation_dataset = tfds.load(\n    \"mnist\",\n    split=[\"train\", \"test\"],\n    shuffle_files=True,\n    as_supervised=True,\n    with_info=False,\n)\ntraining_dataset = training_dataset.map(normalize_img)\nvalidation_dataset = validation_dataset.map(normalize_img)",
      "def train_model(model, training_data, validation_data, **kwargs):\n  model.compile(\n      optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n      metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n      # Uncomment this to ease debugging:\n      # run_eagerly=True,\n  )\n  kwargs.setdefault(\"epochs\", 5)\n  kwargs.setdefault(\"verbose\", 1)\n  log = model.fit(\n      training_data.batch(128).prefetch(8),\n      validation_data=validation_data.batch(128).cache(),\n      validation_freq=1,\n      **kwargs,\n  )\n  return log.history[\"val_sparse_categorical_accuracy\"][-1]\n\nclassifier_accuracy = train_model(\n    classifier, training_dataset, validation_dataset)\n\nprint(f\"Accuracy: {classifier_accuracy:0.4f}\")",
      "_ = tf.linspace(-5., 5., 501)\nplt.plot(_, tfc.PowerLawEntropyModel(0).penalty(_));",
      "class PowerLawRegularizer(tf.keras.regularizers.Regularizer):\n\n  def __init__(self, lmbda):\n    super().__init__()\n    self.lmbda = lmbda\n\n  def __call__(self, variable):\n    em = tfc.PowerLawEntropyModel(coding_rank=variable.shape.rank)\n    return self.lmbda * em.penalty(variable)\n\n# Normalizing the weight of the penalty by the number of model parameters is a\n# good rule of thumb to produce comparable results across models.\nregularizer = PowerLawRegularizer(lmbda=2./classifier.count_params())",
      "def quantize(latent, log_step):\n  step = tf.exp(log_step)\n  return tfc.round_st(latent / step) * step",
      "class CompressibleDense(CustomDense):\n\n  def __init__(self, regularizer, *args, **kwargs):\n    super().__init__(*args, **kwargs)\n    self.regularizer = regularizer\n\n  def build(self, input_shape, other=None):\n    \"\"\"Instantiates weights, optionally initializing them from `other`.\"\"\"\n    super().build(input_shape, other=other)\n    if other is not None and hasattr(other, \"kernel_log_step\"):\n      kernel_log_step = other.kernel_log_step\n      bias_log_step = other.bias_log_step\n    else:\n      kernel_log_step = bias_log_step = -4.\n    self.kernel_log_step = tf.Variable(\n        tf.cast(kernel_log_step, self.variable_dtype), name=\"kernel_log_step\")\n    self.bias_log_step = tf.Variable(\n        tf.cast(bias_log_step, self.variable_dtype), name=\"bias_log_step\")\n    self.add_loss(lambda: self.regularizer(\n        self.kernel_latent / tf.exp(self.kernel_log_step)))\n    self.add_loss(lambda: self.regularizer(\n        self.bias_latent / tf.exp(self.bias_log_step)))\n\n  @property\n  def kernel(self):\n    return quantize(self.kernel_latent, self.kernel_log_step)\n\n  @kernel.setter\n  def kernel(self, kernel):\n    self.kernel_latent = tf.Variable(kernel, name=\"kernel_latent\")\n\n  @property\n  def bias(self):\n    return quantize(self.bias_latent, self.bias_log_step)\n\n  @bias.setter\n  def bias(self, bias):\n    self.bias_latent = tf.Variable(bias, name=\"bias_latent\")",
      "def to_rdft(kernel, kernel_size):\n  # The kernel has shape (H, W, I, O) -> transpose to take DFT over last two\n  # dimensions.\n  kernel = tf.transpose(kernel, (2, 3, 0, 1))\n  # The RDFT has type complex64 and shape (I, O, FH, FW).\n  kernel_rdft = tf.signal.rfft2d(kernel)\n  # Map real and imaginary parts into regular floats. The result is float32\n  # and has shape (I, O, FH, FW, 2).\n  kernel_rdft = tf.stack(\n      [tf.math.real(kernel_rdft), tf.math.imag(kernel_rdft)], axis=-1)\n  # Divide by kernel size to make the DFT orthonormal (length-preserving).\n  return kernel_rdft / kernel_size\n\ndef from_rdft(kernel_rdft, kernel_size):\n  # Undoes the transformations in to_rdft.\n  kernel_rdft *= kernel_size\n  kernel_rdft = tf.dtypes.complex(*tf.unstack(kernel_rdft, axis=-1))\n  kernel = tf.signal.irfft2d(kernel_rdft, fft_length=2 * (kernel_size,))\n  return tf.transpose(kernel, (2, 3, 0, 1))",
      "class CompressibleConv2D(CustomConv2D):\n\n  def __init__(self, regularizer, *args, **kwargs):\n    super().__init__(*args, **kwargs)\n    self.regularizer = regularizer\n\n  def build(self, input_shape, other=None):\n    \"\"\"Instantiates weights, optionally initializing them from `other`.\"\"\"\n    super().build(input_shape, other=other)\n    if other is not None and hasattr(other, \"kernel_log_step\"):\n      kernel_log_step = other.kernel_log_step\n      bias_log_step = other.bias_log_step\n    else:\n      kernel_log_step = tf.fill(self.kernel_latent.shape[2:], -4.)\n      bias_log_step = -4.\n    self.kernel_log_step = tf.Variable(\n        tf.cast(kernel_log_step, self.variable_dtype), name=\"kernel_log_step\")\n    self.bias_log_step = tf.Variable(\n        tf.cast(bias_log_step, self.variable_dtype), name=\"bias_log_step\")\n    self.add_loss(lambda: self.regularizer(\n        self.kernel_latent / tf.exp(self.kernel_log_step)))\n    self.add_loss(lambda: self.regularizer(\n        self.bias_latent / tf.exp(self.bias_log_step)))\n\n  @property\n  def kernel(self):\n    kernel_rdft = quantize(self.kernel_latent, self.kernel_log_step)\n    return from_rdft(kernel_rdft, self.kernel_size)\n\n  @kernel.setter\n  def kernel(self, kernel):\n    kernel_rdft = to_rdft(kernel, self.kernel_size)\n    self.kernel_latent = tf.Variable(kernel_rdft, name=\"kernel_latent\")\n\n  @property\n  def bias(self):\n    return quantize(self.bias_latent, self.bias_log_step)\n\n  @bias.setter\n  def bias(self, bias):\n    self.bias_latent = tf.Variable(bias, name=\"bias_latent\")",
      "def make_mnist_classifier(regularizer):\n  return tf.keras.Sequential([\n      CompressibleConv2D(regularizer, 20, 5, strides=2, name=\"conv_1\"),\n      CompressibleConv2D(regularizer, 50, 5, strides=2, name=\"conv_2\"),\n      tf.keras.layers.Flatten(),\n      CompressibleDense(regularizer, 500, name=\"fc_1\"),\n      CompressibleDense(regularizer, 10, name=\"fc_2\"),\n  ], name=\"classifier\")\n\ncompressible_classifier = make_mnist_classifier(regularizer)",
      "def compress_latent(latent, log_step, name):\n  em = tfc.PowerLawEntropyModel(latent.shape.rank)\n  compressed = em.compress(latent / tf.exp(log_step))\n  compressed = tf.Variable(compressed, name=f\"{name}_compressed\")\n  log_step = tf.cast(log_step, tf.float16)\n  log_step = tf.Variable(log_step, name=f\"{name}_log_step\")\n  return compressed, log_step\n\ndef decompress_latent(compressed, shape, log_step):\n  latent = tfc.PowerLawEntropyModel(len(shape)).decompress(compressed, shape)\n  step = tf.exp(tf.cast(log_step, latent.dtype))\n  return latent * step",
      "class CompressedDense(CustomDense):\n\n  def build(self, input_shape, other=None):\n    assert isinstance(other, CompressibleDense)\n    self.input_channels = other.kernel.shape[0]\n    self.kernel_compressed, self.kernel_log_step = compress_latent(\n        other.kernel_latent, other.kernel_log_step, \"kernel\")\n    self.bias_compressed, self.bias_log_step = compress_latent(\n        other.bias_latent, other.bias_log_step, \"bias\")\n    self.built = True\n\n  @property\n  def kernel(self):\n    kernel_shape = (self.input_channels, self.filters)\n    return decompress_latent(\n        self.kernel_compressed, kernel_shape, self.kernel_log_step)\n\n  @property\n  def bias(self):\n    bias_shape = (self.filters,)\n    return decompress_latent(\n        self.bias_compressed, bias_shape, self.bias_log_step)",
      "class CompressedConv2D(CustomConv2D):\n\n  def build(self, input_shape, other=None):\n    assert isinstance(other, CompressibleConv2D)\n    self.input_channels = other.kernel.shape[2]\n    self.kernel_compressed, self.kernel_log_step = compress_latent(\n        other.kernel_latent, other.kernel_log_step, \"kernel\")\n    self.bias_compressed, self.bias_log_step = compress_latent(\n        other.bias_latent, other.bias_log_step, \"bias\")\n    self.built = True\n\n  @property\n  def kernel(self):\n    rdft_shape = (self.input_channels, self.filters,\n                  self.kernel_size, self.kernel_size // 2 + 1, 2)\n    kernel_rdft = decompress_latent(\n        self.kernel_compressed, rdft_shape, self.kernel_log_step)\n    return from_rdft(kernel_rdft, self.kernel_size)\n\n  @property\n  def bias(self):\n    bias_shape = (self.filters,)\n    return decompress_latent(\n        self.bias_compressed, bias_shape, self.bias_log_step)",
      "def compress_layer(layer):\n  if isinstance(layer, CompressibleDense):\n    return CompressedDense.copy(layer)\n  if isinstance(layer, CompressibleConv2D):\n    return CompressedConv2D.copy(layer)\n  return type(layer).from_config(layer.get_config())\n\ncompressed_classifier = tf.keras.models.clone_model(\n    compressible_classifier, clone_function=compress_layer)",
      "compressed_classifier.compile(metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n_, compressed_accuracy = compressed_classifier.evaluate(validation_dataset.batch(128))\n\nprint(f\"Accuracy of the compressible classifier: {penalized_accuracy:0.4f}\")\nprint(f\"Accuracy of the compressed classifier: {compressed_accuracy:0.4f}\")",
      "def get_weight_size_in_bytes(weight):\n  if weight.dtype == tf.string:\n    return tf.reduce_sum(tf.strings.length(weight, unit=\"BYTE\"))\n  else:\n    return tf.size(weight) * weight.dtype.size\n\noriginal_size = sum(map(get_weight_size_in_bytes, classifier.weights))\ncompressed_size = sum(map(get_weight_size_in_bytes, compressed_classifier.weights))\n\nprint(f\"Size of original model weights: {original_size} bytes\")\nprint(f\"Size of compressed model weights: {compressed_size} bytes\")\nprint(f\"Compression ratio: {(original_size/compressed_size):0.0f}x\")",
      "import os\nimport shutil\n\ndef get_disk_size(model, path):\n  model.save(path)\n  zip_path = shutil.make_archive(path, \"zip\", path)\n  return os.path.getsize(zip_path)\n\noriginal_zip_size = get_disk_size(classifier, \"/tmp/classifier\")\ncompressed_zip_size = get_disk_size(\n    compressed_classifier, \"/tmp/compressed_classifier\")\n\nprint(f\"Original on-disk size (ZIP compressed): {original_zip_size} bytes\")\nprint(f\"Compressed on-disk size (ZIP compressed): {compressed_zip_size} bytes\")\nprint(f\"Compression ratio: {(original_zip_size/compressed_zip_size):0.0f}x\")",
      "def compress_and_evaluate_model(lmbda):\n  print(f\"lambda={lmbda:0.0f}: training...\", flush=True)\n  regularizer = PowerLawRegularizer(lmbda=lmbda/classifier.count_params())\n  compressible_classifier = make_mnist_classifier(regularizer)\n  train_model(\n      compressible_classifier, training_dataset, validation_dataset, verbose=0)\n  print(\"compressing...\", flush=True)\n  compressed_classifier = tf.keras.models.clone_model(\n      compressible_classifier, clone_function=compress_layer)\n  compressed_size = sum(map(\n      get_weight_size_in_bytes, compressed_classifier.weights))\n  compressed_zip_size = float(get_disk_size(\n      compressed_classifier, \"/tmp/compressed_classifier\"))\n  print(\"evaluating...\", flush=True)\n  compressed_classifier = tf.keras.models.load_model(\n      \"/tmp/compressed_classifier\")\n  compressed_classifier.compile(\n      metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n  _, compressed_accuracy = compressed_classifier.evaluate(\n      validation_dataset.batch(128), verbose=0)\n  print()\n  return compressed_size, compressed_zip_size, compressed_accuracy\n\nlambdas = (2., 5., 10., 20., 50.)\nmetrics = [compress_and_evaluate_model(l) for l in lambdas]\nmetrics = tf.convert_to_tensor(metrics, tf.float32)",
      "def plot_broken_xaxis(ax, compressed_sizes, original_size, original_accuracy):\n  xticks = list(range(\n      int(tf.math.floor(min(compressed_sizes) / 5) * 5),\n      int(tf.math.ceil(max(compressed_sizes) / 5) * 5) + 1,\n      5))\n  xticks.append(xticks[-1] + 10)\n  ax.set_xlim(xticks[0], xticks[-1] + 2)\n  ax.set_xticks(xticks[1:])\n  ax.set_xticklabels(xticks[1:-1] + [f\"{original_size:0.2f}\"])\n  ax.plot(xticks[-1], original_accuracy, \"o\", label=\"float32\")\n\nsizes, zip_sizes, accuracies = tf.transpose(metrics)\nsizes /= 1024\nzip_sizes /= 1024\n\nfig, (axl, axr) = plt.subplots(1, 2, sharey=True, figsize=(10, 4))\naxl.plot(sizes, accuracies, \"o-\", label=\"EPR compressed\")\naxr.plot(zip_sizes, accuracies, \"o-\", label=\"EPR compressed\")\nplot_broken_xaxis(axl, sizes, original_size/1024, classifier_accuracy)\nplot_broken_xaxis(axr, zip_sizes, original_zip_size/1024, classifier_accuracy)\n\naxl.set_xlabel(\"size of model weights [kbytes]\")\naxr.set_xlabel(\"ZIP compressed on-disk model size [kbytes]\")\naxl.set_ylabel(\"accuracy\")\naxl.legend(loc=\"lower right\")\naxr.legend(loc=\"lower right\")\naxl.grid()\naxr.grid()\nfor i in range(len(lambdas)):\n  axl.annotate(f\"$\\lambda = {lambdas[i]:0.0f}$\", (sizes[i], accuracies[i]),\n               xytext=(10, -5), xycoords=\"data\", textcoords=\"offset points\")\n  axr.annotate(f\"$\\lambda = {lambdas[i]:0.0f}$\", (zip_sizes[i], accuracies[i]),\n               xytext=(10, -5), xycoords=\"data\", textcoords=\"offset points\")\nplt.tight_layout()",
      "def decompress_layer(layer):\n  if isinstance(layer, CompressedDense):\n    return CustomDense.copy(layer)\n  if isinstance(layer, CompressedConv2D):\n    return CustomConv2D.copy(layer)\n  return type(layer).from_config(layer.get_config())\n\ndecompressed_classifier = tf.keras.models.clone_model(\n    compressed_classifier, clone_function=decompress_layer)",
      "def decompress_layer_with_penalty(layer):\n  if isinstance(layer, CompressedDense):\n    return CompressibleDense.copy(layer, regularizer=regularizer)\n  if isinstance(layer, CompressedConv2D):\n    return CompressibleConv2D.copy(layer, regularizer=regularizer)\n  return type(layer).from_config(layer.get_config())\n\ndecompressed_classifier = tf.keras.models.clone_model(\n    compressed_classifier, clone_function=decompress_layer_with_penalty)"
    ]
  },
  {
    "url": "https://www.tensorflow.org/tutorials/distribute/multi_worker_with_ctl",
    "title": "Custom training loop with Keras and MultiWorkerMirroredStrategy\n      \n\n\n      \n      Stay organized with collections\n    \n\n      \n      Save and categorize content based on your preferences.",
    "code_snippets": [
      "import json\nimport os\nimport sys",
      "import tensorflow as tf",
      "%%writefile mnist.py\n\nimport os\nimport tensorflow as tf\nimport numpy as np\n\ndef mnist_dataset(batch_size):\n  (x_train, y_train), _ = tf.keras.datasets.mnist.load_data()\n  # The `x` arrays are in uint8 and have values in the range [0, 255].\n  # You need to convert them to float32 with values in the range [0, 1]\n  x_train = x_train / np.float32(255)\n  y_train = y_train.astype(np.int64)\n  train_dataset = tf.data.Dataset.from_tensor_slices(\n      (x_train, y_train)).shuffle(60000)\n  return train_dataset\n\ndef dataset_fn(global_batch_size, input_context):\n  batch_size = input_context.get_per_replica_batch_size(global_batch_size)\n  dataset = mnist_dataset(batch_size)\n  dataset = dataset.shard(input_context.num_input_pipelines,\n                          input_context.input_pipeline_id)\n  dataset = dataset.batch(batch_size)\n  return dataset\n\ndef build_cnn_model():\n  regularizer = tf.keras.regularizers.L2(1e-5)\n  return tf.keras.Sequential([\n      tf.keras.Input(shape=(28, 28)),\n      tf.keras.layers.Reshape(target_shape=(28, 28, 1)),\n      tf.keras.layers.Conv2D(32, 3,\n                             activation='relu',\n                             kernel_regularizer=regularizer),\n      tf.keras.layers.Flatten(),\n      tf.keras.layers.Dense(128,\n                            activation='relu',\n                            kernel_regularizer=regularizer),\n      tf.keras.layers.Dense(10, kernel_regularizer=regularizer)\n  ])",
      "strategy = tf.distribute.MultiWorkerMirroredStrategy()",
      "import mnist\nwith strategy.scope():\n  # Model building needs to be within `strategy.scope()`.\n  multi_worker_model = mnist.build_cnn_model()",
      "with strategy.scope():\n  # The creation of optimizer and train_accuracy needs to be in\n  # `strategy.scope()` as well, since they create variables.\n  optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n  train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n      name='train_accuracy')",
      "@tf.function\ndef train_step(iterator):\n  \"\"\"Training step function.\"\"\"\n\n  def step_fn(inputs):\n    \"\"\"Per-Replica step function.\"\"\"\n    x, y = inputs\n    with tf.GradientTape() as tape:\n      predictions = multi_worker_model(x, training=True)\n      per_example_loss = tf.keras.losses.SparseCategoricalCrossentropy(\n          from_logits=True,\n          reduction=tf.keras.losses.Reduction.NONE)(y, predictions)\n      loss = tf.nn.compute_average_loss(per_example_loss)\n      model_losses = multi_worker_model.losses\n      if model_losses:\n        loss += tf.nn.scale_regularization_loss(tf.add_n(model_losses))\n\n    grads = tape.gradient(loss, multi_worker_model.trainable_variables)\n    optimizer.apply_gradients(\n        zip(grads, multi_worker_model.trainable_variables))\n    train_accuracy.update_state(y, predictions)\n    return loss\n\n  per_replica_losses = strategy.run(step_fn, args=(next(iterator),))\n  return strategy.reduce(\n      tf.distribute.ReduceOp.SUM, per_replica_losses, axis=None)",
      "from multiprocessing import util\ncheckpoint_dir = os.path.join(util.get_temp_dir(), 'ckpt')\n\ndef _is_chief(task_type, task_id, cluster_spec):\n  return (task_type is None\n          or task_type == 'chief'\n          or (task_type == 'worker'\n              and task_id == 0\n              and \"chief\" not in cluster_spec.as_dict()))\n\ndef _get_temp_dir(dirpath, task_id):\n  base_dirpath = 'workertemp_' + str(task_id)\n  temp_dir = os.path.join(dirpath, base_dirpath)\n  tf.io.gfile.makedirs(temp_dir)\n  return temp_dir\n\ndef write_filepath(filepath, task_type, task_id, cluster_spec):\n  dirpath = os.path.dirname(filepath)\n  base = os.path.basename(filepath)\n  if not _is_chief(task_type, task_id, cluster_spec):\n    dirpath = _get_temp_dir(dirpath, task_id)\n  return os.path.join(dirpath, base)",
      "epoch = tf.Variable(\n    initial_value=tf.constant(0, dtype=tf.dtypes.int64), name='epoch')\nstep_in_epoch = tf.Variable(\n    initial_value=tf.constant(0, dtype=tf.dtypes.int64),\n    name='step_in_epoch')\ntask_type, task_id = (strategy.cluster_resolver.task_type,\n                      strategy.cluster_resolver.task_id)\n# Normally, you don't need to manually instantiate a `ClusterSpec`, but in this\n# illustrative example you did not set `'TF_CONFIG'` before initializing the\n# strategy. Check out the next section for \"real-world\" usage.\ncluster_spec = tf.train.ClusterSpec(tf_config['cluster'])\n\ncheckpoint = tf.train.Checkpoint(\n    model=multi_worker_model, epoch=epoch, step_in_epoch=step_in_epoch)\n\nwrite_checkpoint_dir = write_filepath(checkpoint_dir, task_type, task_id,\n                                      cluster_spec)\ncheckpoint_manager = tf.train.CheckpointManager(\n    checkpoint, directory=write_checkpoint_dir, max_to_keep=1)",
      "latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\nif latest_checkpoint:\n  checkpoint.restore(latest_checkpoint)",
      "num_epochs = 3\nnum_steps_per_epoch = 70\n\nwhile epoch.numpy() < num_epochs:\n  iterator = iter(multi_worker_dataset)\n  total_loss = 0.0\n  num_batches = 0\n\n  while step_in_epoch.numpy() < num_steps_per_epoch:\n    total_loss += train_step(iterator)\n    num_batches += 1\n    step_in_epoch.assign_add(1)\n\n  train_loss = total_loss / num_batches\n  print('Epoch: %d, accuracy: %f, train_loss: %f.'\n                %(epoch.numpy(), train_accuracy.result(), train_loss))\n\n  train_accuracy.reset_states()\n\n  # Once the `CheckpointManager` is set up, you're now ready to save, and remove\n  # the checkpoints non-chief workers saved.\n  checkpoint_manager.save()\n  if not _is_chief(task_type, task_id, cluster_spec):\n    tf.io.gfile.rmtree(write_checkpoint_dir)\n\n  epoch.assign_add(1)\n  step_in_epoch.assign(0)",
      "%%writefile main.py\nimport os\nimport json\nimport tensorflow as tf\nimport mnist\nfrom multiprocessing import util\n\nper_worker_batch_size = 64\ntf_config = json.loads(os.environ['TF_CONFIG'])\nnum_workers = len(tf_config['cluster']['worker'])\nglobal_batch_size = per_worker_batch_size * num_workers\n\nnum_epochs = 3\nnum_steps_per_epoch=70\n\n# Checkpoint saving and restoring\ndef _is_chief(task_type, task_id, cluster_spec):\n  return (task_type is None\n          or task_type == 'chief'\n          or (task_type == 'worker'\n              and task_id == 0\n              and 'chief' not in cluster_spec.as_dict()))\n\ndef _get_temp_dir(dirpath, task_id):\n  base_dirpath = 'workertemp_' + str(task_id)\n  temp_dir = os.path.join(dirpath, base_dirpath)\n  tf.io.gfile.makedirs(temp_dir)\n  return temp_dir\n\ndef write_filepath(filepath, task_type, task_id, cluster_spec):\n  dirpath = os.path.dirname(filepath)\n  base = os.path.basename(filepath)\n  if not _is_chief(task_type, task_id, cluster_spec):\n    dirpath = _get_temp_dir(dirpath, task_id)\n  return os.path.join(dirpath, base)\n\ncheckpoint_dir = os.path.join(util.get_temp_dir(), 'ckpt')\n\n# Define Strategy\nstrategy = tf.distribute.MultiWorkerMirroredStrategy()\n\nwith strategy.scope():\n  # Model building/compiling need to be within `tf.distribute.Strategy.scope`.\n  multi_worker_model = mnist.build_cnn_model()\n\n  multi_worker_dataset = strategy.distribute_datasets_from_function(\n      lambda input_context: mnist.dataset_fn(global_batch_size, input_context))\n  optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n  train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n      name='train_accuracy')\n\n@tf.function\ndef train_step(iterator):\n  \"\"\"Training step function.\"\"\"\n\n  def step_fn(inputs):\n    \"\"\"Per-Replica step function.\"\"\"\n    x, y = inputs\n    with tf.GradientTape() as tape:\n      predictions = multi_worker_model(x, training=True)\n      per_example_loss = tf.keras.losses.SparseCategoricalCrossentropy(\n          from_logits=True,\n          reduction=tf.keras.losses.Reduction.NONE)(y, predictions)\n      loss = tf.nn.compute_average_loss(per_example_loss)\n      model_losses = multi_worker_model.losses\n      if model_losses:\n        loss += tf.nn.scale_regularization_loss(tf.add_n(model_losses))\n\n    grads = tape.gradient(loss, multi_worker_model.trainable_variables)\n    optimizer.apply_gradients(\n        zip(grads, multi_worker_model.trainable_variables))\n    train_accuracy.update_state(y, predictions)\n\n    return loss\n\n  per_replica_losses = strategy.run(step_fn, args=(next(iterator),))\n  return strategy.reduce(\n      tf.distribute.ReduceOp.SUM, per_replica_losses, axis=None)\n\nepoch = tf.Variable(\n    initial_value=tf.constant(0, dtype=tf.dtypes.int64), name='epoch')\nstep_in_epoch = tf.Variable(\n    initial_value=tf.constant(0, dtype=tf.dtypes.int64),\n    name='step_in_epoch')\n\ntask_type, task_id, cluster_spec = (strategy.cluster_resolver.task_type,\n                                    strategy.cluster_resolver.task_id,\n                                    strategy.cluster_resolver.cluster_spec())\n\ncheckpoint = tf.train.Checkpoint(\n    model=multi_worker_model, epoch=epoch, step_in_epoch=step_in_epoch)\n\nwrite_checkpoint_dir = write_filepath(checkpoint_dir, task_type, task_id,\n                                      cluster_spec)\ncheckpoint_manager = tf.train.CheckpointManager(\n    checkpoint, directory=write_checkpoint_dir, max_to_keep=1)\n\n# Restoring the checkpoint\nlatest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\nif latest_checkpoint:\n  checkpoint.restore(latest_checkpoint)\n\n# Resume our CTL training\nwhile epoch.numpy() < num_epochs:\n  iterator = iter(multi_worker_dataset)\n  total_loss = 0.0\n  num_batches = 0\n\n  while step_in_epoch.numpy() < num_steps_per_epoch:\n    total_loss += train_step(iterator)\n    num_batches += 1\n    step_in_epoch.assign_add(1)\n\n  train_loss = total_loss / num_batches\n  print('Epoch: %d, accuracy: %f, train_loss: %f.'\n                %(epoch.numpy(), train_accuracy.result(), train_loss))\n\n  train_accuracy.reset_states()\n\n  checkpoint_manager.save()\n  if not _is_chief(task_type, task_id, cluster_spec):\n    tf.io.gfile.rmtree(write_checkpoint_dir)\n\n  epoch.assign_add(1)\n  step_in_epoch.assign(0)",
      "import time\ntime.sleep(20)"
    ]
  },
  {
    "url": "https://www.tensorflow.org/tutorials/customization/custom_training_walkthrough",
    "title": "Custom training: walkthrough\n      \n\n\n      \n      Stay organized with collections\n    \n\n      \n      Save and categorize content based on your preferences.",
    "code_snippets": [
      "import os\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\nimport matplotlib.pyplot as plt\n\nprint(\"TensorFlow version: {}\".format(tf.__version__))\nprint(\"TensorFlow Datasets version: \",tfds.__version__)",
      "class_names = ['Ad\u00e9lie', 'Chinstrap', 'Gentoo']",
      "ds_split, info = tfds.load(\"penguins/processed\", split=['train[:20%]', 'train[20%:]'], as_supervised=True, with_info=True)\n\nds_test = ds_split[0]\nds_train = ds_split[1]\nassert isinstance(ds_test, tf.data.Dataset)\n\nprint(info.features)\ndf_test = tfds.as_dataframe(ds_test.take(5), info)\nprint(\"Test dataset sample: \")\nprint(df_test)\n\ndf_train = tfds.as_dataframe(ds_train.take(5), info)\nprint(\"Train dataset sample: \")\nprint(df_train)\n\nds_train_batch = ds_train.batch(32)",
      "tf.Tensor(\n[[0.49818182 0.6904762  0.42372882 0.4027778 ]\n [0.48       0.07142857 0.6440678  0.44444445]\n [0.7236364  0.9047619  0.6440678  0.5833333 ]\n [0.34545454 0.5833333  0.33898306 0.3472222 ]\n [0.10909091 0.75       0.3559322  0.41666666]\n [0.6690909  0.63095236 0.47457626 0.19444445]\n [0.8036364  0.9166667  0.4915254  0.44444445]\n [0.4909091  0.75       0.37288135 0.22916667]\n [0.33454546 0.85714287 0.37288135 0.2361111 ]\n [0.32       0.41666666 0.2542373  0.1388889 ]\n [0.41454545 0.5952381  0.5084746  0.19444445]\n [0.14909092 0.48809522 0.2542373  0.125     ]\n [0.23636363 0.4642857  0.27118644 0.05555556]\n [0.22181818 0.5952381  0.22033899 0.3472222 ]\n [0.24727273 0.5595238  0.15254237 0.25694445]\n [0.63272727 0.35714287 0.88135594 0.8194444 ]\n [0.47272727 0.15476191 0.6440678  0.4722222 ]\n [0.6036364  0.23809524 0.84745765 0.7361111 ]\n [0.26909092 0.5595238  0.27118644 0.16666667]\n [0.28       0.71428573 0.20338982 0.5416667 ]\n [0.10545454 0.5714286  0.33898306 0.2847222 ]\n [0.18545455 0.5952381  0.10169491 0.33333334]\n [0.47272727 0.16666667 0.7288136  0.6388889 ]\n [0.45090908 0.1904762  0.7118644  0.5972222 ]\n [0.49454546 0.5        0.3559322  0.25      ]\n [0.6363636  0.22619048 0.7457627  0.5694444 ]\n [0.08727273 0.5952381  0.2542373  0.05555556]\n [0.52       0.22619048 0.7457627  0.5555556 ]\n [0.5090909  0.23809524 0.7288136  0.6666667 ]\n [0.56       0.22619048 0.779661   0.625     ]\n [0.6363636  0.3452381  0.89830506 0.8333333 ]\n [0.15636364 0.47619048 0.20338982 0.04166667]], shape=(32, 4), dtype=float32)\ntf.Tensor([0 2 1 0 0 1 1 1 0 1 1 0 0 0 0 2 2 2 0 0 0 0 2 2 1 2 0 2 2 2 2 0], shape=(32,), dtype=int64)\n2024-08-16 02:28:39.428348: W tensorflow/core/kernels/data/cache_dataset_ops.cc:913] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.",
      "model = tf.keras.Sequential([\n  tf.keras.layers.Dense(10, activation=tf.nn.relu, input_shape=(4,)),  # input shape required\n  tf.keras.layers.Dense(10, activation=tf.nn.relu),\n  tf.keras.layers.Dense(3)\n])",
      "<tf.Tensor: shape=(5, 3), dtype=float32, numpy=\narray([[-0.01486555, -0.2774356 , -0.4446883 ],\n       [-0.01989127, -0.18148412, -0.23701203],\n       [-0.01520463, -0.38729626, -0.6065093 ],\n       [-0.01831748, -0.22101548, -0.35036355],\n       [-0.05447623, -0.24796928, -0.36515424]], dtype=float32)>",
      "tf.nn.softmax(predictions[:5])",
      "<tf.Tensor: shape=(5, 3), dtype=float32, numpy=\narray([[0.41327488, 0.31783837, 0.26888674],\n       [0.37655985, 0.32037243, 0.3030677 ],\n       [0.44585222, 0.30732197, 0.24682581],\n       [0.39463627, 0.32223028, 0.2831335 ],\n       [0.39107943, 0.322279  , 0.2866416 ]], dtype=float32)>",
      "print(\"Prediction: {}\".format(tf.math.argmax(predictions, axis=1)))\nprint(\"    Labels: {}\".format(labels))",
      "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)",
      "def loss(model, x, y, training):\n  # training=training is needed only if there are layers with different\n  # behavior during training versus inference (e.g. Dropout).\n  y_ = model(x, training=training)\n\n  return loss_object(y_true=y, y_pred=y_)\n\nl = loss(model, features, labels, training=False)\nprint(\"Loss test: {}\".format(l))",
      "def grad(model, inputs, targets):\n  with tf.GradientTape() as tape:\n    loss_value = loss(model, inputs, targets, training=True)\n  return loss_value, tape.gradient(loss_value, model.trainable_variables)",
      "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)",
      "## Note: Rerunning this cell uses the same model parameters\n\n# Keep results for plotting\ntrain_loss_results = []\ntrain_accuracy_results = []\n\nnum_epochs = 201\n\nfor epoch in range(num_epochs):\n  epoch_loss_avg = tf.keras.metrics.Mean()\n  epoch_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n\n  # Training loop - using batches of 32\n  for x, y in ds_train_batch:\n    # Optimize the model\n    loss_value, grads = grad(model, x, y)\n    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n\n    # Track progress\n    epoch_loss_avg.update_state(loss_value)  # Add current batch loss\n    # Compare predicted label to actual label\n    # training=True is needed only if there are layers with different\n    # behavior during training versus inference (e.g. Dropout).\n    epoch_accuracy.update_state(y, model(x, training=True))\n\n  # End epoch\n  train_loss_results.append(epoch_loss_avg.result())\n  train_accuracy_results.append(epoch_accuracy.result())\n\n  if epoch % 50 == 0:\n    print(\"Epoch {:03d}: Loss: {:.3f}, Accuracy: {:.3%}\".format(epoch,\n                                                                epoch_loss_avg.result(),\n                                                                epoch_accuracy.result()))",
      "test_accuracy = tf.keras.metrics.Accuracy()\nds_test_batch = ds_test.batch(10)\n\nfor (x, y) in ds_test_batch:\n  # training=False is needed only if there are layers with different\n  # behavior during training versus inference (e.g. Dropout).\n  logits = model(x, training=False)\n  prediction = tf.math.argmax(logits, axis=1, output_type=tf.int64)\n  test_accuracy(prediction, y)\n\nprint(\"Test set accuracy: {:.3%}\".format(test_accuracy.result()))",
      "tf.stack([y,prediction],axis=1)",
      "<tf.Tensor: shape=(7, 2), dtype=int64, numpy=\narray([[1, 1],\n       [0, 0],\n       [2, 2],\n       [0, 0],\n       [1, 1],\n       [2, 2],\n       [0, 0]])>",
      "predict_dataset = tf.convert_to_tensor([\n    [0.3, 0.8, 0.4, 0.5,],\n    [0.4, 0.1, 0.8, 0.5,],\n    [0.7, 0.9, 0.8, 0.4]\n])\n\n# training=False is needed only if there are layers with different\n# behavior during training versus inference (e.g. Dropout).\npredictions = model(predict_dataset, training=False)\n\nfor i, logits in enumerate(predictions):\n  class_idx = tf.math.argmax(logits).numpy()\n  p = tf.nn.softmax(logits)[class_idx]\n  name = class_names[class_idx]\n  print(\"Example {} prediction: {} ({:4.1f}%)\".format(i, name, 100*p))"
    ]
  },
  {
    "url": "https://www.tensorflow.org/tutorials/quickstart/beginner",
    "title": "TensorFlow 2 quickstart for beginners\n      \n\n\n      \n      Stay organized with collections\n    \n\n      \n      Save and categorize content based on your preferences.",
    "code_snippets": [
      "import tensorflow as tf\nprint(\"TensorFlow version:\", tf.__version__)",
      "mnist = tf.keras.datasets.mnist\n\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0",
      "model = tf.keras.models.Sequential([\n  tf.keras.layers.Flatten(input_shape=(28, 28)),\n  tf.keras.layers.Dense(128, activation='relu'),\n  tf.keras.layers.Dropout(0.2),\n  tf.keras.layers.Dense(10)\n])",
      "tf.nn.softmax(predictions).numpy()",
      "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)",
      "probability_model = tf.keras.Sequential([\n  model,\n  tf.keras.layers.Softmax()\n])",
      "<tf.Tensor: shape=(5, 10), dtype=float32, numpy=\narray([[1.5427084e-07, 7.5027339e-11, 3.1343968e-06, 4.6326011e-05,\n        8.9990645e-13, 1.5266414e-07, 2.0456495e-13, 9.9994934e-01,\n        2.1858141e-07, 7.8530559e-07],\n       [1.7771253e-08, 8.4947787e-05, 9.9989736e-01, 1.8331458e-06,\n        8.3026415e-15, 3.4793761e-08, 6.2480517e-08, 7.9319728e-12,\n        1.5733674e-05, 3.5440111e-15],\n       [3.3602277e-07, 9.9804592e-01, 5.7737787e-05, 5.8099768e-06,\n        6.3599517e-05, 2.3768812e-06, 2.3459031e-06, 1.6781164e-03,\n        1.4260423e-04, 1.0617223e-06],\n       [9.9997318e-01, 8.7561805e-11, 9.8983969e-07, 9.0878149e-10,\n        1.0803159e-07, 3.3033965e-07, 2.3622524e-05, 6.7567669e-07,\n        4.7765565e-09, 1.1131582e-06],\n       [1.1404303e-05, 2.4895797e-09, 6.0792736e-06, 4.9114313e-08,\n        9.9449867e-01, 5.9158310e-06, 2.9842497e-05, 4.8574508e-05,\n        8.5193824e-06, 5.3910208e-03]], dtype=float32)>"
    ]
  },
  {
    "url": "https://www.tensorflow.org/tutorials/video/video_classification",
    "title": "Video classification with a 3D convolutional neural network\n      \n\n\n      \n      Stay organized with collections\n    \n\n      \n      Save and categorize content based on your preferences.",
    "code_snippets": [
      "import tqdm\nimport random\nimport pathlib\nimport itertools\nimport collections\n\nimport cv2\nimport einops\nimport numpy as np\nimport remotezip as rz\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\nimport keras\nfrom keras import layers",
      "def list_files_per_class(zip_url):\n  \"\"\"\n    List the files in each class of the dataset given the zip URL.\n\n    Args:\n      zip_url: URL from which the files can be unzipped. \n\n    Return:\n      files: List of files in each of the classes.\n  \"\"\"\n  files = []\n  with rz.RemoteZip(URL) as zip:\n    for zip_info in zip.infolist():\n      files.append(zip_info.filename)\n  return files\n\ndef get_class(fname):\n  \"\"\"\n    Retrieve the name of the class given a filename.\n\n    Args:\n      fname: Name of the file in the UCF101 dataset.\n\n    Return:\n      Class that the file belongs to.\n  \"\"\"\n  return fname.split('_')[-3]\n\ndef get_files_per_class(files):\n  \"\"\"\n    Retrieve the files that belong to each class. \n\n    Args:\n      files: List of files in the dataset.\n\n    Return:\n      Dictionary of class names (key) and files (values).\n  \"\"\"\n  files_for_class = collections.defaultdict(list)\n  for fname in files:\n    class_name = get_class(fname)\n    files_for_class[class_name].append(fname)\n  return files_for_class\n\ndef download_from_zip(zip_url, to_dir, file_names):\n  \"\"\"\n    Download the contents of the zip file from the zip URL.\n\n    Args:\n      zip_url: Zip URL containing data.\n      to_dir: Directory to download data to.\n      file_names: Names of files to download.\n  \"\"\"\n  with rz.RemoteZip(zip_url) as zip:\n    for fn in tqdm.tqdm(file_names):\n      class_name = get_class(fn)\n      zip.extract(fn, str(to_dir / class_name))\n      unzipped_file = to_dir / class_name / fn\n\n      fn = pathlib.Path(fn).parts[-1]\n      output_file = to_dir / class_name / fn\n      unzipped_file.rename(output_file,)\n\ndef split_class_lists(files_for_class, count):\n  \"\"\"\n    Returns the list of files belonging to a subset of data as well as the remainder of\n    files that need to be downloaded.\n\n    Args:\n      files_for_class: Files belonging to a particular class of data.\n      count: Number of files to download.\n\n    Return:\n      split_files: Files belonging to the subset of data.\n      remainder: Dictionary of the remainder of files that need to be downloaded.\n  \"\"\"\n  split_files = []\n  remainder = {}\n  for cls in files_for_class:\n    split_files.extend(files_for_class[cls][:count])\n    remainder[cls] = files_for_class[cls][count:]\n  return split_files, remainder\n\ndef download_ufc_101_subset(zip_url, num_classes, splits, download_dir):\n  \"\"\"\n    Download a subset of the UFC101 dataset and split them into various parts, such as\n    training, validation, and test. \n\n    Args:\n      zip_url: Zip URL containing data.\n      num_classes: Number of labels.\n      splits: Dictionary specifying the training, validation, test, etc. (key) division of data \n              (value is number of files per split).\n      download_dir: Directory to download data to.\n\n    Return:\n      dir: Posix path of the resulting directories containing the splits of data.\n  \"\"\"\n  files = list_files_per_class(zip_url)\n  for f in files:\n    tokens = f.split('/')\n    if len(tokens) <= 2:\n      files.remove(f) # Remove that item from the list if it does not have a filename\n\n  files_for_class = get_files_per_class(files)\n\n  classes = list(files_for_class.keys())[:num_classes]\n\n  for cls in classes:\n    new_files_for_class = files_for_class[cls]\n    random.shuffle(new_files_for_class)\n    files_for_class[cls] = new_files_for_class\n\n  # Only use the number of classes you want in the dictionary\n  files_for_class = {x: files_for_class[x] for x in list(files_for_class)[:num_classes]}\n\n  dirs = {}\n  for split_name, split_count in splits.items():\n    print(split_name, \":\")\n    split_dir = download_dir / split_name\n    split_files, files_for_class = split_class_lists(files_for_class, split_count)\n    download_from_zip(zip_url, split_dir, split_files)\n    dirs[split_name] = split_dir\n\n  return dirs\n\ndef format_frames(frame, output_size):\n  \"\"\"\n    Pad and resize an image from a video.\n\n    Args:\n      frame: Image that needs to resized and padded. \n      output_size: Pixel size of the output frame image.\n\n    Return:\n      Formatted frame with padding of specified output size.\n  \"\"\"\n  frame = tf.image.convert_image_dtype(frame, tf.float32)\n  frame = tf.image.resize_with_pad(frame, *output_size)\n  return frame\n\ndef frames_from_video_file(video_path, n_frames, output_size = (224,224), frame_step = 15):\n  \"\"\"\n    Creates frames from each video file present for each category.\n\n    Args:\n      video_path: File path to the video.\n      n_frames: Number of frames to be created per video file.\n      output_size: Pixel size of the output frame image.\n\n    Return:\n      An NumPy array of frames in the shape of (n_frames, height, width, channels).\n  \"\"\"\n  # Read each video frame by frame\n  result = []\n  src = cv2.VideoCapture(str(video_path))  \n\n  video_length = src.get(cv2.CAP_PROP_FRAME_COUNT)\n\n  need_length = 1 + (n_frames - 1) * frame_step\n\n  if need_length > video_length:\n    start = 0\n  else:\n    max_start = video_length - need_length\n    start = random.randint(0, max_start + 1)\n\n  src.set(cv2.CAP_PROP_POS_FRAMES, start)\n  # ret is a boolean indicating whether read was successful, frame is the image itself\n  ret, frame = src.read()\n  result.append(format_frames(frame, output_size))\n\n  for _ in range(n_frames - 1):\n    for _ in range(frame_step):\n      ret, frame = src.read()\n    if ret:\n      frame = format_frames(frame, output_size)\n      result.append(frame)\n    else:\n      result.append(np.zeros_like(result[0]))\n  src.release()\n  result = np.array(result)[..., [2, 1, 0]]\n\n  return result\n\nclass FrameGenerator:\n  def __init__(self, path, n_frames, training = False):\n    \"\"\" Returns a set of frames with their associated label. \n\n      Args:\n        path: Video file paths.\n        n_frames: Number of frames. \n        training: Boolean to determine if training dataset is being created.\n    \"\"\"\n    self.path = path\n    self.n_frames = n_frames\n    self.training = training\n    self.class_names = sorted(set(p.name for p in self.path.iterdir() if p.is_dir()))\n    self.class_ids_for_name = dict((name, idx) for idx, name in enumerate(self.class_names))\n\n  def get_files_and_class_names(self):\n    video_paths = list(self.path.glob('*/*.avi'))\n    classes = [p.parent.name for p in video_paths] \n    return video_paths, classes\n\n  def __call__(self):\n    video_paths, classes = self.get_files_and_class_names()\n\n    pairs = list(zip(video_paths, classes))\n\n    if self.training:\n      random.shuffle(pairs)\n\n    for path, name in pairs:\n      video_frames = frames_from_video_file(path, self.n_frames) \n      label = self.class_ids_for_name[name] # Encode labels\n      yield video_frames, label",
      "n_frames = 10\nbatch_size = 8\n\noutput_signature = (tf.TensorSpec(shape = (None, None, None, 3), dtype = tf.float32),\n                    tf.TensorSpec(shape = (), dtype = tf.int16))\n\ntrain_ds = tf.data.Dataset.from_generator(FrameGenerator(subset_paths['train'], n_frames, training=True),\n                                          output_signature = output_signature)\n\n\n# Batch the data\ntrain_ds = train_ds.batch(batch_size)\n\nval_ds = tf.data.Dataset.from_generator(FrameGenerator(subset_paths['val'], n_frames),\n                                        output_signature = output_signature)\nval_ds = val_ds.batch(batch_size)\n\ntest_ds = tf.data.Dataset.from_generator(FrameGenerator(subset_paths['test'], n_frames),\n                                         output_signature = output_signature)\n\ntest_ds = test_ds.batch(batch_size)",
      "class Conv2Plus1D(keras.layers.Layer):\n  def __init__(self, filters, kernel_size, padding):\n    \"\"\"\n      A sequence of convolutional layers that first apply the convolution operation over the\n      spatial dimensions, and then the temporal dimension. \n    \"\"\"\n    super().__init__()\n    self.seq = keras.Sequential([  \n        # Spatial decomposition\n        layers.Conv3D(filters=filters,\n                      kernel_size=(1, kernel_size[1], kernel_size[2]),\n                      padding=padding),\n        # Temporal decomposition\n        layers.Conv3D(filters=filters, \n                      kernel_size=(kernel_size[0], 1, 1),\n                      padding=padding)\n        ])\n\n  def call(self, x):\n    return self.seq(x)",
      "class ResidualMain(keras.layers.Layer):\n  \"\"\"\n    Residual block of the model with convolution, layer normalization, and the\n    activation function, ReLU.\n  \"\"\"\n  def __init__(self, filters, kernel_size):\n    super().__init__()\n    self.seq = keras.Sequential([\n        Conv2Plus1D(filters=filters,\n                    kernel_size=kernel_size,\n                    padding='same'),\n        layers.LayerNormalization(),\n        layers.ReLU(),\n        Conv2Plus1D(filters=filters, \n                    kernel_size=kernel_size,\n                    padding='same'),\n        layers.LayerNormalization()\n    ])\n\n  def call(self, x):\n    return self.seq(x)",
      "class Project(keras.layers.Layer):\n  \"\"\"\n    Project certain dimensions of the tensor as the data is passed through different \n    sized filters and downsampled. \n  \"\"\"\n  def __init__(self, units):\n    super().__init__()\n    self.seq = keras.Sequential([\n        layers.Dense(units),\n        layers.LayerNormalization()\n    ])\n\n  def call(self, x):\n    return self.seq(x)",
      "def add_residual_block(input, filters, kernel_size):\n  \"\"\"\n    Add residual blocks to the model. If the last dimensions of the input data\n    and filter size does not match, project it such that last dimension matches.\n  \"\"\"\n  out = ResidualMain(filters, \n                     kernel_size)(input)\n\n  res = input\n  # Using the Keras functional APIs, project the last dimension of the tensor to\n  # match the new filter size\n  if out.shape[-1] != input.shape[-1]:\n    res = Project(out.shape[-1])(res)\n\n  return layers.add([res, out])",
      "class ResizeVideo(keras.layers.Layer):\n  def __init__(self, height, width):\n    super().__init__()\n    self.height = height\n    self.width = width\n    self.resizing_layer = layers.Resizing(self.height, self.width)\n\n  def call(self, video):\n    \"\"\"\n      Use the einops library to resize the tensor.  \n\n      Args:\n        video: Tensor representation of the video, in the form of a set of frames.\n\n      Return:\n        A downsampled size of the video according to the new height and width it should be resized to.\n    \"\"\"\n    # b stands for batch size, t stands for time, h stands for height, \n    # w stands for width, and c stands for the number of channels.\n    old_shape = einops.parse_shape(video, 'b t h w c')\n    images = einops.rearrange(video, 'b t h w c -> (b t) h w c')\n    images = self.resizing_layer(images)\n    videos = einops.rearrange(\n        images, '(b t) h w c -> b t h w c',\n        t = old_shape['t'])\n    return videos",
      "def plot_history(history):\n  \"\"\"\n    Plotting training and validation learning curves.\n\n    Args:\n      history: model history with all the metric measures\n  \"\"\"\n  fig, (ax1, ax2) = plt.subplots(2)\n\n  fig.set_size_inches(18.5, 10.5)\n\n  # Plot loss\n  ax1.set_title('Loss')\n  ax1.plot(history.history['loss'], label = 'train')\n  ax1.plot(history.history['val_loss'], label = 'test')\n  ax1.set_ylabel('Loss')\n\n  # Determine upper bound of y-axis\n  max_loss = max(history.history['loss'] + history.history['val_loss'])\n\n  ax1.set_ylim([0, np.ceil(max_loss)])\n  ax1.set_xlabel('Epoch')\n  ax1.legend(['Train', 'Validation']) \n\n  # Plot accuracy\n  ax2.set_title('Accuracy')\n  ax2.plot(history.history['accuracy'],  label = 'train')\n  ax2.plot(history.history['val_accuracy'], label = 'test')\n  ax2.set_ylabel('Accuracy')\n  ax2.set_ylim([0, 1])\n  ax2.set_xlabel('Epoch')\n  ax2.legend(['Train', 'Validation'])\n\n  plt.show()\n\nplot_history(history)",
      "def get_actual_predicted_labels(dataset): \n  \"\"\"\n    Create a list of actual ground truth values and the predictions from the model.\n\n    Args:\n      dataset: An iterable data structure, such as a TensorFlow Dataset, with features and labels.\n\n    Return:\n      Ground truth and predicted values for a particular dataset.\n  \"\"\"\n  actual = [labels for _, labels in dataset.unbatch()]\n  predicted = model.predict(dataset)\n\n  actual = tf.stack(actual, axis=0)\n  predicted = tf.concat(predicted, axis=0)\n  predicted = tf.argmax(predicted, axis=1)\n\n  return actual, predicted",
      "def plot_confusion_matrix(actual, predicted, labels, ds_type):\n  cm = tf.math.confusion_matrix(actual, predicted)\n  ax = sns.heatmap(cm, annot=True, fmt='g')\n  sns.set(rc={'figure.figsize':(12, 12)})\n  sns.set(font_scale=1.4)\n  ax.set_title('Confusion matrix of action recognition for ' + ds_type)\n  ax.set_xlabel('Predicted Action')\n  ax.set_ylabel('Actual Action')\n  plt.xticks(rotation=90)\n  plt.yticks(rotation=0)\n  ax.xaxis.set_ticklabels(labels)\n  ax.yaxis.set_ticklabels(labels)",
      "def calculate_classification_metrics(y_actual, y_pred, labels):\n  \"\"\"\n    Calculate the precision and recall of a classification model using the ground truth and\n    predicted values. \n\n    Args:\n      y_actual: Ground truth labels.\n      y_pred: Predicted labels.\n      labels: List of classification labels.\n\n    Return:\n      Precision and recall measures.\n  \"\"\"\n  cm = tf.math.confusion_matrix(y_actual, y_pred)\n  tp = np.diag(cm) # Diagonal represents true positives\n  precision = dict()\n  recall = dict()\n  for i in range(len(labels)):\n    col = cm[:, i]\n    fp = np.sum(col) - tp[i] # Sum of column minus true positive is false negative\n\n    row = cm[i, :]\n    fn = np.sum(row) - tp[i] # Sum of row minus true positive, is false negative\n\n    precision[labels[i]] = tp[i] / (tp[i] + fp) # Precision \n\n    recall[labels[i]] = tp[i] / (tp[i] + fn) # Recall\n\n  return precision, recall"
    ]
  },
  {
    "url": "https://www.tensorflow.org/tutorials/text",
    "title": "Text and natural language processing with TensorFlow\n      \n\n\n      \n      Stay organized with collections\n    \n\n      \n      Save and categorize content based on your preferences.",
    "code_snippets": []
  },
  {
    "url": "https://www.tensorflow.org/tutorials/load_data/images",
    "title": "Load and preprocess images\n      \n\n\n      \n      Stay organized with collections\n    \n\n      \n      Save and categorize content based on your preferences.",
    "code_snippets": [
      "import numpy as np\nimport os\nimport PIL\nimport PIL.Image\nimport tensorflow as tf\nimport tensorflow_datasets as tfds",
      "print(tf.__version__)",
      "import pathlib\ndataset_url = \"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\"\narchive = tf.keras.utils.get_file(origin=dataset_url, extract=True)\ndata_dir = pathlib.Path(archive).with_suffix('')",
      "train_ds = tf.keras.utils.image_dataset_from_directory(\n  data_dir,\n  validation_split=0.2,\n  subset=\"training\",\n  seed=123,\n  image_size=(img_height, img_width),\n  batch_size=batch_size)",
      "val_ds = tf.keras.utils.image_dataset_from_directory(\n  data_dir,\n  validation_split=0.2,\n  subset=\"validation\",\n  seed=123,\n  image_size=(img_height, img_width),\n  batch_size=batch_size)",
      "class_names = train_ds.class_names\nprint(class_names)",
      "import matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 10))\nfor images, labels in train_ds.take(1):\n  for i in range(9):\n    ax = plt.subplot(3, 3, i + 1)\n    plt.imshow(images[i].numpy().astype(\"uint8\"))\n    plt.title(class_names[labels[i]])\n    plt.axis(\"off\")",
      "normalization_layer = tf.keras.layers.Rescaling(1./255)",
      "AUTOTUNE = tf.data.AUTOTUNE\n\ntrain_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\nval_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)",
      "num_classes = 5\n\nmodel = tf.keras.Sequential([\n  tf.keras.layers.Rescaling(1./255),\n  tf.keras.layers.Conv2D(32, 3, activation='relu'),\n  tf.keras.layers.MaxPooling2D(),\n  tf.keras.layers.Conv2D(32, 3, activation='relu'),\n  tf.keras.layers.MaxPooling2D(),\n  tf.keras.layers.Conv2D(32, 3, activation='relu'),\n  tf.keras.layers.MaxPooling2D(),\n  tf.keras.layers.Flatten(),\n  tf.keras.layers.Dense(128, activation='relu'),\n  tf.keras.layers.Dense(num_classes)\n])",
      "model.compile(\n  optimizer='adam',\n  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n  metrics=['accuracy'])",
      "list_ds = tf.data.Dataset.list_files(str(data_dir/'*/*'), shuffle=False)\nlist_ds = list_ds.shuffle(image_count, reshuffle_each_iteration=False)",
      "class_names = np.array(sorted([item.name for item in data_dir.glob('*') if item.name != \"LICENSE.txt\"]))\nprint(class_names)",
      "print(tf.data.experimental.cardinality(train_ds).numpy())\nprint(tf.data.experimental.cardinality(val_ds).numpy())",
      "def get_label(file_path):\n  # Convert the path to a list of path components\n  parts = tf.strings.split(file_path, os.path.sep)\n  # The second to last is the class-directory\n  one_hot = parts[-2] == class_names\n  # Integer encode the label\n  return tf.argmax(one_hot)",
      "def decode_img(img):\n  # Convert the compressed string to a 3D uint8 tensor\n  img = tf.io.decode_jpeg(img, channels=3)\n  # Resize the image to the desired size\n  return tf.image.resize(img, [img_height, img_width])",
      "def process_path(file_path):\n  label = get_label(file_path)\n  # Load the raw data from the file as a string\n  img = tf.io.read_file(file_path)\n  img = decode_img(img)\n  return img, label",
      "def configure_for_performance(ds):\n  ds = ds.cache()\n  ds = ds.shuffle(buffer_size=1000)\n  ds = ds.batch(batch_size)\n  ds = ds.prefetch(buffer_size=AUTOTUNE)\n  return ds\n\ntrain_ds = configure_for_performance(train_ds)\nval_ds = configure_for_performance(val_ds)"
    ]
  },
  {
    "url": "https://www.tensorflow.org/tutorials/generative/style_transfer",
    "title": "Neural style transfer\n      \n\n\n      \n      Stay organized with collections\n    \n\n      \n      Save and categorize content based on your preferences.",
    "code_snippets": [
      "import os\nimport tensorflow as tf\n# Load compressed models from tensorflow_hub\nos.environ['TFHUB_MODEL_LOAD_FORMAT'] = 'COMPRESSED'",
      "import IPython.display as display\n\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nmpl.rcParams['figure.figsize'] = (12, 12)\nmpl.rcParams['axes.grid'] = False\n\nimport numpy as np\nimport PIL.Image\nimport time\nimport functools",
      "def tensor_to_image(tensor):\n  tensor = tensor*255\n  tensor = np.array(tensor, dtype=np.uint8)\n  if np.ndim(tensor)>3:\n    assert tensor.shape[0] == 1\n    tensor = tensor[0]\n  return PIL.Image.fromarray(tensor)",
      "content_path = tf.keras.utils.get_file('YellowLabradorLooking_new.jpg', 'https://storage.googleapis.com/download.tensorflow.org/example_images/YellowLabradorLooking_new.jpg')\nstyle_path = tf.keras.utils.get_file('kandinsky5.jpg','https://storage.googleapis.com/download.tensorflow.org/example_images/Vassily_Kandinsky%2C_1913_-_Composition_7.jpg')",
      "def load_img(path_to_img):\n  max_dim = 512\n  img = tf.io.read_file(path_to_img)\n  img = tf.image.decode_image(img, channels=3)\n  img = tf.image.convert_image_dtype(img, tf.float32)\n\n  shape = tf.cast(tf.shape(img)[:-1], tf.float32)\n  long_dim = max(shape)\n  scale = max_dim / long_dim\n\n  new_shape = tf.cast(shape * scale, tf.int32)\n\n  img = tf.image.resize(img, new_shape)\n  img = img[tf.newaxis, :]\n  return img",
      "def imshow(image, title=None):\n  if len(image.shape) > 3:\n    image = tf.squeeze(image, axis=0)\n\n  plt.imshow(image)\n  if title:\n    plt.title(title)",
      "import tensorflow_hub as hub\nhub_model = hub.load('https://tfhub.dev/google/magenta/arbitrary-image-stylization-v1-256/2')\nstylized_image = hub_model(tf.constant(content_image), tf.constant(style_image))[0]\ntensor_to_image(stylized_image)",
      "x = tf.keras.applications.vgg19.preprocess_input(content_image*255)\nx = tf.image.resize(x, (224, 224))\nvgg = tf.keras.applications.VGG19(include_top=True, weights='imagenet')\nprediction_probabilities = vgg(x)\nprediction_probabilities.shape",
      "predicted_top_5 = tf.keras.applications.vgg19.decode_predictions(prediction_probabilities.numpy())[0]\n[(class_name, prob) for (number, class_name, prob) in predicted_top_5]",
      "vgg = tf.keras.applications.VGG19(include_top=False, weights='imagenet')\n\nprint()\nfor layer in vgg.layers:\n  print(layer.name)",
      "def vgg_layers(layer_names):\n  \"\"\" Creates a VGG model that returns a list of intermediate output values.\"\"\"\n  # Load our model. Load pretrained VGG, trained on ImageNet data\n  vgg = tf.keras.applications.VGG19(include_top=False, weights='imagenet')\n  vgg.trainable = False\n\n  outputs = [vgg.get_layer(name).output for name in layer_names]\n\n  model = tf.keras.Model([vgg.input], outputs)\n  return model",
      "def gram_matrix(input_tensor):\n  result = tf.linalg.einsum('bijc,bijd->bcd', input_tensor, input_tensor)\n  input_shape = tf.shape(input_tensor)\n  num_locations = tf.cast(input_shape[1]*input_shape[2], tf.float32)\n  return result/(num_locations)",
      "class StyleContentModel(tf.keras.models.Model):\n  def __init__(self, style_layers, content_layers):\n    super(StyleContentModel, self).__init__()\n    self.vgg = vgg_layers(style_layers + content_layers)\n    self.style_layers = style_layers\n    self.content_layers = content_layers\n    self.num_style_layers = len(style_layers)\n    self.vgg.trainable = False\n\n  def call(self, inputs):\n    \"Expects float input in [0,1]\"\n    inputs = inputs*255.0\n    preprocessed_input = tf.keras.applications.vgg19.preprocess_input(inputs)\n    outputs = self.vgg(preprocessed_input)\n    style_outputs, content_outputs = (outputs[:self.num_style_layers],\n                                      outputs[self.num_style_layers:])\n\n    style_outputs = [gram_matrix(style_output)\n                     for style_output in style_outputs]\n\n    content_dict = {content_name: value\n                    for content_name, value\n                    in zip(self.content_layers, content_outputs)}\n\n    style_dict = {style_name: value\n                  for style_name, value\n                  in zip(self.style_layers, style_outputs)}\n\n    return {'content': content_dict, 'style': style_dict}",
      "extractor = StyleContentModel(style_layers, content_layers)\n\nresults = extractor(tf.constant(content_image))\n\nprint('Styles:')\nfor name, output in sorted(results['style'].items()):\n  print(\"  \", name)\n  print(\"    shape: \", output.numpy().shape)\n  print(\"    min: \", output.numpy().min())\n  print(\"    max: \", output.numpy().max())\n  print(\"    mean: \", output.numpy().mean())\n  print()\n\nprint(\"Contents:\")\nfor name, output in sorted(results['content'].items()):\n  print(\"  \", name)\n  print(\"    shape: \", output.numpy().shape)\n  print(\"    min: \", output.numpy().min())\n  print(\"    max: \", output.numpy().max())\n  print(\"    mean: \", output.numpy().mean())",
      "image = tf.Variable(content_image)",
      "def clip_0_1(image):\n  return tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=1.0)",
      "opt = tf.keras.optimizers.Adam(learning_rate=0.02, beta_1=0.99, epsilon=1e-1)",
      "def style_content_loss(outputs):\n    style_outputs = outputs['style']\n    content_outputs = outputs['content']\n    style_loss = tf.add_n([tf.reduce_mean((style_outputs[name]-style_targets[name])**2) \n                           for name in style_outputs.keys()])\n    style_loss *= style_weight / num_style_layers\n\n    content_loss = tf.add_n([tf.reduce_mean((content_outputs[name]-content_targets[name])**2) \n                             for name in content_outputs.keys()])\n    content_loss *= content_weight / num_content_layers\n    loss = style_loss + content_loss\n    return loss",
      "@tf.function()\ndef train_step(image):\n  with tf.GradientTape() as tape:\n    outputs = extractor(image)\n    loss = style_content_loss(outputs)\n\n  grad = tape.gradient(loss, image)\n  opt.apply_gradients([(grad, image)])\n  image.assign(clip_0_1(image))",
      "import time\nstart = time.time()\n\nepochs = 10\nsteps_per_epoch = 100\n\nstep = 0\nfor n in range(epochs):\n  for m in range(steps_per_epoch):\n    step += 1\n    train_step(image)\n    print(\".\", end='', flush=True)\n  display.clear_output(wait=True)\n  display.display(tensor_to_image(image))\n  print(\"Train step: {}\".format(step))\n\nend = time.time()\nprint(\"Total time: {:.1f}\".format(end-start))",
      "def high_pass_x_y(image):\n  x_var = image[:, :, 1:, :] - image[:, :, :-1, :]\n  y_var = image[:, 1:, :, :] - image[:, :-1, :, :]\n\n  return x_var, y_var",
      "plt.figure(figsize=(14, 10))\n\nsobel = tf.image.sobel_edges(content_image)\nplt.subplot(1, 2, 1)\nimshow(clip_0_1(sobel[..., 0]/4+0.5), \"Horizontal Sobel-edges\")\nplt.subplot(1, 2, 2)\nimshow(clip_0_1(sobel[..., 1]/4+0.5), \"Vertical Sobel-edges\")",
      "def total_variation_loss(image):\n  x_deltas, y_deltas = high_pass_x_y(image)\n  return tf.reduce_sum(tf.abs(x_deltas)) + tf.reduce_sum(tf.abs(y_deltas))",
      "tf.image.total_variation(image).numpy()",
      "@tf.function()\ndef train_step(image):\n  with tf.GradientTape() as tape:\n    outputs = extractor(image)\n    loss = style_content_loss(outputs)\n    loss += total_variation_weight*tf.image.total_variation(image)\n\n  grad = tape.gradient(loss, image)\n  opt.apply_gradients([(grad, image)])\n  image.assign(clip_0_1(image))",
      "opt = tf.keras.optimizers.Adam(learning_rate=0.02, beta_1=0.99, epsilon=1e-1)\nimage = tf.Variable(content_image)",
      "import time\nstart = time.time()\n\nepochs = 10\nsteps_per_epoch = 100\n\nstep = 0\nfor n in range(epochs):\n  for m in range(steps_per_epoch):\n    step += 1\n    train_step(image)\n    print(\".\", end='', flush=True)\n  display.clear_output(wait=True)\n  display.display(tensor_to_image(image))\n  print(\"Train step: {}\".format(step))\n\nend = time.time()\nprint(\"Total time: {:.1f}\".format(end-start))"
    ]
  },
  {
    "url": "https://www.tensorflow.org/tutorials/generative/cvae",
    "title": "Convolutional Variational Autoencoder\n      \n\n\n      \n      Stay organized with collections\n    \n\n      \n      Save and categorize content based on your preferences.",
    "code_snippets": [
      "(train_images, _), (test_images, _) = tf.keras.datasets.mnist.load_data()",
      "def preprocess_images(images):\n  images = images.reshape((images.shape[0], 28, 28, 1)) / 255.\n  return np.where(images > .5, 1.0, 0.0).astype('float32')\n\ntrain_images = preprocess_images(train_images)\ntest_images = preprocess_images(test_images)",
      "train_dataset = (tf.data.Dataset.from_tensor_slices(train_images)\n                 .shuffle(train_size).batch(batch_size))\ntest_dataset = (tf.data.Dataset.from_tensor_slices(test_images)\n                .shuffle(test_size).batch(batch_size))",
      "class CVAE(tf.keras.Model):\n  \"\"\"Convolutional variational autoencoder.\"\"\"\n\n  def __init__(self, latent_dim):\n    super(CVAE, self).__init__()\n    self.latent_dim = latent_dim\n    self.encoder = tf.keras.Sequential(\n        [\n            tf.keras.layers.InputLayer(input_shape=(28, 28, 1)),\n            tf.keras.layers.Conv2D(\n                filters=32, kernel_size=3, strides=(2, 2), activation='relu'),\n            tf.keras.layers.Conv2D(\n                filters=64, kernel_size=3, strides=(2, 2), activation='relu'),\n            tf.keras.layers.Flatten(),\n            # No activation\n            tf.keras.layers.Dense(latent_dim + latent_dim),\n        ]\n    )\n\n    self.decoder = tf.keras.Sequential(\n        [\n            tf.keras.layers.InputLayer(input_shape=(latent_dim,)),\n            tf.keras.layers.Dense(units=7*7*32, activation=tf.nn.relu),\n            tf.keras.layers.Reshape(target_shape=(7, 7, 32)),\n            tf.keras.layers.Conv2DTranspose(\n                filters=64, kernel_size=3, strides=2, padding='same',\n                activation='relu'),\n            tf.keras.layers.Conv2DTranspose(\n                filters=32, kernel_size=3, strides=2, padding='same',\n                activation='relu'),\n            # No activation\n            tf.keras.layers.Conv2DTranspose(\n                filters=1, kernel_size=3, strides=1, padding='same'),\n        ]\n    )\n\n  @tf.function\n  def sample(self, eps=None):\n    if eps is None:\n      eps = tf.random.normal(shape=(100, self.latent_dim))\n    return self.decode(eps, apply_sigmoid=True)\n\n  def encode(self, x):\n    mean, logvar = tf.split(self.encoder(x), num_or_size_splits=2, axis=1)\n    return mean, logvar\n\n  def reparameterize(self, mean, logvar):\n    eps = tf.random.normal(shape=mean.shape)\n    return eps * tf.exp(logvar * .5) + mean\n\n  def decode(self, z, apply_sigmoid=False):\n    logits = self.decoder(z)\n    if apply_sigmoid:\n      probs = tf.sigmoid(logits)\n      return probs\n    return logits",
      "optimizer = tf.keras.optimizers.Adam(1e-4)\n\n\ndef log_normal_pdf(sample, mean, logvar, raxis=1):\n  log2pi = tf.math.log(2. * np.pi)\n  return tf.reduce_sum(\n      -.5 * ((sample - mean) ** 2. * tf.exp(-logvar) + logvar + log2pi),\n      axis=raxis)\n\n\ndef compute_loss(model, x):\n  mean, logvar = model.encode(x)\n  z = model.reparameterize(mean, logvar)\n  x_logit = model.decode(z)\n  cross_ent = tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=x)\n  logpx_z = -tf.reduce_sum(cross_ent, axis=[1, 2, 3])\n  logpz = log_normal_pdf(z, 0., 0.)\n  logqz_x = log_normal_pdf(z, mean, logvar)\n  return -tf.reduce_mean(logpx_z + logpz - logqz_x)\n\n\n@tf.function\ndef train_step(model, x, optimizer):\n  \"\"\"Executes one training step and returns the loss.\n\n  This function computes the loss and gradients, and uses the latter to\n  update the model's parameters.\n  \"\"\"\n  with tf.GradientTape() as tape:\n    loss = compute_loss(model, x)\n  gradients = tape.gradient(loss, model.trainable_variables)\n  optimizer.apply_gradients(zip(gradients, model.trainable_variables))",
      "epochs = 10\n# set the dimensionality of the latent space to a plane for visualization later\nlatent_dim = 2\nnum_examples_to_generate = 16\n\n# keeping the random vector constant for generation (prediction) so\n# it will be easier to see the improvement.\nrandom_vector_for_generation = tf.random.normal(\n    shape=[num_examples_to_generate, latent_dim])\nmodel = CVAE(latent_dim)",
      "def generate_and_save_images(model, epoch, test_sample):\n  mean, logvar = model.encode(test_sample)\n  z = model.reparameterize(mean, logvar)\n  predictions = model.sample(z)\n  fig = plt.figure(figsize=(4, 4))\n\n  for i in range(predictions.shape[0]):\n    plt.subplot(4, 4, i + 1)\n    plt.imshow(predictions[i, :, :, 0], cmap='gray')\n    plt.axis('off')\n\n  # tight_layout minimizes the overlap between 2 sub-plots\n  plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n  plt.show()",
      "generate_and_save_images(model, 0, test_sample)\n\nfor epoch in range(1, epochs + 1):\n  start_time = time.time()\n  for train_x in train_dataset:\n    train_step(model, train_x, optimizer)\n  end_time = time.time()\n\n  loss = tf.keras.metrics.Mean()\n  for test_x in test_dataset:\n    loss(compute_loss(model, test_x))\n  elbo = -loss.result()\n  display.clear_output(wait=False)\n  print('Epoch: {}, Test set ELBO: {}, time elapse for current epoch: {}'\n        .format(epoch, elbo, end_time - start_time))\n  generate_and_save_images(model, epoch, test_sample)",
      "def display_image(epoch_no):\n  return PIL.Image.open('image_at_epoch_{:04d}.png'.format(epoch_no))",
      "import tensorflow_docs.vis.embed as embed\nembed.embed_file(anim_file)",
      "def plot_latent_images(model, n, digit_size=28):\n  \"\"\"Plots n x n digit images decoded from the latent space.\"\"\"\n\n  norm = tfp.distributions.Normal(0, 1)\n  grid_x = norm.quantile(np.linspace(0.05, 0.95, n))\n  grid_y = norm.quantile(np.linspace(0.05, 0.95, n))\n  image_width = digit_size*n\n  image_height = image_width\n  image = np.zeros((image_height, image_width))\n\n  for i, yi in enumerate(grid_x):\n    for j, xi in enumerate(grid_y):\n      z = np.array([[xi, yi]])\n      x_decoded = model.sample(z)\n      digit = tf.reshape(x_decoded[0], (digit_size, digit_size))\n      image[i * digit_size: (i + 1) * digit_size,\n            j * digit_size: (j + 1) * digit_size] = digit.numpy()\n\n  plt.figure(figsize=(10, 10))\n  plt.imshow(image, cmap='Greys_r')\n  plt.axis('Off')\n  plt.show()"
    ]
  },
  {
    "url": "https://www.tensorflow.org/tutorials/distribute/input",
    "title": "Distributed Input\n      \n\n\n      \n      Stay organized with collections\n    \n\n      \n      Save and categorize content based on your preferences.",
    "code_snippets": [
      "import tensorflow as tf\n\n# Helper libraries\nimport numpy as np\nimport os\n\nprint(tf.__version__)",
      "# Simulate multiple CPUs with virtual devices\nN_VIRTUAL_DEVICES = 2\nphysical_devices = tf.config.list_physical_devices(\"CPU\")\ntf.config.set_logical_device_configuration(\n    physical_devices[0], [tf.config.LogicalDeviceConfiguration() for _ in range(N_VIRTUAL_DEVICES)])",
      "print(\"Available devices:\")\nfor i, device in enumerate(tf.config.list_logical_devices()):\n  print(\"%d) %s\" % (i, device))",
      "global_batch_size = 16\n# Create a tf.data.Dataset object.\ndataset = tf.data.Dataset.from_tensors(([1.], [1.])).repeat(100).batch(global_batch_size)\n\n@tf.function\ndef train_step(inputs):\n  features, labels = inputs\n  return labels - 0.3 * features\n\n# Iterate over the dataset using the for..in construct.\nfor inputs in dataset:\n  print(train_step(inputs))",
      "tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]\n [0.7]\n [0.7]\n [0.7]\n [0.7]\n [0.7]\n [0.7]\n [0.7]\n [0.7]\n [0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(16, 1), dtype=float32)\ntf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]\n [0.7]\n [0.7]\n [0.7]\n [0.7]\n [0.7]\n [0.7]\n [0.7]\n [0.7]\n [0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(16, 1), dtype=float32)\ntf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]\n [0.7]\n [0.7]\n [0.7]\n [0.7]\n [0.7]\n [0.7]\n [0.7]\n [0.7]\n [0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(16, 1), dtype=float32)\ntf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]\n [0.7]\n [0.7]\n [0.7]\n [0.7]\n [0.7]\n [0.7]\n [0.7]\n [0.7]\n [0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(16, 1), dtype=float32)\ntf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]\n [0.7]\n [0.7]\n [0.7]\n [0.7]\n [0.7]\n [0.7]\n [0.7]\n [0.7]\n [0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(16, 1), dtype=float32)\ntf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]\n [0.7]\n [0.7]\n [0.7]\n [0.7]\n [0.7]\n [0.7]\n [0.7]\n [0.7]\n [0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(16, 1), dtype=float32)\ntf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32)",
      "global_batch_size = 16\nmirrored_strategy = tf.distribute.MirroredStrategy()\n\ndataset = tf.data.Dataset.from_tensors(([1.], [1.])).repeat(100).batch(global_batch_size)\n# Distribute input using the `experimental_distribute_dataset`.\ndist_dataset = mirrored_strategy.experimental_distribute_dataset(dataset)\n# 1 global batch of data fed to the model in 1 step.\nprint(next(iter(dist_dataset)))",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\n(PerReplica:{\n  0: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  1: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  2: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  3: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>\n}, PerReplica:{\n  0: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  1: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  2: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  3: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>\n})",
      "dataset = tf.data.Dataset.from_tensors(([1.], [1.])).repeat(64).batch(16)\noptions = tf.data.Options()\noptions.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.DATA\ndataset = dataset.with_options(options)",
      "mirrored_strategy = tf.distribute.MirroredStrategy()\n\ndef dataset_fn(input_context):\n  batch_size = input_context.get_per_replica_batch_size(global_batch_size)\n  dataset = tf.data.Dataset.from_tensors(([1.], [1.])).repeat(64).batch(16)\n  dataset = dataset.shard(\n      input_context.num_input_pipelines, input_context.input_pipeline_id)\n  dataset = dataset.batch(batch_size)\n  dataset = dataset.prefetch(2)  # This prefetches 2 batches per device.\n  return dataset\n\ndist_dataset = mirrored_strategy.distribute_datasets_from_function(dataset_fn)",
      "global_batch_size = 16\nmirrored_strategy = tf.distribute.MirroredStrategy()\n\ndataset = tf.data.Dataset.from_tensors(([1.], [1.])).repeat(100).batch(global_batch_size)\ndist_dataset = mirrored_strategy.experimental_distribute_dataset(dataset)\n\n@tf.function\ndef train_step(inputs):\n  features, labels = inputs\n  return labels - 0.3 * features\n\nfor x in dist_dataset:\n  # train_step trains the model using the dataset elements\n  loss = mirrored_strategy.run(train_step, args=(x,))\n  print(\"Loss is \", loss)",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\nLoss is  PerReplica:{\n  0: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  1: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  2: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  3: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32)\n}\nLoss is  PerReplica:{\n  0: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  1: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  2: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  3: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32)\n}\nLoss is  PerReplica:{\n  0: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  1: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  2: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  3: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32)\n}\nLoss is  PerReplica:{\n  0: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  1: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  2: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  3: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32)\n}\nLoss is  PerReplica:{\n  0: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  1: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  2: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  3: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32)\n}\nLoss is  PerReplica:{\n  0: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  1: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  2: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  3: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32)\n}\nLoss is  PerReplica:{\n  0: tf.Tensor([[0.7]], shape=(1, 1), dtype=float32),\n  1: tf.Tensor([[0.7]], shape=(1, 1), dtype=float32),\n  2: tf.Tensor([[0.7]], shape=(1, 1), dtype=float32),\n  3: tf.Tensor([[0.7]], shape=(1, 1), dtype=float32)\n}",
      "Loss is  PerReplica:{\n  0: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  1: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  2: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  3: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32)\n}\nLoss is  PerReplica:{\n  0: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  1: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  2: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  3: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32)\n}\nLoss is  PerReplica:{\n  0: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  1: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  2: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  3: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32)\n}\nLoss is  PerReplica:{\n  0: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  1: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  2: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  3: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32)\n}\nLoss is  PerReplica:{\n  0: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  1: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  2: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  3: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32)\n}\nLoss is  PerReplica:{\n  0: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  1: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  2: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  3: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32)\n}\nLoss is  PerReplica:{\n  0: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  1: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  2: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  3: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32)\n}\nLoss is  PerReplica:{\n  0: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  1: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  2: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  3: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32)\n}\nLoss is  PerReplica:{\n  0: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  1: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  2: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  3: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32)\n}\nLoss is  PerReplica:{\n  0: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  1: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  2: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  3: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32)\n}\nLoss is  PerReplica:{\n  0: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  1: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  2: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  3: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32)\n}\nLoss is  PerReplica:{\n  0: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  1: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  2: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  3: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32)\n}\nLoss is  PerReplica:{\n  0: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  1: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  2: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  3: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32)\n}\nLoss is  PerReplica:{\n  0: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  1: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  2: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  3: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32)\n}\nLoss is  PerReplica:{\n  0: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  1: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  2: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  3: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32)\n}\nLoss is  PerReplica:{\n  0: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  1: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  2: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  3: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32)\n}\nLoss is  PerReplica:{\n  0: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  1: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  2: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  3: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32)\n}\nLoss is  PerReplica:{\n  0: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  1: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  2: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  3: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32)\n}\nLoss is  PerReplica:{\n  0: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  1: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  2: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  3: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32)\n}\nLoss is  PerReplica:{\n  0: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  1: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  2: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  3: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32)\n}\nLoss is  PerReplica:{\n  0: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  1: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  2: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  3: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32)\n}\nLoss is  PerReplica:{\n  0: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  1: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  2: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  3: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32)\n}\nLoss is  PerReplica:{\n  0: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  1: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  2: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  3: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32)\n}\nLoss is  PerReplica:{\n  0: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  1: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  2: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  3: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32)\n}\nLoss is  PerReplica:{\n  0: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  1: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  2: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  3: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32)\n}\nLoss is  PerReplica:{\n  0: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  1: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  2: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  3: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32)\n}\nLoss is  PerReplica:{\n  0: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  1: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  2: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  3: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32)\n}\nLoss is  PerReplica:{\n  0: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  1: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  2: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  3: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32)\n}\nLoss is  PerReplica:{\n  0: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  1: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  2: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  3: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32)\n}\nLoss is  PerReplica:{\n  0: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  1: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  2: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  3: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32)\n}\nLoss is  PerReplica:{\n  0: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  1: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  2: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  3: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32)\n}\nLoss is  PerReplica:{\n  0: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  1: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  2: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  3: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32)\n}\nLoss is  PerReplica:{\n  0: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  1: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  2: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  3: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32)\n}\nLoss is  PerReplica:{\n  0: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  1: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  2: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  3: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32)\n}\nLoss is  PerReplica:{\n  0: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  1: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  2: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  3: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32)\n}\nLoss is  PerReplica:{\n  0: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  1: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  2: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  3: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32)\n}\nLoss is  PerReplica:{\n  0: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  1: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  2: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  3: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32)\n}\nLoss is  PerReplica:{\n  0: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  1: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  2: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  3: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32)\n}\nLoss is  PerReplica:{\n  0: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  1: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  2: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  3: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32)\n}\nLoss is  PerReplica:{\n  0: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  1: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  2: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  3: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32)\n}\nLoss is  PerReplica:{\n  0: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  1: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  2: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  3: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32)\n}\nLoss is  PerReplica:{\n  0: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  1: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  2: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  3: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32)\n}\nLoss is  PerReplica:{\n  0: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  1: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  2: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  3: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32)\n}\nLoss is  PerReplica:{\n  0: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  1: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  2: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  3: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32)\n}\nLoss is  PerReplica:{\n  0: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  1: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  2: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  3: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32)\n}\nLoss is  PerReplica:{\n  0: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  1: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  2: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  3: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32)\n}\nLoss is  PerReplica:{\n  0: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  1: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  2: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  3: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32)\n}\nLoss is  PerReplica:{\n  0: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  1: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  2: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  3: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32)\n}\nLoss is  PerReplica:{\n  0: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  1: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  2: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  3: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32)\n}\nLoss is  PerReplica:{\n  0: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  1: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  2: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32),\n  3: tf.Tensor(\n[[0.7]\n [0.7]\n [0.7]\n [0.7]], shape=(4, 1), dtype=float32)\n}",
      "@tf.function\ndef train_fn(iterator):\n  for _ in tf.range(steps_per_loop):\n    strategy.run(step_fn, args=(next(iterator),))",
      "# You can break the loop with `get_next_as_optional` by checking if the `Optional` contains a value\nglobal_batch_size = 4\nsteps_per_loop = 5\nstrategy = tf.distribute.MirroredStrategy()\n\ndataset = tf.data.Dataset.range(9).batch(global_batch_size)\ndistributed_iterator = iter(strategy.experimental_distribute_dataset(dataset))\n\n@tf.function\ndef train_fn(distributed_iterator):\n  for _ in tf.range(steps_per_loop):\n    optional_data = distributed_iterator.get_next_as_optional()\n    if not optional_data.has_value():\n      break\n    per_replica_results = strategy.run(lambda x: x, args=(optional_data.get_value(),))\n    tf.print(strategy.experimental_local_results(per_replica_results))\ntrain_fn(distributed_iterator)",
      "global_batch_size = 16\nepochs = 5\nsteps_per_epoch = 5\nmirrored_strategy = tf.distribute.MirroredStrategy()\n\ndataset = tf.data.Dataset.from_tensors(([1.], [1.])).repeat(100).batch(global_batch_size)\ndist_dataset = mirrored_strategy.experimental_distribute_dataset(dataset)\n\n@tf.function(input_signature=[dist_dataset.element_spec])\ndef train_step(per_replica_inputs):\n  def step_fn(inputs):\n    return 2 * inputs\n\n  return mirrored_strategy.run(step_fn, args=(per_replica_inputs,))\n\nfor _ in range(epochs):\n  iterator = iter(dist_dataset)\n  for _ in range(steps_per_epoch):\n    output = train_step(next(iterator))\n    tf.print(output)",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\n(PerReplica:{\n  0: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  1: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  2: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  3: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>\n},\n PerReplica:{\n  0: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  1: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  2: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  3: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>\n},\n PerReplica:{\n  0: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  1: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  2: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  3: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>\n},\n PerReplica:{\n  0: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  1: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  2: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  3: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>\n})\n(PerReplica:{\n  0: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  1: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  2: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  3: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>\n},\n PerReplica:{\n  0: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  1: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  2: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  3: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>\n},\n PerReplica:{\n  0: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  1: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  2: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  3: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>\n},\n PerReplica:{\n  0: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  1: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  2: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  3: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>\n})\n(PerReplica:{\n  0: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  1: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  2: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  3: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>\n},\n PerReplica:{\n  0: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  1: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  2: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  3: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>\n},\n PerReplica:{\n  0: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  1: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  2: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  3: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>\n},\n PerReplica:{\n  0: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  1: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  2: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  3: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>\n})\n(PerReplica:{\n  0: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  1: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  2: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  3: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>\n},\n PerReplica:{\n  0: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  1: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  2: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  3: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>\n},\n PerReplica:{\n  0: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  1: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  2: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  3: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>\n},\n PerReplica:{\n  0: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  1: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  2: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  3: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>\n})\n(PerReplica:{\n  0: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  1: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  2: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  3: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>\n},\n PerReplica:{\n  0: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  1: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  2: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  3: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>\n},\n PerReplica:{\n  0: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  1: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  2: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  3: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>\n},\n PerReplica:{\n  0: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  1: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  2: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  3: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>\n})\n(PerReplica:{\n  0: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  1: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  2: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  3: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>\n},\n PerReplica:{\n  0: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  1: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  2: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  3: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>\n},\n PerReplica:{\n  0: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  1: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  2: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  3: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>\n},\n PerReplica:{\n  0: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  1: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  2: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  3: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>\n})\n(PerReplica:{\n  0: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  1: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  2: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  3: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>\n},\n PerReplica:{\n  0: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  1: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  2: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  3: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>\n},\n PerReplica:{\n  0: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  1: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  2: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  3: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>\n},\n PerReplica:{\n  0: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  1: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  2: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  3: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>\n})\n(PerReplica:{\n  0: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  1: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  2: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  3: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>\n},\n PerReplica:{\n  0: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  1: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  2: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  3: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>\n},\n PerReplica:{\n  0: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  1: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  2: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  3: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>\n},\n PerReplica:{\n  0: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  1: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  2: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  3: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>\n})\n(PerReplica:{\n  0: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  1: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  2: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  3: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>\n},\n PerReplica:{\n  0: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  1: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  2: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  3: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>\n},\n PerReplica:{\n  0: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  1: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  2: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  3: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>\n},\n PerReplica:{\n  0: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  1: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  2: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  3: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>\n})\n(PerReplica:{\n  0: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  1: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  2: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  3: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>\n},\n PerReplica:{\n  0: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  1: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  2: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  3: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>\n},\n PerReplica:{\n  0: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  1: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  2: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  3: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>\n},\n PerReplica:{\n  0: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  1: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  2: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  3: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>\n})\n(PerReplica:{\n  0: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  1: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  2: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  3: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>\n},\n PerReplica:{\n  0: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  1: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  2: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  3: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>\n},\n PerReplica:{\n  0: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  1: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  2: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  3: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>\n},\n PerReplica:{\n  0: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  1: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  2: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  3: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>\n})\n(PerReplica:{\n  0: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  1: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  2: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  3: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>\n},\n PerReplica:{\n  0: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  1: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  2: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  3: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>\n},\n PerReplica:{\n  0: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  1: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  2: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  3: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>\n},\n PerReplica:{\n  0: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  1: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  2: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  3: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>\n})\n(PerReplica:{\n  0: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  1: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  2: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  3: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>\n},\n PerReplica:{\n  0: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  1: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  2: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  3: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>\n},\n PerReplica:{\n  0: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  1: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  2: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  3: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>\n},\n PerReplica:{\n  0: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  1: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  2: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  3: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>\n})\n(PerReplica:{\n  0: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  1: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  2: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  3: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>\n},\n PerReplica:{\n  0: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  1: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  2: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  3: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>\n},\n PerReplica:{\n  0: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  1: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  2: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  3: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>\n},\n PerReplica:{\n  0: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  1: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  2: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  3: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>\n})\n(PerReplica:{\n  0: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  1: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  2: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  3: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>\n},\n PerReplica:{\n  0: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  1: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  2: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  3: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>\n},\n PerReplica:{\n  0: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  1: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  2: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  3: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>\n},\n PerReplica:{\n  0: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  1: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  2: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  3: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>\n})\n(PerReplica:{\n  0: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  1: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  2: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  3: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>\n},\n PerReplica:{\n  0: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  1: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  2: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  3: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>\n},\n PerReplica:{\n  0: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  1: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  2: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  3: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>\n},\n PerReplica:{\n  0: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  1: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  2: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  3: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>\n})\n(PerReplica:{\n  0: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  1: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  2: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  3: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>\n},\n PerReplica:{\n  0: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  1: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  2: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  3: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>\n},\n PerReplica:{\n  0: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  1: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  2: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  3: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>\n},\n PerReplica:{\n  0: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  1: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  2: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  3: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>\n})\n(PerReplica:{\n  0: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  1: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  2: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  3: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>\n},\n PerReplica:{\n  0: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  1: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  2: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  3: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>\n},\n PerReplica:{\n  0: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  1: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  2: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  3: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>\n},\n PerReplica:{\n  0: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  1: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  2: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  3: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>\n})\n(PerReplica:{\n  0: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  1: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  2: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  3: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>\n},\n PerReplica:{\n  0: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  1: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  2: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  3: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>\n},\n PerReplica:{\n  0: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  1: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  2: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  3: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>\n},\n PerReplica:{\n  0: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  1: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  2: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  3: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>\n})\n(PerReplica:{\n  0: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  1: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  2: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  3: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>\n},\n PerReplica:{\n  0: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  1: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  2: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  3: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>\n},\n PerReplica:{\n  0: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  1: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  2: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  3: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>\n},\n PerReplica:{\n  0: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  1: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  2: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  3: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>\n})\n(PerReplica:{\n  0: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  1: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  2: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  3: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>\n},\n PerReplica:{\n  0: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  1: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  2: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  3: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>\n},\n PerReplica:{\n  0: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  1: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  2: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  3: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>\n},\n PerReplica:{\n  0: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  1: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  2: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  3: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>\n})\n(PerReplica:{\n  0: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  1: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  2: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  3: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>\n},\n PerReplica:{\n  0: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  1: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  2: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  3: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>\n},\n PerReplica:{\n  0: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  1: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  2: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  3: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>\n},\n PerReplica:{\n  0: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  1: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  2: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  3: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>\n})\n(PerReplica:{\n  0: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  1: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  2: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  3: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>\n},\n PerReplica:{\n  0: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  1: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  2: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  3: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>\n},\n PerReplica:{\n  0: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  1: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  2: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  3: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>\n},\n PerReplica:{\n  0: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  1: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  2: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  3: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>\n})\n(PerReplica:{\n  0: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  1: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  2: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  3: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>\n},\n PerReplica:{\n  0: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  1: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  2: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  3: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>\n},\n PerReplica:{\n  0: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  1: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  2: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  3: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>\n},\n PerReplica:{\n  0: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  1: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  2: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  3: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>\n})\n(PerReplica:{\n  0: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  1: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  2: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  3: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>\n},\n PerReplica:{\n  0: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  1: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  2: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  3: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>\n},\n PerReplica:{\n  0: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  1: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  2: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  3: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>\n},\n PerReplica:{\n  0: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  1: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  2: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>,\n  3: <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>\n})",
      "strategy = tf.distribute.MirroredStrategy()\nwith strategy.scope():\n  # Create the layer(s) under scope.\n  integer_preprocessing_layer = tf.keras.layers.IntegerLookup(vocabulary=FILE_PATH)\n  model = ...\n  model.compile(...)\ndataset = dataset.map(lambda x, y: (integer_preprocessing_layer(x), y))\nmodel.fit(dataset)",
      "strategy = tf.distribute.experimental.ParameterServerStrategy(\n    cluster_resolver,\n    variable_partitioner=variable_partitioner)\n\nwith strategy.scope():\n  preprocessing_layer = tf.keras.layers.StringLookup(vocabulary=FILE_PATH)\n  model = ...\n  model.compile(...)\n\ndef dataset_fn(input_context):\n  ...\n  dataset = dataset.map(preprocessing_layer)\n  ...\n  return dataset\n\ndataset_creator = tf.keras.utils.experimental.DatasetCreator(dataset_fn)\nmodel.fit(dataset_creator, epochs=5, steps_per_epoch=20, callbacks=callbacks)",
      "strategy = tf.distribute.MirroredStrategy()\nvocab = [\"a\", \"b\", \"c\", \"d\", \"f\"]\n\nwith strategy.scope():\n  # Create the layer(s) under scope.\n  layer = tf.keras.layers.StringLookup(vocabulary=vocab)\n\ndef dataset_fn(input_context):\n  # a tf.data.Dataset\n  dataset = tf.data.Dataset.from_tensor_slices([\"a\", \"c\", \"e\"]).repeat()\n\n  # Custom your batching, sharding, prefetching, etc.\n  global_batch_size = 4\n  batch_size = input_context.get_per_replica_batch_size(global_batch_size)\n  dataset = dataset.batch(batch_size)\n  dataset = dataset.shard(\n      input_context.num_input_pipelines,\n      input_context.input_pipeline_id)\n\n  # Apply the preprocessing layer(s) to the tf.data.Dataset\n  def preprocess_with_kpl(input):\n    return layer(input)\n\n  processed_ds = dataset.map(preprocess_with_kpl)\n  return processed_ds\n\ndistributed_dataset = strategy.distribute_datasets_from_function(dataset_fn)\n\n# Print out a few example batches.\ndistributed_dataset_iterator = iter(distributed_dataset)\nfor _ in range(3):\n  print(next(distributed_dataset_iterator))",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\nPerReplica:{\n  0: tf.Tensor([1], shape=(1,), dtype=int64),\n  1: tf.Tensor([3], shape=(1,), dtype=int64),\n  2: tf.Tensor([0], shape=(1,), dtype=int64),\n  3: tf.Tensor([1], shape=(1,), dtype=int64)\n}\nPerReplica:{\n  0: tf.Tensor([3], shape=(1,), dtype=int64),\n  1: tf.Tensor([0], shape=(1,), dtype=int64),\n  2: tf.Tensor([1], shape=(1,), dtype=int64),\n  3: tf.Tensor([3], shape=(1,), dtype=int64)\n}\nPerReplica:{\n  0: tf.Tensor([0], shape=(1,), dtype=int64),\n  1: tf.Tensor([1], shape=(1,), dtype=int64),\n  2: tf.Tensor([3], shape=(1,), dtype=int64),\n  3: tf.Tensor([0], shape=(1,), dtype=int64)\n}",
      "@tf.function\ndef per_worker_dataset_fn():\n  return strategy.distribute_datasets_from_function(dataset_fn)\n\nper_worker_dataset = coordinator.create_per_worker_dataset(per_worker_dataset_fn)\nper_worker_iterator = iter(per_worker_dataset)",
      "with strategy.scope():\n  # working_dir contains the tf.Transform output.\n  tf_transform_output = tft.TFTransformOutput(working_dir)\n  # Loading from working_dir to create a Keras layer for applying the tf.Transform output to data\n  tft_layer = tf_transform_output.transform_features_layer()\n  ...\n\ndef dataset_fn(input_context):\n  ...\n  dataset.map(tft_layer, num_parallel_calls=tf.data.AUTOTUNE)\n  ...\n  return dataset\n\ndistributed_dataset = strategy.distribute_datasets_from_function(dataset_fn)",
      "d = tf.data.Dataset.list_files(pattern, shuffle=False)\nd = d.shard(num_workers, worker_index)\nd = d.repeat(num_epochs)\nd = d.shuffle(shuffle_buffer_size)\nd = d.interleave(tf.data.TFRecordDataset,\n                 cycle_length=num_readers, block_length=1)\nd = d.map(parser_fn, num_parallel_calls=num_map_threads)",
      "mirrored_strategy = tf.distribute.MirroredStrategy()\ndataset_size = 24\nbatch_size = 6\ndataset = tf.data.Dataset.range(dataset_size).enumerate().batch(batch_size)\ndist_dataset = mirrored_strategy.experimental_distribute_dataset(dataset)\n\ndef predict(index, inputs):\n  outputs = 2 * inputs\n  return index, outputs\n\nresult = {}\nfor index, inputs in dist_dataset:\n  output_index, outputs = mirrored_strategy.run(predict, args=(index, inputs))\n  indices = list(mirrored_strategy.experimental_local_results(output_index))\n  rindices = []\n  for a in indices:\n    rindices.extend(a.numpy())\n  outputs = list(mirrored_strategy.experimental_local_results(outputs))\n  routputs = []\n  for a in outputs:\n    routputs.extend(a.numpy())\n  for i, value in zip(rindices, routputs):\n    result[i] = value\n\nprint(result)",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\nWARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `run` inside a tf.function to get the best performance.\nWARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `run` inside a tf.function to get the best performance.\nWARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `run` inside a tf.function to get the best performance.\nWARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `run` inside a tf.function to get the best performance.\n{0: 0, 1: 2, 2: 4, 3: 6, 4: 8, 5: 10, 6: 12, 7: 14, 8: 16, 9: 18, 10: 20, 11: 22, 12: 24, 13: 26, 14: 28, 15: 30, 16: 32, 17: 34, 18: 36, 19: 38, 20: 40, 21: 42, 22: 44, 23: 46}",
      "mirrored_strategy = tf.distribute.MirroredStrategy()\n\ndef value_fn(ctx):\n  return tf.constant(ctx.replica_id_in_sync_group)\n\ndistributed_values = mirrored_strategy.experimental_distribute_values_from_function(value_fn)\nfor _ in range(4):\n  result = mirrored_strategy.run(lambda x: x, args=(distributed_values,))\n  print(result)",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\nWARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `run` inside a tf.function to get the best performance.\nPerReplica:{\n  0: tf.Tensor(0, shape=(), dtype=int32),\n  1: tf.Tensor(1, shape=(), dtype=int32),\n  2: tf.Tensor(2, shape=(), dtype=int32),\n  3: tf.Tensor(3, shape=(), dtype=int32)\n}\nPerReplica:{\n  0: tf.Tensor(0, shape=(), dtype=int32),\n  1: tf.Tensor(1, shape=(), dtype=int32),\n  2: tf.Tensor(2, shape=(), dtype=int32),\n  3: tf.Tensor(3, shape=(), dtype=int32)\n}\nPerReplica:{\n  0: tf.Tensor(0, shape=(), dtype=int32),\n  1: tf.Tensor(1, shape=(), dtype=int32),\n  2: tf.Tensor(2, shape=(), dtype=int32),\n  3: tf.Tensor(3, shape=(), dtype=int32)\n}\nPerReplica:{\n  0: tf.Tensor(0, shape=(), dtype=int32),\n  1: tf.Tensor(1, shape=(), dtype=int32),\n  2: tf.Tensor(2, shape=(), dtype=int32),\n  3: tf.Tensor(3, shape=(), dtype=int32)\n}",
      "mirrored_strategy = tf.distribute.MirroredStrategy()\ndef input_gen():\n  while True:\n    yield np.random.rand(4)\n\n# use Dataset.from_generator\ndataset = tf.data.Dataset.from_generator(\n    input_gen, output_types=(tf.float32), output_shapes=tf.TensorShape([4]))\ndist_dataset = mirrored_strategy.experimental_distribute_dataset(dataset)\niterator = iter(dist_dataset)\nfor _ in range(4):\n  result = mirrored_strategy.run(lambda x: x, args=(next(iterator),))\n  print(result)",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\nPerReplica:{\n  0: tf.Tensor([0.28548497], shape=(1,), dtype=float32),\n  1: tf.Tensor([0.30408835], shape=(1,), dtype=float32),\n  2: tf.Tensor([0.5395937], shape=(1,), dtype=float32),\n  3: tf.Tensor([0.02621447], shape=(1,), dtype=float32)\n}\nPerReplica:{\n  0: tf.Tensor([0.05446655], shape=(1,), dtype=float32),\n  1: tf.Tensor([0.35898355], shape=(1,), dtype=float32),\n  2: tf.Tensor([0.97658086], shape=(1,), dtype=float32),\n  3: tf.Tensor([0.20252395], shape=(1,), dtype=float32)\n}\nPerReplica:{\n  0: tf.Tensor([0.5492602], shape=(1,), dtype=float32),\n  1: tf.Tensor([0.7265879], shape=(1,), dtype=float32),\n  2: tf.Tensor([0.76111615], shape=(1,), dtype=float32),\n  3: tf.Tensor([0.7545076], shape=(1,), dtype=float32)\n}\nPerReplica:{\n  0: tf.Tensor([0.7143968], shape=(1,), dtype=float32),\n  1: tf.Tensor([0.8691852], shape=(1,), dtype=float32),\n  2: tf.Tensor([0.28626952], shape=(1,), dtype=float32),\n  3: tf.Tensor([0.827588], shape=(1,), dtype=float32)\n}"
    ]
  },
  {
    "url": "https://www.tensorflow.org/tutorials/generative/data_compression",
    "title": "Learned data compression\n      \n\n\n      \n      Stay organized with collections\n    \n\n      \n      Save and categorize content based on your preferences.",
    "code_snippets": [
      "import matplotlib.pyplot as plt\nimport tensorflow as tf\nimport tensorflow_compression as tfc\nimport tensorflow_datasets as tfds",
      "def make_analysis_transform(latent_dims):\n  \"\"\"Creates the analysis (encoder) transform.\"\"\"\n  return tf.keras.Sequential([\n      tf.keras.layers.Conv2D(\n          20, 5, use_bias=True, strides=2, padding=\"same\",\n          activation=\"leaky_relu\", name=\"conv_1\"),\n      tf.keras.layers.Conv2D(\n          50, 5, use_bias=True, strides=2, padding=\"same\",\n          activation=\"leaky_relu\", name=\"conv_2\"),\n      tf.keras.layers.Flatten(),\n      tf.keras.layers.Dense(\n          500, use_bias=True, activation=\"leaky_relu\", name=\"fc_1\"),\n      tf.keras.layers.Dense(\n          latent_dims, use_bias=True, activation=None, name=\"fc_2\"),\n  ], name=\"analysis_transform\")",
      "def make_synthesis_transform():\n  \"\"\"Creates the synthesis (decoder) transform.\"\"\"\n  return tf.keras.Sequential([\n      tf.keras.layers.Dense(\n          500, use_bias=True, activation=\"leaky_relu\", name=\"fc_1\"),\n      tf.keras.layers.Dense(\n          2450, use_bias=True, activation=\"leaky_relu\", name=\"fc_2\"),\n      tf.keras.layers.Reshape((7, 7, 50)),\n      tf.keras.layers.Conv2DTranspose(\n          20, 5, use_bias=True, strides=2, padding=\"same\",\n          activation=\"leaky_relu\", name=\"conv_1\"),\n      tf.keras.layers.Conv2DTranspose(\n          1, 5, use_bias=True, strides=2, padding=\"same\",\n          activation=\"leaky_relu\", name=\"conv_2\"),\n  ], name=\"synthesis_transform\")",
      "class MNISTCompressionTrainer(tf.keras.Model):\n  \"\"\"Model that trains a compressor/decompressor for MNIST.\"\"\"\n\n  def __init__(self, latent_dims):\n    super().__init__()\n    self.analysis_transform = make_analysis_transform(latent_dims)\n    self.synthesis_transform = make_synthesis_transform()\n    self.prior_log_scales = tf.Variable(tf.zeros((latent_dims,)))\n\n  @property\n  def prior(self):\n    return tfc.NoisyLogistic(loc=0., scale=tf.exp(self.prior_log_scales))\n\n  def call(self, x, training):\n    \"\"\"Computes rate and distortion losses.\"\"\"\n    # Ensure inputs are floats in the range (0, 1).\n    x = tf.cast(x, self.compute_dtype) / 255.\n    x = tf.reshape(x, (-1, 28, 28, 1))\n\n    # Compute latent space representation y, perturb it and model its entropy,\n    # then compute the reconstructed pixel-level representation x_hat.\n    y = self.analysis_transform(x)\n    entropy_model = tfc.ContinuousBatchedEntropyModel(\n        self.prior, coding_rank=1, compression=False)\n    y_tilde, rate = entropy_model(y, training=training)\n    x_tilde = self.synthesis_transform(y_tilde)\n\n    # Average number of bits per MNIST digit.\n    rate = tf.reduce_mean(rate)\n\n    # Mean absolute difference across pixels.\n    distortion = tf.reduce_mean(abs(x - x_tilde))\n\n    return dict(rate=rate, distortion=distortion)",
      "(x, _), = validation_dataset.take(1)\n\nplt.imshow(tf.squeeze(x))\nprint(f\"Data type: {x.dtype}\")\nprint(f\"Shape: {x.shape}\")",
      "x = tf.cast(x, tf.float32) / 255.\nx = tf.reshape(x, (-1, 28, 28, 1))\ny = make_analysis_transform(10)(x)\n\nprint(\"y:\", y)",
      "y: tf.Tensor(\n[[-0.03515958 -0.04426444  0.02830836 -0.00623044  0.00728801 -0.01185333\n  -0.06493839  0.02908771 -0.05313966 -0.01150604]], shape=(1, 10), dtype=float32)",
      "y_tilde = y + tf.random.uniform(y.shape, -.5, .5)\n\nprint(\"y_tilde:\", y_tilde)",
      "y_tilde: tf.Tensor(\n[[ 0.23809135 -0.00584603  0.09636745  0.20950142  0.13149777 -0.12119483\n  -0.3411804   0.1945247   0.43893054  0.08348517]], shape=(1, 10), dtype=float32)",
      "prior = tfc.NoisyLogistic(loc=0., scale=tf.linspace(.01, 2., 10))\n\n_ = tf.linspace(-6., 6., 501)[:, None]\nplt.plot(_, prior.prob(_));",
      "rate: tf.Tensor([18.123432], shape=(1,), dtype=float32)\ny_tilde: tf.Tensor(\n[[ 0.2644348   0.00915181  0.23324046 -0.40824887  0.424091    0.3612876\n   0.02335981  0.2968393  -0.1207529   0.05890064]], shape=(1, 10), dtype=float32)",
      "x_tilde = make_synthesis_transform()(y_tilde)\n\n# Mean absolute difference across pixels.\ndistortion = tf.reduce_mean(abs(x - x_tilde))\nprint(\"distortion:\", distortion)\n\nx_tilde = tf.saturate_cast(x_tilde[0] * 255, tf.uint8)\nplt.imshow(tf.squeeze(x_tilde))\nprint(f\"Data type: {x_tilde.dtype}\")\nprint(f\"Shape: {x_tilde.shape}\")",
      "distortion: tf.Tensor(0.17096801, shape=(), dtype=float32)\nData type: <dtype: 'uint8'>\nShape: (28, 28, 1)",
      "rate:  tf.Tensor(20.296253, shape=(), dtype=float32)\ndistortion:  tf.Tensor(0.14659302, shape=(), dtype=float32)\n2024-08-16 06:46:35.159201: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.",
      "def pass_through_loss(_, x):\n  # Since rate and distortion are unsupervised, the loss doesn't need a target.\n  return x\n\ndef make_mnist_compression_trainer(lmbda, latent_dims=50):\n  trainer = MNISTCompressionTrainer(latent_dims)\n  trainer.compile(\n    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n    # Just pass through rate and distortion as losses/metrics.\n    loss=dict(rate=pass_through_loss, distortion=pass_through_loss),\n    metrics=dict(rate=pass_through_loss, distortion=pass_through_loss),\n    loss_weights=dict(rate=1., distortion=lmbda),\n  )\n  return trainer",
      "def add_rd_targets(image, label):\n  # Training is unsupervised, so labels aren't necessary here. However, we\n  # need to add \"dummy\" targets for rate and distortion.\n  return image, dict(rate=0., distortion=0.)\n\ndef train_mnist_model(lmbda):\n  trainer = make_mnist_compression_trainer(lmbda)\n  trainer.fit(\n      training_dataset.map(add_rd_targets).batch(128).prefetch(8),\n      epochs=15,\n      validation_data=validation_dataset.map(add_rd_targets).batch(128).cache(),\n      validation_freq=1,\n      verbose=1,\n  )\n  return trainer\n\ntrainer = train_mnist_model(lmbda=2000)",
      "Epoch 1/15\n468/469 [============================>.] - ETA: 0s - loss: 216.8709 - distortion_loss: 0.0584 - rate_loss: 100.1461 - distortion_pass_through_loss: 0.0584 - rate_pass_through_loss: 100.1461\nWARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n469/469 [==============================] - 13s 22ms/step - loss: 216.7950 - distortion_loss: 0.0583 - rate_loss: 100.1332 - distortion_pass_through_loss: 0.0583 - rate_pass_through_loss: 100.1289 - val_loss: 176.3082 - val_distortion_loss: 0.0421 - val_rate_loss: 92.0145 - val_distortion_pass_through_loss: 0.0421 - val_rate_pass_through_loss: 92.0216\nEpoch 2/15\n469/469 [==============================] - 10s 20ms/step - loss: 165.3472 - distortion_loss: 0.0407 - rate_loss: 83.9473 - distortion_pass_through_loss: 0.0407 - rate_pass_through_loss: 83.9427 - val_loss: 155.3631 - val_distortion_loss: 0.0397 - val_rate_loss: 75.9290 - val_distortion_pass_through_loss: 0.0397 - val_rate_pass_through_loss: 75.9372\nEpoch 3/15\n469/469 [==============================] - 9s 20ms/step - loss: 150.4180 - distortion_loss: 0.0396 - rate_loss: 71.1453 - distortion_pass_through_loss: 0.0396 - rate_pass_through_loss: 71.1426 - val_loss: 144.3111 - val_distortion_loss: 0.0401 - val_rate_loss: 64.0771 - val_distortion_pass_through_loss: 0.0401 - val_rate_pass_through_loss: 64.0679\nEpoch 4/15\n469/469 [==============================] - 9s 20ms/step - loss: 142.1455 - distortion_loss: 0.0395 - rate_loss: 63.1692 - distortion_pass_through_loss: 0.0395 - rate_pass_through_loss: 63.1674 - val_loss: 136.5376 - val_distortion_loss: 0.0404 - val_rate_loss: 55.8161 - val_distortion_pass_through_loss: 0.0404 - val_rate_pass_through_loss: 55.8286\nEpoch 5/15\n469/469 [==============================] - 10s 20ms/step - loss: 136.9274 - distortion_loss: 0.0393 - rate_loss: 58.3971 - distortion_pass_through_loss: 0.0393 - rate_pass_through_loss: 58.3959 - val_loss: 131.0551 - val_distortion_loss: 0.0407 - val_rate_loss: 49.5784 - val_distortion_pass_through_loss: 0.0407 - val_rate_pass_through_loss: 49.5649\nEpoch 6/15\n469/469 [==============================] - 10s 20ms/step - loss: 133.3907 - distortion_loss: 0.0390 - rate_loss: 55.3780 - distortion_pass_through_loss: 0.0390 - rate_pass_through_loss: 55.3764 - val_loss: 128.4564 - val_distortion_loss: 0.0417 - val_rate_loss: 45.1091 - val_distortion_pass_through_loss: 0.0417 - val_rate_pass_through_loss: 45.0806\nEpoch 7/15\n469/469 [==============================] - 10s 20ms/step - loss: 130.3171 - distortion_loss: 0.0385 - rate_loss: 53.2658 - distortion_pass_through_loss: 0.0385 - rate_pass_through_loss: 53.2643 - val_loss: 124.0318 - val_distortion_loss: 0.0405 - val_rate_loss: 42.9713 - val_distortion_pass_through_loss: 0.0405 - val_rate_pass_through_loss: 42.9749\nEpoch 8/15\n469/469 [==============================] - 10s 20ms/step - loss: 127.8933 - distortion_loss: 0.0381 - rate_loss: 51.6005 - distortion_pass_through_loss: 0.0381 - rate_pass_through_loss: 51.5994 - val_loss: 123.1978 - val_distortion_loss: 0.0405 - val_rate_loss: 42.1649 - val_distortion_pass_through_loss: 0.0405 - val_rate_pass_through_loss: 42.1638\nEpoch 9/15\n469/469 [==============================] - 10s 20ms/step - loss: 125.6461 - distortion_loss: 0.0377 - rate_loss: 50.1962 - distortion_pass_through_loss: 0.0377 - rate_pass_through_loss: 50.1950 - val_loss: 120.4046 - val_distortion_loss: 0.0394 - val_rate_loss: 41.6417 - val_distortion_pass_through_loss: 0.0394 - val_rate_pass_through_loss: 41.6490\nEpoch 10/15\n469/469 [==============================] - 9s 20ms/step - loss: 123.5309 - distortion_loss: 0.0373 - rate_loss: 48.8519 - distortion_pass_through_loss: 0.0373 - rate_pass_through_loss: 48.8512 - val_loss: 117.5625 - val_distortion_loss: 0.0381 - val_rate_loss: 41.4515 - val_distortion_pass_through_loss: 0.0381 - val_rate_pass_through_loss: 41.4562\nEpoch 11/15\n469/469 [==============================] - 10s 21ms/step - loss: 121.6021 - distortion_loss: 0.0370 - rate_loss: 47.7015 - distortion_pass_through_loss: 0.0369 - rate_pass_through_loss: 47.7005 - val_loss: 115.9406 - val_distortion_loss: 0.0374 - val_rate_loss: 41.1221 - val_distortion_pass_through_loss: 0.0374 - val_rate_pass_through_loss: 41.1329\nEpoch 12/15\n469/469 [==============================] - 10s 20ms/step - loss: 119.9221 - distortion_loss: 0.0366 - rate_loss: 46.7678 - distortion_pass_through_loss: 0.0366 - rate_pass_through_loss: 46.7669 - val_loss: 115.2949 - val_distortion_loss: 0.0370 - val_rate_loss: 41.2657 - val_distortion_pass_through_loss: 0.0370 - val_rate_pass_through_loss: 41.2901\nEpoch 13/15\n469/469 [==============================] - 9s 20ms/step - loss: 118.3295 - distortion_loss: 0.0362 - rate_loss: 46.0043 - distortion_pass_through_loss: 0.0362 - rate_pass_through_loss: 46.0035 - val_loss: 114.3115 - val_distortion_loss: 0.0365 - val_rate_loss: 41.3752 - val_distortion_pass_through_loss: 0.0365 - val_rate_pass_through_loss: 41.3830\nEpoch 14/15\n469/469 [==============================] - 9s 20ms/step - loss: 117.2416 - distortion_loss: 0.0359 - rate_loss: 45.3939 - distortion_pass_through_loss: 0.0359 - rate_pass_through_loss: 45.3932 - val_loss: 113.1450 - val_distortion_loss: 0.0357 - val_rate_loss: 41.7618 - val_distortion_pass_through_loss: 0.0357 - val_rate_pass_through_loss: 41.7776\nEpoch 15/15\n469/469 [==============================] - 9s 20ms/step - loss: 116.2503 - distortion_loss: 0.0357 - rate_loss: 44.9476 - distortion_pass_through_loss: 0.0356 - rate_pass_through_loss: 44.9467 - val_loss: 112.1821 - val_distortion_loss: 0.0355 - val_rate_loss: 41.1623 - val_distortion_pass_through_loss: 0.0355 - val_rate_pass_through_loss: 41.1731",
      "class MNISTCompressor(tf.keras.Model):\n  \"\"\"Compresses MNIST images to strings.\"\"\"\n\n  def __init__(self, analysis_transform, entropy_model):\n    super().__init__()\n    self.analysis_transform = analysis_transform\n    self.entropy_model = entropy_model\n\n  def call(self, x):\n    # Ensure inputs are floats in the range (0, 1).\n    x = tf.cast(x, self.compute_dtype) / 255.\n    y = self.analysis_transform(x)\n    # Also return the exact information content of each digit.\n    _, bits = self.entropy_model(y, training=False)\n    return self.entropy_model.compress(y), bits",
      "class MNISTDecompressor(tf.keras.Model):\n  \"\"\"Decompresses MNIST images from strings.\"\"\"\n\n  def __init__(self, entropy_model, synthesis_transform):\n    super().__init__()\n    self.entropy_model = entropy_model\n    self.synthesis_transform = synthesis_transform\n\n  def call(self, string):\n    y_hat = self.entropy_model.decompress(string, ())\n    x_hat = self.synthesis_transform(y_hat)\n    # Scale and cast back to 8-bit integer.\n    return tf.saturate_cast(tf.round(x_hat * 255.), tf.uint8)",
      "def make_mnist_codec(trainer, **kwargs):\n  # The entropy model must be created with `compression=True` and the same\n  # instance must be shared between compressor and decompressor.\n  entropy_model = tfc.ContinuousBatchedEntropyModel(\n      trainer.prior, coding_rank=1, compression=True, **kwargs)\n  compressor = MNISTCompressor(trainer.analysis_transform, entropy_model)\n  decompressor = MNISTDecompressor(entropy_model, trainer.synthesis_transform)\n  return compressor, decompressor\n\ncompressor, decompressor = make_mnist_codec(trainer)",
      "def display_digits(originals, strings, entropies, reconstructions):\n  \"\"\"Visualizes 16 digits together with their reconstructions.\"\"\"\n  fig, axes = plt.subplots(4, 4, sharex=True, sharey=True, figsize=(12.5, 5))\n  axes = axes.ravel()\n  for i in range(len(axes)):\n    image = tf.concat([\n        tf.squeeze(originals[i]),\n        tf.zeros((28, 14), tf.uint8),\n        tf.squeeze(reconstructions[i]),\n    ], 1)\n    axes[i].imshow(image)\n    axes[i].text(\n        .5, .5, f\"\u2192 0x{strings[i].numpy().hex()} \u2192\\n{entropies[i]:0.2f} bits\",\n        ha=\"center\", va=\"top\", color=\"white\", fontsize=\"small\",\n        transform=axes[i].transAxes)\n    axes[i].axis(\"off\")\n  plt.subplots_adjust(wspace=0, hspace=0, left=0, right=1, bottom=0, top=1)",
      "def train_and_visualize_model(lmbda):\n  trainer = train_mnist_model(lmbda=lmbda)\n  compressor, decompressor = make_mnist_codec(trainer)\n  strings, entropies = compressor(originals)\n  reconstructions = decompressor(strings)\n  display_digits(originals, strings, entropies, reconstructions)\n\ntrain_and_visualize_model(lmbda=500)",
      "Epoch 1/15\n469/469 [==============================] - ETA: 0s - loss: 127.7013 - distortion_loss: 0.0703 - rate_loss: 92.5737 - distortion_pass_through_loss: 0.0702 - rate_pass_through_loss: 92.5675\nWARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n469/469 [==============================] - 12s 21ms/step - loss: 127.7013 - distortion_loss: 0.0703 - rate_loss: 92.5737 - distortion_pass_through_loss: 0.0702 - rate_pass_through_loss: 92.5675 - val_loss: 107.4871 - val_distortion_loss: 0.0552 - val_rate_loss: 79.8772 - val_distortion_pass_through_loss: 0.0552 - val_rate_pass_through_loss: 79.8765\nEpoch 2/15\n469/469 [==============================] - 9s 20ms/step - loss: 97.3885 - distortion_loss: 0.0541 - rate_loss: 70.3172 - distortion_pass_through_loss: 0.0541 - rate_pass_through_loss: 70.3120 - val_loss: 86.6660 - val_distortion_loss: 0.0612 - val_rate_loss: 56.0898 - val_distortion_pass_through_loss: 0.0611 - val_rate_pass_through_loss: 56.0988\nEpoch 3/15\n469/469 [==============================] - 9s 20ms/step - loss: 81.3244 - distortion_loss: 0.0564 - rate_loss: 53.1456 - distortion_pass_through_loss: 0.0564 - rate_pass_through_loss: 53.1418 - val_loss: 71.7392 - val_distortion_loss: 0.0673 - val_rate_loss: 38.0721 - val_distortion_pass_through_loss: 0.0673 - val_rate_pass_through_loss: 38.0759\nEpoch 4/15\n469/469 [==============================] - 9s 20ms/step - loss: 71.6396 - distortion_loss: 0.0595 - rate_loss: 41.9071 - distortion_pass_through_loss: 0.0595 - rate_pass_through_loss: 41.9044 - val_loss: 63.2747 - val_distortion_loss: 0.0767 - val_rate_loss: 24.9434 - val_distortion_pass_through_loss: 0.0766 - val_rate_pass_through_loss: 24.9559\nEpoch 5/15\n469/469 [==============================] - 9s 19ms/step - loss: 65.9924 - distortion_loss: 0.0622 - rate_loss: 34.8828 - distortion_pass_through_loss: 0.0622 - rate_pass_through_loss: 34.8815 - val_loss: 58.0754 - val_distortion_loss: 0.0797 - val_rate_loss: 18.2346 - val_distortion_pass_through_loss: 0.0797 - val_rate_pass_through_loss: 18.2361\nEpoch 6/15\n469/469 [==============================] - 9s 20ms/step - loss: 62.4760 - distortion_loss: 0.0643 - rate_loss: 30.3295 - distortion_pass_through_loss: 0.0643 - rate_pass_through_loss: 30.3285 - val_loss: 54.3282 - val_distortion_loss: 0.0804 - val_rate_loss: 14.1167 - val_distortion_pass_through_loss: 0.0805 - val_rate_pass_through_loss: 14.0998\nEpoch 7/15\n469/469 [==============================] - 9s 20ms/step - loss: 59.9234 - distortion_loss: 0.0655 - rate_loss: 27.1496 - distortion_pass_through_loss: 0.0655 - rate_pass_through_loss: 27.1490 - val_loss: 51.5607 - val_distortion_loss: 0.0785 - val_rate_loss: 12.3280 - val_distortion_pass_through_loss: 0.0785 - val_rate_pass_through_loss: 12.3260\nEpoch 8/15\n469/469 [==============================] - 9s 20ms/step - loss: 57.8360 - distortion_loss: 0.0662 - rate_loss: 24.7523 - distortion_pass_through_loss: 0.0662 - rate_pass_through_loss: 24.7515 - val_loss: 49.1663 - val_distortion_loss: 0.0739 - val_rate_loss: 12.2225 - val_distortion_pass_through_loss: 0.0739 - val_rate_pass_through_loss: 12.2252\nEpoch 9/15\n469/469 [==============================] - 9s 20ms/step - loss: 55.8557 - distortion_loss: 0.0661 - rate_loss: 22.8270 - distortion_pass_through_loss: 0.0661 - rate_pass_through_loss: 22.8269 - val_loss: 47.8735 - val_distortion_loss: 0.0703 - val_rate_loss: 12.7483 - val_distortion_pass_through_loss: 0.0703 - val_rate_pass_through_loss: 12.7435\nEpoch 10/15\n469/469 [==============================] - 9s 20ms/step - loss: 53.8931 - distortion_loss: 0.0653 - rate_loss: 21.2611 - distortion_pass_through_loss: 0.0653 - rate_pass_through_loss: 21.2607 - val_loss: 46.9746 - val_distortion_loss: 0.0678 - val_rate_loss: 13.0869 - val_distortion_pass_through_loss: 0.0677 - val_rate_pass_through_loss: 13.0893\nEpoch 11/15\n469/469 [==============================] - 9s 20ms/step - loss: 52.2135 - distortion_loss: 0.0645 - rate_loss: 19.9613 - distortion_pass_through_loss: 0.0645 - rate_pass_through_loss: 19.9613 - val_loss: 46.1504 - val_distortion_loss: 0.0649 - val_rate_loss: 13.6771 - val_distortion_pass_through_loss: 0.0649 - val_rate_pass_through_loss: 13.6784\nEpoch 12/15\n469/469 [==============================] - 9s 20ms/step - loss: 50.7047 - distortion_loss: 0.0635 - rate_loss: 18.9639 - distortion_pass_through_loss: 0.0635 - rate_pass_through_loss: 18.9636 - val_loss: 45.6434 - val_distortion_loss: 0.0636 - val_rate_loss: 13.8268 - val_distortion_pass_through_loss: 0.0636 - val_rate_pass_through_loss: 13.8222\nEpoch 13/15\n469/469 [==============================] - 9s 20ms/step - loss: 49.5375 - distortion_loss: 0.0627 - rate_loss: 18.1879 - distortion_pass_through_loss: 0.0627 - rate_pass_through_loss: 18.1875 - val_loss: 45.3913 - val_distortion_loss: 0.0623 - val_rate_loss: 14.2331 - val_distortion_pass_through_loss: 0.0623 - val_rate_pass_through_loss: 14.2288\nEpoch 14/15\n469/469 [==============================] - 9s 19ms/step - loss: 48.5012 - distortion_loss: 0.0617 - rate_loss: 17.6528 - distortion_pass_through_loss: 0.0617 - rate_pass_through_loss: 17.6525 - val_loss: 45.0980 - val_distortion_loss: 0.0618 - val_rate_loss: 14.1822 - val_distortion_pass_through_loss: 0.0619 - val_rate_pass_through_loss: 14.1783\nEpoch 15/15\n469/469 [==============================] - 9s 20ms/step - loss: 47.7803 - distortion_loss: 0.0611 - rate_loss: 17.2333 - distortion_pass_through_loss: 0.0611 - rate_pass_through_loss: 17.2331 - val_loss: 44.7918 - val_distortion_loss: 0.0601 - val_rate_loss: 14.7223 - val_distortion_pass_through_loss: 0.0602 - val_rate_pass_through_loss: 14.7139",
      "Epoch 1/15\n469/469 [==============================] - ETA: 0s - loss: 113.5871 - distortion_loss: 0.0750 - rate_loss: 91.0911 - distortion_pass_through_loss: 0.0750 - rate_pass_through_loss: 91.0843\nWARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n469/469 [==============================] - 11s 20ms/step - loss: 113.5871 - distortion_loss: 0.0750 - rate_loss: 91.0911 - distortion_pass_through_loss: 0.0750 - rate_pass_through_loss: 91.0843 - val_loss: 96.3590 - val_distortion_loss: 0.0666 - val_rate_loss: 76.3773 - val_distortion_pass_through_loss: 0.0666 - val_rate_pass_through_loss: 76.3787\nEpoch 2/15\n469/469 [==============================] - 9s 20ms/step - loss: 85.8210 - distortion_loss: 0.0610 - rate_loss: 67.5072 - distortion_pass_through_loss: 0.0610 - rate_pass_through_loss: 67.5020 - val_loss: 73.8655 - val_distortion_loss: 0.0754 - val_rate_loss: 51.2544 - val_distortion_pass_through_loss: 0.0754 - val_rate_pass_through_loss: 51.2604\nEpoch 3/15\n469/469 [==============================] - 9s 20ms/step - loss: 68.8192 - distortion_loss: 0.0646 - rate_loss: 49.4383 - distortion_pass_through_loss: 0.0646 - rate_pass_through_loss: 49.4348 - val_loss: 58.8346 - val_distortion_loss: 0.0914 - val_rate_loss: 31.4027 - val_distortion_pass_through_loss: 0.0914 - val_rate_pass_through_loss: 31.4040\nEpoch 4/15\n469/469 [==============================] - 9s 20ms/step - loss: 58.2441 - distortion_loss: 0.0693 - rate_loss: 37.4659 - distortion_pass_through_loss: 0.0693 - rate_pass_through_loss: 37.4635 - val_loss: 48.4599 - val_distortion_loss: 0.0967 - val_rate_loss: 19.4409 - val_distortion_pass_through_loss: 0.0967 - val_rate_pass_through_loss: 19.4465\nEpoch 5/15\n469/469 [==============================] - 9s 20ms/step - loss: 51.9693 - distortion_loss: 0.0735 - rate_loss: 29.9285 - distortion_pass_through_loss: 0.0735 - rate_pass_through_loss: 29.9274 - val_loss: 42.3357 - val_distortion_loss: 0.1013 - val_rate_loss: 11.9475 - val_distortion_pass_through_loss: 0.1013 - val_rate_pass_through_loss: 11.9459\nEpoch 6/15\n469/469 [==============================] - 9s 20ms/step - loss: 48.0556 - distortion_loss: 0.0769 - rate_loss: 24.9786 - distortion_pass_through_loss: 0.0769 - rate_pass_through_loss: 24.9773 - val_loss: 39.0373 - val_distortion_loss: 0.1047 - val_rate_loss: 7.6218 - val_distortion_pass_through_loss: 0.1047 - val_rate_pass_through_loss: 7.6315\nEpoch 7/15\n469/469 [==============================] - 9s 19ms/step - loss: 45.3187 - distortion_loss: 0.0795 - rate_loss: 21.4538 - distortion_pass_through_loss: 0.0795 - rate_pass_through_loss: 21.4531 - val_loss: 36.2421 - val_distortion_loss: 0.1005 - val_rate_loss: 6.1012 - val_distortion_pass_through_loss: 0.1005 - val_rate_pass_through_loss: 6.1087\nEpoch 8/15\n469/469 [==============================] - 9s 20ms/step - loss: 43.1370 - distortion_loss: 0.0811 - rate_loss: 18.7982 - distortion_pass_through_loss: 0.0811 - rate_pass_through_loss: 18.7976 - val_loss: 34.1957 - val_distortion_loss: 0.0933 - val_rate_loss: 6.2151 - val_distortion_pass_through_loss: 0.0934 - val_rate_pass_through_loss: 6.2087\nEpoch 9/15\n469/469 [==============================] - 9s 20ms/step - loss: 41.1743 - distortion_loss: 0.0814 - rate_loss: 16.7616 - distortion_pass_through_loss: 0.0814 - rate_pass_through_loss: 16.7611 - val_loss: 33.2329 - val_distortion_loss: 0.0865 - val_rate_loss: 7.2854 - val_distortion_pass_through_loss: 0.0865 - val_rate_pass_through_loss: 7.2807\nEpoch 10/15\n469/469 [==============================] - 9s 20ms/step - loss: 39.4184 - distortion_loss: 0.0805 - rate_loss: 15.2631 - distortion_pass_through_loss: 0.0805 - rate_pass_through_loss: 15.2625 - val_loss: 32.7048 - val_distortion_loss: 0.0833 - val_rate_loss: 7.7035 - val_distortion_pass_through_loss: 0.0834 - val_rate_pass_through_loss: 7.7043\nEpoch 11/15\n469/469 [==============================] - 9s 20ms/step - loss: 37.9319 - distortion_loss: 0.0793 - rate_loss: 14.1282 - distortion_pass_through_loss: 0.0793 - rate_pass_through_loss: 14.1282 - val_loss: 32.4213 - val_distortion_loss: 0.0820 - val_rate_loss: 7.8242 - val_distortion_pass_through_loss: 0.0820 - val_rate_pass_through_loss: 7.8276\nEpoch 12/15\n469/469 [==============================] - 9s 20ms/step - loss: 36.7734 - distortion_loss: 0.0783 - rate_loss: 13.2686 - distortion_pass_through_loss: 0.0783 - rate_pass_through_loss: 13.2680 - val_loss: 32.1318 - val_distortion_loss: 0.0808 - val_rate_loss: 7.8993 - val_distortion_pass_through_loss: 0.0808 - val_rate_pass_through_loss: 7.9013\nEpoch 13/15\n469/469 [==============================] - 9s 20ms/step - loss: 35.7413 - distortion_loss: 0.0772 - rate_loss: 12.5748 - distortion_pass_through_loss: 0.0772 - rate_pass_through_loss: 12.5750 - val_loss: 31.8718 - val_distortion_loss: 0.0788 - val_rate_loss: 8.2290 - val_distortion_pass_through_loss: 0.0789 - val_rate_pass_through_loss: 8.2357\nEpoch 14/15\n469/469 [==============================] - 9s 20ms/step - loss: 34.9921 - distortion_loss: 0.0767 - rate_loss: 11.9826 - distortion_pass_through_loss: 0.0767 - rate_pass_through_loss: 11.9821 - val_loss: 31.6609 - val_distortion_loss: 0.0783 - val_rate_loss: 8.1636 - val_distortion_pass_through_loss: 0.0784 - val_rate_pass_through_loss: 8.1657\nEpoch 15/15\n469/469 [==============================] - 9s 20ms/step - loss: 34.3623 - distortion_loss: 0.0760 - rate_loss: 11.5580 - distortion_pass_through_loss: 0.0760 - rate_pass_through_loss: 11.5578 - val_loss: 31.5164 - val_distortion_loss: 0.0762 - val_rate_loss: 8.6582 - val_distortion_pass_through_loss: 0.0762 - val_rate_pass_through_loss: 8.6660",
      "import os\n\nstrings = tf.constant([os.urandom(8) for _ in range(16)])\nsamples = decompressor(strings)\n\nfig, axes = plt.subplots(4, 4, sharex=True, sharey=True, figsize=(5, 5))\naxes = axes.ravel()\nfor i in range(len(axes)):\n  axes[i].imshow(tf.squeeze(samples[i]))\n  axes[i].axis(\"off\")\nplt.subplots_adjust(wspace=0, hspace=0, left=0, right=1, bottom=0, top=1)"
    ]
  },
  {
    "url": "https://www.tensorflow.org/tutorials/images/transfer_learning",
    "title": "Transfer learning and fine-tuning\n      \n\n\n      \n      Stay organized with collections\n    \n\n      \n      Save and categorize content based on your preferences.",
    "code_snippets": [
      "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport tensorflow as tf",
      "_URL = 'https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip'\npath_to_zip = tf.keras.utils.get_file('cats_and_dogs.zip', origin=_URL, extract=True)\nPATH = os.path.join(os.path.dirname(path_to_zip), 'cats_and_dogs_filtered')\n\ntrain_dir = os.path.join(PATH, 'train')\nvalidation_dir = os.path.join(PATH, 'validation')\n\nBATCH_SIZE = 32\nIMG_SIZE = (160, 160)\n\ntrain_dataset = tf.keras.utils.image_dataset_from_directory(train_dir,\n                                                            shuffle=True,\n                                                            batch_size=BATCH_SIZE,\n                                                            image_size=IMG_SIZE)",
      "validation_dataset = tf.keras.utils.image_dataset_from_directory(validation_dir,\n                                                                 shuffle=True,\n                                                                 batch_size=BATCH_SIZE,\n                                                                 image_size=IMG_SIZE)",
      "class_names = train_dataset.class_names\n\nplt.figure(figsize=(10, 10))\nfor images, labels in train_dataset.take(1):\n  for i in range(9):\n    ax = plt.subplot(3, 3, i + 1)\n    plt.imshow(images[i].numpy().astype(\"uint8\"))\n    plt.title(class_names[labels[i]])\n    plt.axis(\"off\")",
      "val_batches = tf.data.experimental.cardinality(validation_dataset)\ntest_dataset = validation_dataset.take(val_batches // 5)\nvalidation_dataset = validation_dataset.skip(val_batches // 5)",
      "print('Number of validation batches: %d' % tf.data.experimental.cardinality(validation_dataset))\nprint('Number of test batches: %d' % tf.data.experimental.cardinality(test_dataset))",
      "AUTOTUNE = tf.data.AUTOTUNE\n\ntrain_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)\nvalidation_dataset = validation_dataset.prefetch(buffer_size=AUTOTUNE)\ntest_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE)",
      "data_augmentation = tf.keras.Sequential([\n  tf.keras.layers.RandomFlip('horizontal'),\n  tf.keras.layers.RandomRotation(0.2),\n])",
      "for image, _ in train_dataset.take(1):\n  plt.figure(figsize=(10, 10))\n  first_image = image[0]\n  for i in range(9):\n    ax = plt.subplot(3, 3, i + 1)\n    augmented_image = data_augmentation(tf.expand_dims(first_image, 0))\n    plt.imshow(augmented_image[0] / 255)\n    plt.axis('off')",
      "preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input",
      "rescale = tf.keras.layers.Rescaling(1./127.5, offset=-1)",
      "# Create the base model from the pre-trained model MobileNet V2\nIMG_SHAPE = IMG_SIZE + (3,)\nbase_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n                                               include_top=False,\n                                               weights='imagenet')",
      "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\nfeature_batch_average = global_average_layer(feature_batch)\nprint(feature_batch_average.shape)",
      "prediction_layer = tf.keras.layers.Dense(1, activation='sigmoid')\nprediction_batch = prediction_layer(feature_batch_average)\nprint(prediction_batch.shape)",
      "inputs = tf.keras.Input(shape=(160, 160, 3))\nx = data_augmentation(inputs)\nx = preprocess_input(x)\nx = base_model(x, training=False)\nx = global_average_layer(x)\nx = tf.keras.layers.Dropout(0.2)(x)\noutputs = prediction_layer(x)\nmodel = tf.keras.Model(inputs, outputs)",
      "tf.keras.utils.plot_model(model, show_shapes=True)",
      "base_learning_rate = 0.0001\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=base_learning_rate),\n              loss=tf.keras.losses.BinaryCrossentropy(),\n              metrics=[tf.keras.metrics.BinaryAccuracy(threshold=0.5, name='accuracy')])",
      "model.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n              optimizer = tf.keras.optimizers.RMSprop(learning_rate=base_learning_rate/10),\n              metrics=[tf.keras.metrics.BinaryAccuracy(threshold=0.5, name='accuracy')])",
      "# Retrieve a batch of images from the test set\nimage_batch, label_batch = test_dataset.as_numpy_iterator().next()\npredictions = model.predict_on_batch(image_batch).flatten()\npredictions = tf.where(predictions < 0.5, 0, 1)\n\nprint('Predictions:\\n', predictions.numpy())\nprint('Labels:\\n', label_batch)\n\nplt.figure(figsize=(10, 10))\nfor i in range(9):\n  ax = plt.subplot(3, 3, i + 1)\n  plt.imshow(image_batch[i].astype(\"uint8\"))\n  plt.title(class_names[predictions[i]])\n  plt.axis(\"off\")"
    ]
  },
  {
    "url": "https://www.tensorflow.org/tutorials/load_data/tfrecord",
    "title": "TFRecord and tf.train.Example\n      \n\n\n      \n      Stay organized with collections\n    \n\n      \n      Save and categorize content based on your preferences.",
    "code_snippets": [
      "import tensorflow as tf\n\nimport numpy as np\nimport IPython.display as display",
      "# The following functions can be used to convert a value to a type compatible\n# with tf.train.Example.\n\ndef _bytes_feature(value):\n  \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n  if isinstance(value, type(tf.constant(0))):\n    value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n\ndef _float_feature(value):\n  \"\"\"Returns a float_list from a float / double.\"\"\"\n  return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n\ndef _int64_feature(value):\n  \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n  return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))",
      "@tf.py_function(Tout=tf.string)\ndef serialize_example(feature0, feature1, feature2, feature3):\n  \"\"\"\n  Creates a tf.train.Example message ready to be written to a file.\n  \"\"\"\n  # Create a dictionary mapping the feature name to the tf.train.Example-compatible\n  # data type.\n  feature = {\n      'feature0': _int64_feature(feature0),\n      'feature1': _int64_feature(feature1),\n      'feature2': _bytes_feature(feature2),\n      'feature3': _float_feature(feature3),\n  }\n\n  # Create a Features message using tf.train.Example.\n\n  example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n  return example_proto.SerializeToString()",
      "<tf.Tensor: shape=(), dtype=string, numpy=b'\\nR\\n\\x11\\n\\x08feature1\\x12\\x05\\x1a\\x03\\n\\x01\\x04\\n\\x14\\n\\x08feature3\\x12\\x08\\x12\\x06\\n\\x04[\\xd3|?\\n\\x11\\n\\x08feature0\\x12\\x05\\x1a\\x03\\n\\x01\\x00\\n\\x14\\n\\x08feature2\\x12\\x08\\n\\x06\\n\\x04goat'>",
      "example_proto = tf.train.Example.FromString(serialized_example.numpy())\nexample_proto",
      "# Write the `tf.train.Example` observations to the file.\nwith tf.io.TFRecordWriter(filename) as writer:\n  for i in range(n_observations):\n    example = serialize_example(feature0[i], feature1[i], feature2[i], feature3[i])\n    writer.write(example.numpy())",
      "filenames = [filename]\nraw_dataset = tf.data.TFRecordDataset(filenames)\nraw_dataset",
      "<TFRecordDatasetV2 element_spec=TensorSpec(shape=(), dtype=tf.string, name=None)>",
      "for raw_record in raw_dataset.take(1):\n  example = tf.train.Example()\n  example.ParseFromString(raw_record.numpy())\n  print(example)",
      "filenames = [filename]\nraw_dataset = tf.data.TFRecordDataset(filenames)\nraw_dataset",
      "<TFRecordDatasetV2 element_spec=TensorSpec(shape=(), dtype=tf.string, name=None)>",
      "<tf.Tensor: shape=(), dtype=string, numpy=b'\\nS\\n\\x11\\n\\x08feature1\\x12\\x05\\x1a\\x03\\n\\x01\\x03\\n\\x11\\n\\x08feature0\\x12\\x05\\x1a\\x03\\n\\x01\\x00\\n\\x15\\n\\x08feature2\\x12\\t\\n\\x07\\n\\x05horse\\n\\x14\\n\\x08feature3\\x12\\x08\\x12\\x06\\n\\x04\\x85-%\\xbf'>\n<tf.Tensor: shape=(), dtype=string, numpy=b'\\nU\\n\\x14\\n\\x08feature3\\x12\\x08\\x12\\x06\\n\\x04\\xc0,\\xec\\xbe\\n\\x17\\n\\x08feature2\\x12\\x0b\\n\\t\\n\\x07chicken\\n\\x11\\n\\x08feature0\\x12\\x05\\x1a\\x03\\n\\x01\\x01\\n\\x11\\n\\x08feature1\\x12\\x05\\x1a\\x03\\n\\x01\\x02'>\n<tf.Tensor: shape=(), dtype=string, numpy=b'\\nR\\n\\x14\\n\\x08feature3\\x12\\x08\\x12\\x06\\n\\x04\\xd6O\\xb8?\\n\\x11\\n\\x08feature1\\x12\\x05\\x1a\\x03\\n\\x01\\x04\\n\\x11\\n\\x08feature0\\x12\\x05\\x1a\\x03\\n\\x01\\x01\\n\\x14\\n\\x08feature2\\x12\\x08\\n\\x06\\n\\x04goat'>\n<tf.Tensor: shape=(), dtype=string, numpy=b'\\nR\\n\\x14\\n\\x08feature3\\x12\\x08\\x12\\x06\\n\\x04\\xf3\\xbf\\xa0\\xbf\\n\\x14\\n\\x08feature2\\x12\\x08\\n\\x06\\n\\x04goat\\n\\x11\\n\\x08feature1\\x12\\x05\\x1a\\x03\\n\\x01\\x04\\n\\x11\\n\\x08feature0\\x12\\x05\\x1a\\x03\\n\\x01\\x01'>\n<tf.Tensor: shape=(), dtype=string, numpy=b'\\nS\\n\\x15\\n\\x08feature2\\x12\\t\\n\\x07\\n\\x05horse\\n\\x11\\n\\x08feature0\\x12\\x05\\x1a\\x03\\n\\x01\\x01\\n\\x14\\n\\x08feature3\\x12\\x08\\x12\\x06\\n\\x04\\x97\\x1aE\\xbe\\n\\x11\\n\\x08feature1\\x12\\x05\\x1a\\x03\\n\\x01\\x03'>\n<tf.Tensor: shape=(), dtype=string, numpy=b'\\nU\\n\\x14\\n\\x08feature3\\x12\\x08\\x12\\x06\\n\\x04~<\\xe0\\xbe\\n\\x11\\n\\x08feature1\\x12\\x05\\x1a\\x03\\n\\x01\\x02\\n\\x11\\n\\x08feature0\\x12\\x05\\x1a\\x03\\n\\x01\\x00\\n\\x17\\n\\x08feature2\\x12\\x0b\\n\\t\\n\\x07chicken'>\n<tf.Tensor: shape=(), dtype=string, numpy=b'\\nU\\n\\x11\\n\\x08feature1\\x12\\x05\\x1a\\x03\\n\\x01\\x02\\n\\x17\\n\\x08feature2\\x12\\x0b\\n\\t\\n\\x07chicken\\n\\x14\\n\\x08feature3\\x12\\x08\\x12\\x06\\n\\x04\\x0cW\\x18>\\n\\x11\\n\\x08feature0\\x12\\x05\\x1a\\x03\\n\\x01\\x01'>\n<tf.Tensor: shape=(), dtype=string, numpy=b'\\nQ\\n\\x14\\n\\x08feature3\\x12\\x08\\x12\\x06\\n\\x04\\x8d\\xdd\\xcb\\xbf\\n\\x11\\n\\x08feature0\\x12\\x05\\x1a\\x03\\n\\x01\\x01\\n\\x11\\n\\x08feature1\\x12\\x05\\x1a\\x03\\n\\x01\\x00\\n\\x13\\n\\x08feature2\\x12\\x07\\n\\x05\\n\\x03cat'>\n<tf.Tensor: shape=(), dtype=string, numpy=b'\\nQ\\n\\x14\\n\\x08feature3\\x12\\x08\\x12\\x06\\n\\x04\\x95\\xdf=?\\n\\x11\\n\\x08feature1\\x12\\x05\\x1a\\x03\\n\\x01\\x01\\n\\x13\\n\\x08feature2\\x12\\x07\\n\\x05\\n\\x03dog\\n\\x11\\n\\x08feature0\\x12\\x05\\x1a\\x03\\n\\x01\\x01'>\n<tf.Tensor: shape=(), dtype=string, numpy=b'\\nQ\\n\\x14\\n\\x08feature3\\x12\\x08\\x12\\x06\\n\\x04\\x8dC\\xb8\\xbd\\n\\x11\\n\\x08feature0\\x12\\x05\\x1a\\x03\\n\\x01\\x00\\n\\x13\\n\\x08feature2\\x12\\x07\\n\\x05\\n\\x03cat\\n\\x11\\n\\x08feature1\\x12\\x05\\x1a\\x03\\n\\x01\\x00'>",
      "# Create a description of the features.\nfeature_description = {\n    'feature0': tf.io.FixedLenFeature([], tf.int64, default_value=0),\n    'feature1': tf.io.FixedLenFeature([], tf.int64, default_value=0),\n    'feature2': tf.io.FixedLenFeature([], tf.string, default_value=''),\n    'feature3': tf.io.FixedLenFeature([], tf.float32, default_value=0.0),\n}\n\ndef _parse_function(example_proto):\n  # Parse the input `tf.train.Example` proto using the dictionary above.\n  return tf.io.parse_single_example(example_proto, feature_description)",
      "<_MapDataset element_spec={'feature0': TensorSpec(shape=(), dtype=tf.int64, name=None), 'feature1': TensorSpec(shape=(), dtype=tf.int64, name=None), 'feature2': TensorSpec(shape=(), dtype=tf.string, name=None), 'feature3': TensorSpec(shape=(), dtype=tf.float32, name=None)}>",
      "{'feature0': <tf.Tensor: shape=(), dtype=int64, numpy=0>, 'feature1': <tf.Tensor: shape=(), dtype=int64, numpy=3>, 'feature2': <tf.Tensor: shape=(), dtype=string, numpy=b'horse'>, 'feature3': <tf.Tensor: shape=(), dtype=float32, numpy=-0.6452258>}\n{'feature0': <tf.Tensor: shape=(), dtype=int64, numpy=1>, 'feature1': <tf.Tensor: shape=(), dtype=int64, numpy=2>, 'feature2': <tf.Tensor: shape=(), dtype=string, numpy=b'chicken'>, 'feature3': <tf.Tensor: shape=(), dtype=float32, numpy=-0.46127892>}\n{'feature0': <tf.Tensor: shape=(), dtype=int64, numpy=1>, 'feature1': <tf.Tensor: shape=(), dtype=int64, numpy=4>, 'feature2': <tf.Tensor: shape=(), dtype=string, numpy=b'goat'>, 'feature3': <tf.Tensor: shape=(), dtype=float32, numpy=1.4399364>}\n{'feature0': <tf.Tensor: shape=(), dtype=int64, numpy=1>, 'feature1': <tf.Tensor: shape=(), dtype=int64, numpy=4>, 'feature2': <tf.Tensor: shape=(), dtype=string, numpy=b'goat'>, 'feature3': <tf.Tensor: shape=(), dtype=float32, numpy=-1.2558578>}\n{'feature0': <tf.Tensor: shape=(), dtype=int64, numpy=1>, 'feature1': <tf.Tensor: shape=(), dtype=int64, numpy=3>, 'feature2': <tf.Tensor: shape=(), dtype=string, numpy=b'horse'>, 'feature3': <tf.Tensor: shape=(), dtype=float32, numpy=-0.19248424>}\n{'feature0': <tf.Tensor: shape=(), dtype=int64, numpy=0>, 'feature1': <tf.Tensor: shape=(), dtype=int64, numpy=2>, 'feature2': <tf.Tensor: shape=(), dtype=string, numpy=b'chicken'>, 'feature3': <tf.Tensor: shape=(), dtype=float32, numpy=-0.43796152>}\n{'feature0': <tf.Tensor: shape=(), dtype=int64, numpy=1>, 'feature1': <tf.Tensor: shape=(), dtype=int64, numpy=2>, 'feature2': <tf.Tensor: shape=(), dtype=string, numpy=b'chicken'>, 'feature3': <tf.Tensor: shape=(), dtype=float32, numpy=0.14876956>}\n{'feature0': <tf.Tensor: shape=(), dtype=int64, numpy=1>, 'feature1': <tf.Tensor: shape=(), dtype=int64, numpy=0>, 'feature2': <tf.Tensor: shape=(), dtype=string, numpy=b'cat'>, 'feature3': <tf.Tensor: shape=(), dtype=float32, numpy=-1.5926987>}\n{'feature0': <tf.Tensor: shape=(), dtype=int64, numpy=1>, 'feature1': <tf.Tensor: shape=(), dtype=int64, numpy=1>, 'feature2': <tf.Tensor: shape=(), dtype=string, numpy=b'dog'>, 'feature3': <tf.Tensor: shape=(), dtype=float32, numpy=0.74169284>}\n{'feature0': <tf.Tensor: shape=(), dtype=int64, numpy=0>, 'feature1': <tf.Tensor: shape=(), dtype=int64, numpy=0>, 'feature2': <tf.Tensor: shape=(), dtype=string, numpy=b'cat'>, 'feature3': <tf.Tensor: shape=(), dtype=float32, numpy=-0.08997259>}",
      "cat_in_snow  = tf.keras.utils.get_file(\n    '320px-Felis_catus-cat_on_snow.jpg',\n    'https://storage.googleapis.com/download.tensorflow.org/example_images/320px-Felis_catus-cat_on_snow.jpg')\n\nwilliamsburg_bridge = tf.keras.utils.get_file(\n    '194px-New_East_River_Bridge_from_Brooklyn_det.4a09796u.jpg',\n    'https://storage.googleapis.com/download.tensorflow.org/example_images/194px-New_East_River_Bridge_from_Brooklyn_det.4a09796u.jpg')",
      "# This is an example, just using the cat image.\nimage_string = open(cat_in_snow, 'rb').read()\n\nlabel = image_labels[cat_in_snow]\n\n# Create a dictionary with features that may be relevant.\ndef image_example(image_string, label):\n  image_shape = tf.io.decode_jpeg(image_string).shape\n\n  feature = {\n      'height': _int64_feature(image_shape[0]),\n      'width': _int64_feature(image_shape[1]),\n      'depth': _int64_feature(image_shape[2]),\n      'label': _int64_feature(label),\n      'image_raw': _bytes_feature(image_string),\n  }\n\n  return tf.train.Example(features=tf.train.Features(feature=feature))\n\nfor line in str(image_example(image_string, label)).split('\\n')[:15]:\n  print(line)\nprint('...')",
      "# Write the raw image files to `images.tfrecords`.\n# First, process the two images into `tf.train.Example` messages.\n# Then, write to a `.tfrecords` file.\nrecord_file = 'images.tfrecords'\nwith tf.io.TFRecordWriter(record_file) as writer:\n  for filename, label in image_labels.items():\n    image_string = open(filename, 'rb').read()\n    tf_example = image_example(image_string, label)\n    writer.write(tf_example.SerializeToString())",
      "raw_image_dataset = tf.data.TFRecordDataset('images.tfrecords')\n\n# Create a dictionary describing the features.\nimage_feature_description = {\n    'height': tf.io.FixedLenFeature([], tf.int64),\n    'width': tf.io.FixedLenFeature([], tf.int64),\n    'depth': tf.io.FixedLenFeature([], tf.int64),\n    'label': tf.io.FixedLenFeature([], tf.int64),\n    'image_raw': tf.io.FixedLenFeature([], tf.string),\n}\n\ndef _parse_image_function(example_proto):\n  # Parse the input tf.train.Example proto using the dictionary above.\n  return tf.io.parse_single_example(example_proto, image_feature_description)\n\nparsed_image_dataset = raw_image_dataset.map(_parse_image_function)\nparsed_image_dataset",
      "<_MapDataset element_spec={'depth': TensorSpec(shape=(), dtype=tf.int64, name=None), 'height': TensorSpec(shape=(), dtype=tf.int64, name=None), 'image_raw': TensorSpec(shape=(), dtype=tf.string, name=None), 'label': TensorSpec(shape=(), dtype=tf.int64, name=None), 'width': TensorSpec(shape=(), dtype=tf.int64, name=None)}>"
    ]
  },
  {
    "url": "https://www.tensorflow.org/tutorials/load_data/numpy",
    "title": "Load NumPy data\n      \n\n\n      \n      Stay organized with collections\n    \n\n      \n      Save and categorize content based on your preferences.",
    "code_snippets": [
      "import numpy as np\nimport tensorflow as tf",
      "DATA_URL = 'https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz'\n\npath = tf.keras.utils.get_file('mnist.npz', DATA_URL)\nwith np.load(path) as data:\n  train_examples = data['x_train']\n  train_labels = data['y_train']\n  test_examples = data['x_test']\n  test_labels = data['y_test']",
      "train_dataset = tf.data.Dataset.from_tensor_slices((train_examples, train_labels))\ntest_dataset = tf.data.Dataset.from_tensor_slices((test_examples, test_labels))",
      "model = tf.keras.Sequential([\n    tf.keras.layers.Flatten(input_shape=(28, 28)),\n    tf.keras.layers.Dense(128, activation='relu'),\n    tf.keras.layers.Dense(10)\n])\n\nmodel.compile(optimizer=tf.keras.optimizers.RMSprop(),\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['sparse_categorical_accuracy'])"
    ]
  },
  {
    "url": "https://www.tensorflow.org/tutorials/load_data/video",
    "title": "Load video data\n      \n\n\n      \n      Stay organized with collections\n    \n\n      \n      Save and categorize content based on your preferences.",
    "code_snippets": [
      "import tqdm\nimport random\nimport pathlib\nimport itertools\nimport collections\n\nimport os\nimport cv2\nimport numpy as np\nimport remotezip as rz\n\nimport tensorflow as tf\n\n# Some modules to display an animation using imageio.\nimport imageio\nfrom IPython import display\nfrom urllib import request\nfrom tensorflow_docs.vis import embed",
      "def list_files_from_zip_url(zip_url):\n  \"\"\" List the files in each class of the dataset given a URL with the zip file.\n\n    Args:\n      zip_url: A URL from which the files can be extracted from.\n\n    Returns:\n      List of files in each of the classes.\n  \"\"\"\n  files = []\n  with rz.RemoteZip(zip_url) as zip:\n    for zip_info in zip.infolist():\n      files.append(zip_info.filename)\n  return files",
      "def get_class(fname):\n  \"\"\" Retrieve the name of the class given a filename.\n\n    Args:\n      fname: Name of the file in the UCF101 dataset.\n\n    Returns:\n      Class that the file belongs to.\n  \"\"\"\n  return fname.split('_')[-3]",
      "def get_files_per_class(files):\n  \"\"\" Retrieve the files that belong to each class.\n\n    Args:\n      files: List of files in the dataset.\n\n    Returns:\n      Dictionary of class names (key) and files (values). \n  \"\"\"\n  files_for_class = collections.defaultdict(list)\n  for fname in files:\n    class_name = get_class(fname)\n    files_for_class[class_name].append(fname)\n  return files_for_class",
      "def select_subset_of_classes(files_for_class, classes, files_per_class):\n  \"\"\" Create a dictionary with the class name and a subset of the files in that class.\n\n    Args:\n      files_for_class: Dictionary of class names (key) and files (values).\n      classes: List of classes.\n      files_per_class: Number of files per class of interest.\n\n    Returns:\n      Dictionary with class as key and list of specified number of video files in that class.\n  \"\"\"\n  files_subset = dict()\n\n  for class_name in classes:\n    class_files = files_for_class[class_name]\n    files_subset[class_name] = class_files[:files_per_class]\n\n  return files_subset",
      "def download_from_zip(zip_url, to_dir, file_names):\n  \"\"\" Download the contents of the zip file from the zip URL.\n\n    Args:\n      zip_url: A URL with a zip file containing data.\n      to_dir: A directory to download data to.\n      file_names: Names of files to download.\n  \"\"\"\n  with rz.RemoteZip(zip_url) as zip:\n    for fn in tqdm.tqdm(file_names):\n      class_name = get_class(fn)\n      zip.extract(fn, str(to_dir / class_name))\n      unzipped_file = to_dir / class_name / fn\n\n      fn = pathlib.Path(fn).parts[-1]\n      output_file = to_dir / class_name / fn\n      unzipped_file.rename(output_file)",
      "def split_class_lists(files_for_class, count):\n  \"\"\" Returns the list of files belonging to a subset of data as well as the remainder of\n    files that need to be downloaded.\n\n    Args:\n      files_for_class: Files belonging to a particular class of data.\n      count: Number of files to download.\n\n    Returns:\n      Files belonging to the subset of data and dictionary of the remainder of files that need to be downloaded.\n  \"\"\"\n  split_files = []\n  remainder = {}\n  for cls in files_for_class:\n    split_files.extend(files_for_class[cls][:count])\n    remainder[cls] = files_for_class[cls][count:]\n  return split_files, remainder",
      "def download_ucf_101_subset(zip_url, num_classes, splits, download_dir):\n  \"\"\" Download a subset of the UCF101 dataset and split them into various parts, such as\n    training, validation, and test.\n\n    Args:\n      zip_url: A URL with a ZIP file with the data.\n      num_classes: Number of labels.\n      splits: Dictionary specifying the training, validation, test, etc. (key) division of data \n              (value is number of files per split).\n      download_dir: Directory to download data to.\n\n    Return:\n      Mapping of the directories containing the subsections of data.\n  \"\"\"\n  files = list_files_from_zip_url(zip_url)\n  for f in files:\n    path = os.path.normpath(f)\n    tokens = path.split(os.sep)\n    if len(tokens) <= 2:\n      files.remove(f) # Remove that item from the list if it does not have a filename\n\n  files_for_class = get_files_per_class(files)\n\n  classes = list(files_for_class.keys())[:num_classes]\n\n  for cls in classes:\n    random.shuffle(files_for_class[cls])\n\n  # Only use the number of classes you want in the dictionary\n  files_for_class = {x: files_for_class[x] for x in classes}\n\n  dirs = {}\n  for split_name, split_count in splits.items():\n    print(split_name, \":\")\n    split_dir = download_dir / split_name\n    split_files, files_for_class = split_class_lists(files_for_class, split_count)\n    download_from_zip(zip_url, split_dir, split_files)\n    dirs[split_name] = split_dir\n\n  return dirs",
      "def format_frames(frame, output_size):\n  \"\"\"\n    Pad and resize an image from a video.\n\n    Args:\n      frame: Image that needs to resized and padded. \n      output_size: Pixel size of the output frame image.\n\n    Return:\n      Formatted frame with padding of specified output size.\n  \"\"\"\n  frame = tf.image.convert_image_dtype(frame, tf.float32)\n  frame = tf.image.resize_with_pad(frame, *output_size)\n  return frame",
      "def frames_from_video_file(video_path, n_frames, output_size = (224,224), frame_step = 15):\n  \"\"\"\n    Creates frames from each video file present for each category.\n\n    Args:\n      video_path: File path to the video.\n      n_frames: Number of frames to be created per video file.\n      output_size: Pixel size of the output frame image.\n\n    Return:\n      An NumPy array of frames in the shape of (n_frames, height, width, channels).\n  \"\"\"\n  # Read each video frame by frame\n  result = []\n  src = cv2.VideoCapture(str(video_path))  \n\n  video_length = src.get(cv2.CAP_PROP_FRAME_COUNT)\n\n  need_length = 1 + (n_frames - 1) * frame_step\n\n  if need_length > video_length:\n    start = 0\n  else:\n    max_start = video_length - need_length\n    start = random.randint(0, max_start + 1)\n\n  src.set(cv2.CAP_PROP_POS_FRAMES, start)\n  # ret is a boolean indicating whether read was successful, frame is the image itself\n  ret, frame = src.read()\n  result.append(format_frames(frame, output_size))\n\n  for _ in range(n_frames - 1):\n    for _ in range(frame_step):\n      ret, frame = src.read()\n    if ret:\n      frame = format_frames(frame, output_size)\n      result.append(frame)\n    else:\n      result.append(np.zeros_like(result[0]))\n  src.release()\n  result = np.array(result)[..., [2, 1, 0]]\n\n  return result",
      "def to_gif(images):\n  converted_images = np.clip(images * 255, 0, 255).astype(np.uint8)\n  imageio.mimsave('./animation.gif', converted_images, fps=10)\n  return embed.embed_file('./animation.gif')",
      "class FrameGenerator:\n  def __init__(self, path, n_frames, training = False):\n    \"\"\" Returns a set of frames with their associated label. \n\n      Args:\n        path: Video file paths.\n        n_frames: Number of frames. \n        training: Boolean to determine if training dataset is being created.\n    \"\"\"\n    self.path = path\n    self.n_frames = n_frames\n    self.training = training\n    self.class_names = sorted(set(p.name for p in self.path.iterdir() if p.is_dir()))\n    self.class_ids_for_name = dict((name, idx) for idx, name in enumerate(self.class_names))\n\n  def get_files_and_class_names(self):\n    video_paths = list(self.path.glob('*/*.avi'))\n    classes = [p.parent.name for p in video_paths] \n    return video_paths, classes\n\n  def __call__(self):\n    video_paths, classes = self.get_files_and_class_names()\n\n    pairs = list(zip(video_paths, classes))\n\n    if self.training:\n      random.shuffle(pairs)\n\n    for path, name in pairs:\n      video_frames = frames_from_video_file(path, self.n_frames) \n      label = self.class_ids_for_name[name] # Encode labels\n      yield video_frames, label",
      "# Create the training set\noutput_signature = (tf.TensorSpec(shape = (None, None, None, 3), dtype = tf.float32),\n                    tf.TensorSpec(shape = (), dtype = tf.int16))\ntrain_ds = tf.data.Dataset.from_generator(FrameGenerator(subset_paths['train'], 10, training=True),\n                                          output_signature = output_signature)",
      "# Create the validation set\nval_ds = tf.data.Dataset.from_generator(FrameGenerator(subset_paths['val'], 10),\n                                        output_signature = output_signature)",
      "AUTOTUNE = tf.data.AUTOTUNE\n\ntrain_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size = AUTOTUNE)\nval_ds = val_ds.cache().shuffle(1000).prefetch(buffer_size = AUTOTUNE)",
      "net = tf.keras.applications.EfficientNetB0(include_top = False)\nnet.trainable = False\n\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Rescaling(scale=255),\n    tf.keras.layers.TimeDistributed(net),\n    tf.keras.layers.Dense(10),\n    tf.keras.layers.GlobalAveragePooling3D()\n])\n\nmodel.compile(optimizer = 'adam',\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True),\n              metrics=['accuracy'])\n\nmodel.fit(train_ds, \n          epochs = 10,\n          validation_data = val_ds,\n          callbacks = tf.keras.callbacks.EarlyStopping(patience = 2, monitor = 'val_loss'))"
    ]
  },
  {
    "url": "https://www.tensorflow.org/tutorials/distribute/keras",
    "title": "Distributed training with Keras\n      \n\n\n      \n      Stay organized with collections\n    \n\n      \n      Save and categorize content based on your preferences.",
    "code_snippets": [
      "import tensorflow_datasets as tfds\nimport tensorflow as tf\n\nimport os\n\n# Load the TensorBoard notebook extension.\n%load_ext tensorboard",
      "print(tf.__version__)",
      "strategy = tf.distribute.MirroredStrategy()",
      "def scale(image, label):\n  image = tf.cast(image, tf.float32)\n  image /= 255\n\n  return image, label",
      "with strategy.scope():\n  model = tf.keras.Sequential([\n      tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(28, 28, 1)),\n      tf.keras.layers.MaxPooling2D(),\n      tf.keras.layers.Flatten(),\n      tf.keras.layers.Dense(64, activation='relu'),\n      tf.keras.layers.Dense(10)\n  ])\n\n  model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n                optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n                metrics=['accuracy'])",
      "# Define a callback for printing the learning rate at the end of each epoch.\nclass PrintLR(tf.keras.callbacks.Callback):\n  def on_epoch_end(self, epoch, logs=None):\n    print('\\nLearning rate for epoch {} is {}'.format(        epoch + 1, model.optimizer.lr.numpy()))",
      "# Put all the callbacks together.\ncallbacks = [\n    tf.keras.callbacks.TensorBoard(log_dir='./logs'),\n    tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix,\n                                       save_weights_only=True),\n    tf.keras.callbacks.LearningRateScheduler(decay),\n    PrintLR()\n]",
      "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n\neval_loss, eval_acc = model.evaluate(eval_dataset)\n\nprint('Eval loss: {}, Eval accuracy: {}'.format(eval_loss, eval_acc))",
      "unreplicated_model = tf.keras.models.load_model(path)\n\nunreplicated_model.compile(\n    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    optimizer=tf.keras.optimizers.Adam(),\n    metrics=['accuracy'])\n\neval_loss, eval_acc = unreplicated_model.evaluate(eval_dataset)\n\nprint('Eval loss: {}, Eval Accuracy: {}'.format(eval_loss, eval_acc))",
      "with strategy.scope():\n  replicated_model = tf.keras.models.load_model(path)\n  replicated_model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n                           optimizer=tf.keras.optimizers.Adam(),\n                           metrics=['accuracy'])\n\n  eval_loss, eval_acc = replicated_model.evaluate(eval_dataset)\n  print ('Eval loss: {}, Eval Accuracy: {}'.format(eval_loss, eval_acc))"
    ]
  },
  {
    "url": "https://www.tensorflow.org/tutorials/audio/music_generation",
    "title": "Generate music with an RNN\n      \n\n\n      \n      Stay organized with collections\n    \n\n      \n      Save and categorize content based on your preferences.",
    "code_snippets": [
      "import collections\nimport datetime\nimport fluidsynth\nimport glob\nimport numpy as np\nimport pathlib\nimport pandas as pd\nimport pretty_midi\nimport seaborn as sns\nimport tensorflow as tf\n\nfrom IPython import display\nfrom matplotlib import pyplot as plt\nfrom typing import Optional",
      "seed = 42\ntf.random.set_seed(seed)\nnp.random.seed(seed)\n\n# Sampling rate for audio playback\n_SAMPLING_RATE = 16000",
      "data_dir = pathlib.Path('data/maestro-v2.0.0')\nif not data_dir.exists():\n  tf.keras.utils.get_file(\n      'maestro-v2.0.0-midi.zip',\n      origin='https://storage.googleapis.com/magentadata/datasets/maestro/v2.0.0/maestro-v2.0.0-midi.zip',\n      extract=True,\n      cache_dir='.', cache_subdir='data',\n  )",
      "def display_audio(pm: pretty_midi.PrettyMIDI, seconds=30):\n  waveform = pm.fluidsynth(fs=_SAMPLING_RATE)\n  # Take a sample of the generated waveform to mitigate kernel resets\n  waveform_short = waveform[:seconds*_SAMPLING_RATE]\n  return display.Audio(waveform_short, rate=_SAMPLING_RATE)",
      "def midi_to_notes(midi_file: str) -> pd.DataFrame:\n  pm = pretty_midi.PrettyMIDI(midi_file)\n  instrument = pm.instruments[0]\n  notes = collections.defaultdict(list)\n\n  # Sort the notes by start time\n  sorted_notes = sorted(instrument.notes, key=lambda note: note.start)\n  prev_start = sorted_notes[0].start\n\n  for note in sorted_notes:\n    start = note.start\n    end = note.end\n    notes['pitch'].append(note.pitch)\n    notes['start'].append(start)\n    notes['end'].append(end)\n    notes['step'].append(start - prev_start)\n    notes['duration'].append(end - start)\n    prev_start = start\n\n  return pd.DataFrame({name: np.array(value) for name, value in notes.items()})",
      "def plot_piano_roll(notes: pd.DataFrame, count: Optional[int] = None):\n  if count:\n    title = f'First {count} notes'\n  else:\n    title = f'Whole track'\n    count = len(notes['pitch'])\n  plt.figure(figsize=(20, 4))\n  plot_pitch = np.stack([notes['pitch'], notes['pitch']], axis=0)\n  plot_start_stop = np.stack([notes['start'], notes['end']], axis=0)\n  plt.plot(\n      plot_start_stop[:, :count], plot_pitch[:, :count], color=\"b\", marker=\".\")\n  plt.xlabel('Time [s]')\n  plt.ylabel('Pitch')\n  _ = plt.title(title)",
      "def plot_distributions(notes: pd.DataFrame, drop_percentile=2.5):\n  plt.figure(figsize=[15, 5])\n  plt.subplot(1, 3, 1)\n  sns.histplot(notes, x=\"pitch\", bins=20)\n\n  plt.subplot(1, 3, 2)\n  max_step = np.percentile(notes['step'], 100 - drop_percentile)\n  sns.histplot(notes, x=\"step\", bins=np.linspace(0, max_step, 21))\n\n  plt.subplot(1, 3, 3)\n  max_duration = np.percentile(notes['duration'], 100 - drop_percentile)\n  sns.histplot(notes, x=\"duration\", bins=np.linspace(0, max_duration, 21))",
      "def notes_to_midi(\n  notes: pd.DataFrame,\n  out_file: str, \n  instrument_name: str,\n  velocity: int = 100,  # note loudness\n) -> pretty_midi.PrettyMIDI:\n\n  pm = pretty_midi.PrettyMIDI()\n  instrument = pretty_midi.Instrument(\n      program=pretty_midi.instrument_name_to_program(\n          instrument_name))\n\n  prev_start = 0\n  for i, note in notes.iterrows():\n    start = float(prev_start + note['step'])\n    end = float(start + note['duration'])\n    note = pretty_midi.Note(\n        velocity=velocity,\n        pitch=int(note['pitch']),\n        start=start,\n        end=end,\n    )\n    instrument.notes.append(note)\n    prev_start = start\n\n  pm.instruments.append(instrument)\n  pm.write(out_file)\n  return pm",
      "notes_ds = tf.data.Dataset.from_tensor_slices(train_notes)\nnotes_ds.element_spec",
      "def create_sequences(\n    dataset: tf.data.Dataset, \n    seq_length: int,\n    vocab_size = 128,\n) -> tf.data.Dataset:\n  \"\"\"Returns TF Dataset of sequence and label examples.\"\"\"\n  seq_length = seq_length+1\n\n  # Take 1 extra for the labels\n  windows = dataset.window(seq_length, shift=1, stride=1,\n                              drop_remainder=True)\n\n  # `flat_map` flattens the\" dataset of datasets\" into a dataset of tensors\n  flatten = lambda x: x.batch(seq_length, drop_remainder=True)\n  sequences = windows.flat_map(flatten)\n\n  # Normalize note pitch\n  def scale_pitch(x):\n    x = x/[vocab_size,1.0,1.0]\n    return x\n\n  # Split the labels\n  def split_labels(sequences):\n    inputs = sequences[:-1]\n    labels_dense = sequences[-1]\n    labels = {key:labels_dense[i] for i,key in enumerate(key_order)}\n\n    return scale_pitch(inputs), labels\n\n  return sequences.map(split_labels, num_parallel_calls=tf.data.AUTOTUNE)",
      "batch_size = 64\nbuffer_size = n_notes - seq_length  # the number of items in the dataset\ntrain_ds = (seq_ds\n            .shuffle(buffer_size)\n            .batch(batch_size, drop_remainder=True)\n            .cache()\n            .prefetch(tf.data.experimental.AUTOTUNE))",
      "def mse_with_positive_pressure(y_true: tf.Tensor, y_pred: tf.Tensor):\n  mse = (y_true - y_pred) ** 2\n  positive_pressure = 10 * tf.maximum(-y_pred, 0.0)\n  return tf.reduce_mean(mse + positive_pressure)",
      "input_shape = (seq_length, 3)\nlearning_rate = 0.005\n\ninputs = tf.keras.Input(input_shape)\nx = tf.keras.layers.LSTM(128)(inputs)\n\noutputs = {\n  'pitch': tf.keras.layers.Dense(128, name='pitch')(x),\n  'step': tf.keras.layers.Dense(1, name='step')(x),\n  'duration': tf.keras.layers.Dense(1, name='duration')(x),\n}\n\nmodel = tf.keras.Model(inputs, outputs)\n\nloss = {\n      'pitch': tf.keras.losses.SparseCategoricalCrossentropy(\n          from_logits=True),\n      'step': mse_with_positive_pressure,\n      'duration': mse_with_positive_pressure,\n}\n\noptimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n\nmodel.compile(loss=loss, optimizer=optimizer)\n\nmodel.summary()",
      "callbacks = [\n    tf.keras.callbacks.ModelCheckpoint(\n        filepath='./training_checkpoints/ckpt_{epoch}',\n        save_weights_only=True),\n    tf.keras.callbacks.EarlyStopping(\n        monitor='loss',\n        patience=5,\n        verbose=1,\n        restore_best_weights=True),\n]",
      "def predict_next_note(\n    notes: np.ndarray, \n    model: tf.keras.Model, \n    temperature: float = 1.0) -> tuple[int, float, float]:\n  \"\"\"Generates a note as a tuple of (pitch, step, duration), using a trained sequence model.\"\"\"\n\n  assert temperature > 0\n\n  # Add batch dimension\n  inputs = tf.expand_dims(notes, 0)\n\n  predictions = model.predict(inputs)\n  pitch_logits = predictions['pitch']\n  step = predictions['step']\n  duration = predictions['duration']\n\n  pitch_logits /= temperature\n  pitch = tf.random.categorical(pitch_logits, num_samples=1)\n  pitch = tf.squeeze(pitch, axis=-1)\n  duration = tf.squeeze(duration, axis=-1)\n  step = tf.squeeze(step, axis=-1)\n\n  # `step` and `duration` values should be non-negative\n  step = tf.maximum(0, step)\n  duration = tf.maximum(0, duration)\n\n  return int(pitch), float(step), float(duration)"
    ]
  },
  {
    "url": "https://www.tensorflow.org/tutorials/estimator/premade",
    "title": "Premade Estimators\n      \n\n\n      \n      Stay organized with collections\n    \n\n      \n      Save and categorize content based on your preferences.",
    "code_snippets": [
      "import tensorflow as tf\n\nimport pandas as pd",
      "train_path = tf.keras.utils.get_file(\n    \"iris_training.csv\", \"https://storage.googleapis.com/download.tensorflow.org/data/iris_training.csv\")\ntest_path = tf.keras.utils.get_file(\n    \"iris_test.csv\", \"https://storage.googleapis.com/download.tensorflow.org/data/iris_test.csv\")\n\ntrain = pd.read_csv(train_path, names=CSV_COLUMN_NAMES, header=0)\ntest = pd.read_csv(test_path, names=CSV_COLUMN_NAMES, header=0)",
      "def input_evaluation_set():\n    features = {'SepalLength': np.array([6.4, 5.0]),\n                'SepalWidth':  np.array([2.8, 2.3]),\n                'PetalLength': np.array([5.6, 3.3]),\n                'PetalWidth':  np.array([2.2, 1.0])}\n    labels = np.array([2, 1])\n    return features, labels",
      "def input_fn(features, labels, training=True, batch_size=256):\n    \"\"\"An input function for training or evaluating\"\"\"\n    # Convert the inputs to a Dataset.\n    dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))\n\n    # Shuffle and repeat if you are in training mode.\n    if training:\n        dataset = dataset.shuffle(1000).repeat()\n\n    return dataset.batch(batch_size)",
      "# Feature columns describe how to use the input.\nmy_feature_columns = []\nfor key in train.keys():\n    my_feature_columns.append(tf.feature_column.numeric_column(key=key))",
      "# Build a DNN with 2 hidden layers with 30 and 10 hidden nodes each.\nclassifier = tf.estimator.DNNClassifier(\n    feature_columns=my_feature_columns,\n    # Two hidden layers of 30 and 10 nodes respectively.\n    hidden_units=[30, 10],\n    # The model must choose between 3 classes.\n    n_classes=3)",
      "# Generate predictions from the model\nexpected = ['Setosa', 'Versicolor', 'Virginica']\npredict_x = {\n    'SepalLength': [5.1, 5.9, 6.9],\n    'SepalWidth': [3.3, 3.0, 3.1],\n    'PetalLength': [1.7, 4.2, 5.4],\n    'PetalWidth': [0.5, 1.5, 2.1],\n}\n\ndef input_fn(features, batch_size=256):\n    \"\"\"An input function for prediction.\"\"\"\n    # Convert the inputs to a Dataset without labels.\n    return tf.data.Dataset.from_tensor_slices(dict(features)).batch(batch_size)\n\npredictions = classifier.predict(\n    input_fn=lambda: input_fn(predict_x))"
    ]
  },
  {
    "url": "https://www.tensorflow.org/tutorials/generative/adversarial_fgsm",
    "title": "Adversarial example using FGSM\n      \n\n\n      \n      Stay organized with collections\n    \n\n      \n      Save and categorize content based on your preferences.",
    "code_snippets": [
      "import tensorflow as tf\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n\nmpl.rcParams['figure.figsize'] = (8, 8)\nmpl.rcParams['axes.grid'] = False",
      "pretrained_model = tf.keras.applications.MobileNetV2(include_top=True,\n                                                     weights='imagenet')\npretrained_model.trainable = False\n\n# ImageNet labels\ndecode_predictions = tf.keras.applications.mobilenet_v2.decode_predictions",
      "# Helper function to preprocess the image so that it can be inputted in MobileNetV2\ndef preprocess(image):\n  image = tf.cast(image, tf.float32)\n  image = tf.image.resize(image, (224, 224))\n  image = tf.keras.applications.mobilenet_v2.preprocess_input(image)\n  image = image[None, ...]\n  return image\n\n# Helper function to extract labels from probability vector\ndef get_imagenet_label(probs):\n  return decode_predictions(probs, top=1)[0][0]",
      "image_path = tf.keras.utils.get_file('YellowLabradorLooking_new.jpg', 'https://storage.googleapis.com/download.tensorflow.org/example_images/YellowLabradorLooking_new.jpg')\nimage_raw = tf.io.read_file(image_path)\nimage = tf.image.decode_image(image_raw)\n\nimage = preprocess(image)\nimage_probs = pretrained_model.predict(image)",
      "loss_object = tf.keras.losses.CategoricalCrossentropy()\n\ndef create_adversarial_pattern(input_image, input_label):\n  with tf.GradientTape() as tape:\n    tape.watch(input_image)\n    prediction = pretrained_model(input_image)\n    loss = loss_object(input_label, prediction)\n\n  # Get the gradients of the loss w.r.t to the input image.\n  gradient = tape.gradient(loss, input_image)\n  # Get the sign of the gradients to create the perturbation\n  signed_grad = tf.sign(gradient)\n  return signed_grad",
      "# Get the input label of the image.\nlabrador_retriever_index = 208\nlabel = tf.one_hot(labrador_retriever_index, image_probs.shape[-1])\nlabel = tf.reshape(label, (1, image_probs.shape[-1]))\n\nperturbations = create_adversarial_pattern(image, label)\nplt.imshow(perturbations[0] * 0.5 + 0.5);  # To change [-1, 1] to [0,1]",
      "def display_images(image, description):\n  _, label, confidence = get_imagenet_label(pretrained_model.predict(image))\n  plt.figure()\n  plt.imshow(image[0]*0.5+0.5)\n  plt.title('{} \\n {} : {:.2f}% Confidence'.format(description,\n                                                   label, confidence*100))\n  plt.show()",
      "epsilons = [0, 0.01, 0.1, 0.15]\ndescriptions = [('Epsilon = {:0.3f}'.format(eps) if eps else 'Input')\n                for eps in epsilons]\n\nfor i, eps in enumerate(epsilons):\n  adv_x = image + eps*perturbations\n  adv_x = tf.clip_by_value(adv_x, -1, 1)\n  display_images(adv_x, descriptions[i])"
    ]
  },
  {
    "url": "https://www.tensorflow.org/tutorials/customization/basics",
    "title": "Customization basics: tensors and operations\n      \n\n\n      \n      Stay organized with collections\n    \n\n      \n      Save and categorize content based on your preferences.",
    "code_snippets": [
      "import tensorflow as tf",
      "print(tf.math.add(1, 2))\nprint(tf.math.add([1, 2], [3, 4]))\nprint(tf.math.square(5))\nprint(tf.math.reduce_sum([1, 2, 3]))\n\n# Operator overloading is also supported\nprint(tf.math.square(2) + tf.math.square(3))",
      "tf.Tensor(3, shape=(), dtype=int32)\ntf.Tensor([4 6], shape=(2,), dtype=int32)\ntf.Tensor(25, shape=(), dtype=int32)\ntf.Tensor(6, shape=(), dtype=int32)\ntf.Tensor(13, shape=(), dtype=int32)\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1723775459.220860   70727 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723775459.224736   70727 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723775459.228526   70727 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723775459.231708   70727 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723775459.243472   70727 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723775459.247007   70727 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723775459.250510   70727 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723775459.253505   70727 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723775459.256964   70727 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723775459.260375   70727 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723775459.263839   70727 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723775459.266764   70727 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723775460.484960   70727 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723775460.487122   70727 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723775460.489159   70727 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723775460.491212   70727 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723775460.493195   70727 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723775460.495178   70727 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723775460.497138   70727 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723775460.499106   70727 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723775460.500943   70727 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723775460.502910   70727 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723775460.504850   70727 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723775460.506819   70727 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723775460.545061   70727 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723775460.547158   70727 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723775460.549140   70727 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723775460.551249   70727 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723775460.553080   70727 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723775460.555080   70727 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723775460.557010   70727 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723775460.558972   70727 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723775460.560817   70727 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723775460.563305   70727 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723775460.565638   70727 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723775460.568007   70727 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355",
      "x = tf.linalg.matmul([[1]], [[2, 3]])\nprint(x)\nprint(x.shape)\nprint(x.dtype)",
      "tf.Tensor([[2 3]], shape=(1, 2), dtype=int32)\n(1, 2)\n<dtype: 'int32'>",
      "import numpy as np\n\nndarray = np.ones([3, 3])\n\nprint(\"TensorFlow operations convert numpy arrays to Tensors automatically\")\ntensor = tf.math.multiply(ndarray, 42)\nprint(tensor)\n\n\nprint(\"And NumPy operations convert Tensors to NumPy arrays automatically\")\nprint(np.add(tensor, 1))\n\nprint(\"The .numpy() method explicitly converts a Tensor to a numpy array\")\nprint(tensor.numpy())",
      "TensorFlow operations convert numpy arrays to Tensors automatically\ntf.Tensor(\n[[42. 42. 42.]\n [42. 42. 42.]\n [42. 42. 42.]], shape=(3, 3), dtype=float64)\nAnd NumPy operations convert Tensors to NumPy arrays automatically\n[[43. 43. 43.]\n [43. 43. 43.]\n [43. 43. 43.]]\nThe .numpy() method explicitly converts a Tensor to a numpy array\n[[42. 42. 42.]\n [42. 42. 42.]\n [42. 42. 42.]]",
      "x = tf.random.uniform([3, 3])\n\nprint(\"Is there a GPU available: \"),\nprint(tf.config.list_physical_devices(\"GPU\"))\n\nprint(\"Is the Tensor on GPU #0:  \"),\nprint(x.device.endswith('GPU:0'))",
      "import time\n\ndef time_matmul(x):\n  start = time.time()\n  for loop in range(10):\n    tf.linalg.matmul(x, x)\n\n  result = time.time()-start\n\n  print(\"10 loops: {:0.2f}ms\".format(1000*result))\n\n# Force execution on CPU\nprint(\"On CPU:\")\nwith tf.device(\"CPU:0\"):\n  x = tf.random.uniform([1000, 1000])\n  assert x.device.endswith(\"CPU:0\")\n  time_matmul(x)\n\n# Force execution on GPU #0 if available\nif tf.config.list_physical_devices(\"GPU\"):\n  print(\"On GPU:\")\n  with tf.device(\"GPU:0\"): # Or GPU:1 for the 2nd GPU, GPU:2 for the 3rd etc.\n    x = tf.random.uniform([1000, 1000])\n    assert x.device.endswith(\"GPU:0\")\n    time_matmul(x)",
      "ds_tensors = tf.data.Dataset.from_tensor_slices([1, 2, 3, 4, 5, 6])\n\n# Create a CSV file\nimport tempfile\n_, filename = tempfile.mkstemp()\n\nwith open(filename, 'w') as f:\n  f.write(\"\"\"Line 1\nLine 2\nLine 3\n  \"\"\")\n\nds_file = tf.data.TextLineDataset(filename)",
      "ds_tensors = ds_tensors.map(tf.math.square).shuffle(2).batch(2)\n\nds_file = ds_file.batch(2)",
      "Elements of ds_tensors:\ntf.Tensor([4 1], shape=(2,), dtype=int32)\ntf.Tensor([16  9], shape=(2,), dtype=int32)\ntf.Tensor([25 36], shape=(2,), dtype=int32)\n\nElements in ds_file:\ntf.Tensor([b'Line 1' b'Line 2'], shape=(2,), dtype=string)\ntf.Tensor([b'Line 3' b'  '], shape=(2,), dtype=string)"
    ]
  },
  {
    "url": "https://www.tensorflow.org/tutorials/structured_data/time_series",
    "title": "Time series forecasting\n      \n\n\n      \n      Stay organized with collections\n    \n\n      \n      Save and categorize content based on your preferences.",
    "code_snippets": [
      "import os\nimport datetime\n\nimport IPython\nimport IPython.display\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport tensorflow as tf\n\nmpl.rcParams['figure.figsize'] = (8, 6)\nmpl.rcParams['axes.grid'] = False",
      "zip_path = tf.keras.utils.get_file(\n    origin='https://storage.googleapis.com/tensorflow/tf-keras-datasets/jena_climate_2009_2016.csv.zip',\n    fname='jena_climate_2009_2016.csv.zip',\n    extract=True)\ncsv_path, _ = os.path.splitext(zip_path)",
      "fft = tf.signal.rfft(df['T (degC)'])\nf_per_dataset = np.arange(0, len(fft))\n\nn_samples_h = len(df['T (degC)'])\nhours_per_year = 24*365.2524\nyears_per_dataset = n_samples_h/(hours_per_year)\n\nf_per_year = f_per_dataset/years_per_dataset\nplt.step(f_per_year, np.abs(fft))\nplt.xscale('log')\nplt.ylim(0, 400000)\nplt.xlim([0.1, max(plt.xlim())])\nplt.xticks([1, 365.2524], labels=['1/Year', '1/day'])\n_ = plt.xlabel('Frequency (log scale)')",
      "class WindowGenerator():\n  def __init__(self, input_width, label_width, shift,\n               train_df=train_df, val_df=val_df, test_df=test_df,\n               label_columns=None):\n    # Store the raw data.\n    self.train_df = train_df\n    self.val_df = val_df\n    self.test_df = test_df\n\n    # Work out the label column indices.\n    self.label_columns = label_columns\n    if label_columns is not None:\n      self.label_columns_indices = {name: i for i, name in\n                                    enumerate(label_columns)}\n    self.column_indices = {name: i for i, name in\n                           enumerate(train_df.columns)}\n\n    # Work out the window parameters.\n    self.input_width = input_width\n    self.label_width = label_width\n    self.shift = shift\n\n    self.total_window_size = input_width + shift\n\n    self.input_slice = slice(0, input_width)\n    self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n\n    self.label_start = self.total_window_size - self.label_width\n    self.labels_slice = slice(self.label_start, None)\n    self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n\n  def __repr__(self):\n    return '\\n'.join([\n        f'Total window size: {self.total_window_size}',\n        f'Input indices: {self.input_indices}',\n        f'Label indices: {self.label_indices}',\n        f'Label column name(s): {self.label_columns}'])",
      "def split_window(self, features):\n  inputs = features[:, self.input_slice, :]\n  labels = features[:, self.labels_slice, :]\n  if self.label_columns is not None:\n    labels = tf.stack(\n        [labels[:, :, self.column_indices[name]] for name in self.label_columns],\n        axis=-1)\n\n  # Slicing doesn't preserve static shape information, so set the shapes\n  # manually. This way the `tf.data.Datasets` are easier to inspect.\n  inputs.set_shape([None, self.input_width, None])\n  labels.set_shape([None, self.label_width, None])\n\n  return inputs, labels\n\nWindowGenerator.split_window = split_window",
      "# Stack three slices, the length of the total window.\nexample_window = tf.stack([np.array(train_df[:w2.total_window_size]),\n                           np.array(train_df[100:100+w2.total_window_size]),\n                           np.array(train_df[200:200+w2.total_window_size])])\n\nexample_inputs, example_labels = w2.split_window(example_window)\n\nprint('All shapes are: (batch, time, features)')\nprint(f'Window shape: {example_window.shape}')\nprint(f'Inputs shape: {example_inputs.shape}')\nprint(f'Labels shape: {example_labels.shape}')",
      "def plot(self, model=None, plot_col='T (degC)', max_subplots=3):\n  inputs, labels = self.example\n  plt.figure(figsize=(12, 8))\n  plot_col_index = self.column_indices[plot_col]\n  max_n = min(max_subplots, len(inputs))\n  for n in range(max_n):\n    plt.subplot(max_n, 1, n+1)\n    plt.ylabel(f'{plot_col} [normed]')\n    plt.plot(self.input_indices, inputs[n, :, plot_col_index],\n             label='Inputs', marker='.', zorder=-10)\n\n    if self.label_columns:\n      label_col_index = self.label_columns_indices.get(plot_col, None)\n    else:\n      label_col_index = plot_col_index\n\n    if label_col_index is None:\n      continue\n\n    plt.scatter(self.label_indices, labels[n, :, label_col_index],\n                edgecolors='k', label='Labels', c='#2ca02c', s=64)\n    if model is not None:\n      predictions = model(inputs)\n      plt.scatter(self.label_indices, predictions[n, :, label_col_index],\n                  marker='X', edgecolors='k', label='Predictions',\n                  c='#ff7f0e', s=64)\n\n    if n == 0:\n      plt.legend()\n\n  plt.xlabel('Time [h]')\n\nWindowGenerator.plot = plot",
      "def make_dataset(self, data):\n  data = np.array(data, dtype=np.float32)\n  ds = tf.keras.utils.timeseries_dataset_from_array(\n      data=data,\n      targets=None,\n      sequence_length=self.total_window_size,\n      sequence_stride=1,\n      shuffle=True,\n      batch_size=32,)\n\n  ds = ds.map(self.split_window)\n\n  return ds\n\nWindowGenerator.make_dataset = make_dataset",
      "(TensorSpec(shape=(None, 6, 19), dtype=tf.float32, name=None),\n TensorSpec(shape=(None, 1, 1), dtype=tf.float32, name=None))",
      "class Baseline(tf.keras.Model):\n  def __init__(self, label_index=None):\n    super().__init__()\n    self.label_index = label_index\n\n  def call(self, inputs):\n    if self.label_index is None:\n      return inputs\n    result = inputs[:, :, self.label_index]\n    return result[:, :, tf.newaxis]",
      "baseline = Baseline(label_index=column_indices['T (degC)'])\n\nbaseline.compile(loss=tf.keras.losses.MeanSquaredError(),\n                 metrics=[tf.keras.metrics.MeanAbsoluteError()])\n\nval_performance = {}\nperformance = {}\nval_performance['Baseline'] = baseline.evaluate(single_step_window.val, return_dict=True)\nperformance['Baseline'] = baseline.evaluate(single_step_window.test, verbose=0, return_dict=True)",
      "linear = tf.keras.Sequential([\n    tf.keras.layers.Dense(units=1)\n])",
      "MAX_EPOCHS = 20\n\ndef compile_and_fit(model, window, patience=2):\n  early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n                                                    patience=patience,\n                                                    mode='min')\n\n  model.compile(loss=tf.keras.losses.MeanSquaredError(),\n                optimizer=tf.keras.optimizers.Adam(),\n                metrics=[tf.keras.metrics.MeanAbsoluteError()])\n\n  history = model.fit(window.train, epochs=MAX_EPOCHS,\n                      validation_data=window.val,\n                      callbacks=[early_stopping])\n  return history",
      "dense = tf.keras.Sequential([\n    tf.keras.layers.Dense(units=64, activation='relu'),\n    tf.keras.layers.Dense(units=64, activation='relu'),\n    tf.keras.layers.Dense(units=1)\n])\n\nhistory = compile_and_fit(dense, single_step_window)\n\nval_performance['Dense'] = dense.evaluate(single_step_window.val, return_dict=True)\nperformance['Dense'] = dense.evaluate(single_step_window.test, verbose=0, return_dict=True)",
      "multi_step_dense = tf.keras.Sequential([\n    # Shape: (time, features) => (time*features)\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(units=32, activation='relu'),\n    tf.keras.layers.Dense(units=32, activation='relu'),\n    tf.keras.layers.Dense(units=1),\n    # Add back the time dimension.\n    # Shape: (outputs) => (1, outputs)\n    tf.keras.layers.Reshape([1, -1]),\n])",
      "Input shape: (32, 24, 19)\n\nValueError:Exception encountered when calling Sequential.call().\n\nInput 0 of layer \"dense_4\" is incompatible with the layer: expected axis -1 of input shape to have value 57, but received input with shape (32, 456)\n\nArguments received by Sequential.call():\n  \u2022 inputs=tf.Tensor(shape=(32, 24, 19), dtype=float32)\n  \u2022 training=None\n  \u2022 mask=None",
      "conv_model = tf.keras.Sequential([\n    tf.keras.layers.Conv1D(filters=32,\n                           kernel_size=(CONV_WIDTH,),\n                           activation='relu'),\n    tf.keras.layers.Dense(units=32, activation='relu'),\n    tf.keras.layers.Dense(units=1),\n])",
      "lstm_model = tf.keras.models.Sequential([\n    # Shape [batch, time, features] => [batch, time, lstm_units]\n    tf.keras.layers.LSTM(32, return_sequences=True),\n    # Shape => [batch, time, features]\n    tf.keras.layers.Dense(units=1)\n])",
      "baseline = Baseline()\nbaseline.compile(loss=tf.keras.losses.MeanSquaredError(),\n                 metrics=[tf.keras.metrics.MeanAbsoluteError()])",
      "dense = tf.keras.Sequential([\n    tf.keras.layers.Dense(units=64, activation='relu'),\n    tf.keras.layers.Dense(units=64, activation='relu'),\n    tf.keras.layers.Dense(units=num_features)\n])",
      "%%time\nwide_window = WindowGenerator(\n    input_width=24, label_width=24, shift=1)\n\nlstm_model = tf.keras.models.Sequential([\n    # Shape [batch, time, features] => [batch, time, lstm_units]\n    tf.keras.layers.LSTM(32, return_sequences=True),\n    # Shape => [batch, time, features]\n    tf.keras.layers.Dense(units=num_features)\n])\n\nhistory = compile_and_fit(lstm_model, wide_window)\n\nIPython.display.clear_output()\nval_performance['LSTM'] = lstm_model.evaluate( wide_window.val, return_dict=True)\nperformance['LSTM'] = lstm_model.evaluate( wide_window.test, verbose=0, return_dict=True)\n\nprint()",
      "class ResidualWrapper(tf.keras.Model):\n  def __init__(self, model):\n    super().__init__()\n    self.model = model\n\n  def call(self, inputs, *args, **kwargs):\n    delta = self.model(inputs, *args, **kwargs)\n\n    # The prediction for each time step is the input\n    # from the previous time step plus the delta\n    # calculated by the model.\n    return inputs + delta",
      "%%time\nresidual_lstm = ResidualWrapper(\n    tf.keras.Sequential([\n    tf.keras.layers.LSTM(32, return_sequences=True),\n    tf.keras.layers.Dense(\n        num_features,\n        # The predicted deltas should start small.\n        # Therefore, initialize the output layer with zeros.\n        kernel_initializer=tf.initializers.zeros())\n]))\n\nhistory = compile_and_fit(residual_lstm, wide_window)\n\nIPython.display.clear_output()\nval_performance['Residual LSTM'] = residual_lstm.evaluate(wide_window.val, return_dict=True)\nperformance['Residual LSTM'] = residual_lstm.evaluate(wide_window.test, verbose=0, return_dict=True)\nprint()",
      "class MultiStepLastBaseline(tf.keras.Model):\n  def call(self, inputs):\n    return tf.tile(inputs[:, -1:, :], [1, OUT_STEPS, 1])\n\nlast_baseline = MultiStepLastBaseline()\nlast_baseline.compile(loss=tf.keras.losses.MeanSquaredError(),\n                      metrics=[tf.keras.metrics.MeanAbsoluteError()])\n\nmulti_val_performance = {}\nmulti_performance = {}\n\nmulti_val_performance['Last'] = last_baseline.evaluate(multi_window.val, return_dict=True)\nmulti_performance['Last'] = last_baseline.evaluate(multi_window.test, verbose=0, return_dict=True)\nmulti_window.plot(last_baseline)",
      "class RepeatBaseline(tf.keras.Model):\n  def call(self, inputs):\n    return inputs\n\nrepeat_baseline = RepeatBaseline()\nrepeat_baseline.compile(loss=tf.keras.losses.MeanSquaredError(),\n                        metrics=[tf.keras.metrics.MeanAbsoluteError()])\n\nmulti_val_performance['Repeat'] = repeat_baseline.evaluate(multi_window.val, return_dict=True)\nmulti_performance['Repeat'] = repeat_baseline.evaluate(multi_window.test, verbose=0, return_dict=True)\nmulti_window.plot(repeat_baseline)",
      "multi_linear_model = tf.keras.Sequential([\n    # Take the last time-step.\n    # Shape [batch, time, features] => [batch, 1, features]\n    tf.keras.layers.Lambda(lambda x: x[:, -1:, :]),\n    # Shape => [batch, 1, out_steps*features]\n    tf.keras.layers.Dense(OUT_STEPS*num_features,\n                          kernel_initializer=tf.initializers.zeros()),\n    # Shape => [batch, out_steps, features]\n    tf.keras.layers.Reshape([OUT_STEPS, num_features])\n])\n\nhistory = compile_and_fit(multi_linear_model, multi_window)\n\nIPython.display.clear_output()\nmulti_val_performance['Linear'] = multi_linear_model.evaluate(multi_window.val, return_dict=True)\nmulti_performance['Linear'] = multi_linear_model.evaluate(multi_window.test, verbose=0, return_dict=True)\nmulti_window.plot(multi_linear_model)",
      "multi_dense_model = tf.keras.Sequential([\n    # Take the last time step.\n    # Shape [batch, time, features] => [batch, 1, features]\n    tf.keras.layers.Lambda(lambda x: x[:, -1:, :]),\n    # Shape => [batch, 1, dense_units]\n    tf.keras.layers.Dense(512, activation='relu'),\n    # Shape => [batch, out_steps*features]\n    tf.keras.layers.Dense(OUT_STEPS*num_features,\n                          kernel_initializer=tf.initializers.zeros()),\n    # Shape => [batch, out_steps, features]\n    tf.keras.layers.Reshape([OUT_STEPS, num_features])\n])\n\nhistory = compile_and_fit(multi_dense_model, multi_window)\n\nIPython.display.clear_output()\nmulti_val_performance['Dense'] = multi_dense_model.evaluate(multi_window.val, return_dict=True)\nmulti_performance['Dense'] = multi_dense_model.evaluate(multi_window.test, verbose=0, return_dict=True)\nmulti_window.plot(multi_dense_model)",
      "CONV_WIDTH = 3\nmulti_conv_model = tf.keras.Sequential([\n    # Shape [batch, time, features] => [batch, CONV_WIDTH, features]\n    tf.keras.layers.Lambda(lambda x: x[:, -CONV_WIDTH:, :]),\n    # Shape => [batch, 1, conv_units]\n    tf.keras.layers.Conv1D(256, activation='relu', kernel_size=(CONV_WIDTH)),\n    # Shape => [batch, 1,  out_steps*features]\n    tf.keras.layers.Dense(OUT_STEPS*num_features,\n                          kernel_initializer=tf.initializers.zeros()),\n    # Shape => [batch, out_steps, features]\n    tf.keras.layers.Reshape([OUT_STEPS, num_features])\n])\n\nhistory = compile_and_fit(multi_conv_model, multi_window)\n\nIPython.display.clear_output()\n\nmulti_val_performance['Conv'] = multi_conv_model.evaluate(multi_window.val, return_dict=True)\nmulti_performance['Conv'] = multi_conv_model.evaluate(multi_window.test, verbose=0, return_dict=True)\nmulti_window.plot(multi_conv_model)",
      "multi_lstm_model = tf.keras.Sequential([\n    # Shape [batch, time, features] => [batch, lstm_units].\n    # Adding more `lstm_units` just overfits more quickly.\n    tf.keras.layers.LSTM(32, return_sequences=False),\n    # Shape => [batch, out_steps*features].\n    tf.keras.layers.Dense(OUT_STEPS*num_features,\n                          kernel_initializer=tf.initializers.zeros()),\n    # Shape => [batch, out_steps, features].\n    tf.keras.layers.Reshape([OUT_STEPS, num_features])\n])\n\nhistory = compile_and_fit(multi_lstm_model, multi_window)\n\nIPython.display.clear_output()\n\nmulti_val_performance['LSTM'] = multi_lstm_model.evaluate(multi_window.val, return_dict=True)\nmulti_performance['LSTM'] = multi_lstm_model.evaluate(multi_window.test, verbose=0, return_dict=True)\nmulti_window.plot(multi_lstm_model)",
      "class FeedBack(tf.keras.Model):\n  def __init__(self, units, out_steps):\n    super().__init__()\n    self.out_steps = out_steps\n    self.units = units\n    self.lstm_cell = tf.keras.layers.LSTMCell(units)\n    # Also wrap the LSTMCell in an RNN to simplify the `warmup` method.\n    self.lstm_rnn = tf.keras.layers.RNN(self.lstm_cell, return_state=True)\n    self.dense = tf.keras.layers.Dense(num_features)",
      "def warmup(self, inputs):\n  # inputs.shape => (batch, time, features)\n  # x.shape => (batch, lstm_units)\n  x, *state = self.lstm_rnn(inputs)\n\n  # predictions.shape => (batch, features)\n  prediction = self.dense(x)\n  return prediction, state\n\nFeedBack.warmup = warmup",
      "def call(self, inputs, training=None):\n  # Use a TensorArray to capture dynamically unrolled outputs.\n  predictions = []\n  # Initialize the LSTM state.\n  prediction, state = self.warmup(inputs)\n\n  # Insert the first prediction.\n  predictions.append(prediction)\n\n  # Run the rest of the prediction steps.\n  for n in range(1, self.out_steps):\n    # Use the last prediction as input.\n    x = prediction\n    # Execute one lstm step.\n    x, state = self.lstm_cell(x, states=state,\n                              training=training)\n    # Convert the lstm output to a prediction.\n    prediction = self.dense(x)\n    # Add the prediction to the output.\n    predictions.append(prediction)\n\n  # predictions.shape => (time, batch, features)\n  predictions = tf.stack(predictions)\n  # predictions.shape => (batch, time, features)\n  predictions = tf.transpose(predictions, [1, 0, 2])\n  return predictions\n\nFeedBack.call = call"
    ]
  },
  {
    "url": "https://www.tensorflow.org/tutorials/generative/generate_images_with_stable_diffusion",
    "title": "High-performance image generation using Stable Diffusion in KerasCV\n      \n\n\n      \n      Stay organized with collections\n    \n\n      \n      Save and categorize content based on your preferences.",
    "code_snippets": [
      "import time\nimport keras_cv\nfrom tensorflow import keras\nimport matplotlib.pyplot as plt"
    ]
  },
  {
    "url": "https://www.tensorflow.org/tutorials/interpretability/integrated_gradients",
    "title": "Integrated gradients\n      \n\n\n      \n      Stay organized with collections\n    \n\n      \n      Save and categorize content based on your preferences.",
    "code_snippets": [
      "import matplotlib.pylab as plt\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow_hub as hub",
      "model = tf.keras.Sequential([\n    hub.KerasLayer(\n        name='inception_v1',\n        handle='https://tfhub.dev/google/imagenet/inception_v1/classification/4',\n        trainable=False),\n])\nmodel.build([None, 224, 224, 3])\nmodel.summary()",
      "def load_imagenet_labels(file_path):\n  labels_file = tf.keras.utils.get_file('ImageNetLabels.txt', file_path)\n  with open(labels_file) as reader:\n    f = reader.read()\n    labels = f.splitlines()\n  return np.array(labels)",
      "def read_image(file_name):\n  image = tf.io.read_file(file_name)\n  image = tf.io.decode_jpeg(image, channels=3)\n  image = tf.image.convert_image_dtype(image, tf.float32)\n  image = tf.image.resize_with_pad(image, target_height=224, target_width=224)\n  return image",
      "img_url = {\n    'Fireboat': 'http://storage.googleapis.com/download.tensorflow.org/example_images/San_Francisco_fireboat_showing_off.jpg',\n    'Giant Panda': 'http://storage.googleapis.com/download.tensorflow.org/example_images/Giant_Panda_2.jpeg',\n}\n\nimg_paths = {name: tf.keras.utils.get_file(name, url) for (name, url) in img_url.items()}\nimg_name_tensors = {name: read_image(img_path) for (name, img_path) in img_paths.items()}",
      "def top_k_predictions(img, k=3):\n  image_batch = tf.expand_dims(img, 0)\n  predictions = model(image_batch)\n  probs = tf.nn.softmax(predictions, axis=-1)\n  top_probs, top_idxs = tf.math.top_k(input=probs, k=k)\n  top_labels = imagenet_labels[tuple(top_idxs)]\n  return top_labels, top_probs[0]",
      "def f(x):\n  \"\"\"A simplified model function.\"\"\"\n  return tf.where(x < 0.8, x, 0.8)\n\ndef interpolated_path(x):\n  \"\"\"A straight line path.\"\"\"\n  return tf.zeros_like(x)\n\nx = tf.linspace(start=0.0, stop=1.0, num=6)\ny = f(x)",
      "fig = plt.figure(figsize=(12, 5))\nax0 = fig.add_subplot(121)\nax0.plot(x, f(x), marker='o')\nax0.set_title('Gradients saturate over F(x)', fontweight='bold')\nax0.text(0.2, 0.5, 'Gradients > 0 = \\n x is important')\nax0.text(0.7, 0.85, 'Gradients = 0 \\n x not important')\nax0.set_yticks(tf.range(0, 1.5, 0.5))\nax0.set_xticks(tf.range(0, 1.5, 0.5))\nax0.set_ylabel('F(x) - model true class predicted probability')\nax0.set_xlabel('x - (pixel value)')\n\nax1 = fig.add_subplot(122)\nax1.plot(x, f(x), marker='o')\nax1.plot(x, interpolated_path(x), marker='>')\nax1.set_title('IG intuition', fontweight='bold')\nax1.text(0.25, 0.1, 'Accumulate gradients along path')\nax1.set_ylabel('F(x) - model true class predicted probability')\nax1.set_xlabel('x - (pixel value)')\nax1.set_yticks(tf.range(0, 1.5, 0.5))\nax1.set_xticks(tf.range(0, 1.5, 0.5))\nax1.annotate('Baseline', xy=(0.0, 0.0), xytext=(0.0, 0.2),\n             arrowprops=dict(facecolor='black', shrink=0.1))\nax1.annotate('Input', xy=(1.0, 0.0), xytext=(0.95, 0.2),\n             arrowprops=dict(facecolor='black', shrink=0.1))\nplt.show();",
      "baseline = tf.zeros(shape=(224,224,3))",
      "m_steps=50\nalphas = tf.linspace(start=0.0, stop=1.0, num=m_steps+1) # Generate m_steps intervals for integral_approximation() below.",
      "def interpolate_images(baseline,\n                       image,\n                       alphas):\n  alphas_x = alphas[:, tf.newaxis, tf.newaxis, tf.newaxis]\n  baseline_x = tf.expand_dims(baseline, axis=0)\n  input_x = tf.expand_dims(image, axis=0)\n  delta = input_x - baseline_x\n  images = baseline_x +  alphas_x * delta\n  return images",
      "def compute_gradients(images, target_class_idx):\n  with tf.GradientTape() as tape:\n    tape.watch(images)\n    logits = model(images)\n    probs = tf.nn.softmax(logits, axis=-1)[:, target_class_idx]\n  return tape.gradient(probs, images)",
      "pred = model(interpolated_images)\npred_proba = tf.nn.softmax(pred, axis=-1)[:, 555]",
      "plt.figure(figsize=(10, 4))\nax1 = plt.subplot(1, 2, 1)\nax1.plot(alphas, pred_proba)\nax1.set_title('Target class predicted probability over alpha')\nax1.set_ylabel('model p(target class)')\nax1.set_xlabel('alpha')\nax1.set_ylim([0, 1])\n\nax2 = plt.subplot(1, 2, 2)\n# Average across interpolation steps\naverage_grads = tf.reduce_mean(path_gradients, axis=[1, 2, 3])\n# Normalize gradients to 0 to 1 scale. E.g., (x - min(x))/(max(x)-min(x))\naverage_grads_norm = (average_grads-tf.math.reduce_min(average_grads))/(tf.math.reduce_max(average_grads)-tf.reduce_min(average_grads))\nax2.plot(alphas, average_grads_norm)\nax2.set_title('Average pixel gradients (normalized) over alpha')\nax2.set_ylabel('Average pixel gradients')\nax2.set_xlabel('alpha')\nax2.set_ylim([0, 1]);",
      "def integral_approximation(gradients):\n  # riemann_trapezoidal\n  grads = (gradients[:-1] + gradients[1:]) / tf.constant(2.0)\n  integrated_gradients = tf.math.reduce_mean(grads, axis=0)\n  return integrated_gradients",
      "def integrated_gradients(baseline,\n                         image,\n                         target_class_idx,\n                         m_steps=50,\n                         batch_size=32):\n  # Generate alphas.\n  alphas = tf.linspace(start=0.0, stop=1.0, num=m_steps+1)\n\n  # Collect gradients.    \n  gradient_batches = []\n\n  # Iterate alphas range and batch computation for speed, memory efficiency, and scaling to larger m_steps.\n  for alpha in tf.range(0, len(alphas), batch_size):\n    from_ = alpha\n    to = tf.minimum(from_ + batch_size, len(alphas))\n    alpha_batch = alphas[from_:to]\n\n    gradient_batch = one_batch(baseline, image, alpha_batch, target_class_idx)\n    gradient_batches.append(gradient_batch)\n\n  # Concatenate path gradients together row-wise into single tensor.\n  total_gradients = tf.concat(gradient_batches, axis=0)\n\n  # Integral approximation through averaging gradients.\n  avg_gradients = integral_approximation(gradients=total_gradients)\n\n  # Scale integrated gradients with respect to input.\n  integrated_gradients = (image - baseline) * avg_gradients\n\n  return integrated_gradients",
      "@tf.function\ndef one_batch(baseline, image, alpha_batch, target_class_idx):\n    # Generate interpolated inputs between baseline and input.\n    interpolated_path_input_batch = interpolate_images(baseline=baseline,\n                                                       image=image,\n                                                       alphas=alpha_batch)\n\n    # Compute gradients between model outputs and interpolated inputs.\n    gradient_batch = compute_gradients(images=interpolated_path_input_batch,\n                                       target_class_idx=target_class_idx)\n    return gradient_batch",
      "def plot_img_attributions(baseline,\n                          image,\n                          target_class_idx,\n                          m_steps=50,\n                          cmap=None,\n                          overlay_alpha=0.4):\n\n  attributions = integrated_gradients(baseline=baseline,\n                                      image=image,\n                                      target_class_idx=target_class_idx,\n                                      m_steps=m_steps)\n\n  # Sum of the attributions across color channels for visualization.\n  # The attribution mask shape is a grayscale image with height and width\n  # equal to the original image.\n  attribution_mask = tf.reduce_sum(tf.math.abs(attributions), axis=-1)\n\n  fig, axs = plt.subplots(nrows=2, ncols=2, squeeze=False, figsize=(8, 8))\n\n  axs[0, 0].set_title('Baseline image')\n  axs[0, 0].imshow(baseline)\n  axs[0, 0].axis('off')\n\n  axs[0, 1].set_title('Original image')\n  axs[0, 1].imshow(image)\n  axs[0, 1].axis('off')\n\n  axs[1, 0].set_title('Attribution mask')\n  axs[1, 0].imshow(attribution_mask, cmap=cmap)\n  axs[1, 0].axis('off')\n\n  axs[1, 1].set_title('Overlay')\n  axs[1, 1].imshow(attribution_mask, cmap=cmap)\n  axs[1, 1].imshow(image, alpha=overlay_alpha)\n  axs[1, 1].axis('off')\n\n  plt.tight_layout()\n  return fig"
    ]
  },
  {
    "url": "https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras",
    "title": "Multi-worker training with Keras\n      \n\n\n      \n      Stay organized with collections\n    \n\n      \n      Save and categorize content based on your preferences.",
    "code_snippets": [
      "import json\nimport os\nimport sys",
      "import tensorflow as tf",
      "%%writefile mnist_setup.py\n\nimport os\nimport tensorflow as tf\nimport numpy as np\n\ndef mnist_dataset(batch_size):\n  (x_train, y_train), _ = tf.keras.datasets.mnist.load_data()\n  # The `x` arrays are in uint8 and have values in the [0, 255] range.\n  # You need to convert them to float32 with values in the [0, 1] range.\n  x_train = x_train / np.float32(255)\n  y_train = y_train.astype(np.int64)\n  train_dataset = tf.data.Dataset.from_tensor_slices(\n      (x_train, y_train)).shuffle(60000).repeat().batch(batch_size)\n  return train_dataset\n\ndef build_and_compile_cnn_model():\n  model = tf.keras.Sequential([\n      tf.keras.layers.InputLayer(input_shape=(28, 28)),\n      tf.keras.layers.Reshape(target_shape=(28, 28, 1)),\n      tf.keras.layers.Conv2D(32, 3, activation='relu'),\n      tf.keras.layers.Flatten(),\n      tf.keras.layers.Dense(128, activation='relu'),\n      tf.keras.layers.Dense(10)\n  ])\n  model.compile(\n      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n      optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),\n      metrics=['accuracy'])\n  return model",
      "import mnist_setup\n\nbatch_size = 64\nsingle_worker_dataset = mnist_setup.mnist_dataset(batch_size)\nsingle_worker_model = mnist_setup.build_and_compile_cnn_model()\nsingle_worker_model.fit(single_worker_dataset, epochs=3, steps_per_epoch=70)",
      "strategy = tf.distribute.MultiWorkerMirroredStrategy()",
      "%%writefile main.py\n\nimport os\nimport json\n\nimport tensorflow as tf\nimport mnist_setup\n\nper_worker_batch_size = 64\ntf_config = json.loads(os.environ['TF_CONFIG'])\nnum_workers = len(tf_config['cluster']['worker'])\n\nstrategy = tf.distribute.MultiWorkerMirroredStrategy()\n\nglobal_batch_size = per_worker_batch_size * num_workers\nmulti_worker_dataset = mnist_setup.mnist_dataset(global_batch_size)\n\nwith strategy.scope():\n  # Model building/compiling need to be within `strategy.scope()`.\n  multi_worker_model = mnist_setup.build_and_compile_cnn_model()\n\n\nmulti_worker_model.fit(multi_worker_dataset, epochs=3, steps_per_epoch=70)",
      "import time\ntime.sleep(10)",
      "options = tf.data.Options()\noptions.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.OFF\n\nglobal_batch_size = 64\nmulti_worker_dataset = mnist_setup.mnist_dataset(batch_size=64)\ndataset_no_auto_shard = multi_worker_dataset.with_options(options)",
      "communication_options=tf.distribute.experimental.CommunicationOptions(implementation=tf.distribute.experimental.CommunicationImplementation.NCCL)",
      "model_path = '/tmp/keras-model'\n\ndef _is_chief(task_type, task_id):\n  # Note: there are two possible `TF_CONFIG` configurations.\n  #   1) In addition to `worker` tasks, a `chief` task type is use;\n  #      in this case, this function should be modified to\n  #      `return task_type == 'chief'`.\n  #   2) Only `worker` task type is used; in this case, worker 0 is\n  #      regarded as the chief. The implementation demonstrated here\n  #      is for this case.\n  # For the purpose of this Colab section, the `task_type` is `None` case\n  # is added because it is effectively run with only a single worker.\n  return (task_type == 'worker' and task_id == 0) or task_type is None\n\ndef _get_temp_dir(dirpath, task_id):\n  base_dirpath = 'workertemp_' + str(task_id)\n  temp_dir = os.path.join(dirpath, base_dirpath)\n  tf.io.gfile.makedirs(temp_dir)\n  return temp_dir\n\ndef write_filepath(filepath, task_type, task_id):\n  dirpath = os.path.dirname(filepath)\n  base = os.path.basename(filepath)\n  if not _is_chief(task_type, task_id):\n    dirpath = _get_temp_dir(dirpath, task_id)\n  return os.path.join(dirpath, base)\n\ntask_type, task_id = (strategy.cluster_resolver.task_type,\n                      strategy.cluster_resolver.task_id)\nwrite_model_path = write_filepath(model_path, task_type, task_id)",
      "if not _is_chief(task_type, task_id):\n  tf.io.gfile.rmtree(os.path.dirname(write_model_path))",
      "loaded_model = tf.keras.models.load_model(model_path)\n\n# Now that the model is restored, and can continue with the training.\nloaded_model.fit(single_worker_dataset, epochs=2, steps_per_epoch=20)",
      "checkpoint_dir = '/tmp/ckpt'\n\ncheckpoint = tf.train.Checkpoint(model=multi_worker_model)\nwrite_checkpoint_dir = write_filepath(checkpoint_dir, task_type, task_id)\ncheckpoint_manager = tf.train.CheckpointManager(\n    checkpoint, directory=write_checkpoint_dir, max_to_keep=1)",
      "checkpoint_manager.save()\nif not _is_chief(task_type, task_id):\n  tf.io.gfile.rmtree(write_checkpoint_dir)",
      "latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\ncheckpoint.restore(latest_checkpoint)\nmulti_worker_model.fit(multi_worker_dataset, epochs=2, steps_per_epoch=20)",
      "# Multi-worker training with `MultiWorkerMirroredStrategy`\n# and the `BackupAndRestore` callback. The training state \n# is backed up at epoch boundaries by default.\n\ncallbacks = [tf.keras.callbacks.BackupAndRestore(backup_dir='/tmp/backup')]\nwith strategy.scope():\n  multi_worker_model = mnist_setup.build_and_compile_cnn_model()\nmulti_worker_model.fit(multi_worker_dataset,\n                       epochs=3,\n                       steps_per_epoch=70,\n                       callbacks=callbacks)",
      "# The training state is backed up at epoch boundaries because `save_freq` is\n# set to `epoch`.\n\ncallbacks = [tf.keras.callbacks.BackupAndRestore(backup_dir='/tmp/backup')]\nwith strategy.scope():\n  multi_worker_model = mnist_setup.build_and_compile_cnn_model()\nmulti_worker_model.fit(multi_worker_dataset,\n                       epochs=3,\n                       steps_per_epoch=70,\n                       callbacks=callbacks)",
      "# The training state is backed up at every 30 steps because `save_freq` is set\n# to an integer value of `30`.\n\ncallbacks = [tf.keras.callbacks.BackupAndRestore(backup_dir='/tmp/backup', save_freq=30)]\nwith strategy.scope():\n  multi_worker_model = mnist_setup.build_and_compile_cnn_model()\nmulti_worker_model.fit(multi_worker_dataset,\n                       epochs=3,\n                       steps_per_epoch=70,\n                       callbacks=callbacks)"
    ]
  },
  {
    "url": "https://www.tensorflow.org/tutorials/load_data/pandas_dataframe",
    "title": "Load a pandas DataFrame\n      \n\n\n      \n      Stay organized with collections\n    \n\n      \n      Save and categorize content based on your preferences.",
    "code_snippets": [
      "import numpy as np\nimport pandas as pd\nimport tensorflow as tf\n\nSHUFFLE_BUFFER = 500\nBATCH_SIZE = 2",
      "csv_file = tf.keras.utils.get_file('heart.csv', 'https://storage.googleapis.com/download.tensorflow.org/data/heart.csv')",
      "tf.convert_to_tensor(numeric_features)",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1723791580.394635  189972 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723791580.398535  189972 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723791580.402304  189972 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723791580.406024  189972 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723791580.417906  189972 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723791580.423162  189972 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723791580.426583  189972 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723791580.430147  189972 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723791580.433654  189972 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723791580.437125  189972 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723791580.440588  189972 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723791580.443941  189972 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723791581.673556  189972 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723791581.675740  189972 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723791581.677839  189972 cuda_executor.cc:1015] successful NUMA node read from SysFS ha\n<tf.Tensor: shape=(303, 5), dtype=float64, numpy=\narray([[ 63. , 150. , 145. , 233. ,   2.3],\n       [ 67. , 108. , 160. , 286. ,   1.5],\n       [ 67. , 129. , 120. , 229. ,   2.6],\n       ...,\n       [ 65. , 127. , 135. , 254. ,   2.8],\n       [ 48. , 150. , 130. , 256. ,   0. ],\n       [ 63. , 154. , 150. , 407. ,   4. ]])>\nd negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723791581.679903  189972 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723791581.681941  189972 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723791581.683916  189972 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723791581.685875  189972 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723791581.687852  189972 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723791581.689776  189972 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723791581.691767  189972 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723791581.693745  189972 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723791581.695704  189972 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723791581.733646  189972 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723791581.735847  189972 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723791581.737865  189972 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723791581.739866  189972 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723791581.741853  189972 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723791581.743844  189972 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723791581.745817  189972 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723791581.747788  189972 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723791581.749827  189972 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723791581.752365  189972 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723791581.754754  189972 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723791581.757175  189972 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355",
      "normalizer = tf.keras.layers.Normalization(axis=-1)\nnormalizer.adapt(np.array(numeric_features))",
      "<tf.Tensor: shape=(3, 5), dtype=float32, numpy=\narray([[ 0.93384   ,  0.03480783,  0.74578166, -0.26008663,  1.0680454 ],\n       [ 1.3782114 , -1.7806157 ,  1.5923294 ,  0.75738776,  0.3802287 ],\n       [ 1.3782114 , -0.87290394, -0.6651312 , -0.33687717,  1.3259766 ]],\n      dtype=float32)>",
      "def get_basic_model():\n  model = tf.keras.Sequential([\n    normalizer,\n    tf.keras.layers.Dense(10, activation='relu'),\n    tf.keras.layers.Dense(10, activation='relu'),\n    tf.keras.layers.Dense(1)\n  ])\n\n  model.compile(optimizer='adam',\n                loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n                metrics=['accuracy'])\n  return model",
      "numeric_dataset = tf.data.Dataset.from_tensor_slices((numeric_features, target))\n\nfor row in numeric_dataset.take(3):\n  print(row)",
      "(<tf.Tensor: shape=(5,), dtype=float64, numpy=array([ 63. , 150. , 145. , 233. ,   2.3])>, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n(<tf.Tensor: shape=(5,), dtype=float64, numpy=array([ 67. , 108. , 160. , 286. ,   1.5])>, <tf.Tensor: shape=(), dtype=int64, numpy=1>)\n(<tf.Tensor: shape=(5,), dtype=float64, numpy=array([ 67. , 129. , 120. , 229. ,   2.6])>, <tf.Tensor: shape=(), dtype=int64, numpy=0>)",
      "numeric_features_dict = {key: value.to_numpy()[:, tf.newaxis] for key, value in dict(numeric_features).items()}\ntarget_array =  target.to_numpy()[:, tf.newaxis]",
      "numeric_dict_ds = tf.data.Dataset.from_tensor_slices((numeric_features_dict , target_array))",
      "({'age': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([63])>, 'thalach': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([150])>, 'trestbps': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([145])>, 'chol': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([233])>, 'oldpeak': <tf.Tensor: shape=(1,), dtype=float64, numpy=array([2.3])>}, <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>)\n({'age': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([67])>, 'thalach': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([108])>, 'trestbps': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([160])>, 'chol': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([286])>, 'oldpeak': <tf.Tensor: shape=(1,), dtype=float64, numpy=array([1.5])>}, <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>)\n({'age': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([67])>, 'thalach': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([129])>, 'trestbps': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([120])>, 'chol': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([229])>, 'oldpeak': <tf.Tensor: shape=(1,), dtype=float64, numpy=array([2.6])>}, <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>)",
      "class MyModel(tf.keras.Model):\n  def __init__(self):\n    # Create all the internal layers in init.\n    super().__init__()\n\n    self.normalizer = tf.keras.layers.Normalization(axis=-1)\n\n    self.seq = tf.keras.Sequential([\n      self.normalizer,\n      tf.keras.layers.Dense(10, activation='relu'),\n      tf.keras.layers.Dense(10, activation='relu'),\n      tf.keras.layers.Dense(1)\n    ])\n\n    self.concat = tf.keras.layers.Concatenate(axis=1)\n\n  def _stack(self, input_dict):\n    values = []\n    for key, value in sorted(input_dict.items()):\n      values.append(value)\n\n    return self.concat(values)\n\n  def adapt(self, inputs):\n    # Stack the inputs and `adapt` the normalization layer.\n    inputs = self._stack(inputs)\n    self.normalizer.adapt(inputs)\n\n  def call(self, inputs):\n    # Stack the inputs\n    inputs = self._stack(inputs)\n    # Run them through all the layers.\n    result = self.seq(inputs)\n\n    return result\n\nmodel = MyModel()\n\nmodel.adapt(numeric_features_dict)\n\nmodel.compile(optimizer='adam',\n              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n              metrics=['accuracy'],\n              run_eagerly=True)",
      "inputs = {}\nfor name, column in numeric_features.items():\n  inputs[name] = tf.keras.Input(\n      shape=(1,), name=name, dtype=tf.float32)\n\ninputs",
      "xs = [value for key, value in sorted(inputs.items())]\n\nconcat = tf.keras.layers.Concatenate(axis=1)\nx = concat(xs)\n\nnormalizer = tf.keras.layers.Normalization(axis=-1)\nnormalizer.adapt(np.concatenate([value for key, value in sorted(numeric_features_dict.items())], axis=1))\n\nx = normalizer(x)\nx = tf.keras.layers.Dense(10, activation='relu')(x)\nx = tf.keras.layers.Dense(10, activation='relu')(x)\nx = tf.keras.layers.Dense(1)(x)\n\nmodel = tf.keras.Model(inputs, x)\n\nmodel.compile(optimizer='adam',\n              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n              metrics=['accuracy'],\n              run_eagerly=True)",
      "tf.keras.utils.plot_model(model, rankdir=\"LR\", show_shapes=True,  show_layer_names=True)",
      "inputs = {}\nfor name, column in df.items():\n  if type(column[0]) == str:\n    dtype = tf.string\n  elif (name in categorical_feature_names or\n        name in binary_feature_names):\n    dtype = tf.int64\n  else:\n    dtype = tf.float32\n\n  inputs[name] = tf.keras.Input(shape=(1,), name=name, dtype=dtype)",
      "normalizer = tf.keras.layers.Normalization(axis=-1)\nnormalizer.adapt(np.concatenate([value for key, value in sorted(numeric_features_dict.items())], axis=1))",
      "numeric_inputs = []\nfor name in numeric_feature_names:\n  numeric_inputs.append(inputs[name])\n\nnumeric_inputs = tf.keras.layers.Concatenate(axis=-1)(numeric_inputs)\nnumeric_normalized = normalizer(numeric_inputs)\n\npreprocessed.append(numeric_normalized)\n\npreprocessed",
      "vocab = ['a','b','c']\nlookup = tf.keras.layers.StringLookup(vocabulary=vocab, output_mode='one_hot')\nlookup(['c','a','a','b','zzz'])",
      "<tf.Tensor: shape=(5, 4), dtype=int64, numpy=\narray([[0, 0, 0, 1],\n       [0, 1, 0, 0],\n       [0, 1, 0, 0],\n       [0, 0, 1, 0],\n       [1, 0, 0, 0]])>",
      "vocab = [1,4,7,99]\nlookup = tf.keras.layers.IntegerLookup(vocabulary=vocab, output_mode='one_hot')\n\nlookup([-1,4,1])",
      "<tf.Tensor: shape=(3, 5), dtype=int64, numpy=\narray([[1, 0, 0, 0, 0],\n       [0, 0, 1, 0, 0],\n       [0, 1, 0, 0, 0]])>",
      "for name in categorical_feature_names:\n  vocab = sorted(set(df[name]))\n  print(f'name: {name}')\n  print(f'vocab: {vocab}\\n')\n\n  if type(vocab[0]) is str:\n    lookup = tf.keras.layers.StringLookup(vocabulary=vocab, output_mode='one_hot')\n  else:\n    lookup = tf.keras.layers.IntegerLookup(vocabulary=vocab, output_mode='one_hot')\n\n  x = inputs[name]\n  x = lookup(x)\n  preprocessed.append(x)",
      "preprocessed_result = tf.keras.layers.Concatenate(axis=1)(preprocessed)\npreprocessed_result",
      "preprocessor = tf.keras.Model(inputs, preprocessed_result)",
      "tf.keras.utils.plot_model(preprocessor, rankdir=\"LR\", show_shapes=True,  show_layer_names=True)",
      "<tf.Tensor: shape=(1, 33), dtype=float32, numpy=\narray([[  1.       ,   1.       ,   0.       ,   0.93384  ,  -1.8534899,\n        123.75736  ,   3.6224306,  -7.3077087,   0.       ,   0.       ,\n\n          1.       ,   0.       ,   0.       ,   0.       ,   0.       ,\n          0.       ,   0.       ,   1.       ,   0.       ,   0.       ,\n          0.       ,   1.       ,   0.       ,   0.       ,   0.       ,\n          1.       ,   0.       ,   0.       ,   0.       ,   1.       ,\n          0.       ,   0.       ,   0.       ]], dtype=float32)>",
      "body = tf.keras.Sequential([\n  tf.keras.layers.Dense(10, activation='relu'),\n  tf.keras.layers.Dense(10, activation='relu'),\n  tf.keras.layers.Dense(1)\n])",
      "model = tf.keras.Model(inputs, result)\n\nmodel.compile(optimizer='adam',\n                loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n                metrics=['accuracy'])",
      "tf.keras.utils.plot_model(model, show_shapes=True,  show_layer_names=True)",
      "ds = tf.data.Dataset.from_tensor_slices((\n    dict(df),\n    target\n))\n\nds = ds.batch(BATCH_SIZE)",
      "import pprint\n\nfor x, y in ds.take(1):\n  pprint.pprint(x)\n  print()\n  print(y)",
      "{'age': <tf.Tensor: shape=(2,), dtype=int64, numpy=array([63, 67])>,\n 'ca': <tf.Tensor: shape=(2,), dtype=int64, numpy=array([0, 3])>,\n 'chol': <tf.Tensor: shape=(2,), dtype=int64, numpy=array([233, 286])>,\n 'cp': <tf.Tensor: shape=(2,), dtype=int64, numpy=array([1, 4])>,\n 'exang': <tf.Tensor: shape=(2,), dtype=int64, numpy=array([0, 1])>,\n 'fbs': <tf.Tensor: shape=(2,), dtype=int64, numpy=array([1, 0])>,\n 'oldpeak': <tf.Tensor: shape=(2,), dtype=float64, numpy=array([2.3, 1.5])>,\n 'restecg': <tf.Tensor: shape=(2,), dtype=int64, numpy=array([2, 2])>,\n 'sex': <tf.Tensor: shape=(2,), dtype=int64, numpy=array([1, 1])>,\n 'slope': <tf.Tensor: shape=(2,), dtype=int64, numpy=array([3, 2])>,\n 'thal': <tf.Tensor: shape=(2,), dtype=string, numpy=array([b'fixed', b'normal'], dtype=object)>,\n 'thalach': <tf.Tensor: shape=(2,), dtype=int64, numpy=array([150, 108])>,\n 'trestbps': <tf.Tensor: shape=(2,), dtype=int64, numpy=array([145, 160])>}\n\ntf.Tensor([0 1], shape=(2,), dtype=int64)"
    ]
  },
  {
    "url": "https://www.tensorflow.org/tutorials/images/transfer_learning_with_hub",
    "title": "Transfer learning with TensorFlow Hub\n      \n\n\n      \n      Stay organized with collections\n    \n\n      \n      Save and categorize content based on your preferences.",
    "code_snippets": [
      "import numpy as np\nimport time\n\nimport PIL.Image as Image\nimport matplotlib.pylab as plt\n\nimport tensorflow as tf\nimport tensorflow_hub as hub\n\nimport datetime\n\n%load_ext tensorboard",
      "IMAGE_SHAPE = (224, 224)\n\nclassifier = tf.keras.Sequential([\n    hub.KerasLayer(classifier_model, input_shape=IMAGE_SHAPE+(3,))\n])",
      "grace_hopper = tf.keras.utils.get_file('image.jpg','https://storage.googleapis.com/download.tensorflow.org/example_images/grace_hopper.jpg')\ngrace_hopper = Image.open(grace_hopper).resize(IMAGE_SHAPE)\ngrace_hopper",
      "predicted_class = tf.math.argmax(result[0], axis=-1)\npredicted_class",
      "labels_path = tf.keras.utils.get_file('ImageNetLabels.txt','https://storage.googleapis.com/download.tensorflow.org/data/ImageNetLabels.txt')\nimagenet_labels = np.array(open(labels_path).read().splitlines())",
      "import pathlib\n\ndata_file = tf.keras.utils.get_file(\n  'flower_photos.tgz',\n  'https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz',\n  cache_dir='.',\n   extract=True)\n\ndata_root = pathlib.Path(data_file).with_suffix('')",
      "batch_size = 32\nimg_height = 224\nimg_width = 224\n\ntrain_ds = tf.keras.utils.image_dataset_from_directory(\n  str(data_root),\n  validation_split=0.2,\n  subset=\"training\",\n  seed=123,\n  image_size=(img_height, img_width),\n  batch_size=batch_size\n)\n\nval_ds = tf.keras.utils.image_dataset_from_directory(\n  str(data_root),\n  validation_split=0.2,\n  subset=\"validation\",\n  seed=123,\n  image_size=(img_height, img_width),\n  batch_size=batch_size\n)",
      "class_names = np.array(train_ds.class_names)\nprint(class_names)",
      "normalization_layer = tf.keras.layers.Rescaling(1./255)\ntrain_ds = train_ds.map(lambda x, y: (normalization_layer(x), y)) # Where x\u2014images, y\u2014labels.\nval_ds = val_ds.map(lambda x, y: (normalization_layer(x), y)) # Where x\u2014images, y\u2014labels.",
      "AUTOTUNE = tf.data.AUTOTUNE\ntrain_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\nval_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)",
      "predicted_class_names = imagenet_labels[tf.math.argmax(result_batch, axis=-1)]\npredicted_class_names",
      "num_classes = len(class_names)\n\nmodel = tf.keras.Sequential([\n  feature_extractor_layer,\n  tf.keras.layers.Dense(num_classes)\n])\n\nmodel.summary()",
      "model.compile(\n  optimizer=tf.keras.optimizers.Adam(),\n  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n  metrics=['acc'])\n\nlog_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\ntensorboard_callback = tf.keras.callbacks.TensorBoard(\n    log_dir=log_dir,\n    histogram_freq=1) # Enable histogram computation for every epoch.",
      "predicted_batch = model.predict(image_batch)\npredicted_id = tf.math.argmax(predicted_batch, axis=-1)\npredicted_label_batch = class_names[predicted_id]\nprint(predicted_label_batch)",
      "reloaded = tf.keras.models.load_model(export_path)",
      "reloaded_predicted_id = tf.math.argmax(reloaded_result_batch, axis=-1)\nreloaded_predicted_label_batch = class_names[reloaded_predicted_id]\nprint(reloaded_predicted_label_batch)"
    ]
  },
  {
    "url": "https://www.tensorflow.org/tutorials/distribute/multi_worker_with_estimator",
    "title": "Multi-worker training with Estimator\n      \n\n\n      \n      Stay organized with collections\n    \n\n      \n      Save and categorize content based on your preferences.",
    "code_snippets": [
      "import tensorflow_datasets as tfds\nimport tensorflow as tf\n\nimport os, json",
      "tf.compat.v1.disable_eager_execution()",
      "BUFFER_SIZE = 10000\nBATCH_SIZE = 64\n\ndef input_fn(mode, input_context=None):\n  datasets, info = tfds.load(name='mnist',\n                                with_info=True,\n                                as_supervised=True)\n  mnist_dataset = (datasets['train'] if mode == tf.estimator.ModeKeys.TRAIN else\n                   datasets['test'])\n\n  def scale(image, label):\n    image = tf.cast(image, tf.float32)\n    image /= 255\n    return image, label\n\n  if input_context:\n    mnist_dataset = mnist_dataset.shard(input_context.num_input_pipelines,\n                                        input_context.input_pipeline_id)\n  return mnist_dataset.map(scale).cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)",
      "LEARNING_RATE = 1e-4\ndef model_fn(features, labels, mode):\n  model = tf.keras.Sequential([\n      tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(28, 28, 1)),\n      tf.keras.layers.MaxPooling2D(),\n      tf.keras.layers.Flatten(),\n      tf.keras.layers.Dense(64, activation='relu'),\n      tf.keras.layers.Dense(10)\n  ])\n  logits = model(features, training=False)\n\n  if mode == tf.estimator.ModeKeys.PREDICT:\n    predictions = {'logits': logits}\n    return tf.estimator.EstimatorSpec(labels=labels, predictions=predictions)\n\n  optimizer = tf.compat.v1.train.GradientDescentOptimizer(\n      learning_rate=LEARNING_RATE)\n  loss = tf.keras.losses.SparseCategoricalCrossentropy(\n      from_logits=True, reduction=tf.keras.losses.Reduction.NONE)(labels, logits)\n  loss = tf.reduce_sum(loss) * (1. / BATCH_SIZE)\n  if mode == tf.estimator.ModeKeys.EVAL:\n    return tf.estimator.EstimatorSpec(mode, loss=loss)\n\n  return tf.estimator.EstimatorSpec(\n      mode=mode,\n      loss=loss,\n      train_op=optimizer.minimize(\n          loss, tf.compat.v1.train.get_or_create_global_step()))",
      "strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()",
      "config = tf.estimator.RunConfig(train_distribute=strategy)\n\nclassifier = tf.estimator.Estimator(\n    model_fn=model_fn, model_dir='/tmp/multiworker', config=config)\ntf.estimator.train_and_evaluate(\n    classifier,\n    train_spec=tf.estimator.TrainSpec(input_fn=input_fn),\n    eval_spec=tf.estimator.EvalSpec(input_fn=input_fn)\n)"
    ]
  },
  {
    "url": "https://www.tensorflow.org/tutorials/distribute/custom_training",
    "title": "Custom training with tf.distribute.Strategy\n      \n\n\n      \n      Stay organized with collections\n    \n\n      \n      Save and categorize content based on your preferences.",
    "code_snippets": [
      "# Import TensorFlow\nimport tensorflow as tf\n\n# Helper libraries\nimport numpy as np\nimport os\n\nprint(tf.__version__)",
      "fashion_mnist = tf.keras.datasets.fashion_mnist\n\n(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n\n# Add a dimension to the array -> new shape == (28, 28, 1)\n# This is done because the first layer in our model is a convolutional\n# layer and it requires a 4D input (batch_size, height, width, channels).\n# batch_size dimension will be added later on.\ntrain_images = train_images[..., None]\ntest_images = test_images[..., None]\n\n# Scale the images to the [0, 1] range.\ntrain_images = train_images / np.float32(255)\ntest_images = test_images / np.float32(255)",
      "# If the list of devices is not specified in\n# `tf.distribute.MirroredStrategy` constructor, they will be auto-detected.\nstrategy = tf.distribute.MirroredStrategy()",
      "train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels)).shuffle(BUFFER_SIZE).batch(GLOBAL_BATCH_SIZE)\ntest_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels)).batch(GLOBAL_BATCH_SIZE)\n\ntrain_dist_dataset = strategy.experimental_distribute_dataset(train_dataset)\ntest_dist_dataset = strategy.experimental_distribute_dataset(test_dataset)",
      "def create_model():\n  regularizer = tf.keras.regularizers.L2(1e-5)\n  model = tf.keras.Sequential([\n      tf.keras.layers.Conv2D(32, 3,\n                             activation='relu',\n                             kernel_regularizer=regularizer),\n      tf.keras.layers.MaxPooling2D(),\n      tf.keras.layers.Conv2D(64, 3,\n                             activation='relu',\n                             kernel_regularizer=regularizer),\n      tf.keras.layers.MaxPooling2D(),\n      tf.keras.layers.Flatten(),\n      tf.keras.layers.Dense(64,\n                            activation='relu',\n                            kernel_regularizer=regularizer),\n      tf.keras.layers.Dense(10, kernel_regularizer=regularizer)\n    ])\n\n  return model",
      "with strategy.scope():\n  # Set reduction to `NONE` so you can do the reduction yourself.\n  loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n      from_logits=True,\n      reduction=tf.keras.losses.Reduction.NONE)\n  def compute_loss(labels, predictions, model_losses):\n    per_example_loss = loss_object(labels, predictions)\n    loss = tf.nn.compute_average_loss(per_example_loss)\n    if model_losses:\n      loss += tf.nn.scale_regularization_loss(tf.add_n(model_losses))\n    return loss",
      "with strategy.scope():\n  test_loss = tf.keras.metrics.Mean(name='test_loss')\n\n  train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n      name='train_accuracy')\n  test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n      name='test_accuracy')",
      "# A model, an optimizer, and a checkpoint must be created under `strategy.scope`.\nwith strategy.scope():\n  model = create_model()\n\n  optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n\n  checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=model)",
      "def train_step(inputs):\n  images, labels = inputs\n\n  with tf.GradientTape() as tape:\n    predictions = model(images, training=True)\n    loss = compute_loss(labels, predictions, model.losses)\n\n  gradients = tape.gradient(loss, model.trainable_variables)\n  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n\n  train_accuracy.update_state(labels, predictions)\n  return loss\n\ndef test_step(inputs):\n  images, labels = inputs\n\n  predictions = model(images, training=False)\n  t_loss = loss_object(labels, predictions)\n\n  test_loss.update_state(t_loss)\n  test_accuracy.update_state(labels, predictions)",
      "# `run` replicates the provided computation and runs it\n# with the distributed input.\n@tf.function\ndef distributed_train_step(dataset_inputs):\n  per_replica_losses = strategy.run(train_step, args=(dataset_inputs,))\n  return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses,\n                         axis=None)\n\n@tf.function\ndef distributed_test_step(dataset_inputs):\n  return strategy.run(test_step, args=(dataset_inputs,))\n\nfor epoch in range(EPOCHS):\n  # TRAIN LOOP\n  total_loss = 0.0\n  num_batches = 0\n  for x in train_dist_dataset:\n    total_loss += distributed_train_step(x)\n    num_batches += 1\n  train_loss = total_loss / num_batches\n\n  # TEST LOOP\n  for x in test_dist_dataset:\n    distributed_test_step(x)\n\n  if epoch % 2 == 0:\n    checkpoint.save(checkpoint_prefix)\n\n  template = (\"Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, \"\n              \"Test Accuracy: {}\")\n  print(template.format(epoch + 1, train_loss,\n                         train_accuracy.result() * 100, test_loss.result(),\n                         test_accuracy.result() * 100))\n\n  test_loss.reset_states()\n  train_accuracy.reset_states()\n  test_accuracy.reset_states()",
      "eval_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n      name='eval_accuracy')\n\nnew_model = create_model()\nnew_optimizer = tf.keras.optimizers.Adam()\n\ntest_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels)).batch(GLOBAL_BATCH_SIZE)",
      "@tf.function\ndef eval_step(images, labels):\n  predictions = new_model(images, training=False)\n  eval_accuracy(labels, predictions)",
      "checkpoint = tf.train.Checkpoint(optimizer=new_optimizer, model=new_model)\ncheckpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n\nfor images, labels in test_dataset:\n  eval_step(images, labels)\n\nprint('Accuracy after restoring the saved model without strategy: {}'.format(\n    eval_accuracy.result() * 100))",
      "@tf.function\ndef distributed_train_epoch(dataset):\n  total_loss = 0.0\n  num_batches = 0\n  for x in dataset:\n    per_replica_losses = strategy.run(train_step, args=(x,))\n    total_loss += strategy.reduce(\n      tf.distribute.ReduceOp.SUM, per_replica_losses, axis=None)\n    num_batches += 1\n  return total_loss / tf.cast(num_batches, dtype=tf.float32)\n\nfor epoch in range(EPOCHS):\n  train_loss = distributed_train_epoch(train_dist_dataset)\n\n  template = (\"Epoch {}, Loss: {}, Accuracy: {}\")\n  print(template.format(epoch + 1, train_loss, train_accuracy.result() * 100))\n\n  train_accuracy.reset_states()"
    ]
  },
  {
    "url": "https://www.tensorflow.org/tutorials/quickstart/advanced",
    "title": "TensorFlow 2 quickstart for experts\n      \n\n\n      \n      Stay organized with collections\n    \n\n      \n      Save and categorize content based on your preferences.",
    "code_snippets": [
      "import tensorflow as tf\nprint(\"TensorFlow version:\", tf.__version__)\n\nfrom tensorflow.keras.layers import Dense, Flatten, Conv2D\nfrom tensorflow.keras import Model",
      "mnist = tf.keras.datasets.mnist\n\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0\n\n# Add a channels dimension\nx_train = x_train[..., tf.newaxis].astype(\"float32\")\nx_test = x_test[..., tf.newaxis].astype(\"float32\")",
      "train_ds = tf.data.Dataset.from_tensor_slices(\n    (x_train, y_train)).shuffle(10000).batch(32)\n\ntest_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)",
      "class MyModel(Model):\n  def __init__(self):\n    super().__init__()\n    self.conv1 = Conv2D(32, 3, activation='relu')\n    self.flatten = Flatten()\n    self.d1 = Dense(128, activation='relu')\n    self.d2 = Dense(10)\n\n  def call(self, x):\n    x = self.conv1(x)\n    x = self.flatten(x)\n    x = self.d1(x)\n    return self.d2(x)\n\n# Create an instance of the model\nmodel = MyModel()",
      "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n\noptimizer = tf.keras.optimizers.Adam()",
      "train_loss = tf.keras.metrics.Mean(name='train_loss')\ntrain_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n\ntest_loss = tf.keras.metrics.Mean(name='test_loss')\ntest_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')",
      "@tf.function\ndef train_step(images, labels):\n  with tf.GradientTape() as tape:\n    # training=True is only needed if there are layers with different\n    # behavior during training versus inference (e.g. Dropout).\n    predictions = model(images, training=True)\n    loss = loss_object(labels, predictions)\n  gradients = tape.gradient(loss, model.trainable_variables)\n  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n\n  train_loss(loss)\n  train_accuracy(labels, predictions)",
      "@tf.function\ndef test_step(images, labels):\n  # training=False is only needed if there are layers with different\n  # behavior during training versus inference (e.g. Dropout).\n  predictions = model(images, training=False)\n  t_loss = loss_object(labels, predictions)\n\n  test_loss(t_loss)\n  test_accuracy(labels, predictions)"
    ]
  },
  {
    "url": "https://www.tensorflow.org/tutorials/images/classification",
    "title": "Image classification\n      \n\n\n      \n      Stay organized with collections\n    \n\n      \n      Save and categorize content based on your preferences.",
    "code_snippets": [
      "import matplotlib.pyplot as plt\nimport numpy as np\nimport PIL\nimport tensorflow as tf\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Sequential",
      "import pathlib\n\ndataset_url = \"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\"\ndata_dir = tf.keras.utils.get_file('flower_photos.tar', origin=dataset_url, extract=True)\ndata_dir = pathlib.Path(data_dir).with_suffix('')",
      "train_ds = tf.keras.utils.image_dataset_from_directory(\n  data_dir,\n  validation_split=0.2,\n  subset=\"training\",\n  seed=123,\n  image_size=(img_height, img_width),\n  batch_size=batch_size)",
      "val_ds = tf.keras.utils.image_dataset_from_directory(\n  data_dir,\n  validation_split=0.2,\n  subset=\"validation\",\n  seed=123,\n  image_size=(img_height, img_width),\n  batch_size=batch_size)",
      "class_names = train_ds.class_names\nprint(class_names)",
      "import matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 10))\nfor images, labels in train_ds.take(1):\n  for i in range(9):\n    ax = plt.subplot(3, 3, i + 1)\n    plt.imshow(images[i].numpy().astype(\"uint8\"))\n    plt.title(class_names[labels[i]])\n    plt.axis(\"off\")",
      "AUTOTUNE = tf.data.AUTOTUNE\n\ntrain_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\nval_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)",
      "model.compile(optimizer='adam',\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])",
      "model.compile(optimizer='adam',\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])",
      "sunflower_url = \"https://storage.googleapis.com/download.tensorflow.org/example_images/592px-Red_sunflower.jpg\"\nsunflower_path = tf.keras.utils.get_file('Red_sunflower', origin=sunflower_url)\n\nimg = tf.keras.utils.load_img(\n    sunflower_path, target_size=(img_height, img_width)\n)\nimg_array = tf.keras.utils.img_to_array(img)\nimg_array = tf.expand_dims(img_array, 0) # Create a batch\n\npredictions = model.predict(img_array)\nscore = tf.nn.softmax(predictions[0])\n\nprint(\n    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n    .format(class_names[np.argmax(score)], 100 * np.max(score))\n)",
      "# Convert the model.\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\ntflite_model = converter.convert()\n\n# Save the model.\nwith open('model.tflite', 'wb') as f:\n  f.write(tflite_model)",
      "TF_MODEL_FILE_PATH = 'model.tflite' # The default path to the saved TensorFlow Lite model\n\ninterpreter = tf.lite.Interpreter(model_path=TF_MODEL_FILE_PATH)",
      "classify_lite = interpreter.get_signature_runner('serving_default')\nclassify_lite",
      "predictions_lite = classify_lite(sequential_1_input=img_array)['outputs']\nscore_lite = tf.nn.softmax(predictions_lite)"
    ]
  },
  {
    "url": "https://www.tensorflow.org/tutorials/audio/transfer_learning_audio",
    "title": "Transfer learning with YAMNet for environmental sound classification\n      \n\n\n      \n      Stay organized with collections\n    \n\n      \n      Save and categorize content based on your preferences.",
    "code_snippets": [
      "import os\n\nfrom IPython import display\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport tensorflow_io as tfio",
      "testing_wav_file_name = tf.keras.utils.get_file('miaow_16k.wav',\n                                                'https://storage.googleapis.com/audioset/miaow_16k.wav',\n                                                cache_dir='./',\n                                                cache_subdir='test_data')\n\nprint(testing_wav_file_name)",
      "# Utility functions for loading audio files and making sure the sample rate is correct.\n\n@tf.function\ndef load_wav_16k_mono(filename):\n    \"\"\" Load a WAV file, convert it to a float tensor, resample to 16 kHz single-channel audio. \"\"\"\n    file_contents = tf.io.read_file(filename)\n    wav, sample_rate = tf.audio.decode_wav(\n          file_contents,\n          desired_channels=1)\n    wav = tf.squeeze(wav, axis=-1)\n    sample_rate = tf.cast(sample_rate, dtype=tf.int64)\n    wav = tfio.audio.resample(wav, rate_in=sample_rate, rate_out=16000)\n    return wav",
      "class_map_path = yamnet_model.class_map_path().numpy().decode('utf-8')\nclass_names =list(pd.read_csv(class_map_path)['display_name'])\n\nfor name in class_names[:20]:\n  print(name)\nprint('...')",
      "scores, embeddings, spectrogram = yamnet_model(testing_wav_data)\nclass_scores = tf.reduce_mean(scores, axis=0)\ntop_class = tf.math.argmax(class_scores)\ninferred_class = class_names[top_class]\n\nprint(f'The main sound is: {inferred_class}')\nprint(f'The embeddings shape: {embeddings.shape}')",
      "_ = tf.keras.utils.get_file('esc-50.zip',\n                        'https://github.com/karoldvl/ESC-50/archive/master.zip',\n                        cache_dir='./',\n                        cache_subdir='datasets',\n                        extract=True)",
      "filenames = filtered_pd['filename']\ntargets = filtered_pd['target']\nfolds = filtered_pd['fold']\n\nmain_ds = tf.data.Dataset.from_tensor_slices((filenames, targets, folds))\nmain_ds.element_spec",
      "(TensorSpec(shape=(), dtype=tf.string, name=None),\n TensorSpec(shape=(), dtype=tf.int64, name=None),\n TensorSpec(shape=(), dtype=tf.int64, name=None))",
      "def load_wav_for_map(filename, label, fold):\n  return load_wav_16k_mono(filename), label, fold\n\nmain_ds = main_ds.map(load_wav_for_map)\nmain_ds.element_spec",
      "WARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\nWARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\n(TensorSpec(shape=<unknown>, dtype=tf.float32, name=None),\n TensorSpec(shape=(), dtype=tf.int64, name=None),\n TensorSpec(shape=(), dtype=tf.int64, name=None))",
      "# applies the embedding extraction model to a wav data\ndef extract_embedding(wav_data, label, fold):\n  ''' run YAMNet to extract embedding from the wav data '''\n  scores, embeddings, spectrogram = yamnet_model(wav_data)\n  num_embeddings = tf.shape(embeddings)[0]\n  return (embeddings,\n            tf.repeat(label, num_embeddings),\n            tf.repeat(fold, num_embeddings))\n\n# extract embedding\nmain_ds = main_ds.map(extract_embedding).unbatch()\nmain_ds.element_spec",
      "(TensorSpec(shape=(1024,), dtype=tf.float32, name=None),\n TensorSpec(shape=(), dtype=tf.int64, name=None),\n TensorSpec(shape=(), dtype=tf.int64, name=None))",
      "cached_ds = main_ds.cache()\ntrain_ds = cached_ds.filter(lambda embedding, label, fold: fold < 4)\nval_ds = cached_ds.filter(lambda embedding, label, fold: fold == 4)\ntest_ds = cached_ds.filter(lambda embedding, label, fold: fold == 5)\n\n# remove the folds column now that it's not needed anymore\nremove_fold_column = lambda embedding, label, fold: (embedding, label)\n\ntrain_ds = train_ds.map(remove_fold_column)\nval_ds = val_ds.map(remove_fold_column)\ntest_ds = test_ds.map(remove_fold_column)\n\ntrain_ds = train_ds.cache().shuffle(1000).batch(32).prefetch(tf.data.AUTOTUNE)\nval_ds = val_ds.cache().batch(32).prefetch(tf.data.AUTOTUNE)\ntest_ds = test_ds.cache().batch(32).prefetch(tf.data.AUTOTUNE)",
      "my_model = tf.keras.Sequential([\n    tf.keras.layers.Input(shape=(1024), dtype=tf.float32,\n                          name='input_embedding'),\n    tf.keras.layers.Dense(512, activation='relu'),\n    tf.keras.layers.Dense(len(my_classes))\n], name='my_model')\n\nmy_model.summary()",
      "my_model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n                 optimizer=\"adam\",\n                 metrics=['accuracy'])\n\ncallback = tf.keras.callbacks.EarlyStopping(monitor='loss',\n                                            patience=3,\n                                            restore_best_weights=True)",
      "class ReduceMeanLayer(tf.keras.layers.Layer):\n  def __init__(self, axis=0, **kwargs):\n    super(ReduceMeanLayer, self).__init__(**kwargs)\n    self.axis = axis\n\n  def call(self, input):\n    return tf.math.reduce_mean(input, axis=self.axis)",
      "saved_model_path = './dogs_and_cats_yamnet'\n\ninput_segment = tf.keras.layers.Input(shape=(), dtype=tf.float32, name='audio')\nembedding_extraction_layer = hub.KerasLayer(yamnet_model_handle,\n                                            trainable=False, name='yamnet')\n_, embeddings_output, _ = embedding_extraction_layer(input_segment)\nserving_outputs = my_model(embeddings_output)\nserving_outputs = ReduceMeanLayer(axis=0, name='classifier')(serving_outputs)\nserving_model = tf.keras.Model(input_segment, serving_outputs)\nserving_model.save(saved_model_path, include_optimizer=False)",
      "tf.keras.utils.plot_model(serving_model)",
      "reloaded_model = tf.saved_model.load(saved_model_path)",
      "reloaded_results = reloaded_model(testing_wav_data)\ncat_or_dog = my_classes[tf.math.argmax(reloaded_results)]\nprint(f'The main sound is: {cat_or_dog}')",
      "serving_results = reloaded_model.signatures['serving_default'](testing_wav_data)\ncat_or_dog = my_classes[tf.math.argmax(serving_results['classifier'])]\nprint(f'The main sound is: {cat_or_dog}')",
      "# Run the model, check the output.\nscores, embeddings, spectrogram = yamnet_model(waveform)\nclass_scores = tf.reduce_mean(scores, axis=0)\ntop_class = tf.math.argmax(class_scores)\ninferred_class = class_names[top_class]\ntop_score = class_scores[top_class]\nprint(f'[YAMNet] The main sound is: {inferred_class} ({top_score})')\n\nreloaded_results = reloaded_model(waveform)\nyour_top_class = tf.math.argmax(reloaded_results)\nyour_inferred_class = my_classes[your_top_class]\nclass_probabilities = tf.nn.softmax(reloaded_results, axis=-1)\nyour_top_score = class_probabilities[your_top_class]\nprint(f'[Your model] The main sound is: {your_inferred_class} ({your_top_score})')"
    ]
  },
  {
    "url": "https://www.tensorflow.org/tutorials/reinforcement_learning/actor_critic",
    "title": "Playing CartPole with the Actor-Critic method\n      \n\n\n      \n      Stay organized with collections\n    \n\n      \n      Save and categorize content based on your preferences.",
    "code_snippets": [
      "import collections\nimport gym\nimport numpy as np\nimport statistics\nimport tensorflow as tf\nimport tqdm\n\nfrom matplotlib import pyplot as plt\nfrom tensorflow.keras import layers\nfrom typing import Any, List, Sequence, Tuple\n\n\n# Create the environment\nenv = gym.make(\"CartPole-v1\")\n\n# Set seed for experiment reproducibility\nseed = 42\ntf.random.set_seed(seed)\nnp.random.seed(seed)\n\n# Small epsilon value for stabilizing division operations\neps = np.finfo(np.float32).eps.item()",
      "class ActorCritic(tf.keras.Model):\n  \"\"\"Combined actor-critic network.\"\"\"\n\n  def __init__(\n      self,\n      num_actions: int,\n      num_hidden_units: int):\n    \"\"\"Initialize.\"\"\"\n    super().__init__()\n\n    self.common = layers.Dense(num_hidden_units, activation=\"relu\")\n    self.actor = layers.Dense(num_actions)\n    self.critic = layers.Dense(1)\n\n  def call(self, inputs: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor]:\n    x = self.common(inputs)\n    return self.actor(x), self.critic(x)",
      "# Wrap Gym's `env.step` call as an operation in a TensorFlow function.\n# This would allow it to be included in a callable TensorFlow graph.\n\n@tf.numpy_function(Tout=[tf.float32, tf.int32, tf.int32])\ndef env_step(action: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n  \"\"\"Returns state, reward and done flag given an action.\"\"\"\n\n  state, reward, done, truncated, info = env.step(action)\n  return (state.astype(np.float32),\n          np.array(reward, np.int32),\n          np.array(done, np.int32))",
      "def run_episode(\n    initial_state: tf.Tensor,\n    model: tf.keras.Model,\n    max_steps: int) -> Tuple[tf.Tensor, tf.Tensor, tf.Tensor]:\n  \"\"\"Runs a single episode to collect training data.\"\"\"\n\n  action_probs = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n  values = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n  rewards = tf.TensorArray(dtype=tf.int32, size=0, dynamic_size=True)\n\n  initial_state_shape = initial_state.shape\n  state = initial_state\n\n  for t in tf.range(max_steps):\n    # Convert state into a batched tensor (batch size = 1)\n    state = tf.expand_dims(state, 0)\n\n    # Run the model and to get action probabilities and critic value\n    action_logits_t, value = model(state)\n\n    # Sample next action from the action probability distribution\n    action = tf.random.categorical(action_logits_t, 1)[0, 0]\n    action_probs_t = tf.nn.softmax(action_logits_t)\n\n    # Store critic values\n    values = values.write(t, tf.squeeze(value))\n\n    # Store log probability of the action chosen\n    action_probs = action_probs.write(t, action_probs_t[0, action])\n\n    # Apply action to the environment to get next state and reward\n    state, reward, done = env_step(action)\n    state.set_shape(initial_state_shape)\n\n    # Store reward\n    rewards = rewards.write(t, reward)\n\n    if tf.cast(done, tf.bool):\n      break\n\n  action_probs = action_probs.stack()\n  values = values.stack()\n  rewards = rewards.stack()\n\n  return action_probs, values, rewards",
      "def get_expected_return(\n    rewards: tf.Tensor,\n    gamma: float,\n    standardize: bool = True) -> tf.Tensor:\n  \"\"\"Compute expected returns per timestep.\"\"\"\n\n  n = tf.shape(rewards)[0]\n  returns = tf.TensorArray(dtype=tf.float32, size=n)\n\n  # Start from the end of `rewards` and accumulate reward sums\n  # into the `returns` array\n  rewards = tf.cast(rewards[::-1], dtype=tf.float32)\n  discounted_sum = tf.constant(0.0)\n  discounted_sum_shape = discounted_sum.shape\n  for i in tf.range(n):\n    reward = rewards[i]\n    discounted_sum = reward + gamma * discounted_sum\n    discounted_sum.set_shape(discounted_sum_shape)\n    returns = returns.write(i, discounted_sum)\n  returns = returns.stack()[::-1]\n\n  if standardize:\n    returns = ((returns - tf.math.reduce_mean(returns)) /\n               (tf.math.reduce_std(returns) + eps))\n\n  return returns",
      "huber_loss = tf.keras.losses.Huber(reduction=tf.keras.losses.Reduction.SUM)\n\ndef compute_loss(\n    action_probs: tf.Tensor,\n    values: tf.Tensor,\n    returns: tf.Tensor) -> tf.Tensor:\n  \"\"\"Computes the combined Actor-Critic loss.\"\"\"\n\n  advantage = returns - values\n\n  action_log_probs = tf.math.log(action_probs)\n  actor_loss = -tf.math.reduce_sum(action_log_probs * advantage)\n\n  critic_loss = huber_loss(values, returns)\n\n  return actor_loss + critic_loss",
      "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n\n\n@tf.function\ndef train_step(\n    initial_state: tf.Tensor,\n    model: tf.keras.Model,\n    optimizer: tf.keras.optimizers.Optimizer,\n    gamma: float,\n    max_steps_per_episode: int) -> tf.Tensor:\n  \"\"\"Runs a model training step.\"\"\"\n\n  with tf.GradientTape() as tape:\n\n    # Run the model for one episode to collect training data\n    action_probs, values, rewards = run_episode(\n        initial_state, model, max_steps_per_episode)\n\n    # Calculate the expected returns\n    returns = get_expected_return(rewards, gamma)\n\n    # Convert training data to appropriate TF tensor shapes\n    action_probs, values, returns = [\n        tf.expand_dims(x, 1) for x in [action_probs, values, returns]]\n\n    # Calculate the loss values to update our network\n    loss = compute_loss(action_probs, values, returns)\n\n  # Compute the gradients from the loss\n  grads = tape.gradient(loss, model.trainable_variables)\n\n  # Apply the gradients to the model's parameters\n  optimizer.apply_gradients(zip(grads, model.trainable_variables))\n\n  episode_reward = tf.math.reduce_sum(rewards)\n\n  return episode_reward",
      "%%time\n\nmin_episodes_criterion = 100\nmax_episodes = 10000\nmax_steps_per_episode = 500\n\n# `CartPole-v1` is considered solved if average reward is >= 475 over 500\n# consecutive trials\nreward_threshold = 475\nrunning_reward = 0\n\n# The discount factor for future rewards\ngamma = 0.99\n\n# Keep the last episodes reward\nepisodes_reward: collections.deque = collections.deque(maxlen=min_episodes_criterion)\n\nt = tqdm.trange(max_episodes)\nfor i in t:\n    initial_state, info = env.reset()\n    initial_state = tf.constant(initial_state, dtype=tf.float32)\n    episode_reward = int(train_step(\n        initial_state, model, optimizer, gamma, max_steps_per_episode))\n\n    episodes_reward.append(episode_reward)\n    running_reward = statistics.mean(episodes_reward)\n\n\n    t.set_postfix(\n        episode_reward=episode_reward, running_reward=running_reward)\n\n    # Show the average episode reward every 10 episodes\n    if i % 10 == 0:\n      pass # print(f'Episode {i}: average reward: {avg_reward}')\n\n    if running_reward > reward_threshold and i >= min_episodes_criterion:\n        break\n\nprint(f'\\nSolved at episode {i}: average reward: {running_reward:.2f}!')",
      "# Render an episode and save as a GIF file\n\nfrom IPython import display as ipythondisplay\nfrom PIL import Image\n\nrender_env = gym.make(\"CartPole-v1\", render_mode='rgb_array')\n\ndef render_episode(env: gym.Env, model: tf.keras.Model, max_steps: int):\n  state, info = env.reset()\n  state = tf.constant(state, dtype=tf.float32)\n  screen = env.render()\n  images = [Image.fromarray(screen)]\n\n  for i in range(1, max_steps + 1):\n    state = tf.expand_dims(state, 0)\n    action_probs, _ = model(state)\n    action = np.argmax(np.squeeze(action_probs))\n\n    state, reward, done, truncated, info = env.step(action)\n    state = tf.constant(state, dtype=tf.float32)\n\n    # Render screen every 10 steps\n    if i % 10 == 0:\n      screen = env.render()\n      images.append(Image.fromarray(screen))\n\n    if done:\n      break\n\n  return images\n\n\n# Save GIF image\nimages = render_episode(render_env, model, max_steps_per_episode)\nimage_file = 'cartpole-v1.gif'\n# loop=0: loop forever, duration=1: play each frame for 1ms\nimages[0].save(\n    image_file, save_all=True, append_images=images[1:], loop=0, duration=1)",
      "import tensorflow_docs.vis.embed as embed\nembed.embed_file(image_file)"
    ]
  },
  {
    "url": "https://www.tensorflow.org/tutorials/keras/save_and_load",
    "title": "Save and load models\n      \n\n\n      \n      Stay organized with collections\n    \n\n      \n      Save and categorize content based on your preferences.",
    "code_snippets": [
      "import os\n\nimport tensorflow as tf\nfrom tensorflow import keras\n\nprint(tf.version.VERSION)",
      "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n\ntrain_labels = train_labels[:1000]\ntest_labels = test_labels[:1000]\n\ntrain_images = train_images[:1000].reshape(-1, 28 * 28) / 255.0\ntest_images = test_images[:1000].reshape(-1, 28 * 28) / 255.0",
      "# Define a simple sequential model\ndef create_model():\n  model = tf.keras.Sequential([\n    keras.layers.Dense(512, activation='relu', input_shape=(784,)),\n    keras.layers.Dropout(0.2),\n    keras.layers.Dense(10)\n  ])\n\n  model.compile(optimizer='adam',\n                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n                metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n\n  return model\n\n# Create a basic model instance\nmodel = create_model()\n\n# Display the model's architecture\nmodel.summary()",
      "checkpoint_path = \"training_1/cp.ckpt\"\ncheckpoint_dir = os.path.dirname(checkpoint_path)\n\n# Create a callback that saves the model's weights\ncp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n                                                 save_weights_only=True,\n                                                 verbose=1)\n\n# Train the model with the new callback\nmodel.fit(train_images, \n          train_labels,  \n          epochs=10,\n          validation_data=(test_images, test_labels),\n          callbacks=[cp_callback])  # Pass callback to training\n\n# This may generate warnings related to saving the state of the optimizer.\n# These warnings (and similar warnings throughout this notebook)\n# are in place to discourage outdated usage, and can be ignored.",
      "# Include the epoch in the file name (uses `str.format`)\ncheckpoint_path = \"training_2/cp-{epoch:04d}.ckpt\"\ncheckpoint_dir = os.path.dirname(checkpoint_path)\n\nbatch_size = 32\n\n# Calculate the number of batches per epoch\nimport math\nn_batches = len(train_images) / batch_size\nn_batches = math.ceil(n_batches)    # round up the number of batches to the nearest whole integer\n\n# Create a callback that saves the model's weights every 5 epochs\ncp_callback = tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_path, \n    verbose=1, \n    save_weights_only=True,\n    save_freq=5*n_batches)\n\n# Create a new model instance\nmodel = create_model()\n\n# Save the weights using the `checkpoint_path` format\nmodel.save_weights(checkpoint_path.format(epoch=0))\n\n# Train the model with the new callback\nmodel.fit(train_images, \n          train_labels,\n          epochs=50, \n          batch_size=batch_size, \n          callbacks=[cp_callback],\n          validation_data=(test_images, test_labels),\n          verbose=0)",
      "latest = tf.train.latest_checkpoint(checkpoint_dir)\nlatest",
      "new_model = tf.keras.models.load_model('my_model.keras')\n\n# Show the model architecture\nnew_model.summary()",
      "new_model = tf.keras.models.load_model('saved_model/my_model')\n\n# Check its architecture\nnew_model.summary()",
      "# Recreate the exact same model, including its weights and the optimizer\nnew_model = tf.keras.models.load_model('my_model.h5')\n\n# Show the model architecture\nnew_model.summary()"
    ]
  },
  {
    "url": "https://www.tensorflow.org/tutorials/keras/regression",
    "title": "Basic regression: Predict fuel efficiency\n      \n\n\n      \n      Stay organized with collections\n    \n\n      \n      Save and categorize content based on your preferences.",
    "code_snippets": [
      "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\n# Make NumPy printouts easier to read.\nnp.set_printoptions(precision=3, suppress=True)",
      "import tensorflow as tf\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nprint(tf.__version__)",
      "normalizer = tf.keras.layers.Normalization(axis=-1)",
      "horsepower_model = tf.keras.Sequential([\n    horsepower_normalizer,\n    layers.Dense(units=1)\n])\n\nhorsepower_model.summary()",
      "horsepower_model.compile(\n    optimizer=tf.keras.optimizers.Adam(learning_rate=0.1),\n    loss='mean_absolute_error')",
      "def plot_loss(history):\n  plt.plot(history.history['loss'], label='loss')\n  plt.plot(history.history['val_loss'], label='val_loss')\n  plt.ylim([0, 10])\n  plt.xlabel('Epoch')\n  plt.ylabel('Error [MPG]')\n  plt.legend()\n  plt.grid(True)",
      "x = tf.linspace(0.0, 250, 251)\ny = horsepower_model.predict(x)",
      "def plot_horsepower(x, y):\n  plt.scatter(train_features['Horsepower'], train_labels, label='Data')\n  plt.plot(x, y, color='k', label='Predictions')\n  plt.xlabel('Horsepower')\n  plt.ylabel('MPG')\n  plt.legend()",
      "linear_model = tf.keras.Sequential([\n    normalizer,\n    layers.Dense(units=1)\n])",
      "linear_model.compile(\n    optimizer=tf.keras.optimizers.Adam(learning_rate=0.1),\n    loss='mean_absolute_error')",
      "def build_and_compile_model(norm):\n  model = keras.Sequential([\n      norm,\n      layers.Dense(64, activation='relu'),\n      layers.Dense(64, activation='relu'),\n      layers.Dense(1)\n  ])\n\n  model.compile(loss='mean_absolute_error',\n                optimizer=tf.keras.optimizers.Adam(0.001))\n  return model",
      "x = tf.linspace(0.0, 250, 251)\ny = dnn_horsepower_model.predict(x)",
      "reloaded = tf.keras.models.load_model('dnn_model.keras')\n\ntest_results['reloaded'] = reloaded.evaluate(\n    test_features, test_labels, verbose=0)"
    ]
  },
  {
    "url": "https://www.tensorflow.org/tutorials/images",
    "title": "Computer vision with TensorFlow\n      \n\n\n      \n      Stay organized with collections\n    \n\n      \n      Save and categorize content based on your preferences.",
    "code_snippets": []
  },
  {
    "url": "https://www.tensorflow.org/tutorials/audio/simple_audio",
    "title": "Simple audio recognition: Recognizing keywords\n      \n\n\n      \n      Stay organized with collections\n    \n\n      \n      Save and categorize content based on your preferences.",
    "code_snippets": [
      "import os\nimport pathlib\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nimport tensorflow as tf\n\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import models\nfrom IPython import display\n\n# Set the seed value for experiment reproducibility.\nseed = 42\ntf.random.set_seed(seed)\nnp.random.seed(seed)",
      "DATASET_PATH = 'data/mini_speech_commands'\n\ndata_dir = pathlib.Path(DATASET_PATH)\nif not data_dir.exists():\n  tf.keras.utils.get_file(\n      'mini_speech_commands.zip',\n      origin=\"http://storage.googleapis.com/download.tensorflow.org/data/mini_speech_commands.zip\",\n      extract=True,\n      cache_dir='.', cache_subdir='data')",
      "commands = np.array(tf.io.gfile.listdir(str(data_dir)))\ncommands = commands[(commands != 'README.md') & (commands != '.DS_Store')]\nprint('Commands:', commands)",
      "train_ds, val_ds = tf.keras.utils.audio_dataset_from_directory(\n    directory=data_dir,\n    batch_size=64,\n    validation_split=0.2,\n    seed=0,\n    output_sequence_length=16000,\n    subset='both')\n\nlabel_names = np.array(train_ds.class_names)\nprint()\nprint(\"label names:\", label_names)",
      "(TensorSpec(shape=(None, 16000, None), dtype=tf.float32, name=None),\n TensorSpec(shape=(None,), dtype=tf.int32, name=None))",
      "def squeeze(audio, labels):\n  audio = tf.squeeze(audio, axis=-1)\n  return audio, labels\n\ntrain_ds = train_ds.map(squeeze, tf.data.AUTOTUNE)\nval_ds = val_ds.map(squeeze, tf.data.AUTOTUNE)",
      "def get_spectrogram(waveform):\n  # Convert the waveform to a spectrogram via a STFT.\n  spectrogram = tf.signal.stft(\n      waveform, frame_length=255, frame_step=128)\n  # Obtain the magnitude of the STFT.\n  spectrogram = tf.abs(spectrogram)\n  # Add a `channels` dimension, so that the spectrogram can be used\n  # as image-like input data with convolution layers (which expect\n  # shape (`batch_size`, `height`, `width`, `channels`).\n  spectrogram = spectrogram[..., tf.newaxis]\n  return spectrogram",
      "def plot_spectrogram(spectrogram, ax):\n  if len(spectrogram.shape) > 2:\n    assert len(spectrogram.shape) == 3\n    spectrogram = np.squeeze(spectrogram, axis=-1)\n  # Convert the frequencies to log scale and transpose, so that the time is\n  # represented on the x-axis (columns).\n  # Add an epsilon to avoid taking a log of zero.\n  log_spec = np.log(spectrogram.T + np.finfo(float).eps)\n  height = log_spec.shape[0]\n  width = log_spec.shape[1]\n  X = np.linspace(0, np.size(spectrogram), num=width, dtype=int)\n  Y = range(height)\n  ax.pcolormesh(X, Y, log_spec)",
      "def make_spec_ds(ds):\n  return ds.map(\n      map_func=lambda audio,label: (get_spectrogram(audio), label),\n      num_parallel_calls=tf.data.AUTOTUNE)",
      "train_spectrogram_ds = train_spectrogram_ds.cache().shuffle(10000).prefetch(tf.data.AUTOTUNE)\nval_spectrogram_ds = val_spectrogram_ds.cache().prefetch(tf.data.AUTOTUNE)\ntest_spectrogram_ds = test_spectrogram_ds.cache().prefetch(tf.data.AUTOTUNE)",
      "input_shape = example_spectrograms.shape[1:]\nprint('Input shape:', input_shape)\nnum_labels = len(label_names)\n\n# Instantiate the `tf.keras.layers.Normalization` layer.\nnorm_layer = layers.Normalization()\n# Fit the state of the layer to the spectrograms\n# with `Normalization.adapt`.\nnorm_layer.adapt(data=train_spectrogram_ds.map(map_func=lambda spec, label: spec))\n\nmodel = models.Sequential([\n    layers.Input(shape=input_shape),\n    # Downsample the input.\n    layers.Resizing(32, 32),\n    # Normalize.\n    norm_layer,\n    layers.Conv2D(32, 3, activation='relu'),\n    layers.Conv2D(64, 3, activation='relu'),\n    layers.MaxPooling2D(),\n    layers.Dropout(0.25),\n    layers.Flatten(),\n    layers.Dense(128, activation='relu'),\n    layers.Dropout(0.5),\n    layers.Dense(num_labels),\n])\n\nmodel.summary()",
      "model.compile(\n    optimizer=tf.keras.optimizers.Adam(),\n    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    metrics=['accuracy'],\n)",
      "EPOCHS = 10\nhistory = model.fit(\n    train_spectrogram_ds,\n    validation_data=val_spectrogram_ds,\n    epochs=EPOCHS,\n    callbacks=tf.keras.callbacks.EarlyStopping(verbose=1, patience=2),\n)",
      "y_pred = tf.argmax(y_pred, axis=1)",
      "y_true = tf.concat(list(test_spectrogram_ds.map(lambda s,lab: lab)), axis=0)",
      "confusion_mtx = tf.math.confusion_matrix(y_true, y_pred)\nplt.figure(figsize=(10, 8))\nsns.heatmap(confusion_mtx,\n            xticklabels=label_names,\n            yticklabels=label_names,\n            annot=True, fmt='g')\nplt.xlabel('Prediction')\nplt.ylabel('Label')\nplt.show()",
      "x = data_dir/'no/01bb6a2a_nohash_0.wav'\nx = tf.io.read_file(str(x))\nx, sample_rate = tf.audio.decode_wav(x, desired_channels=1, desired_samples=16000,)\nx = tf.squeeze(x, axis=-1)\nwaveform = x\nx = get_spectrogram(x)\nx = x[tf.newaxis,...]\n\nprediction = model(x)\nx_labels = ['no', 'yes', 'down', 'go', 'left', 'up', 'right', 'stop']\nplt.bar(x_labels, tf.nn.softmax(prediction[0]))\nplt.title('No')\nplt.show()\n\ndisplay.display(display.Audio(waveform, rate=16000))",
      "class ExportModel(tf.Module):\n  def __init__(self, model):\n    self.model = model\n\n    # Accept either a string-filename or a batch of waveforms.\n    # YOu could add additional signatures for a single wave, or a ragged-batch. \n    self.__call__.get_concrete_function(\n        x=tf.TensorSpec(shape=(), dtype=tf.string))\n    self.__call__.get_concrete_function(\n       x=tf.TensorSpec(shape=[None, 16000], dtype=tf.float32))\n\n\n  @tf.function\n  def __call__(self, x):\n    # If they pass a string, load the file and decode it. \n    if x.dtype == tf.string:\n      x = tf.io.read_file(x)\n      x, _ = tf.audio.decode_wav(x, desired_channels=1, desired_samples=16000,)\n      x = tf.squeeze(x, axis=-1)\n      x = x[tf.newaxis, :]\n\n    x = get_spectrogram(x)  \n    result = self.model(x, training=False)\n\n    class_ids = tf.argmax(result, axis=-1)\n    class_names = tf.gather(label_names, class_ids)\n    return {'predictions':result,\n            'class_ids': class_ids,\n            'class_names': class_names}",
      "export = ExportModel(model)\nexport(tf.constant(str(data_dir/'no/01bb6a2a_nohash_0.wav')))",
      "{'predictions': <tf.Tensor: shape=(1, 8), dtype=float32, numpy=\n array([[ 1.0958828,  2.526922 , -1.8349309,  4.2553926, -4.2595496,\n         -2.5386834, -3.6104631, -2.295511 ]], dtype=float32)>,\n 'class_ids': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([3])>,\n 'class_names': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'no'], dtype=object)>}",
      "tf.saved_model.save(export, \"saved\")\nimported = tf.saved_model.load(\"saved\")\nimported(waveform[tf.newaxis, :])",
      "INFO:tensorflow:Assets written to: saved/assets\nINFO:tensorflow:Assets written to: saved/assets\n{'predictions': <tf.Tensor: shape=(1, 8), dtype=float32, numpy=\n array([[ 1.0958828,  2.526922 , -1.8349309,  4.2553926, -4.2595496,\n         -2.5386834, -3.6104631, -2.295511 ]], dtype=float32)>,\n 'class_ids': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([3])>,\n 'class_names': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'no'], dtype=object)>}"
    ]
  },
  {
    "url": "https://www.tensorflow.org/tutorials/distribute/dtensor_ml_tutorial",
    "title": "Distributed training with DTensors\n      \n\n\n      \n      Stay organized with collections\n    \n\n      \n      Save and categorize content based on your preferences.",
    "code_snippets": [
      "import tempfile\nimport numpy as np\nimport tensorflow_datasets as tfds\n\nimport tensorflow as tf\n\nfrom tensorflow.experimental import dtensor\n\nprint('TensorFlow version:', tf.__version__)",
      "def configure_virtual_cpus(ncpu):\n  phy_devices = tf.config.list_physical_devices('CPU')\n  tf.config.set_logical_device_configuration(phy_devices[0], [\n        tf.config.LogicalDeviceConfiguration(),\n    ] * ncpu)\n\nconfigure_virtual_cpus(8)\nDEVICES = [f'CPU:{i}' for i in range(8)]\n\ntf.config.list_logical_devices('CPU')",
      "text_vectorization = tf.keras.layers.TextVectorization(output_mode='tf_idf', max_tokens=1200, output_sequence_length=None)\ntext_vectorization.adapt(data=train_data.map(lambda x: x['text']))",
      "def vectorize(features):\n  return text_vectorization(features['text']), features['label']\n\ntrain_data_vec = train_data.map(vectorize)\ntrain_data_vec",
      "class Dense(tf.Module):\n\n  def __init__(self, input_size, output_size,\n               init_seed, weight_layout, activation=None):\n    super().__init__()\n\n    random_normal_initializer = tf.function(tf.random.stateless_normal)\n\n    self.weight = dtensor.DVariable(\n        dtensor.call_with_layout(\n            random_normal_initializer, weight_layout,\n            shape=[input_size, output_size],\n            seed=init_seed\n            ))\n    if activation is None:\n      activation = lambda x:x\n    self.activation = activation\n\n    # bias is sharded the same way as the last axis of weight.\n    bias_layout = weight_layout.delete([0])\n\n    self.bias = dtensor.DVariable(\n        dtensor.call_with_layout(tf.zeros, bias_layout, [output_size]))\n\n  def __call__(self, x):\n    y = tf.matmul(x, self.weight) + self.bias\n    y = self.activation(y)\n\n    return y",
      "class BatchNorm(tf.Module):\n\n  def __init__(self):\n    super().__init__()\n\n  def __call__(self, x, training=True):\n    if not training:\n      # This branch is not used in the Tutorial.\n      pass\n    mean, variance = tf.nn.moments(x, axes=[0])\n    return tf.nn.batch_normalization(x, mean, variance, 0.0, 1.0, 1e-5)",
      "def make_keras_bn(bn_layout):\n  return tf.keras.layers.BatchNormalization(gamma_layout=bn_layout,\n                                            beta_layout=bn_layout,\n                                            moving_mean_layout=bn_layout,\n                                            moving_variance_layout=bn_layout,\n                                            fused=False)",
      "from typing import Tuple\n\nclass MLP(tf.Module):\n\n  def __init__(self, dense_layouts: Tuple[dtensor.Layout, dtensor.Layout]):\n    super().__init__()\n\n    self.dense1 = Dense(\n        1200, 48, (1, 2), dense_layouts[0], activation=tf.nn.relu)\n    self.bn = BatchNorm()\n    self.dense2 = Dense(48, 2, (3, 4), dense_layouts[1])\n\n  def __call__(self, x):\n    y = x\n    y = self.dense1(y)\n    y = self.bn(y)\n    y = self.dense2(y)\n    return y",
      "class MLPStricter(tf.Module):\n\n  def __init__(self, mesh, input_mesh_dim, inner_mesh_dim1, output_mesh_dim):\n    super().__init__()\n\n    self.dense1 = Dense(\n        1200, 48, (1, 2), dtensor.Layout([input_mesh_dim, inner_mesh_dim1], mesh),\n        activation=tf.nn.relu)\n    self.bn = BatchNorm()\n    self.dense2 = Dense(48, 2, (3, 4), dtensor.Layout([inner_mesh_dim1, output_mesh_dim], mesh))\n\n\n  def __call__(self, x):\n    y = x\n    y = self.dense1(y)\n    y = self.bn(y)\n    y = self.dense2(y)\n    return y",
      "def repack_local_tensor(x, layout):\n  \"\"\"Repacks a local Tensor-like to a DTensor with layout.\n\n  This function assumes a single-client application.\n  \"\"\"\n  x = tf.convert_to_tensor(x)\n  sharded_dims = []\n\n  # For every sharded dimension, use tf.split to split the along the dimension.\n  # The result is a nested list of split-tensors in queue[0].\n  queue = [x]\n  for axis, dim in enumerate(layout.sharding_specs):\n    if dim == dtensor.UNSHARDED:\n      continue\n    num_splits = layout.shape[axis]\n    queue = tf.nest.map_structure(lambda x: tf.split(x, num_splits, axis=axis), queue)\n    sharded_dims.append(dim)\n\n  # Now we can build the list of component tensors by looking up the location in\n  # the nested list of split-tensors created in queue[0].\n  components = []\n  for locations in layout.mesh.local_device_locations():\n    t = queue[0]\n    for dim in sharded_dims:\n      split_index = locations[dim]  # Only valid on single-client mesh.\n      t = t[split_index]\n    components.append(t)\n\n  return dtensor.pack(components, layout)",
      "def repack_batch(x, y, mesh):\n  x = repack_local_tensor(x, layout=dtensor.Layout(['batch', dtensor.UNSHARDED], mesh))\n  y = repack_local_tensor(y, layout=dtensor.Layout(['batch'], mesh))\n  return x, y\n\nsample_x, sample_y = train_data_vec.take(1).get_single_element()\nsample_x, sample_y = repack_batch(sample_x, sample_y, mesh)\n\nprint('x', sample_x[:, 0])\nprint('y', sample_y)",
      "# Refer to the CTL (custom training loop guide)\n@tf.function\ndef train_step(model, x, y, learning_rate=tf.constant(1e-4)):\n  with tf.GradientTape() as tape:\n    logits = model(x)\n    # tf.reduce_sum sums the batch sharded per-example loss to a replicated\n    # global loss (scalar).\n    loss = tf.reduce_sum(\n        tf.nn.sparse_softmax_cross_entropy_with_logits(\n            logits=logits, labels=y))\n  parameters = model.trainable_variables\n  gradients = tape.gradient(loss, parameters)\n  for parameter, parameter_gradient in zip(parameters, gradients):\n    parameter.assign_sub(learning_rate * parameter_gradient)\n\n  # Define some metrics\n  accuracy = 1.0 - tf.reduce_sum(tf.cast(tf.argmax(logits, axis=-1, output_type=tf.int64) != y, tf.float32)) / x.shape[0]\n  loss_per_sample = loss / len(x)\n  return {'loss': loss_per_sample, 'accuracy': accuracy}",
      "CHECKPOINT_DIR = tempfile.mkdtemp()\n\ndef start_checkpoint_manager(model):\n  ckpt = tf.train.Checkpoint(root=model)\n  manager = tf.train.CheckpointManager(ckpt, CHECKPOINT_DIR, max_to_keep=3)\n\n  if manager.latest_checkpoint:\n    print(\"Restoring a checkpoint\")\n    ckpt.restore(manager.latest_checkpoint).assert_consumed()\n  else:\n    print(\"New training\")\n  return manager",
      "num_epochs = 2\nmanager = start_checkpoint_manager(model)\n\nfor epoch in range(num_epochs):\n  step = 0\n  pbar = tf.keras.utils.Progbar(target=int(train_data_vec.cardinality()), stateful_metrics=[])\n  metrics = {'epoch': epoch}\n  for x,y in train_data_vec:\n\n    x, y = repack_batch(x, y, mesh)\n\n    metrics.update(train_step(model, x, y, 1e-2))\n\n    pbar.update(step, values=metrics.items(), finalize=False)\n    step += 1\n  manager.save()\n  pbar.update(step, values=metrics.items(), finalize=True)",
      "def repack_batch(x, y, mesh):\n  x = repack_local_tensor(x, layout=dtensor.Layout(['batch', dtensor.UNSHARDED], mesh))\n  y = repack_local_tensor(y, layout=dtensor.Layout(['batch'], mesh))\n  return x, y",
      "num_epochs = 2\nmanager = start_checkpoint_manager(model)\n\nfor epoch in range(num_epochs):\n  step = 0\n  pbar = tf.keras.utils.Progbar(target=int(train_data_vec.cardinality()))\n  metrics = {'epoch': epoch}\n  for x,y in train_data_vec:\n    x, y = repack_batch(x, y, mesh)\n    metrics.update(train_step(model, x, y, 1e-2))\n    pbar.update(step, values=metrics.items(), finalize=False)\n    step += 1\n  manager.save()\n  pbar.update(step, values=metrics.items(), finalize=True)",
      "def repack_batch_for_spt(x, y, mesh):\n    # Shard data on feature dimension, too\n    x = repack_local_tensor(x, layout=dtensor.Layout([\"batch\", 'feature'], mesh))\n    y = repack_local_tensor(y, layout=dtensor.Layout([\"batch\"], mesh))\n    return x, y",
      "num_epochs = 2\n\nmanager = start_checkpoint_manager(model)\nfor epoch in range(num_epochs):\n  step = 0\n  metrics = {'epoch': epoch}\n  pbar = tf.keras.utils.Progbar(target=int(train_data_vec.cardinality()))\n\n  for x, y in train_data_vec:\n    x, y = repack_batch_for_spt(x, y, mesh)\n    metrics.update(train_step(model, x, y, 1e-2))\n\n    pbar.update(step, values=metrics.items(), finalize=False)\n    step += 1\n  manager.save()\n  pbar.update(step, values=metrics.items(), finalize=True)",
      "mesh = dtensor.create_mesh([(\"world\", 1)], devices=DEVICES[:1])\nmlp = MLP([dtensor.Layout([dtensor.UNSHARDED, dtensor.UNSHARDED], mesh), \n           dtensor.Layout([dtensor.UNSHARDED, dtensor.UNSHARDED], mesh)])\n\nmanager = start_checkpoint_manager(mlp)\n\nmodel_for_saving = tf.keras.Sequential([\n  text_vectorization,\n  mlp\n])\n\n@tf.function(input_signature=[tf.TensorSpec([None], tf.string)])\ndef run(inputs):\n  return {'result': model_for_saving(inputs)}\n\ntf.saved_model.save(\n    model_for_saving, \"/tmp/saved_model\",\n    signatures=run)",
      "loaded = tf.saved_model.load(\"/tmp/saved_model\")\n\nrun_sig = loaded.signatures[\"serving_default\"]\nresult = run_sig(sample_batch['text'])['result']",
      "np.mean(tf.argmax(result, axis=-1) == sample_batch['label'])"
    ]
  },
  {
    "url": "https://www.tensorflow.org/tutorials/keras/keras_tuner",
    "title": "Introduction to the Keras Tuner\n      \n\n\n      \n      Stay organized with collections\n    \n\n      \n      Save and categorize content based on your preferences.",
    "code_snippets": [
      "import tensorflow as tf\nfrom tensorflow import keras",
      "import keras_tuner as kt",
      "def model_builder(hp):\n  model = keras.Sequential()\n  model.add(keras.layers.Flatten(input_shape=(28, 28)))\n\n  # Tune the number of units in the first Dense layer\n  # Choose an optimal value between 32-512\n  hp_units = hp.Int('units', min_value=32, max_value=512, step=32)\n  model.add(keras.layers.Dense(units=hp_units, activation='relu'))\n  model.add(keras.layers.Dense(10))\n\n  # Tune the learning rate for the optimizer\n  # Choose an optimal value from 0.01, 0.001, or 0.0001\n  hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n\n  model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n                loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n                metrics=['accuracy'])\n\n  return model",
      "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)"
    ]
  },
  {
    "url": "https://www.tensorflow.org/tutorials/images/cnn",
    "title": "Convolutional Neural Network (CNN)\n      \n\n\n      \n      Stay organized with collections\n    \n\n      \n      Save and categorize content based on your preferences.",
    "code_snippets": [
      "import tensorflow as tf\n\nfrom tensorflow.keras import datasets, layers, models\nimport matplotlib.pyplot as plt",
      "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n               'dog', 'frog', 'horse', 'ship', 'truck']\n\nplt.figure(figsize=(10,10))\nfor i in range(25):\n    plt.subplot(5,5,i+1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.grid(False)\n    plt.imshow(train_images[i])\n    # The CIFAR labels happen to be arrays, \n    # which is why you need the extra index\n    plt.xlabel(class_names[train_labels[i][0]])\nplt.show()",
      "model.compile(optimizer='adam',\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n\nhistory = model.fit(train_images, train_labels, epochs=10, \n                    validation_data=(test_images, test_labels))"
    ]
  },
  {
    "url": "https://www.tensorflow.org/tutorials/structured_data/preprocessing_layers",
    "title": "Classify structured data using Keras preprocessing layers\n      \n\n\n      \n      Stay organized with collections\n    \n\n      \n      Save and categorize content based on your preferences.",
    "code_snippets": [
      "import numpy as np\nimport pandas as pd\nimport tensorflow as tf\n\nfrom tensorflow.keras import layers",
      "tf.__version__",
      "dataset_url = 'http://storage.googleapis.com/download.tensorflow.org/data/petfinder-mini.zip'\ncsv_file = 'datasets/petfinder-mini/petfinder-mini.csv'\n\ntf.keras.utils.get_file('petfinder_mini.zip', dataset_url,\n                        extract=True, cache_dir='.')\ndataframe = pd.read_csv(csv_file)",
      "def df_to_dataset(dataframe, shuffle=True, batch_size=32):\n  df = dataframe.copy()\n  labels = df.pop('target')\n  df = {key: value.to_numpy()[:,tf.newaxis] for key, value in dataframe.items()}\n  ds = tf.data.Dataset.from_tensor_slices((dict(df), labels))\n  if shuffle:\n    ds = ds.shuffle(buffer_size=len(dataframe))\n  ds = ds.batch(batch_size)\n  ds = ds.prefetch(batch_size)\n  return ds",
      "Every feature: ['Type', 'Age', 'Breed1', 'Gender', 'Color1', 'Color2', 'MaturitySize', 'FurLength', 'Vaccinated', 'Sterilized', 'Health', 'Fee', 'PhotoAmt', 'target']\nA batch of ages: tf.Tensor(\n[[5]\n [2]\n [2]\n [3]\n [3]], shape=(5, 1), dtype=int64)\nA batch of targets: tf.Tensor([1 1 1 1 1], shape=(5,), dtype=int64)",
      "def get_normalization_layer(name, dataset):\n  # Create a Normalization layer for the feature.\n  normalizer = layers.Normalization(axis=None)\n\n  # Prepare a Dataset that only yields the feature.\n  feature_ds = dataset.map(lambda x, y: x[name])\n\n  # Learn the statistics of the data.\n  normalizer.adapt(feature_ds)\n\n  return normalizer",
      "<tf.Tensor: shape=(5, 1), dtype=float32, numpy=\narray([[-0.19791041],\n       [-0.82850194],\n       [-0.5132062 ],\n       [-0.5132062 ],\n       [-0.82850194]], dtype=float32)>",
      "def get_category_encoding_layer(name, dataset, dtype, max_tokens=None):\n  # Create a layer that turns strings into integer indices.\n  if dtype == 'string':\n    index = layers.StringLookup(max_tokens=max_tokens)\n  # Otherwise, create a layer that turns integer values into integer indices.\n  else:\n    index = layers.IntegerLookup(max_tokens=max_tokens)\n\n  # Prepare a `tf.data.Dataset` that only yields the feature.\n  feature_ds = dataset.map(lambda x, y: x[name])\n\n  # Learn the set of possible values and assign them a fixed integer index.\n  index.adapt(feature_ds)\n\n  # Encode the integer indices.\n  encoder = layers.CategoryEncoding(num_tokens=index.vocabulary_size())\n\n  # Apply multi-hot encoding to the indices. The lambda function captures the\n  # layer, so you can use them, or include them in the Keras Functional model later.\n  return lambda feature: encoder(index(feature))",
      "<tf.Tensor: shape=(5, 3), dtype=float32, numpy=\narray([[0., 1., 0.],\n       [0., 1., 0.],\n       [0., 1., 0.],\n       [0., 0., 1.],\n       [0., 0., 1.]], dtype=float32)>",
      "<tf.Tensor: shape=(5, 5), dtype=float32, numpy=\narray([[1., 0., 0., 0., 0.],\n       [0., 1., 0., 0., 0.],\n       [0., 1., 0., 0., 0.],\n       [0., 0., 1., 0., 0.],\n       [0., 0., 1., 0., 0.]], dtype=float32)>",
      "all_inputs = {}\nencoded_features = []\n\n# Numerical features.\nfor header in ['PhotoAmt', 'Fee']:\n  numeric_col = tf.keras.Input(shape=(1,), name=header)\n  normalization_layer = get_normalization_layer(header, train_ds)\n  encoded_numeric_col = normalization_layer(numeric_col)\n  all_inputs[header] = numeric_col\n  encoded_features.append(encoded_numeric_col)",
      "age_col = tf.keras.Input(shape=(1,), name='Age', dtype='int64')\n\nencoding_layer = get_category_encoding_layer(name='Age',\n                                             dataset=train_ds,\n                                             dtype='int64',\n                                             max_tokens=5)\nencoded_age_col = encoding_layer(age_col)\nall_inputs['Age'] = age_col\nencoded_features.append(encoded_age_col)",
      "categorical_cols = ['Type', 'Color1', 'Color2', 'Gender', 'MaturitySize',\n                    'FurLength', 'Vaccinated', 'Sterilized', 'Health', 'Breed1']\n\nfor header in categorical_cols:\n  categorical_col = tf.keras.Input(shape=(1,), name=header, dtype='string')\n  encoding_layer = get_category_encoding_layer(name=header,\n                                               dataset=train_ds,\n                                               dtype='string',\n                                               max_tokens=5)\n  encoded_categorical_col = encoding_layer(categorical_col)\n  all_inputs[header] = categorical_col\n  encoded_features.append(encoded_categorical_col)",
      "all_features = tf.keras.layers.concatenate(encoded_features)\nx = tf.keras.layers.Dense(32, activation=\"relu\")(all_features)\nx = tf.keras.layers.Dropout(0.5)(x)\noutput = tf.keras.layers.Dense(1)(x)\n\nmodel = tf.keras.Model(all_inputs, output)",
      "model.compile(optimizer='adam',\n              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n              metrics=[\"accuracy\"],\n              run_eagerly=True)",
      "# Use `rankdir='LR'` to make the graph horizontal.\ntf.keras.utils.plot_model(model, show_shapes=True, show_layer_names=True, rankdir=\"LR\")",
      "model.save('my_pet_classifier.keras')\nreloaded_model = tf.keras.models.load_model('my_pet_classifier.keras')",
      "sample = {\n    'Type': 'Cat',\n    'Age': 3,\n    'Breed1': 'Tabby',\n    'Gender': 'Male',\n    'Color1': 'Black',\n    'Color2': 'White',\n    'MaturitySize': 'Small',\n    'FurLength': 'Short',\n    'Vaccinated': 'No',\n    'Sterilized': 'No',\n    'Health': 'Healthy',\n    'Fee': 100,\n    'PhotoAmt': 2,\n}\n\ninput_dict = {name: tf.convert_to_tensor([value]) for name, value in sample.items()}\npredictions = reloaded_model.predict(input_dict)\nprob = tf.nn.sigmoid(predictions[0])\n\nprint(\n    \"This particular pet had a %.1f percent probability \"\n    \"of getting adopted.\" % (100 * prob)\n)"
    ]
  },
  {
    "url": "https://www.tensorflow.org/tutorials/structured_data/imbalanced_data",
    "title": "Classification on imbalanced data\n      \n\n\n      \n      Stay organized with collections\n    \n\n      \n      Save and categorize content based on your preferences.",
    "code_snippets": [
      "import tensorflow as tf\nfrom tensorflow import keras\n\nimport os\nimport tempfile\n\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\nimport sklearn\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler",
      "file = tf.keras.utils\nraw_df = pd.read_csv('https://storage.googleapis.com/download.tensorflow.org/data/creditcard.csv')\nraw_df.head()",
      "METRICS = [\n      keras.metrics.BinaryCrossentropy(name='cross entropy'),  # same as model's loss\n      keras.metrics.MeanSquaredError(name='Brier score'),\n      keras.metrics.TruePositives(name='tp'),\n      keras.metrics.FalsePositives(name='fp'),\n      keras.metrics.TrueNegatives(name='tn'),\n      keras.metrics.FalseNegatives(name='fn'),\n      keras.metrics.BinaryAccuracy(name='accuracy'),\n      keras.metrics.Precision(name='precision'),\n      keras.metrics.Recall(name='recall'),\n      keras.metrics.AUC(name='auc'),\n      keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n]\n\ndef make_model(metrics=METRICS, output_bias=None):\n  if output_bias is not None:\n    output_bias = tf.keras.initializers.Constant(output_bias)\n  model = keras.Sequential([\n      keras.layers.Dense(\n          16, activation='relu',\n          input_shape=(train_features.shape[-1],)),\n      keras.layers.Dropout(0.5),\n      keras.layers.Dense(1, activation='sigmoid',\n                         bias_initializer=output_bias),\n  ])\n\n  model.compile(\n      optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n      loss=keras.losses.BinaryCrossentropy(),\n      metrics=metrics)\n\n  return model",
      "EPOCHS = 100\nBATCH_SIZE = 2048\n\ndef early_stopping():\n return tf.keras.callbacks.EarlyStopping(\n    monitor='val_prc',\n    verbose=1,\n    patience=10,\n    mode='max',\n    restore_best_weights=True)",
      "def plot_loss(history, label, n):\n  # Use a log scale on y-axis to show the wide range of values.\n  plt.semilogy(history.epoch, history.history['loss'],\n               color=colors[n], label='Train ' + label)\n  plt.semilogy(history.epoch, history.history['val_loss'],\n               color=colors[n], label='Val ' + label,\n               linestyle=\"--\")\n  plt.xlabel('Epoch')\n  plt.ylabel('Loss')\n  plt.legend()",
      "def plot_metrics(history):\n  metrics = ['loss', 'prc', 'precision', 'recall']\n  for n, metric in enumerate(metrics):\n    name = metric.replace(\"_\",\" \").capitalize()\n    plt.subplot(2,2,n+1)\n    plt.plot(history.epoch, history.history[metric], color=colors[0], label='Train')\n    plt.plot(history.epoch, history.history['val_'+metric],\n             color=colors[0], linestyle=\"--\", label='Val')\n    plt.xlabel('Epoch')\n    plt.ylabel(name)\n    if metric == 'loss':\n      plt.ylim([0, plt.ylim()[1]])\n    elif metric == 'auc':\n      plt.ylim([0.8,1])\n    else:\n      plt.ylim([0,1])\n\n    plt.legend()",
      "def plot_cm(labels, predictions, threshold=0.5):\n  cm = confusion_matrix(labels, predictions > threshold)\n  plt.figure(figsize=(5,5))\n  sns.heatmap(cm, annot=True, fmt=\"d\")\n  plt.title('Confusion matrix @{:.2f}'.format(threshold))\n  plt.ylabel('Actual label')\n  plt.xlabel('Predicted label')\n\n  print('Legitimate Transactions Detected (True Negatives): ', cm[0][0])\n  print('Legitimate Transactions Incorrectly Detected (False Positives): ', cm[0][1])\n  print('Fraudulent Transactions Missed (False Negatives): ', cm[1][0])\n  print('Fraudulent Transactions Detected (True Positives): ', cm[1][1])\n  print('Total Fraudulent Transactions: ', np.sum(cm[1]))",
      "def plot_roc(name, labels, predictions, **kwargs):\n  fp, tp, _ = sklearn.metrics.roc_curve(labels, predictions)\n\n  plt.plot(100*fp, 100*tp, label=name, linewidth=2, **kwargs)\n  plt.xlabel('False positives [%]')\n  plt.ylabel('True positives [%]')\n  plt.xlim([-0.5,20])\n  plt.ylim([80,100.5])\n  plt.grid(True)\n  ax = plt.gca()\n  ax.set_aspect('equal')",
      "def plot_prc(name, labels, predictions, **kwargs):\n    precision, recall, _ = sklearn.metrics.precision_recall_curve(labels, predictions)\n\n    plt.plot(precision, recall, label=name, linewidth=2, **kwargs)\n    plt.xlabel('Precision')\n    plt.ylabel('Recall')\n    plt.grid(True)\n    ax = plt.gca()\n    ax.set_aspect('equal')",
      "BUFFER_SIZE = 100000\n\ndef make_ds(features, labels):\n  ds = tf.data.Dataset.from_tensor_slices((features, labels))#.cache()\n  ds = ds.shuffle(BUFFER_SIZE).repeat()\n  return ds\n\npos_ds = make_ds(pos_features, pos_labels)\nneg_ds = make_ds(neg_features, neg_labels)",
      "resampled_ds = tf.data.Dataset.sample_from_datasets([pos_ds, neg_ds], weights=[0.5, 0.5])\nresampled_ds = resampled_ds.batch(BATCH_SIZE).prefetch(2)",
      "resampled_model = make_model()\nresampled_model.load_weights(initial_weights)\n\n# Reset the bias to zero, since this dataset is balanced.\noutput_layer = resampled_model.layers[-1]\noutput_layer.bias.assign([0])\n\nval_ds = tf.data.Dataset.from_tensor_slices((val_features, val_labels)).cache()\nval_ds = val_ds.batch(BATCH_SIZE).prefetch(2)\n\nresampled_history = resampled_model.fit(\n    resampled_ds,\n    epochs=EPOCHS,\n    steps_per_epoch=resampled_steps_per_epoch,\n    callbacks=[early_stopping()],\n    validation_data=val_ds)"
    ]
  },
  {
    "url": "https://www.tensorflow.org/tutorials/understanding/sngp",
    "title": "Uncertainty-aware Deep Learning with SNGP\n      \n\n\n      \n      Stay organized with collections\n    \n\n      \n      Save and categorize content based on your preferences.",
    "code_snippets": [
      "import matplotlib.pyplot as plt\nimport matplotlib.colors as colors\n\nimport sklearn.datasets\n\nimport numpy as np\nimport tensorflow as tf\n\nimport official.nlp.modeling.layers as nlp_layers",
      "def make_training_data(sample_size=500):\n  \"\"\"Create two moon training dataset.\"\"\"\n  train_examples, train_labels = sklearn.datasets.make_moons(\n      n_samples=2 * sample_size, noise=0.1)\n\n  # Adjust data position slightly.\n  train_examples[train_labels == 0] += [-0.1, 0.2]\n  train_examples[train_labels == 1] += [0.1, -0.2]\n\n  return train_examples, train_labels",
      "def make_testing_data(x_range=DEFAULT_X_RANGE, y_range=DEFAULT_Y_RANGE, n_grid=DEFAULT_N_GRID):\n  \"\"\"Create a mesh grid in 2D space.\"\"\"\n  # testing data (mesh grid over data space)\n  x = np.linspace(x_range[0], x_range[1], n_grid)\n  y = np.linspace(y_range[0], y_range[1], n_grid)\n  xv, yv = np.meshgrid(x, y)\n  return np.stack([xv.flatten(), yv.flatten()], axis=-1)",
      "def make_ood_data(sample_size=500, means=(2.5, -1.75), vars=(0.01, 0.01)):\n  return np.random.multivariate_normal(\n      means, cov=np.diag(vars), size=sample_size)",
      "class DeepResNet(tf.keras.Model):\n  \"\"\"Defines a multi-layer residual network.\"\"\"\n  def __init__(self, num_classes, num_layers=3, num_hidden=128,\n               dropout_rate=0.1, **classifier_kwargs):\n    super().__init__()\n    # Defines class meta data.\n    self.num_hidden = num_hidden\n    self.num_layers = num_layers\n    self.dropout_rate = dropout_rate\n    self.classifier_kwargs = classifier_kwargs\n\n    # Defines the hidden layers.\n    self.input_layer = tf.keras.layers.Dense(self.num_hidden, trainable=False)\n    self.dense_layers = [self.make_dense_layer() for _ in range(num_layers)]\n\n    # Defines the output layer.\n    self.classifier = self.make_output_layer(num_classes)\n\n  def call(self, inputs):\n    # Projects the 2d input data to high dimension.\n    hidden = self.input_layer(inputs)\n\n    # Computes the ResNet hidden representations.\n    for i in range(self.num_layers):\n      resid = self.dense_layers[i](hidden)\n      resid = tf.keras.layers.Dropout(self.dropout_rate)(resid)\n      hidden += resid\n\n    return self.classifier(hidden)\n\n  def make_dense_layer(self):\n    \"\"\"Uses the Dense layer as the hidden layer.\"\"\"\n    return tf.keras.layers.Dense(self.num_hidden, activation=\"relu\")\n\n  def make_output_layer(self, num_classes):\n    \"\"\"Uses the Dense layer as the output layer.\"\"\"\n    return tf.keras.layers.Dense(\n        num_classes, **self.classifier_kwargs)",
      "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\nmetrics = tf.keras.metrics.SparseCategoricalAccuracy(),\noptimizer = tf.keras.optimizers.legacy.Adam(learning_rate=1e-4)\n\ntrain_config = dict(loss=loss, metrics=metrics, optimizer=optimizer)",
      "def plot_uncertainty_surface(test_uncertainty, ax, cmap=None):\n  \"\"\"Visualizes the 2D uncertainty surface.\n\n  For simplicity, assume these objects already exist in the memory:\n\n    test_examples: Array of test examples, shape (num_test, 2).\n    train_labels: Array of train labels, shape (num_train, ).\n    train_examples: Array of train examples, shape (num_train, 2).\n\n  Arguments:\n    test_uncertainty: Array of uncertainty scores, shape (num_test,).\n    ax: A matplotlib Axes object that specifies a matplotlib figure.\n    cmap: A matplotlib colormap object specifying the palette of the\n      predictive surface.\n\n  Returns:\n    pcm: A matplotlib PathCollection object that contains the palette\n      information of the uncertainty plot.\n  \"\"\"\n  # Normalize uncertainty for better visualization.\n  test_uncertainty = test_uncertainty / np.max(test_uncertainty)\n\n  # Set view limits.\n  ax.set_ylim(DEFAULT_Y_RANGE)\n  ax.set_xlim(DEFAULT_X_RANGE)\n\n  # Plot normalized uncertainty surface.\n  pcm = ax.imshow(\n      np.reshape(test_uncertainty, [DEFAULT_N_GRID, DEFAULT_N_GRID]),\n      cmap=cmap,\n      origin=\"lower\",\n      extent=DEFAULT_X_RANGE + DEFAULT_Y_RANGE,\n      vmin=DEFAULT_NORM.vmin,\n      vmax=DEFAULT_NORM.vmax,\n      interpolation='bicubic',\n      aspect='auto')\n\n  # Plot training data.\n  ax.scatter(train_examples[:, 0], train_examples[:, 1],\n             c=train_labels, cmap=DEFAULT_CMAP, alpha=0.5)\n  ax.scatter(ood_examples[:, 0], ood_examples[:, 1], c=\"red\", alpha=0.1)\n\n  return pcm",
      "resnet_logits = resnet_model(test_examples)\nresnet_probs = tf.nn.softmax(resnet_logits, axis=-1)[:, 0]  # Take the probability for class 0.",
      "dense = tf.keras.layers.Dense(units=10)\ndense = nlp_layers.SpectralNormalization(dense, norm_multiplier=0.9)",
      "embedding = tf.random.normal(shape=(batch_size, input_dim))\n\nlogits, covmat = gp_layer(embedding)",
      "class DeepResNetSNGP(DeepResNet):\n  def __init__(self, spec_norm_bound=0.9, **kwargs):\n    self.spec_norm_bound = spec_norm_bound\n    super().__init__(**kwargs)\n\n  def make_dense_layer(self):\n    \"\"\"Applies spectral normalization to the hidden layer.\"\"\"\n    dense_layer = super().make_dense_layer()\n    return nlp_layers.SpectralNormalization(\n        dense_layer, norm_multiplier=self.spec_norm_bound)\n\n  def make_output_layer(self, num_classes):\n    \"\"\"Uses Gaussian process as the output layer.\"\"\"\n    return nlp_layers.RandomFeatureGaussianProcess(\n        num_classes,\n        gp_cov_momentum=-1,\n        **self.classifier_kwargs)\n\n  def call(self, inputs, training=False, return_covmat=False):\n    # Gets logits and a covariance matrix from the GP layer.\n    logits, covmat = super().call(inputs)\n\n    # Returns only logits during training.\n    if not training and return_covmat:\n      return logits, covmat\n\n    return logits",
      "class ResetCovarianceCallback(tf.keras.callbacks.Callback):\n\n  def on_epoch_begin(self, epoch, logs=None):\n    \"\"\"Resets covariance matrix at the beginning of the epoch.\"\"\"\n    if epoch > 0:\n      self.model.classifier.reset_covariance_matrix()",
      "class DeepResNetSNGPWithCovReset(DeepResNetSNGP):\n  def fit(self, *args, **kwargs):\n    \"\"\"Adds ResetCovarianceCallback to model callbacks.\"\"\"\n    kwargs[\"callbacks\"] = list(kwargs.get(\"callbacks\", []))\n    kwargs[\"callbacks\"].append(ResetCovarianceCallback())\n\n    return super().fit(*args, **kwargs)",
      "sngp_variance = tf.linalg.diag_part(sngp_covmat)[:, None]",
      "sngp_logits_adjusted = sngp_logits / tf.sqrt(1. + (np.pi / 8.) * sngp_variance)\nsngp_probs = tf.nn.softmax(sngp_logits_adjusted, axis=-1)[:, 0]",
      "def compute_posterior_mean_probability(logits, covmat, lambda_param=np.pi / 8.):\n  # Computes uncertainty-adjusted logits using the built-in method.\n  logits_adjusted = nlp_layers.gaussian_process.mean_field_logits(\n      logits, covmat, mean_field_factor=lambda_param)\n\n  return tf.nn.softmax(logits_adjusted, axis=-1)[:, 0]",
      "def plot_predictions(pred_probs, model_name=\"\"):\n  \"\"\"Plot normalized class probabilities and predictive uncertainties.\"\"\"\n  # Compute predictive uncertainty.\n  uncertainty = pred_probs * (1. - pred_probs)\n\n  # Initialize the plot axes.\n  fig, axs = plt.subplots(1, 2, figsize=(14, 5))\n\n  # Plots the class probability.\n  pcm_0 = plot_uncertainty_surface(pred_probs, ax=axs[0])\n  # Plots the predictive uncertainty.\n  pcm_1 = plot_uncertainty_surface(uncertainty, ax=axs[1])\n\n  # Adds color bars and titles.\n  fig.colorbar(pcm_0, ax=axs[0])\n  fig.colorbar(pcm_1, ax=axs[1])\n\n  axs[0].set_title(f\"Class Probability, {model_name}\")\n  axs[1].set_title(f\"(Normalized) Predictive Uncertainty, {model_name}\")\n\n  plt.show()",
      "def train_and_test_sngp(train_examples, test_examples):\n  sngp_model = DeepResNetSNGPWithCovReset(**resnet_config)\n\n  sngp_model.compile(**train_config)\n  sngp_model.fit(train_examples, train_labels, verbose=0, **fit_config)\n\n  sngp_logits, sngp_covmat = sngp_model(test_examples, return_covmat=True)\n  sngp_probs = compute_posterior_mean_probability(sngp_logits, sngp_covmat)\n\n  return sngp_probs",
      "def mc_dropout_sampling(test_examples):\n  # Enable dropout during inference.\n  return resnet_model(test_examples, training=True)",
      "# Monte Carlo dropout inference.\ndropout_logit_samples = [mc_dropout_sampling(test_examples) for _ in range(num_ensemble)]\ndropout_prob_samples = [tf.nn.softmax(dropout_logits, axis=-1)[:, 0] for dropout_logits in dropout_logit_samples]\ndropout_probs = tf.reduce_mean(dropout_prob_samples, axis=0)",
      "dropout_probs = tf.reduce_mean(dropout_prob_samples, axis=0)",
      "# Deep ensemble inference\nensemble_logit_samples = [model(test_examples) for model in resnet_ensemble]\nensemble_prob_samples = [tf.nn.softmax(logits, axis=-1)[:, 0] for logits in ensemble_logit_samples]\nensemble_probs = tf.reduce_mean(ensemble_prob_samples, axis=0)"
    ]
  },
  {
    "url": "https://www.tensorflow.org/tutorials/estimator/linear",
    "title": "Build a linear model with Estimators\n      \n\n\n      \n      Stay organized with collections\n    \n\n      \n      Save and categorize content based on your preferences.",
    "code_snippets": [
      "import os\nimport sys\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom IPython.display import clear_output\nfrom six.moves import urllib",
      "import tensorflow.compat.v2.feature_column as fc\n\nimport tensorflow as tf",
      "CATEGORICAL_COLUMNS = ['sex', 'n_siblings_spouses', 'parch', 'class', 'deck',\n                       'embark_town', 'alone']\nNUMERIC_COLUMNS = ['age', 'fare']\n\nfeature_columns = []\nfor feature_name in CATEGORICAL_COLUMNS:\n  vocabulary = dftrain[feature_name].unique()\n  feature_columns.append(tf.feature_column.categorical_column_with_vocabulary_list(feature_name, vocabulary))\n\nfor feature_name in NUMERIC_COLUMNS:\n  feature_columns.append(tf.feature_column.numeric_column(feature_name, dtype=tf.float32))",
      "def make_input_fn(data_df, label_df, num_epochs=10, shuffle=True, batch_size=32):\n  def input_function():\n    ds = tf.data.Dataset.from_tensor_slices((dict(data_df), label_df))\n    if shuffle:\n      ds = ds.shuffle(1000)\n    ds = ds.batch(batch_size).repeat(num_epochs)\n    return ds\n  return input_function\n\ntrain_input_fn = make_input_fn(dftrain, y_train)\neval_input_fn = make_input_fn(dfeval, y_eval, num_epochs=1, shuffle=False)",
      "age_column = feature_columns[7]\ntf.keras.layers.DenseFeatures([age_column])(feature_batch).numpy()",
      "gender_column = feature_columns[0]\ntf.keras.layers.DenseFeatures([tf.feature_column.indicator_column(gender_column)])(feature_batch).numpy()",
      "linear_est = tf.estimator.LinearClassifier(feature_columns=feature_columns)\nlinear_est.train(train_input_fn)\nresult = linear_est.evaluate(eval_input_fn)\n\nclear_output()\nprint(result)",
      "age_x_gender = tf.feature_column.crossed_column(['age', 'sex'], hash_bucket_size=100)",
      "derived_feature_columns = [age_x_gender]\nlinear_est = tf.estimator.LinearClassifier(feature_columns=feature_columns+derived_feature_columns)\nlinear_est.train(train_input_fn)\nresult = linear_est.evaluate(eval_input_fn)\n\nclear_output()\nprint(result)"
    ]
  },
  {
    "url": "https://www.tensorflow.org/tutorials/text/nmt_with_attention",
    "title": "Neural machine translation with attention\n      \n\n\n      \n      Stay organized with collections\n    \n\n      \n      Save and categorize content based on your preferences.",
    "code_snippets": [
      "import numpy as np\n\nimport typing\nfrom typing import Any, Tuple\n\nimport einops\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\n\nimport tensorflow as tf\nimport tensorflow_text as tf_text",
      "class ShapeChecker():\n  def __init__(self):\n    # Keep a cache of every axis-name seen\n    self.shapes = {}\n\n  def __call__(self, tensor, names, broadcast=False):\n    if not tf.executing_eagerly():\n      return\n\n    parsed = einops.parse_shape(tensor, names)\n\n    for name, new_dim in parsed.items():\n      old_dim = self.shapes.get(name, None)\n\n      if (broadcast and new_dim == 1):\n        continue\n\n      if old_dim is None:\n        # If the axis name is new, add its length to the cache.\n        self.shapes[name] = new_dim\n        continue\n\n      if new_dim != old_dim:\n        raise ValueError(f\"Shape mismatch for dimension: '{name}'\\n\"\n                         f\"    found: {new_dim}\\n\"\n                         f\"    expected: {old_dim}\\n\")",
      "# Download the file\nimport pathlib\n\npath_to_zip = tf.keras.utils.get_file(\n    'spa-eng.zip', origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',\n    extract=True)\n\npath_to_file = pathlib.Path(path_to_zip).parent/'spa-eng/spa.txt'",
      "def load_data(path):\n  text = path.read_text(encoding='utf-8')\n\n  lines = text.splitlines()\n  pairs = [line.split('\\t') for line in lines]\n\n  context = np.array([context for target, context in pairs])\n  target = np.array([target for target, context in pairs])\n\n  return target, context",
      "BUFFER_SIZE = len(context_raw)\nBATCH_SIZE = 64\n\nis_train = np.random.uniform(size=(len(target_raw),)) < 0.8\n\ntrain_raw = (\n    tf.data.Dataset\n    .from_tensor_slices((context_raw[is_train], target_raw[is_train]))\n    .shuffle(BUFFER_SIZE)\n    .batch(BATCH_SIZE))\nval_raw = (\n    tf.data.Dataset\n    .from_tensor_slices((context_raw[~is_train], target_raw[~is_train]))\n    .shuffle(BUFFER_SIZE)\n    .batch(BATCH_SIZE))",
      "example_text = tf.constant('\u00bfTodav\u00eda est\u00e1 en casa?')\n\nprint(example_text.numpy())\nprint(tf_text.normalize_utf8(example_text, 'NFKD').numpy())",
      "def tf_lower_and_split_punct(text):\n  # Split accented characters.\n  text = tf_text.normalize_utf8(text, 'NFKD')\n  text = tf.strings.lower(text)\n  # Keep space, a to z, and select punctuation.\n  text = tf.strings.regex_replace(text, '[^ a-z.?!,\u00bf]', '')\n  # Add spaces around punctuation.\n  text = tf.strings.regex_replace(text, '[.?!,\u00bf]', r' \\0 ')\n  # Strip whitespace.\n  text = tf.strings.strip(text)\n\n  text = tf.strings.join(['[START]', text, '[END]'], separator=' ')\n  return text",
      "max_vocab_size = 5000\n\ncontext_text_processor = tf.keras.layers.TextVectorization(\n    standardize=tf_lower_and_split_punct,\n    max_tokens=max_vocab_size,\n    ragged=True)",
      "target_text_processor = tf.keras.layers.TextVectorization(\n    standardize=tf_lower_and_split_punct,\n    max_tokens=max_vocab_size,\n    ragged=True)\n\ntarget_text_processor.adapt(train_raw.map(lambda context, target: target))\ntarget_text_processor.get_vocabulary()[:10]",
      "def process_text(context, target):\n  context = context_text_processor(context).to_tensor()\n  target = target_text_processor(target)\n  targ_in = target[:,:-1].to_tensor()\n  targ_out = target[:,1:].to_tensor()\n  return (context, targ_in), targ_out\n\n\ntrain_ds = train_raw.map(process_text, tf.data.AUTOTUNE)\nval_ds = val_raw.map(process_text, tf.data.AUTOTUNE)",
      "class Encoder(tf.keras.layers.Layer):\n  def __init__(self, text_processor, units):\n    super(Encoder, self).__init__()\n    self.text_processor = text_processor\n    self.vocab_size = text_processor.vocabulary_size()\n    self.units = units\n\n    # The embedding layer converts tokens to vectors\n    self.embedding = tf.keras.layers.Embedding(self.vocab_size, units,\n                                               mask_zero=True)\n\n    # The RNN layer processes those vectors sequentially.\n    self.rnn = tf.keras.layers.Bidirectional(\n        merge_mode='sum',\n        layer=tf.keras.layers.GRU(units,\n                            # Return the sequence and state\n                            return_sequences=True,\n                            recurrent_initializer='glorot_uniform'))\n\n  def call(self, x):\n    shape_checker = ShapeChecker()\n    shape_checker(x, 'batch s')\n\n    # 2. The embedding layer looks up the embedding vector for each token.\n    x = self.embedding(x)\n    shape_checker(x, 'batch s units')\n\n    # 3. The GRU processes the sequence of embeddings.\n    x = self.rnn(x)\n    shape_checker(x, 'batch s units')\n\n    # 4. Returns the new sequence of embeddings.\n    return x\n\n  def convert_input(self, texts):\n    texts = tf.convert_to_tensor(texts)\n    if len(texts.shape) == 0:\n      texts = tf.convert_to_tensor(texts)[tf.newaxis]\n    context = self.text_processor(texts).to_tensor()\n    context = self(context)\n    return context",
      "class CrossAttention(tf.keras.layers.Layer):\n  def __init__(self, units, **kwargs):\n    super().__init__()\n    self.mha = tf.keras.layers.MultiHeadAttention(key_dim=units, num_heads=1, **kwargs)\n    self.layernorm = tf.keras.layers.LayerNormalization()\n    self.add = tf.keras.layers.Add()\n\n  def call(self, x, context):\n    shape_checker = ShapeChecker()\n\n    shape_checker(x, 'batch t units')\n    shape_checker(context, 'batch s units')\n\n    attn_output, attn_scores = self.mha(\n        query=x,\n        value=context,\n        return_attention_scores=True)\n\n    shape_checker(x, 'batch t units')\n    shape_checker(attn_scores, 'batch heads t s')\n\n    # Cache the attention scores for plotting later.\n    attn_scores = tf.reduce_mean(attn_scores, axis=1)\n    shape_checker(attn_scores, 'batch t s')\n    self.last_attention_weights = attn_scores\n\n    x = self.add([x, attn_output])\n    x = self.layernorm(x)\n\n    return x",
      "attention_layer = CrossAttention(UNITS)\n\n# Attend to the encoded tokens\nembed = tf.keras.layers.Embedding(target_text_processor.vocabulary_size(),\n                                  output_dim=UNITS, mask_zero=True)\nex_tar_embed = embed(ex_tar_in)\n\nresult = attention_layer(ex_tar_embed, ex_context)\n\nprint(f'Context sequence, shape (batch, s, units): {ex_context.shape}')\nprint(f'Target sequence, shape (batch, t, units): {ex_tar_embed.shape}')\nprint(f'Attention result, shape (batch, t, units): {result.shape}')\nprint(f'Attention weights, shape (batch, t, s):    {attention_layer.last_attention_weights.shape}')",
      "class Decoder(tf.keras.layers.Layer):\n  @classmethod\n  def add_method(cls, fun):\n    setattr(cls, fun.__name__, fun)\n    return fun\n\n  def __init__(self, text_processor, units):\n    super(Decoder, self).__init__()\n    self.text_processor = text_processor\n    self.vocab_size = text_processor.vocabulary_size()\n    self.word_to_id = tf.keras.layers.StringLookup(\n        vocabulary=text_processor.get_vocabulary(),\n        mask_token='', oov_token='[UNK]')\n    self.id_to_word = tf.keras.layers.StringLookup(\n        vocabulary=text_processor.get_vocabulary(),\n        mask_token='', oov_token='[UNK]',\n        invert=True)\n    self.start_token = self.word_to_id('[START]')\n    self.end_token = self.word_to_id('[END]')\n\n    self.units = units\n\n\n    # 1. The embedding layer converts token IDs to vectors\n    self.embedding = tf.keras.layers.Embedding(self.vocab_size,\n                                               units, mask_zero=True)\n\n    # 2. The RNN keeps track of what's been generated so far.\n    self.rnn = tf.keras.layers.GRU(units,\n                                   return_sequences=True,\n                                   return_state=True,\n                                   recurrent_initializer='glorot_uniform')\n\n    # 3. The RNN output will be the query for the attention layer.\n    self.attention = CrossAttention(units)\n\n    # 4. This fully connected layer produces the logits for each\n    # output token.\n    self.output_layer = tf.keras.layers.Dense(self.vocab_size)",
      "@Decoder.add_method\ndef get_initial_state(self, context):\n  batch_size = tf.shape(context)[0]\n  start_tokens = tf.fill([batch_size, 1], self.start_token)\n  done = tf.zeros([batch_size, 1], dtype=tf.bool)\n  embedded = self.embedding(start_tokens)\n  return start_tokens, done, self.rnn.get_initial_state(embedded)[0]",
      "@Decoder.add_method\ndef tokens_to_text(self, tokens):\n  words = self.id_to_word(tokens)\n  result = tf.strings.reduce_join(words, axis=-1, separator=' ')\n  result = tf.strings.regex_replace(result, '^ *\\[START\\] *', '')\n  result = tf.strings.regex_replace(result, ' *\\[END\\] *$', '')\n  return result",
      "@Decoder.add_method\ndef get_next_token(self, context, next_token, done, state, temperature = 0.0):\n  logits, state = self(\n    context, next_token,\n    state = state,\n    return_state=True) \n\n  if temperature == 0.0:\n    next_token = tf.argmax(logits, axis=-1)\n  else:\n    logits = logits[:, -1, :]/temperature\n    next_token = tf.random.categorical(logits, num_samples=1)\n\n  # If a sequence produces an `end_token`, set it `done`\n  done = done | (next_token == self.end_token)\n  # Once a sequence is done it only produces 0-padding.\n  next_token = tf.where(done, tf.constant(0, dtype=tf.int64), next_token)\n\n  return next_token, done, state",
      "# Setup the loop variables.\nnext_token, done, state = decoder.get_initial_state(ex_context)\ntokens = []\n\nfor n in range(10):\n  # Run one step.\n  next_token, done, state = decoder.get_next_token(\n      ex_context, next_token, done, state, temperature=1.0)\n  # Add the token to the output.\n  tokens.append(next_token)\n\n# Stack all the tokens together.\ntokens = tf.concat(tokens, axis=-1) # (batch, t)\n\n# Convert the tokens back to a a string\nresult = decoder.tokens_to_text(tokens)\nresult[:3].numpy()",
      "class Translator(tf.keras.Model):\n  @classmethod\n  def add_method(cls, fun):\n    setattr(cls, fun.__name__, fun)\n    return fun\n\n  def __init__(self, units,\n               context_text_processor,\n               target_text_processor):\n    super().__init__()\n    # Build the encoder and decoder\n    encoder = Encoder(context_text_processor, units)\n    decoder = Decoder(target_text_processor, units)\n\n    self.encoder = encoder\n    self.decoder = decoder\n\n  def call(self, inputs):\n    context, x = inputs\n    context = self.encoder(context)\n    logits = self.decoder(context, x)\n\n    #TODO(b/250038731): remove this\n    try:\n      # Delete the keras mask, so keras doesn't scale the loss+accuracy. \n      del logits._keras_mask\n    except AttributeError:\n      pass\n\n    return logits",
      "def masked_loss(y_true, y_pred):\n    # Calculate the loss for each item in the batch.\n    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(\n        from_logits=True, reduction='none')\n    loss = loss_fn(y_true, y_pred)\n\n    # Mask off the losses on padding.\n    mask = tf.cast(y_true != 0, loss.dtype)\n    loss *= mask\n\n    # Return the total.\n    return tf.reduce_sum(loss)/tf.reduce_sum(mask)",
      "def masked_acc(y_true, y_pred):\n    # Calculate the loss for each item in the batch.\n    y_pred = tf.argmax(y_pred, axis=-1)\n    y_pred = tf.cast(y_pred, y_true.dtype)\n\n    match = tf.cast(y_true == y_pred, tf.float32)\n    mask = tf.cast(y_true != 0, tf.float32)\n\n    return tf.reduce_sum(match)/tf.reduce_sum(mask)",
      "vocab_size = 1.0 * target_text_processor.vocabulary_size()\n\n{\"expected_loss\": tf.math.log(vocab_size).numpy(),\n \"expected_acc\": 1/vocab_size}",
      "history = model.fit(\n    train_ds.repeat(), \n    epochs=100,\n    steps_per_epoch = 100,\n    validation_data=val_ds,\n    validation_steps = 20,\n    callbacks=[\n        tf.keras.callbacks.EarlyStopping(patience=3)])",
      "@Translator.add_method\ndef translate(self,\n              texts, *,\n              max_length=50,\n              temperature=0.0):\n  # Process the input texts\n  context = self.encoder.convert_input(texts)\n  batch_size = tf.shape(texts)[0]\n\n  # Setup the loop inputs\n  tokens = []\n  attention_weights = []\n  next_token, done, state = self.decoder.get_initial_state(context)\n\n  for _ in range(max_length):\n    # Generate the next token\n    next_token, done, state = self.decoder.get_next_token(\n        context, next_token, done,  state, temperature)\n\n    # Collect the generated tokens\n    tokens.append(next_token)\n    attention_weights.append(self.decoder.last_attention_weights)\n\n    if tf.executing_eagerly() and tf.reduce_all(done):\n      break\n\n  # Stack the lists of tokens and attention weights.\n  tokens = tf.concat(tokens, axis=-1)   # t*[(batch 1)] -> (batch, t)\n  self.last_attention_weights = tf.concat(attention_weights, axis=1)  # t*[(batch 1 s)] -> (batch, t s)\n\n  result = self.decoder.tokens_to_text(tokens)\n  return result",
      "class Export(tf.Module):\n  def __init__(self, model):\n    self.model = model\n\n  @tf.function(input_signature=[tf.TensorSpec(dtype=tf.string, shape=[None])])\n  def translate(self, inputs):\n    return self.model.translate(inputs)",
      "%%time\n_ = export.translate(tf.constant(inputs))",
      "%%time\nresult = export.translate(tf.constant(inputs))\n\nprint(result[0].numpy().decode())\nprint(result[1].numpy().decode())\nprint(result[2].numpy().decode())\nprint()",
      "%%time\ntf.saved_model.save(export, 'translator',\n                    signatures={'serving_default': export.translate})",
      "%%time\nreloaded = tf.saved_model.load('translator')\n_ = reloaded.translate(tf.constant(inputs)) #warmup",
      "%%time\nresult = reloaded.translate(tf.constant(inputs))\n\nprint(result[0].numpy().decode())\nprint(result[1].numpy().decode())\nprint(result[2].numpy().decode())\nprint()",
      "for _ in range(max_length):\n  ...\n  if tf.executing_eagerly() and tf.reduce_all(done):\n    break",
      "for t in tf.range(max_length):\n  ...\n  if tf.reduce_all(done):\n      break",
      "tokens = tf.TensorArray(tf.int64, size=1, dynamic_size=True)\n...\nfor t in tf.range(max_length):\n    ...\n    tokens = tokens.write(t, next_token) # next_token shape is (batch, 1)\n  ...\n  tokens = tokens.stack()\n  tokens = einops.rearrange(tokens, 't batch 1 -> batch t')",
      "@Translator.add_method\ndef translate(self,\n              texts,\n              *,\n              max_length=500,\n              temperature=tf.constant(0.0)):\n  shape_checker = ShapeChecker()\n  context = self.encoder.convert_input(texts)\n  batch_size = tf.shape(context)[0]\n  shape_checker(context, 'batch s units')\n\n  next_token, done, state = self.decoder.get_initial_state(context)\n\n  # initialize the accumulator\n  tokens = tf.TensorArray(tf.int64, size=1, dynamic_size=True)\n\n  for t in tf.range(max_length):\n    # Generate the next token\n    next_token, done, state = self.decoder.get_next_token(\n        context, next_token, done, state, temperature)\n    shape_checker(next_token, 'batch t1')\n\n    # Collect the generated tokens\n    tokens = tokens.write(t, next_token)\n\n    # if all the sequences are done, break\n    if tf.reduce_all(done):\n      break\n\n  # Convert the list of generated token ids to a list of strings.\n  tokens = tokens.stack()\n  shape_checker(tokens, 't batch t1')\n  tokens = einops.rearrange(tokens, 't batch 1 -> batch t')\n  shape_checker(tokens, 'batch t')\n\n  text = self.decoder.tokens_to_text(tokens)\n  shape_checker(text, 'batch')\n\n  return text",
      "class Export(tf.Module):\n  def __init__(self, model):\n    self.model = model\n\n  @tf.function(input_signature=[tf.TensorSpec(dtype=tf.string, shape=[None])])\n  def translate(self, inputs):\n    return self.model.translate(inputs)",
      "%%time\ntf.saved_model.save(export, 'dynamic_translator',\n                    signatures={'serving_default': export.translate})",
      "%%time\nreloaded = tf.saved_model.load('dynamic_translator')\n_ = reloaded.translate(tf.constant(inputs)) #warmup",
      "%%time\nresult = reloaded.translate(tf.constant(inputs))\n\nprint(result[0].numpy().decode())\nprint(result[1].numpy().decode())\nprint(result[2].numpy().decode())\nprint()"
    ]
  },
  {
    "url": "https://www.tensorflow.org/tutorials/estimator/keras_model_to_estimator",
    "title": "Create an Estimator from a Keras model\n      \n\n\n      \n      Stay organized with collections\n    \n\n      \n      Save and categorize content based on your preferences.",
    "code_snippets": [
      "import tensorflow as tf\n\nimport numpy as np\nimport tensorflow_datasets as tfds",
      "model = tf.keras.models.Sequential([\n    tf.keras.layers.Dense(16, activation='relu', input_shape=(4,)),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(3)\n])",
      "model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              optimizer='adam')\nmodel.summary()",
      "def input_fn():\n  split = tfds.Split.TRAIN\n  dataset = tfds.load('iris', split=split, as_supervised=True)\n  dataset = dataset.map(lambda features, labels: ({'dense_input':features}, labels))\n  dataset = dataset.batch(32).repeat()\n  return dataset",
      "import tempfile\nmodel_dir = tempfile.mkdtemp()\nkeras_estimator = tf.keras.estimator.model_to_estimator(\n    keras_model=model, model_dir=model_dir)"
    ]
  },
  {
    "url": "https://www.tensorflow.org/tutorials/generative/deepdream",
    "title": "DeepDream\n      \n\n\n      \n      Stay organized with collections\n    \n\n      \n      Save and categorize content based on your preferences.",
    "code_snippets": [
      "import tensorflow as tf",
      "import numpy as np\n\nimport matplotlib as mpl\n\nimport IPython.display as display\nimport PIL.Image",
      "# Download an image and read it into a NumPy array.\ndef download(url, max_dim=None):\n  name = url.split('/')[-1]\n  image_path = tf.keras.utils.get_file(name, origin=url)\n  img = PIL.Image.open(image_path)\n  if max_dim:\n    img.thumbnail((max_dim, max_dim))\n  return np.array(img)\n\n# Normalize an image\ndef deprocess(img):\n  img = 255*(img + 1.0)/2.0\n  return tf.cast(img, tf.uint8)\n\n# Display an image\ndef show(img):\n  display.display(PIL.Image.fromarray(np.array(img)))\n\n\n# Downsizing the image makes it easier to work with.\noriginal_img = download(url, max_dim=500)\nshow(original_img)\ndisplay.display(display.HTML('Image cc-by: <a \"href=https://commons.wikimedia.org/wiki/File:Felis_catus-cat_on_snow.jpg\">Von.grzanka</a>'))",
      "base_model = tf.keras.applications.InceptionV3(include_top=False, weights='imagenet')",
      "# Maximize the activations of these layers\nnames = ['mixed3', 'mixed5']\nlayers = [base_model.get_layer(name).output for name in names]\n\n# Create the feature extraction model\ndream_model = tf.keras.Model(inputs=base_model.input, outputs=layers)",
      "def calc_loss(img, model):\n  # Pass forward the image through the model to retrieve the activations.\n  # Converts the image into a batch of size 1.\n  img_batch = tf.expand_dims(img, axis=0)\n  layer_activations = model(img_batch)\n  if len(layer_activations) == 1:\n    layer_activations = [layer_activations]\n\n  losses = []\n  for act in layer_activations:\n    loss = tf.math.reduce_mean(act)\n    losses.append(loss)\n\n  return  tf.reduce_sum(losses)",
      "class DeepDream(tf.Module):\n  def __init__(self, model):\n    self.model = model\n\n  @tf.function(\n      input_signature=(\n        tf.TensorSpec(shape=[None,None,3], dtype=tf.float32),\n        tf.TensorSpec(shape=[], dtype=tf.int32),\n        tf.TensorSpec(shape=[], dtype=tf.float32),)\n  )\n  def __call__(self, img, steps, step_size):\n      print(\"Tracing\")\n      loss = tf.constant(0.0)\n      for n in tf.range(steps):\n        with tf.GradientTape() as tape:\n          # This needs gradients relative to `img`\n          # `GradientTape` only watches `tf.Variable`s by default\n          tape.watch(img)\n          loss = calc_loss(img, self.model)\n\n        # Calculate the gradient of the loss with respect to the pixels of the input image.\n        gradients = tape.gradient(loss, img)\n\n        # Normalize the gradients.\n        gradients /= tf.math.reduce_std(gradients) + 1e-8 \n\n        # In gradient ascent, the \"loss\" is maximized so that the input image increasingly \"excites\" the layers.\n        # You can update the image by directly adding the gradients (because they're the same shape!)\n        img = img + gradients*step_size\n        img = tf.clip_by_value(img, -1, 1)\n\n      return loss, img",
      "def run_deep_dream_simple(img, steps=100, step_size=0.01):\n  # Convert from uint8 to the range expected by the model.\n  img = tf.keras.applications.inception_v3.preprocess_input(img)\n  img = tf.convert_to_tensor(img)\n  step_size = tf.convert_to_tensor(step_size)\n  steps_remaining = steps\n  step = 0\n  while steps_remaining:\n    if steps_remaining>100:\n      run_steps = tf.constant(100)\n    else:\n      run_steps = tf.constant(steps_remaining)\n    steps_remaining -= run_steps\n    step += run_steps\n\n    loss, img = deepdream(img, run_steps, tf.constant(step_size))\n\n    display.clear_output(wait=True)\n    show(deprocess(img))\n    print (\"Step {}, loss {}\".format(step, loss))\n\n\n  result = deprocess(img)\n  display.clear_output(wait=True)\n  show(result)\n\n  return result",
      "import time\nstart = time.time()\n\nOCTAVE_SCALE = 1.30\n\nimg = tf.constant(np.array(original_img))\nbase_shape = tf.shape(img)[:-1]\nfloat_base_shape = tf.cast(base_shape, tf.float32)\n\nfor n in range(-2, 3):\n  new_shape = tf.cast(float_base_shape*(OCTAVE_SCALE**n), tf.int32)\n\n  img = tf.image.resize(img, new_shape).numpy()\n\n  img = run_deep_dream_simple(img=img, steps=50, step_size=0.01)\n\ndisplay.clear_output(wait=True)\nimg = tf.image.resize(img, base_shape)\nimg = tf.image.convert_image_dtype(img/255.0, dtype=tf.uint8)\nshow(img)\n\nend = time.time()\nend-start",
      "def random_roll(img, maxroll):\n  # Randomly shift the image to avoid tiled boundaries.\n  shift = tf.random.uniform(shape=[2], minval=-maxroll, maxval=maxroll, dtype=tf.int32)\n  img_rolled = tf.roll(img, shift=shift, axis=[0,1])\n  return shift, img_rolled",
      "class TiledGradients(tf.Module):\n  def __init__(self, model):\n    self.model = model\n\n  @tf.function(\n      input_signature=(\n        tf.TensorSpec(shape=[None,None,3], dtype=tf.float32),\n        tf.TensorSpec(shape=[2], dtype=tf.int32),\n        tf.TensorSpec(shape=[], dtype=tf.int32),)\n  )\n  def __call__(self, img, img_size, tile_size=512):\n    shift, img_rolled = random_roll(img, tile_size)\n\n    # Initialize the image gradients to zero.\n    gradients = tf.zeros_like(img_rolled)\n\n    # Skip the last tile, unless there's only one tile.\n    xs = tf.range(0, img_size[1], tile_size)[:-1]\n    if not tf.cast(len(xs), bool):\n      xs = tf.constant([0])\n    ys = tf.range(0, img_size[0], tile_size)[:-1]\n    if not tf.cast(len(ys), bool):\n      ys = tf.constant([0])\n\n    for x in xs:\n      for y in ys:\n        # Calculate the gradients for this tile.\n        with tf.GradientTape() as tape:\n          # This needs gradients relative to `img_rolled`.\n          # `GradientTape` only watches `tf.Variable`s by default.\n          tape.watch(img_rolled)\n\n          # Extract a tile out of the image.\n          img_tile = img_rolled[y:y+tile_size, x:x+tile_size]\n          loss = calc_loss(img_tile, self.model)\n\n        # Update the image gradients for this tile.\n        gradients = gradients + tape.gradient(loss, img_rolled)\n\n    # Undo the random shift applied to the image and its gradients.\n    gradients = tf.roll(gradients, shift=-shift, axis=[0,1])\n\n    # Normalize the gradients.\n    gradients /= tf.math.reduce_std(gradients) + 1e-8 \n\n    return gradients",
      "def run_deep_dream_with_octaves(img, steps_per_octave=100, step_size=0.01, \n                                octaves=range(-2,3), octave_scale=1.3):\n  base_shape = tf.shape(img)\n  img = tf.keras.utils.img_to_array(img)\n  img = tf.keras.applications.inception_v3.preprocess_input(img)\n\n  initial_shape = img.shape[:-1]\n  img = tf.image.resize(img, initial_shape)\n  for octave in octaves:\n    # Scale the image based on the octave\n    new_size = tf.cast(tf.convert_to_tensor(base_shape[:-1]), tf.float32)*(octave_scale**octave)\n    new_size = tf.cast(new_size, tf.int32)\n    img = tf.image.resize(img, new_size)\n\n    for step in range(steps_per_octave):\n      gradients = get_tiled_gradients(img, new_size)\n      img = img + gradients*step_size\n      img = tf.clip_by_value(img, -1, 1)\n\n      if step % 10 == 0:\n        display.clear_output(wait=True)\n        show(deprocess(img))\n        print (\"Octave {}, Step {}\".format(octave, step))\n\n  result = deprocess(img)\n  return result",
      "img = run_deep_dream_with_octaves(img=original_img, step_size=0.01)\n\ndisplay.clear_output(wait=True)\nimg = tf.image.resize(img, base_shape)\nimg = tf.image.convert_image_dtype(img/255.0, dtype=tf.uint8)\nshow(img)"
    ]
  },
  {
    "url": "https://www.tensorflow.org/tutorials/images/segmentation",
    "title": "Image segmentation\n      \n\n\n      \n      Stay organized with collections\n    \n\n      \n      Save and categorize content based on your preferences.",
    "code_snippets": [
      "import numpy as np\n\nimport tensorflow as tf\nimport tensorflow_datasets as tfds",
      "def normalize(input_image, input_mask):\n  input_image = tf.cast(input_image, tf.float32) / 255.0\n  input_mask -= 1\n  return input_image, input_mask",
      "def load_image(datapoint):\n  input_image = tf.image.resize(datapoint['image'], (128, 128))\n  input_mask = tf.image.resize(\n    datapoint['segmentation_mask'],\n    (128, 128),\n    method = tf.image.ResizeMethod.NEAREST_NEIGHBOR,\n  )\n\n  input_image, input_mask = normalize(input_image, input_mask)\n\n  return input_image, input_mask",
      "train_images = dataset['train'].map(load_image, num_parallel_calls=tf.data.AUTOTUNE)\ntest_images = dataset['test'].map(load_image, num_parallel_calls=tf.data.AUTOTUNE)",
      "class Augment(tf.keras.layers.Layer):\n  def __init__(self, seed=42):\n    super().__init__()\n    # both use the same seed, so they'll make the same random changes.\n    self.augment_inputs = tf.keras.layers.RandomFlip(mode=\"horizontal\", seed=seed)\n    self.augment_labels = tf.keras.layers.RandomFlip(mode=\"horizontal\", seed=seed)\n\n  def call(self, inputs, labels):\n    inputs = self.augment_inputs(inputs)\n    labels = self.augment_labels(labels)\n    return inputs, labels",
      "train_batches = (\n    train_images\n    .cache()\n    .shuffle(BUFFER_SIZE)\n    .batch(BATCH_SIZE)\n    .repeat()\n    .map(Augment())\n    .prefetch(buffer_size=tf.data.AUTOTUNE))\n\ntest_batches = test_images.batch(BATCH_SIZE)",
      "def display(display_list):\n  plt.figure(figsize=(15, 15))\n\n  title = ['Input Image', 'True Mask', 'Predicted Mask']\n\n  for i in range(len(display_list)):\n    plt.subplot(1, len(display_list), i+1)\n    plt.title(title[i])\n    plt.imshow(tf.keras.utils.array_to_img(display_list[i]))\n    plt.axis('off')\n  plt.show()",
      "base_model = tf.keras.applications.MobileNetV2(input_shape=[128, 128, 3], include_top=False)\n\n# Use the activations of these layers\nlayer_names = [\n    'block_1_expand_relu',   # 64x64\n    'block_3_expand_relu',   # 32x32\n    'block_6_expand_relu',   # 16x16\n    'block_13_expand_relu',  # 8x8\n    'block_16_project',      # 4x4\n]\nbase_model_outputs = [base_model.get_layer(name).output for name in layer_names]\n\n# Create the feature extraction model\ndown_stack = tf.keras.Model(inputs=base_model.input, outputs=base_model_outputs)\n\ndown_stack.trainable = False",
      "def unet_model(output_channels:int):\n  inputs = tf.keras.layers.Input(shape=[128, 128, 3])\n\n  # Downsampling through the model\n  skips = down_stack(inputs)\n  x = skips[-1]\n  skips = reversed(skips[:-1])\n\n  # Upsampling and establishing the skip connections\n  for up, skip in zip(up_stack, skips):\n    x = up(x)\n    concat = tf.keras.layers.Concatenate()\n    x = concat([x, skip])\n\n  # This is the last layer of the model\n  last = tf.keras.layers.Conv2DTranspose(\n      filters=output_channels, kernel_size=3, strides=2,\n      padding='same')  #64x64 -> 128x128\n\n  x = last(x)\n\n  return tf.keras.Model(inputs=inputs, outputs=x)",
      "OUTPUT_CLASSES = 3\n\nmodel = unet_model(output_channels=OUTPUT_CLASSES)\nmodel.compile(optimizer='adam',\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])",
      "tf.keras.utils.plot_model(model, show_shapes=True, expand_nested=True, dpi=64)",
      "def create_mask(pred_mask):\n  pred_mask = tf.math.argmax(pred_mask, axis=-1)\n  pred_mask = pred_mask[..., tf.newaxis]\n  return pred_mask[0]",
      "def show_predictions(dataset=None, num=1):\n  if dataset:\n    for image, mask in dataset.take(num):\n      pred_mask = model.predict(image)\n      display([image[0], mask[0], create_mask(pred_mask)])\n  else:\n    display([sample_image, sample_mask,\n             create_mask(model.predict(sample_image[tf.newaxis, ...]))])",
      "class DisplayCallback(tf.keras.callbacks.Callback):\n  def on_epoch_end(self, epoch, logs=None):\n    clear_output(wait=True)\n    show_predictions()\n    print ('\\nSample Prediction after epoch {}\\n'.format(epoch+1))",
      "label = np.array([0,0])\nprediction = np.array([[-3., 0], [-3, 0]])\nsample_weight = [1, 10]\n\nloss = tf.keras.losses.SparseCategoricalCrossentropy(\n    from_logits=True,\n    reduction=tf.keras.losses.Reduction.NONE\n)\nloss(label, prediction, sample_weight).numpy()",
      "def add_sample_weights(image, label):\n  # The weights for each class, with the constraint that:\n  #     sum(class_weights) == 1.0\n  class_weights = tf.constant([2.0, 2.0, 1.0])\n  class_weights = class_weights/tf.reduce_sum(class_weights)\n\n  # Create an image of `sample_weights` by using the label at each pixel as an\n  # index into the `class weights` .\n  sample_weights = tf.gather(class_weights, indices=tf.cast(label, tf.int32))\n\n  return image, label, sample_weights",
      "(TensorSpec(shape=(None, 128, 128, 3), dtype=tf.float32, name=None),\n TensorSpec(shape=(None, 128, 128, 1), dtype=tf.float32, name=None),\n TensorSpec(shape=(None, 128, 128, 1), dtype=tf.float32, name=None))",
      "weighted_model = unet_model(OUTPUT_CLASSES)\nweighted_model.compile(\n    optimizer='adam',\n    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    metrics=['accuracy'])"
    ]
  },
  {
    "url": "https://www.tensorflow.org/tutorials/images/data_augmentation",
    "title": "Data augmentation\n      \n\n\n      \n      Stay organized with collections\n    \n\n      \n      Save and categorize content based on your preferences.",
    "code_snippets": [
      "import matplotlib.pyplot as plt\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\n\nfrom tensorflow.keras import layers",
      "IMG_SIZE = 180\n\nresize_and_rescale = tf.keras.Sequential([\n  layers.Resizing(IMG_SIZE, IMG_SIZE),\n  layers.Rescaling(1./255)\n])",
      "data_augmentation = tf.keras.Sequential([\n  layers.RandomFlip(\"horizontal_and_vertical\"),\n  layers.RandomRotation(0.2),\n])",
      "# Add the image to a batch.\nimage = tf.cast(tf.expand_dims(image, 0), tf.float32)",
      "model = tf.keras.Sequential([\n  # Add the preprocessing layers you created earlier.\n  resize_and_rescale,\n  data_augmentation,\n  layers.Conv2D(16, 3, padding='same', activation='relu'),\n  layers.MaxPooling2D(),\n  # Rest of your model.\n])",
      "batch_size = 32\nAUTOTUNE = tf.data.AUTOTUNE\n\ndef prepare(ds, shuffle=False, augment=False):\n  # Resize and rescale all datasets.\n  ds = ds.map(lambda x, y: (resize_and_rescale(x), y), \n              num_parallel_calls=AUTOTUNE)\n\n  if shuffle:\n    ds = ds.shuffle(1000)\n\n  # Batch all datasets.\n  ds = ds.batch(batch_size)\n\n  # Use data augmentation only on the training set.\n  if augment:\n    ds = ds.map(lambda x, y: (data_augmentation(x, training=True), y), \n                num_parallel_calls=AUTOTUNE)\n\n  # Use buffered prefetching on all datasets.\n  return ds.prefetch(buffer_size=AUTOTUNE)",
      "model = tf.keras.Sequential([\n  layers.Conv2D(16, 3, padding='same', activation='relu'),\n  layers.MaxPooling2D(),\n  layers.Conv2D(32, 3, padding='same', activation='relu'),\n  layers.MaxPooling2D(),\n  layers.Conv2D(64, 3, padding='same', activation='relu'),\n  layers.MaxPooling2D(),\n  layers.Flatten(),\n  layers.Dense(128, activation='relu'),\n  layers.Dense(num_classes)\n])",
      "model.compile(optimizer='adam',\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])",
      "def random_invert_img(x, p=0.5):\n  if  tf.random.uniform([]) < p:\n    x = (255-x)\n  else:\n    x\n  return x",
      "def random_invert(factor=0.5):\n  return layers.Lambda(lambda x: random_invert_img(x, factor))\n\nrandom_invert = random_invert()",
      "class RandomInvert(layers.Layer):\n  def __init__(self, factor=0.5, **kwargs):\n    super().__init__(**kwargs)\n    self.factor = factor\n\n  def call(self, x):\n    return random_invert_img(x)",
      "def visualize(original, augmented):\n  fig = plt.figure()\n  plt.subplot(1,2,1)\n  plt.title('Original image')\n  plt.imshow(original)\n\n  plt.subplot(1,2,2)\n  plt.title('Augmented image')\n  plt.imshow(augmented)",
      "flipped = tf.image.flip_left_right(image)\nvisualize(image, flipped)",
      "grayscaled = tf.image.rgb_to_grayscale(image)\nvisualize(image, tf.squeeze(grayscaled))\n_ = plt.colorbar()",
      "saturated = tf.image.adjust_saturation(image, 3)\nvisualize(image, saturated)",
      "bright = tf.image.adjust_brightness(image, 0.4)\nvisualize(image, bright)",
      "cropped = tf.image.central_crop(image, central_fraction=0.5)\nvisualize(image, cropped)",
      "rotated = tf.image.rot90(image)\nvisualize(image, rotated)",
      "for i in range(3):\n  seed = (i, 0)  # tuple of size (2,)\n  stateless_random_brightness = tf.image.stateless_random_brightness(\n      image, max_delta=0.95, seed=seed)\n  visualize(image, stateless_random_brightness)",
      "for i in range(3):\n  seed = (i, 0)  # tuple of size (2,)\n  stateless_random_contrast = tf.image.stateless_random_contrast(\n      image, lower=0.1, upper=0.9, seed=seed)\n  visualize(image, stateless_random_contrast)",
      "for i in range(3):\n  seed = (i, 0)  # tuple of size (2,)\n  stateless_random_crop = tf.image.stateless_random_crop(\n      image, size=[210, 300, 3], seed=seed)\n  visualize(image, stateless_random_crop)",
      "def resize_and_rescale(image, label):\n  image = tf.cast(image, tf.float32)\n  image = tf.image.resize(image, [IMG_SIZE, IMG_SIZE])\n  image = (image / 255.0)\n  return image, label",
      "def augment(image_label, seed):\n  image, label = image_label\n  image, label = resize_and_rescale(image, label)\n  image = tf.image.resize_with_crop_or_pad(image, IMG_SIZE + 6, IMG_SIZE + 6)\n  # Make a new seed.\n  new_seed = tf.random.split(seed, num=1)[0, :]\n  # Random crop back to the original size.\n  image = tf.image.stateless_random_crop(\n      image, size=[IMG_SIZE, IMG_SIZE, 3], seed=seed)\n  # Random brightness.\n  image = tf.image.stateless_random_brightness(\n      image, max_delta=0.5, seed=new_seed)\n  image = tf.clip_by_value(image, 0, 1)\n  return image, label",
      "# Create a `Counter` object and `Dataset.zip` it together with the training set.\ncounter = tf.data.experimental.Counter()\ntrain_ds = tf.data.Dataset.zip((train_datasets, (counter, counter)))",
      "WARNING:tensorflow:From /tmpfs/tmp/ipykernel_85770/587852618.py:2: CounterV2 (from tensorflow.python.data.experimental.ops.counter) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse `tf.data.Dataset.counter(...)` instead.\nWARNING:tensorflow:From /tmpfs/tmp/ipykernel_85770/587852618.py:2: CounterV2 (from tensorflow.python.data.experimental.ops.counter) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse `tf.data.Dataset.counter(...)` instead.",
      "# Create a generator.\nrng = tf.random.Generator.from_seed(123, alg='philox')"
    ]
  },
  {
    "url": "https://www.tensorflow.org/tutorials/customization/custom_layers",
    "title": "Custom layers\n      \n\n\n      \n      Stay organized with collections\n    \n\n      \n      Save and categorize content based on your preferences.",
    "code_snippets": [
      "import tensorflow as tf",
      "print(tf.config.list_physical_devices('GPU'))",
      "# In the tf.keras.layers package, layers are objects. To construct a layer,\n# simply construct the object. Most layers take as a first argument the number\n# of output dimensions / channels.\nlayer = tf.keras.layers.Dense(100)\n# The number of input dimensions is often unnecessary, as it can be inferred\n# the first time the layer is used, but it can be provided if you want to\n# specify it manually, which is useful in some complex models.\nlayer = tf.keras.layers.Dense(10, input_shape=(None, 5))",
      "# To use a layer, simply call it.\nlayer(tf.zeros([10, 5]))",
      "class MyDenseLayer(tf.keras.layers.Layer):\n  def __init__(self, num_outputs):\n    super(MyDenseLayer, self).__init__()\n    self.num_outputs = num_outputs\n\n  def build(self, input_shape):\n    self.kernel = self.add_weight(\"kernel\",\n                                  shape=[int(input_shape[-1]),\n                                         self.num_outputs])\n\n  def call(self, inputs):\n    return tf.matmul(inputs, self.kernel)\n\nlayer = MyDenseLayer(10)",
      "_ = layer(tf.zeros([10, 5])) # Calling the layer `.builds` it.",
      "class ResnetIdentityBlock(tf.keras.Model):\n  def __init__(self, kernel_size, filters):\n    super(ResnetIdentityBlock, self).__init__(name='')\n    filters1, filters2, filters3 = filters\n\n    self.conv2a = tf.keras.layers.Conv2D(filters1, (1, 1))\n    self.bn2a = tf.keras.layers.BatchNormalization()\n\n    self.conv2b = tf.keras.layers.Conv2D(filters2, kernel_size, padding='same')\n    self.bn2b = tf.keras.layers.BatchNormalization()\n\n    self.conv2c = tf.keras.layers.Conv2D(filters3, (1, 1))\n    self.bn2c = tf.keras.layers.BatchNormalization()\n\n  def call(self, input_tensor, training=False):\n    x = self.conv2a(input_tensor)\n    x = self.bn2a(x, training=training)\n    x = tf.nn.relu(x)\n\n    x = self.conv2b(x)\n    x = self.bn2b(x, training=training)\n    x = tf.nn.relu(x)\n\n    x = self.conv2c(x)\n    x = self.bn2c(x, training=training)\n\n    x += input_tensor\n    return tf.nn.relu(x)\n\n\nblock = ResnetIdentityBlock(1, [1, 2, 3])",
      "_ = block(tf.zeros([1, 2, 3, 3]))",
      "my_seq = tf.keras.Sequential([tf.keras.layers.Conv2D(1, (1, 1),\n                                                    input_shape=(\n                                                        None, None, 3)),\n                             tf.keras.layers.BatchNormalization(),\n                             tf.keras.layers.Conv2D(2, 1,\n                                                    padding='same'),\n                             tf.keras.layers.BatchNormalization(),\n                             tf.keras.layers.Conv2D(3, (1, 1)),\n                             tf.keras.layers.BatchNormalization()])\nmy_seq(tf.zeros([1, 2, 3, 3]))"
    ]
  },
  {
    "url": "https://www.tensorflow.org/tutorials/structured_data/feature_columns",
    "title": "Classify structured data with feature columns\n      \n\n\n      \n      Stay organized with collections\n    \n\n      \n      Save and categorize content based on your preferences.",
    "code_snippets": [
      "import numpy as np\nimport pandas as pd\n\nimport tensorflow as tf\n\nfrom tensorflow import feature_column\nfrom tensorflow.keras import layers\nfrom sklearn.model_selection import train_test_split",
      "import pathlib\n\ndataset_url = 'http://storage.googleapis.com/download.tensorflow.org/data/petfinder-mini.zip'\ncsv_file = 'datasets/petfinder-mini/petfinder-mini.csv'\n\ntf.keras.utils.get_file('petfinder_mini.zip', dataset_url,\n                        extract=True, cache_dir='.')\ndataframe = pd.read_csv(csv_file)",
      "# A utility method to create a tf.data dataset from a Pandas Dataframe\ndef df_to_dataset(dataframe, shuffle=True, batch_size=32):\n  dataframe = dataframe.copy()\n  labels = dataframe.pop('target')\n  ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n  if shuffle:\n    ds = ds.shuffle(buffer_size=len(dataframe))\n  ds = ds.batch(batch_size)\n  return ds",
      "Every feature: ['Type', 'Age', 'Breed1', 'Gender', 'Color1', 'Color2', 'MaturitySize', 'FurLength', 'Vaccinated', 'Sterilized', 'Health', 'Fee', 'PhotoAmt']\nA batch of ages: tf.Tensor([ 2 24  1  4  2], shape=(5,), dtype=int64)\nA batch of targets: tf.Tensor([1 1 1 1 1], shape=(5,), dtype=int64)",
      "WARNING:tensorflow:From /tmpfs/tmp/ipykernel_442856/2408317497.py:1: numeric_column (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse Keras preprocessing layers instead, either directly or via the `tf.keras.utils.FeatureSpace` utility. Each of `tf.feature_column.*` has a functional equivalent in `tf.keras.layers` for feature preprocessing when training a Keras model.\n[[2.]\n [1.]\n [2.]\n [3.]\n [1.]]",
      "WARNING:tensorflow:From /tmpfs/tmp/ipykernel_442856/4134348679.py:2: bucketized_column (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse Keras preprocessing layers instead, either directly or via the `tf.keras.utils.FeatureSpace` utility. Each of `tf.feature_column.*` has a functional equivalent in `tf.keras.layers` for feature preprocessing when training a Keras model.\n[[0. 0. 0. 1.]\n [0. 0. 0. 1.]\n [0. 0. 0. 1.]\n [0. 1. 0. 0.]\n [0. 0. 0. 1.]]",
      "WARNING:tensorflow:From /tmpfs/tmp/ipykernel_442856/1157957390.py:1: categorical_column_with_vocabulary_list (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse Keras preprocessing layers instead, either directly or via the `tf.keras.utils.FeatureSpace` utility. Each of `tf.feature_column.*` has a functional equivalent in `tf.keras.layers` for feature preprocessing when training a Keras model.\nWARNING:tensorflow:From /tmpfs/tmp/ipykernel_442856/1157957390.py:4: indicator_column (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse Keras preprocessing layers instead, either directly or via the `tf.keras.utils.FeatureSpace` utility. Each of `tf.feature_column.*` has a functional equivalent in `tf.keras.layers` for feature preprocessing when training a Keras model.\n[[0. 1.]\n [1. 0.]\n [1. 0.]\n [1. 0.]\n [1. 0.]]",
      "WARNING:tensorflow:From /tmpfs/tmp/ipykernel_442856/689811331.py:5: embedding_column (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse Keras preprocessing layers instead, either directly or via the `tf.keras.utils.FeatureSpace` utility. Each of `tf.feature_column.*` has a functional equivalent in `tf.keras.layers` for feature preprocessing when training a Keras model.\n[[ 0.23340847 -0.22288084  0.41993982  0.48253137  0.14740573 -0.30386004\n   0.30413502  0.14656945]\n [-0.23076059 -0.13627627 -0.05317891  0.6952521   0.46279088 -0.5734566\n  -0.04382351 -0.5681491 ]\n [ 0.45319527  0.40937862 -0.21215594  0.4152906  -0.11821023 -0.20306908\n   0.31819987 -0.0359318 ]\n [-0.23076059 -0.13627627 -0.05317891  0.6952521   0.46279088 -0.5734566\n  -0.04382351 -0.5681491 ]\n [-0.23076059 -0.13627627 -0.05317891  0.6952521   0.46279088 -0.5734566\n  -0.04382351 -0.5681491 ]]",
      "WARNING:tensorflow:From /tmpfs/tmp/ipykernel_442856/3606107843.py:1: categorical_column_with_hash_bucket (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse Keras preprocessing layers instead, either directly or via the `tf.keras.utils.FeatureSpace` utility. Each of `tf.feature_column.*` has a functional equivalent in `tf.keras.layers` for feature preprocessing when training a Keras model.\n[[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]",
      "WARNING:tensorflow:From /tmpfs/tmp/ipykernel_442856/3676267184.py:1: crossed_column (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse `tf.keras.layers.experimental.preprocessing.HashedCrossing` instead for feature crossing when preprocessing data to train a Keras model.\n[[0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]",
      "feature_layer = tf.keras.layers.DenseFeatures(feature_columns)",
      "model = tf.keras.Sequential([\n  feature_layer,\n  layers.Dense(128, activation='relu'),\n  layers.Dense(128, activation='relu'),\n  layers.Dropout(.1),\n  layers.Dense(1)\n])\n\nmodel.compile(optimizer='adam',\n              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n\nmodel.fit(train_ds,\n          validation_data=val_ds,\n          epochs=10)"
    ]
  },
  {
    "url": "https://www.tensorflow.org/tutorials/text/transformer",
    "title": "Neural machine translation with a Transformer and Keras\n      \n\n\n      \n      Stay organized with collections\n    \n\n      \n      Save and categorize content based on your preferences.",
    "code_snippets": [
      "# Install the most re version of TensorFlow to use the improved\n# masking support for `tf.keras.layers.MultiHeadAttention`.\napt install --allow-change-held-packages libcudnn8=8.1.0.77-1+cuda11.2\npip uninstall -y -q tensorflow keras tensorflow-estimator tensorflow-text\npip install protobuf~=3.20.3\npip install -q tensorflow_datasets\npip install -q -U tensorflow-text tensorflow",
      "import logging\nimport time\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport tensorflow_datasets as tfds\nimport tensorflow as tf\n\nimport tensorflow_text",
      "model_name = 'ted_hrlr_translate_pt_en_converter'\ntf.keras.utils.get_file(\n    f'{model_name}.zip',\n    f'https://storage.googleapis.com/download.tensorflow.org/models/{model_name}.zip',\n    cache_dir='.', cache_subdir='', extract=True\n)",
      "tokenizers = tf.saved_model.load(model_name)",
      "def make_batches(ds):\n  return (\n      ds\n      .shuffle(BUFFER_SIZE)\n      .batch(BATCH_SIZE)\n      .map(prepare_batch, tf.data.AUTOTUNE)\n      .prefetch(buffer_size=tf.data.AUTOTUNE))",
      "def positional_encoding(length, depth):\n  depth = depth/2\n\n  positions = np.arange(length)[:, np.newaxis]     # (seq, 1)\n  depths = np.arange(depth)[np.newaxis, :]/depth   # (1, depth)\n\n  angle_rates = 1 / (10000**depths)         # (1, depth)\n  angle_rads = positions * angle_rates      # (pos, depth)\n\n  pos_encoding = np.concatenate(\n      [np.sin(angle_rads), np.cos(angle_rads)],\n      axis=-1) \n\n  return tf.cast(pos_encoding, dtype=tf.float32)",
      "pos_encoding/=tf.norm(pos_encoding, axis=1, keepdims=True)\np = pos_encoding[1000]\ndots = tf.einsum('pd,d -> p', pos_encoding, p)\nplt.subplot(2,1,1)\nplt.plot(dots)\nplt.ylim([0,1])\nplt.plot([950, 950, float('nan'), 1050, 1050],\n         [0,1,float('nan'),0,1], color='k', label='Zoom')\nplt.legend()\nplt.subplot(2,1,2)\nplt.plot(dots)\nplt.xlim([950, 1050])\nplt.ylim([0,1])",
      "class PositionalEmbedding(tf.keras.layers.Layer):\n  def __init__(self, vocab_size, d_model):\n    super().__init__()\n    self.d_model = d_model\n    self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero=True) \n    self.pos_encoding = positional_encoding(length=2048, depth=d_model)\n\n  def compute_mask(self, *args, **kwargs):\n    return self.embedding.compute_mask(*args, **kwargs)\n\n  def call(self, x):\n    length = tf.shape(x)[1]\n    x = self.embedding(x)\n    # This factor sets the relative scale of the embedding and positonal_encoding.\n    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n    x = x + self.pos_encoding[tf.newaxis, :length, :]\n    return x",
      "class BaseAttention(tf.keras.layers.Layer):\n  def __init__(self, **kwargs):\n    super().__init__()\n    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n    self.layernorm = tf.keras.layers.LayerNormalization()\n    self.add = tf.keras.layers.Add()",
      "class CrossAttention(BaseAttention):\n  def call(self, x, context):\n    attn_output, attn_scores = self.mha(\n        query=x,\n        key=context,\n        value=context,\n        return_attention_scores=True)\n\n    # Cache the attention scores for plotting later.\n    self.last_attn_scores = attn_scores\n\n    x = self.add([x, attn_output])\n    x = self.layernorm(x)\n\n    return x",
      "class GlobalSelfAttention(BaseAttention):\n  def call(self, x):\n    attn_output = self.mha(\n        query=x,\n        value=x,\n        key=x)\n    x = self.add([x, attn_output])\n    x = self.layernorm(x)\n    return x",
      "class CausalSelfAttention(BaseAttention):\n  def call(self, x):\n    attn_output = self.mha(\n        query=x,\n        value=x,\n        key=x,\n        use_causal_mask = True)\n    x = self.add([x, attn_output])\n    x = self.layernorm(x)\n    return x",
      "out1 = sample_csa(embed_en(en[:, :3])) \nout2 = sample_csa(embed_en(en))[:, :3]\n\ntf.reduce_max(abs(out1 - out2)).numpy()",
      "class FeedForward(tf.keras.layers.Layer):\n  def __init__(self, d_model, dff, dropout_rate=0.1):\n    super().__init__()\n    self.seq = tf.keras.Sequential([\n      tf.keras.layers.Dense(dff, activation='relu'),\n      tf.keras.layers.Dense(d_model),\n      tf.keras.layers.Dropout(dropout_rate)\n    ])\n    self.add = tf.keras.layers.Add()\n    self.layer_norm = tf.keras.layers.LayerNormalization()\n\n  def call(self, x):\n    x = self.add([x, self.seq(x)])\n    x = self.layer_norm(x) \n    return x",
      "class EncoderLayer(tf.keras.layers.Layer):\n  def __init__(self,*, d_model, num_heads, dff, dropout_rate=0.1):\n    super().__init__()\n\n    self.self_attention = GlobalSelfAttention(\n        num_heads=num_heads,\n        key_dim=d_model,\n        dropout=dropout_rate)\n\n    self.ffn = FeedForward(d_model, dff)\n\n  def call(self, x):\n    x = self.self_attention(x)\n    x = self.ffn(x)\n    return x",
      "class Encoder(tf.keras.layers.Layer):\n  def __init__(self, *, num_layers, d_model, num_heads,\n               dff, vocab_size, dropout_rate=0.1):\n    super().__init__()\n\n    self.d_model = d_model\n    self.num_layers = num_layers\n\n    self.pos_embedding = PositionalEmbedding(\n        vocab_size=vocab_size, d_model=d_model)\n\n    self.enc_layers = [\n        EncoderLayer(d_model=d_model,\n                     num_heads=num_heads,\n                     dff=dff,\n                     dropout_rate=dropout_rate)\n        for _ in range(num_layers)]\n    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n\n  def call(self, x):\n    # `x` is token-IDs shape: (batch, seq_len)\n    x = self.pos_embedding(x)  # Shape `(batch_size, seq_len, d_model)`.\n\n    # Add dropout.\n    x = self.dropout(x)\n\n    for i in range(self.num_layers):\n      x = self.enc_layers[i](x)\n\n    return x  # Shape `(batch_size, seq_len, d_model)`.",
      "class DecoderLayer(tf.keras.layers.Layer):\n  def __init__(self,\n               *,\n               d_model,\n               num_heads,\n               dff,\n               dropout_rate=0.1):\n    super(DecoderLayer, self).__init__()\n\n    self.causal_self_attention = CausalSelfAttention(\n        num_heads=num_heads,\n        key_dim=d_model,\n        dropout=dropout_rate)\n\n    self.cross_attention = CrossAttention(\n        num_heads=num_heads,\n        key_dim=d_model,\n        dropout=dropout_rate)\n\n    self.ffn = FeedForward(d_model, dff)\n\n  def call(self, x, context):\n    x = self.causal_self_attention(x=x)\n    x = self.cross_attention(x=x, context=context)\n\n    # Cache the last attention scores for plotting later\n    self.last_attn_scores = self.cross_attention.last_attn_scores\n\n    x = self.ffn(x)  # Shape `(batch_size, seq_len, d_model)`.\n    return x",
      "class Decoder(tf.keras.layers.Layer):\n  def __init__(self, *, num_layers, d_model, num_heads, dff, vocab_size,\n               dropout_rate=0.1):\n    super(Decoder, self).__init__()\n\n    self.d_model = d_model\n    self.num_layers = num_layers\n\n    self.pos_embedding = PositionalEmbedding(vocab_size=vocab_size,\n                                             d_model=d_model)\n    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n    self.dec_layers = [\n        DecoderLayer(d_model=d_model, num_heads=num_heads,\n                     dff=dff, dropout_rate=dropout_rate)\n        for _ in range(num_layers)]\n\n    self.last_attn_scores = None\n\n  def call(self, x, context):\n    # `x` is token-IDs shape (batch, target_seq_len)\n    x = self.pos_embedding(x)  # (batch_size, target_seq_len, d_model)\n\n    x = self.dropout(x)\n\n    for i in range(self.num_layers):\n      x  = self.dec_layers[i](x, context)\n\n    self.last_attn_scores = self.dec_layers[-1].last_attn_scores\n\n    # The shape of x is (batch_size, target_seq_len, d_model).\n    return x",
      "class Transformer(tf.keras.Model):\n  def __init__(self, *, num_layers, d_model, num_heads, dff,\n               input_vocab_size, target_vocab_size, dropout_rate=0.1):\n    super().__init__()\n    self.encoder = Encoder(num_layers=num_layers, d_model=d_model,\n                           num_heads=num_heads, dff=dff,\n                           vocab_size=input_vocab_size,\n                           dropout_rate=dropout_rate)\n\n    self.decoder = Decoder(num_layers=num_layers, d_model=d_model,\n                           num_heads=num_heads, dff=dff,\n                           vocab_size=target_vocab_size,\n                           dropout_rate=dropout_rate)\n\n    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n\n  def call(self, inputs):\n    # To use a Keras model with `.fit` you must pass all your inputs in the\n    # first argument.\n    context, x  = inputs\n\n    context = self.encoder(context)  # (batch_size, context_len, d_model)\n\n    x = self.decoder(x, context)  # (batch_size, target_len, d_model)\n\n    # Final linear layer output.\n    logits = self.final_layer(x)  # (batch_size, target_len, target_vocab_size)\n\n    try:\n      # Drop the keras mask, so it doesn't scale the losses/metrics.\n      # b/250038731\n      del logits._keras_mask\n    except AttributeError:\n      pass\n\n    # Return the final output and the attention weights.\n    return logits",
      "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n  def __init__(self, d_model, warmup_steps=4000):\n    super().__init__()\n\n    self.d_model = d_model\n    self.d_model = tf.cast(self.d_model, tf.float32)\n\n    self.warmup_steps = warmup_steps\n\n  def __call__(self, step):\n    step = tf.cast(step, dtype=tf.float32)\n    arg1 = tf.math.rsqrt(step)\n    arg2 = step * (self.warmup_steps ** -1.5)\n\n    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)",
      "learning_rate = CustomSchedule(d_model)\n\noptimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,\n                                     epsilon=1e-9)",
      "plt.plot(learning_rate(tf.range(40000, dtype=tf.float32)))\nplt.ylabel('Learning Rate')\nplt.xlabel('Train Step')",
      "def masked_loss(label, pred):\n  mask = label != 0\n  loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n    from_logits=True, reduction='none')\n  loss = loss_object(label, pred)\n\n  mask = tf.cast(mask, dtype=loss.dtype)\n  loss *= mask\n\n  loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)\n  return loss\n\n\ndef masked_accuracy(label, pred):\n  pred = tf.argmax(pred, axis=2)\n  label = tf.cast(label, pred.dtype)\n  match = label == pred\n\n  mask = label != 0\n\n  match = match & mask\n\n  match = tf.cast(match, dtype=tf.float32)\n  mask = tf.cast(mask, dtype=tf.float32)\n  return tf.reduce_sum(match)/tf.reduce_sum(mask)",
      "class Translator(tf.Module):\n  def __init__(self, tokenizers, transformer):\n    self.tokenizers = tokenizers\n    self.transformer = transformer\n\n  def __call__(self, sentence, max_length=MAX_TOKENS):\n    # The input sentence is Portuguese, hence adding the `[START]` and `[END]` tokens.\n    assert isinstance(sentence, tf.Tensor)\n    if len(sentence.shape) == 0:\n      sentence = sentence[tf.newaxis]\n\n    sentence = self.tokenizers.pt.tokenize(sentence).to_tensor()\n\n    encoder_input = sentence\n\n    # As the output language is English, initialize the output with the\n    # English `[START]` token.\n    start_end = self.tokenizers.en.tokenize([''])[0]\n    start = start_end[0][tf.newaxis]\n    end = start_end[1][tf.newaxis]\n\n    # `tf.TensorArray` is required here (instead of a Python list), so that the\n    # dynamic-loop can be traced by `tf.function`.\n    output_array = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)\n    output_array = output_array.write(0, start)\n\n    for i in tf.range(max_length):\n      output = tf.transpose(output_array.stack())\n      predictions = self.transformer([encoder_input, output], training=False)\n\n      # Select the last token from the `seq_len` dimension.\n      predictions = predictions[:, -1:, :]  # Shape `(batch_size, 1, vocab_size)`.\n\n      predicted_id = tf.argmax(predictions, axis=-1)\n\n      # Concatenate the `predicted_id` to the output which is given to the\n      # decoder as its input.\n      output_array = output_array.write(i+1, predicted_id[0])\n\n      if predicted_id == end:\n        break\n\n    output = tf.transpose(output_array.stack())\n    # The output shape is `(1, tokens)`.\n    text = tokenizers.en.detokenize(output)[0]  # Shape: `()`.\n\n    tokens = tokenizers.en.lookup(output)[0]\n\n    # `tf.function` prevents us from using the attention_weights that were\n    # calculated on the last iteration of the loop.\n    # So, recalculate them outside the loop.\n    self.transformer([encoder_input, output[:,:-1]], training=False)\n    attention_weights = self.transformer.decoder.last_attn_scores\n\n    return text, tokens, attention_weights",
      "def print_translation(sentence, tokens, ground_truth):\n  print(f'{\"Input:\":15s}: {sentence}')\n  print(f'{\"Prediction\":15s}: {tokens.numpy().decode(\"utf-8\")}')\n  print(f'{\"Ground truth\":15s}: {ground_truth}')",
      "sentence = 'este \u00e9 um problema que temos que resolver.'\nground_truth = 'this is a problem we have to solve .'\n\ntranslated_text, translated_tokens, attention_weights = translator(\n    tf.constant(sentence))\nprint_translation(sentence, translated_text, ground_truth)",
      "sentence = 'os meus vizinhos ouviram sobre esta ideia.'\nground_truth = 'and my neighboring homes heard about this idea .'\n\ntranslated_text, translated_tokens, attention_weights = translator(\n    tf.constant(sentence))\nprint_translation(sentence, translated_text, ground_truth)",
      "sentence = 'vou ent\u00e3o muito rapidamente partilhar convosco algumas hist\u00f3rias de algumas coisas m\u00e1gicas que aconteceram.'\nground_truth = \"so i'll just share with you some stories very quickly of some magical things that have happened.\"\n\ntranslated_text, translated_tokens, attention_weights = translator(\n    tf.constant(sentence))\nprint_translation(sentence, translated_text, ground_truth)",
      "sentence = 'este \u00e9 o primeiro livro que eu fiz.'\nground_truth = \"this is the first book i've ever done.\"\n\ntranslated_text, translated_tokens, attention_weights = translator(\n    tf.constant(sentence))\nprint_translation(sentence, translated_text, ground_truth)",
      "def plot_attention_head(in_tokens, translated_tokens, attention):\n  # The model didn't generate `<START>` in the output. Skip it.\n  translated_tokens = translated_tokens[1:]\n\n  ax = plt.gca()\n  ax.matshow(attention)\n  ax.set_xticks(range(len(in_tokens)))\n  ax.set_yticks(range(len(translated_tokens)))\n\n  labels = [label.decode('utf-8') for label in in_tokens.numpy()]\n  ax.set_xticklabels(\n      labels, rotation=90)\n\n  labels = [label.decode('utf-8') for label in translated_tokens.numpy()]\n  ax.set_yticklabels(labels)",
      "head = 0\n# Shape: `(batch=1, num_heads, seq_len_q, seq_len_k)`.\nattention_heads = tf.squeeze(attention_weights, 0)\nattention = attention_heads[head]\nattention.shape",
      "in_tokens = tf.convert_to_tensor([sentence])\nin_tokens = tokenizers.pt.tokenize(in_tokens).to_tensor()\nin_tokens = tokenizers.pt.lookup(in_tokens)[0]\nin_tokens",
      "def plot_attention_weights(sentence, translated_tokens, attention_heads):\n  in_tokens = tf.convert_to_tensor([sentence])\n  in_tokens = tokenizers.pt.tokenize(in_tokens).to_tensor()\n  in_tokens = tokenizers.pt.lookup(in_tokens)[0]\n\n  fig = plt.figure(figsize=(16, 8))\n\n  for h, head in enumerate(attention_heads):\n    ax = fig.add_subplot(2, 4, h+1)\n\n    plot_attention_head(in_tokens, translated_tokens, head)\n\n    ax.set_xlabel(f'Head {h+1}')\n\n  plt.tight_layout()\n  plt.show()",
      "sentence = 'Eu li sobre triceratops na enciclop\u00e9dia.'\nground_truth = 'I read about triceratops in the encyclopedia.'\n\ntranslated_text, translated_tokens, attention_weights = translator(\n    tf.constant(sentence))\nprint_translation(sentence, translated_text, ground_truth)\n\nplot_attention_weights(sentence, translated_tokens, attention_weights[0])",
      "class ExportTranslator(tf.Module):\n  def __init__(self, translator):\n    self.translator = translator\n\n  @tf.function(input_signature=[tf.TensorSpec(shape=[], dtype=tf.string)])\n  def __call__(self, sentence):\n    (result,\n     tokens,\n     attention_weights) = self.translator(sentence, max_length=MAX_TOKENS)\n\n    return result",
      "tf.saved_model.save(translator, export_dir='translator')",
      "reloaded = tf.saved_model.load('translator')"
    ]
  },
  {
    "url": "https://www.tensorflow.org/tutorials/load_data/csv",
    "title": "Load CSV data\n      \n\n\n      \n      Stay organized with collections\n    \n\n      \n      Save and categorize content based on your preferences.",
    "code_snippets": [
      "import pandas as pd\nimport numpy as np\n\n# Make numpy values easier to read.\nnp.set_printoptions(precision=3, suppress=True)\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers",
      "abalone_model = tf.keras.Sequential([\n  layers.Dense(64, activation='relu'),\n  layers.Dense(1)\n])\n\nabalone_model.compile(loss = tf.keras.losses.MeanSquaredError(),\n                      optimizer = tf.keras.optimizers.Adam())",
      "norm_abalone_model = tf.keras.Sequential([\n  normalize,\n  layers.Dense(64, activation='relu'),\n  layers.Dense(1)\n])\n\nnorm_abalone_model.compile(loss = tf.keras.losses.MeanSquaredError(),\n                           optimizer = tf.keras.optimizers.Adam())\n\nnorm_abalone_model.fit(abalone_features, abalone_labels, epochs=10)",
      "# Create a symbolic input\ninput = tf.keras.Input(shape=(), dtype=tf.float32)\n\n# Perform a calculation using the input\nresult = 2*input + 1\n\n# the result doesn't have a value\nresult",
      "calc = tf.keras.Model(inputs=input, outputs=result)",
      "inputs = {}\n\nfor name, column in titanic_features.items():\n  dtype = column.dtype\n  if dtype == object:\n    dtype = tf.string\n  else:\n    dtype = tf.float32\n\n  inputs[name] = tf.keras.Input(shape=(1,), name=name, dtype=dtype)\n\ninputs",
      "numeric_inputs = {name:input for name,input in inputs.items()\n                  if input.dtype==tf.float32}\n\nx = layers.Concatenate()(list(numeric_inputs.values()))\nnorm = layers.Normalization()\nnorm.adapt(np.array(titanic[numeric_inputs.keys()]))\nall_numeric_inputs = norm(x)\n\nall_numeric_inputs",
      "for name, input in inputs.items():\n  if input.dtype == tf.float32:\n    continue\n\n  lookup = layers.StringLookup(vocabulary=np.unique(titanic_features[name]))\n  one_hot = layers.CategoryEncoding(num_tokens=lookup.vocabulary_size())\n\n  x = lookup(input)\n  x = one_hot(x)\n  preprocessed_inputs.append(x)",
      "preprocessed_inputs_cat = layers.Concatenate()(preprocessed_inputs)\n\ntitanic_preprocessing = tf.keras.Model(inputs, preprocessed_inputs_cat)\n\ntf.keras.utils.plot_model(model = titanic_preprocessing , rankdir=\"LR\", dpi=72, show_shapes=True)",
      "<tf.Tensor: shape=(1, 28), dtype=float32, numpy=\narray([[-0.61 ,  0.395, -0.479, -0.497,  0.   ,  0.   ,  1.   ,  0.   ,\n\n         0.   ,  0.   ,  1.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,\n         0.   ,  0.   ,  0.   ,  1.   ,  0.   ,  0.   ,  0.   ,  1.   ,\n         0.   ,  0.   ,  1.   ,  0.   ]], dtype=float32)>",
      "def titanic_model(preprocessing_head, inputs):\n  body = tf.keras.Sequential([\n    layers.Dense(64, activation='relu'),\n    layers.Dense(1)\n  ])\n\n  preprocessed_inputs = preprocessing_head(inputs)\n  result = body(preprocessed_inputs)\n  model = tf.keras.Model(inputs, result)\n\n  model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n                optimizer=tf.keras.optimizers.Adam())\n  return model\n\ntitanic_model = titanic_model(titanic_preprocessing, inputs)",
      "titanic_model.save('test.keras')\nreloaded = tf.keras.models.load_model('test.keras')",
      "tf.Tensor([[-1.882]], shape=(1, 1), dtype=float32)\ntf.Tensor([[-1.882]], shape=(1, 1), dtype=float32)",
      "import itertools\n\ndef slices(features):\n  for i in itertools.count():\n    # For each feature take index `i`\n    example = {name:values[i] for name, values in features.items()}\n    yield example",
      "features_ds = tf.data.Dataset.from_tensor_slices(titanic_features_dict)",
      "titanic_ds = tf.data.Dataset.from_tensor_slices((titanic_features_dict, titanic_labels))",
      "titanic_file_path = tf.keras.utils.get_file(\"train.csv\", \"https://storage.googleapis.com/tf-datasets/titanic/train.csv\")",
      "titanic_csv_ds = tf.data.experimental.make_csv_dataset(\n    titanic_file_path,\n    batch_size=5, # Artificially small to make examples easier to show.\n    label_name='survived',\n    num_epochs=1,\n    ignore_errors=True,)",
      "WARNING:tensorflow:From /tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow/python/data/experimental/ops/readers.py:572: ignore_errors (from tensorflow.python.data.experimental.ops.error_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse `tf.data.Dataset.ignore_errors` instead.",
      "traffic_volume_csv_gz = tf.keras.utils.get_file(\n    'Metro_Interstate_Traffic_Volume.csv.gz', \n    \"https://archive.ics.uci.edu/ml/machine-learning-databases/00492/Metro_Interstate_Traffic_Volume.csv.gz\",\n    cache_dir='.', cache_subdir='traffic')",
      "traffic_volume_csv_gz_ds = tf.data.experimental.make_csv_dataset(\n    traffic_volume_csv_gz,\n    batch_size=256,\n    label_name='traffic_volume',\n    num_epochs=1,\n    compression_type=\"GZIP\")\n\nfor batch, label in traffic_volume_csv_gz_ds.take(1):\n  for key, value in batch.items():\n    print(f\"{key:20s}: {value[:5]}\")\n  print()\n  print(f\"{'label':20s}: {label[:5]}\")",
      "fonts_zip = tf.keras.utils.get_file(\n    'fonts.zip',  \"https://archive.ics.uci.edu/ml/machine-learning-databases/00417/fonts.zip\",\n    cache_dir='.', cache_subdir='fonts',\n    extract=True)",
      "import pathlib\nfont_csvs =  sorted(str(p) for p in pathlib.Path('fonts').glob(\"*.csv\"))\n\nfont_csvs[:10]",
      "fonts_ds = tf.data.experimental.make_csv_dataset(\n    file_pattern = \"fonts/*.csv\",\n    batch_size=10, num_epochs=1,\n    num_parallel_reads=20,\n    shuffle_buffer_size=10000)",
      "import re\n\ndef make_images(features):\n  image = [None]*400\n  new_feats = {}\n\n  for name, value in features.items():\n    match = re.match('r(\\d+)c(\\d+)', name)\n    if match:\n      image[int(match.group(1))*20+int(match.group(2))] = value\n    else:\n      new_feats[name] = value\n\n  image = tf.stack(image, axis=0)\n  image = tf.reshape(image, [20, 20, -1])\n  new_feats['image'] = image\n\n  return new_feats",
      "features = tf.io.decode_csv(lines, record_defaults=all_strings) \n\nfor f in features:\n  print(f\"type: {f.dtype.name}, shape: {f.shape}\")",
      "features = tf.io.decode_csv(lines, record_defaults=titanic_types) \n\nfor f in features:\n  print(f\"type: {f.dtype.name}, shape: {f.shape}\")",
      "simple_titanic = tf.data.experimental.CsvDataset(titanic_file_path, record_defaults=titanic_types, header=True)\n\nfor example in simple_titanic.take(1):\n  print([e.numpy() for e in example])",
      "def decode_titanic_line(line):\n  return tf.io.decode_csv(line, titanic_types)\n\nmanual_titanic = (\n    # Load the lines of text\n    tf.data.TextLineDataset(titanic_file_path)\n    # Skip the header row.\n    .skip(1)\n    # Decode the line.\n    .map(decode_titanic_line)\n)\n\nfor example in manual_titanic.take(1):\n  print([e.numpy() for e in example])",
      "simple_font_ds = tf.data.experimental.CsvDataset(\n    font_csvs, \n    record_defaults=font_column_types, \n    header=True)",
      "font_files = tf.data.Dataset.list_files(\"fonts/*.csv\")",
      "def make_font_csv_ds(path):\n  return tf.data.experimental.CsvDataset(\n    path, \n    record_defaults=font_column_types, \n    header=True)",
      "BATCH_SIZE=2048\nfonts_ds = tf.data.experimental.make_csv_dataset(\n    file_pattern = \"fonts/*.csv\",\n    batch_size=BATCH_SIZE, num_epochs=1,\n    num_parallel_reads=100)",
      "fonts_files = tf.data.Dataset.list_files(\"fonts/*.csv\")\nfonts_lines = fonts_files.interleave(\n    lambda fname:tf.data.TextLineDataset(fname).skip(1), \n    cycle_length=100).batch(BATCH_SIZE)\n\nfonts_fast = fonts_lines.map(lambda x: tf.io.decode_csv(x, record_defaults=font_column_types))"
    ]
  }
]