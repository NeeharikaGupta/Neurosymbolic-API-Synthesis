API Component,Documentation
tensorflow,"Top-level module of TensorFlow. By convention, we refer to this module as `tf` instead of `tensorflow`, following the common practice of importing TensorFlow via the command `import tensorflow as tf`.  The primary function of this module is to import all of the public TensorFlow interfaces into a single place. The interfaces themselves are located in sub-modules, as described below.  Note that the file `__init__.py` in the TensorFlow source code tree is actually only a placeholder to enable test cases to run. The TensorFlow build replaces this file with a file generated from [`api_template.__init__.py`](https://www.github.com/tensorflow/tensorflow/blob/master/tensorflow/api_template.__init__.py)"
tensorflow.keras,"DO NOT EDIT.  This file was autogenerated. Do not edit it by hand, since your modifications would be overwritten."
Sequential,"`Sequential` groups a linear stack of layers into a `Model`.  Examples:  ```python model = keras.Sequential() model.add(keras.Input(shape=(16,))) model.add(keras.layers.Dense(8))  # Note that you can also omit the initial `Input`. # In that case the model doesn't have any weights until the first call # to a training/evaluation method (since it isn't yet built): model = keras.Sequential() model.add(keras.layers.Dense(8)) model.add(keras.layers.Dense(4)) # model.weights not created yet  # Whereas if you specify an `Input`, the model gets built # continuously as you are adding layers: model = keras.Sequential() model.add(keras.Input(shape=(16,))) model.add(keras.layers.Dense(8)) len(model.weights)  # Returns ""2""  # When using the delayed-build pattern (no input shape specified), you can # choose to manually build your model by calling # `build(batch_input_shape)`: model = keras.Sequential() model.add(keras.layers.Dense(8)) model.add(keras.layers.Dense(4)) model.build((None, 16)) len(model.weights)  # Returns ""4""  # Note that when using the delayed-build pattern (no input shape specified), # the model gets built the first time you call `fit`, `eval`, or `predict`, # or the first time you call the model on some input data. model = keras.Sequential() model.add(keras.layers.Dense(8)) model.add(keras.layers.Dense(1)) model.compile(optimizer='sgd', loss='mse') # This builds the model for the first time: model.fit(x, y, batch_size=32, epochs=10) ```"
Dense,"Just your regular densely-connected NN layer.  `Dense` implements the operation: `output = activation(dot(input, kernel) + bias)` where `activation` is the element-wise activation function passed as the `activation` argument, `kernel` is a weights matrix created by the layer, and `bias` is a bias vector created by the layer (only applicable if `use_bias` is `True`).  Note: If the input to the layer has a rank greater than 2, `Dense` computes the dot product between the `inputs` and the `kernel` along the last axis of the `inputs` and axis 0 of the `kernel` (using `tf.tensordot`). For example, if input has dimensions `(batch_size, d0, d1)`, then we create a `kernel` with shape `(d1, units)`, and the `kernel` operates along axis 2 of the `input`, on every sub-tensor of shape `(1, 1, d1)` (there are `batch_size * d0` such sub-tensors). The output in this case will have shape `(batch_size, d0, units)`.  Args:     units: Positive integer, dimensionality of the output space.     activation: Activation function to use.         If you don't specify anything, no activation is applied         (ie. ""linear"" activation: `a(x) = x`).     use_bias: Boolean, whether the layer uses a bias vector.     kernel_initializer: Initializer for the `kernel` weights matrix.     bias_initializer: Initializer for the bias vector.     kernel_regularizer: Regularizer function applied to         the `kernel` weights matrix.     bias_regularizer: Regularizer function applied to the bias vector.     activity_regularizer: Regularizer function applied to         the output of the layer (its ""activation"").     kernel_constraint: Constraint function applied to         the `kernel` weights matrix.     bias_constraint: Constraint function applied to the bias vector.     lora_rank: Optional integer. If set, the layer's forward pass         will implement LoRA (Low-Rank Adaptation)         with the provided rank. LoRA sets the layer's kernel         to non-trainable and replaces it with a delta over the         original kernel, obtained via multiplying two lower-rank         trainable matrices. This can be useful to reduce the         computation cost of fine-tuning large dense layers.         You can also enable LoRA on an existing         `Dense` layer by calling `layer.enable_lora(rank)`.  Input shape:     N-D tensor with shape: `(batch_size, ..., input_dim)`.     The most common situation would be     a 2D input with shape `(batch_size, input_dim)`.  Output shape:     N-D tensor with shape: `(batch_size, ..., units)`.     For instance, for a 2D input with shape `(batch_size, input_dim)`,     the output would have shape `(batch_size, units)`."
Input,"Used to instantiate a Keras tensor.  A Keras tensor is a symbolic tensor-like object, which we augment with certain attributes that allow us to build a Keras model just by knowing the inputs and outputs of the model.  For instance, if `a`, `b` and `c` are Keras tensors, it becomes possible to do: `model = Model(input=[a, b], output=c)`  Args:     shape: A shape tuple (tuple of integers or `None` objects),         not including the batch size.         For instance, `shape=(32,)` indicates that the expected input         will be batches of 32-dimensional vectors. Elements of this tuple         can be `None`; `None` elements represent dimensions where the shape         is not known and may vary (e.g. sequence length).     batch_size: Optional static batch size (integer).     dtype: The data type expected by the input, as a string         (e.g. `""float32""`, `""int32""`...)     sparse: A boolean specifying whether the expected input will be sparse         tensors. Note that, if `sparse` is `False`, sparse tensors can still         be passed into the input - they will be densified with a default         value of 0. This feature is only supported with the TensorFlow         backend. Defaults to `False`.     batch_shape: Optional shape tuple (tuple of integers or `None` objects),         including the batch size.     name: Optional name string for the layer.         Should be unique in a model (do not reuse the same name twice).         It will be autogenerated if it isn't provided.     tensor: Optional existing tensor to wrap into the `Input` layer.         If set, the layer will use this tensor rather         than creating a new placeholder tensor.     optional: Boolean, whether the input is optional or not.         An optional input can accept `None` values.  Returns:   A Keras tensor.  Example:  ```python # This is a logistic regression in Keras x = Input(shape=(32,)) y = Dense(16, activation='softmax')(x) model = Model(x, y) ```"
Concatenate,"Concatenates a list of inputs.  It takes as input a list of tensors, all of the same shape except for the concatenation axis, and returns a single tensor that is the concatenation of all inputs.  Examples:  >>> x = np.arange(20).reshape(2, 2, 5) >>> y = np.arange(20, 30).reshape(2, 1, 5) >>> keras.layers.Concatenate(axis=1)([x, y])  Usage in a Keras model:  >>> x1 = keras.layers.Dense(8)(np.arange(10).reshape(5, 2)) >>> x2 = keras.layers.Dense(8)(np.arange(10, 20).reshape(5, 2)) >>> y = keras.layers.Concatenate()([x1, x2])  Args:     axis: Axis along which to concatenate.     **kwargs: Standard layer keyword arguments.  Returns:     A tensor, the concatenation of the inputs alongside axis `axis`."
fetch_california_housing,"Load the California housing dataset (regression).  ==============   ============== Samples total             20640 Dimensionality                8 Features                   real Target           real 0.15 - 5. ==============   ==============  Read more in the :ref:`User Guide <california_housing_dataset>`.  Parameters ---------- data_home : str or path-like, default=None     Specify another download and cache folder for the datasets. By default     all scikit-learn data is stored in '~/scikit_learn_data' subfolders.  download_if_missing : bool, default=True     If False, raise an OSError if the data is not locally available     instead of trying to download the data from the source site.  return_X_y : bool, default=False     If True, returns ``(data.data, data.target)`` instead of a Bunch     object.      .. versionadded:: 0.20  as_frame : bool, default=False     If True, the data is a pandas DataFrame including columns with     appropriate dtypes (numeric, string or categorical). The target is     a pandas DataFrame or Series depending on the number of target_columns.      .. versionadded:: 0.23  n_retries : int, default=3     Number of retries when HTTP errors are encountered.      .. versionadded:: 1.5  delay : float, default=1.0     Number of seconds between retries.      .. versionadded:: 1.5  Returns ------- dataset : :class:`~sklearn.utils.Bunch`     Dictionary-like object, with the following attributes.      data : ndarray, shape (20640, 8)         Each row corresponding to the 8 feature values in order.         If ``as_frame`` is True, ``data`` is a pandas object.     target : numpy array of shape (20640,)         Each value corresponds to the average         house value in units of 100,000.         If ``as_frame`` is True, ``target`` is a pandas object.     feature_names : list of length 8         Array of ordered feature names used in the dataset.     DESCR : str         Description of the California housing dataset.     frame : pandas DataFrame         Only present when `as_frame=True`. DataFrame with ``data`` and         ``target``.          .. versionadded:: 0.23  (data, target) : tuple if ``return_X_y`` is True     A tuple of two ndarray. The first containing a 2D array of     shape (n_samples, n_features) with each row representing one     sample and each column representing the features. The second     ndarray of shape (n_samples,) containing the target samples.      .. versionadded:: 0.20  Notes -----  This dataset consists of 20,640 samples and 9 features.  Examples -------- >>> from sklearn.datasets import fetch_california_housing >>> housing = fetch_california_housing() >>> print(housing.data.shape, housing.target.shape) (20640, 8) (20640,) >>> print(housing.feature_names[0:6]) ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup']"
make_classification,"Generate a random n-class classification problem.  This initially creates clusters of points normally distributed (std=1) about vertices of an ``n_informative``-dimensional hypercube with sides of length ``2*class_sep`` and assigns an equal number of clusters to each class. It introduces interdependence between these features and adds various types of further noise to the data.  Without shuffling, ``X`` horizontally stacks features in the following order: the primary ``n_informative`` features, followed by ``n_redundant`` linear combinations of the informative features, followed by ``n_repeated`` duplicates, drawn randomly with replacement from the informative and redundant features. The remaining features are filled with random noise. Thus, without shuffling, all useful features are contained in the columns ``X[:, :n_informative + n_redundant + n_repeated]``.  Read more in the :ref:`User Guide <sample_generators>`.  Parameters ---------- n_samples : int, default=100     The number of samples.  n_features : int, default=20     The total number of features. These comprise ``n_informative``     informative features, ``n_redundant`` redundant features,     ``n_repeated`` duplicated features and     ``n_features-n_informative-n_redundant-n_repeated`` useless features     drawn at random.  n_informative : int, default=2     The number of informative features. Each class is composed of a number     of gaussian clusters each located around the vertices of a hypercube     in a subspace of dimension ``n_informative``. For each cluster,     informative features are drawn independently from  N(0, 1) and then     randomly linearly combined within each cluster in order to add     covariance. The clusters are then placed on the vertices of the     hypercube.  n_redundant : int, default=2     The number of redundant features. These features are generated as     random linear combinations of the informative features.  n_repeated : int, default=0     The number of duplicated features, drawn randomly from the informative     and the redundant features.  n_classes : int, default=2     The number of classes (or labels) of the classification problem.  n_clusters_per_class : int, default=2     The number of clusters per class.  weights : array-like of shape (n_classes,) or (n_classes - 1,),              default=None     The proportions of samples assigned to each class. If None, then     classes are balanced. Note that if ``len(weights) == n_classes - 1``,     then the last class weight is automatically inferred.     More than ``n_samples`` samples may be returned if the sum of     ``weights`` exceeds 1. Note that the actual class proportions will     not exactly match ``weights`` when ``flip_y`` isn't 0.  flip_y : float, default=0.01     The fraction of samples whose class is assigned randomly. Larger     values introduce noise in the labels and make the classification     task harder. Note that the default setting flip_y > 0 might lead     to less than ``n_classes`` in y in some cases.  class_sep : float, default=1.0     The factor multiplying the hypercube size.  Larger values spread     out the clusters/classes and make the classification task easier.  hypercube : bool, default=True     If True, the clusters are put on the vertices of a hypercube. If     False, the clusters are put on the vertices of a random polytope.  shift : float, ndarray of shape (n_features,) or None, default=0.0     Shift features by the specified value. If None, then features     are shifted by a random value drawn in [-class_sep, class_sep].  scale : float, ndarray of shape (n_features,) or None, default=1.0     Multiply features by the specified value. If None, then features     are scaled by a random value drawn in [1, 100]. Note that scaling     happens after shifting.  shuffle : bool, default=True     Shuffle the samples and the features.  random_state : int, RandomState instance or None, default=None     Determines random number generation for dataset creation. Pass an int     for reproducible output across multiple function calls.     See :term:`Glossary <random_state>`.  Returns ------- X : ndarray of shape (n_samples, n_features)     The generated samples.  y : ndarray of shape (n_samples,)     The integer labels for class membership of each sample.  See Also -------- make_blobs : Simplified variant. make_multilabel_classification : Unrelated generator for multilabel tasks.  Notes ----- The algorithm is adapted from Guyon [1] and was designed to generate the ""Madelon"" dataset.  References ---------- .. [1] I. Guyon, ""Design of experiments for the NIPS 2003 variable        selection benchmark"", 2003.  Examples -------- >>> from sklearn.datasets import make_classification >>> X, y = make_classification(random_state=42) >>> X.shape (100, 20) >>> y.shape (100,) >>> list(y[:5]) [np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(0)]"
train_test_split,"Split arrays or matrices into random train and test subsets.  Quick utility that wraps input validation, ``next(ShuffleSplit().split(X, y))``, and application to input data into a single call for splitting (and optionally subsampling) data into a one-liner.  Read more in the :ref:`User Guide <cross_validation>`.  Parameters ---------- *arrays : sequence of indexables with same length / shape[0]     Allowed inputs are lists, numpy arrays, scipy-sparse     matrices or pandas dataframes.  test_size : float or int, default=None     If float, should be between 0.0 and 1.0 and represent the proportion     of the dataset to include in the test split. If int, represents the     absolute number of test samples. If None, the value is set to the     complement of the train size. If ``train_size`` is also None, it will     be set to 0.25.  train_size : float or int, default=None     If float, should be between 0.0 and 1.0 and represent the     proportion of the dataset to include in the train split. If     int, represents the absolute number of train samples. If None,     the value is automatically set to the complement of the test size.  random_state : int, RandomState instance or None, default=None     Controls the shuffling applied to the data before applying the split.     Pass an int for reproducible output across multiple function calls.     See :term:`Glossary <random_state>`.  shuffle : bool, default=True     Whether or not to shuffle the data before splitting. If shuffle=False     then stratify must be None.  stratify : array-like, default=None     If not None, data is split in a stratified fashion, using this as     the class labels.     Read more in the :ref:`User Guide <stratification>`.  Returns ------- splitting : list, length=2 * len(arrays)     List containing train-test split of inputs.      .. versionadded:: 0.16         If the input is sparse, the output will be a         ``scipy.sparse.csr_matrix``. Else, output type is the same as the         input type.  Examples -------- >>> import numpy as np >>> from sklearn.model_selection import train_test_split >>> X, y = np.arange(10).reshape((5, 2)), range(5) >>> X array([[0, 1],        [2, 3],        [4, 5],        [6, 7],        [8, 9]]) >>> list(y) [0, 1, 2, 3, 4]  >>> X_train, X_test, y_train, y_test = train_test_split( ...     X, y, test_size=0.33, random_state=42) ... >>> X_train array([[4, 5],        [0, 1],        [6, 7]]) >>> y_train [2, 0, 3] >>> X_test array([[2, 3],        [8, 9]]) >>> y_test [1, 4]  >>> train_test_split(y, shuffle=False) [[0, 1, 2], [3, 4]]"
StandardScaler,"Standardize features by removing the mean and scaling to unit variance.  The standard score of a sample `x` is calculated as:  .. code-block:: text      z = (x - u) / s  where `u` is the mean of the training samples or zero if `with_mean=False`, and `s` is the standard deviation of the training samples or one if `with_std=False`.  Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Mean and standard deviation are then stored to be used on later data using :meth:`transform`.  Standardization of a dataset is a common requirement for many machine learning estimators: they might behave badly if the individual features do not more or less look like standard normally distributed data (e.g. Gaussian with 0 mean and unit variance).  For instance many elements used in the objective function of a learning algorithm (such as the RBF kernel of Support Vector Machines or the L1 and L2 regularizers of linear models) assume that all features are centered around 0 and have variance in the same order. If a feature has a variance that is orders of magnitude larger than others, it might dominate the objective function and make the estimator unable to learn from other features correctly as expected.  `StandardScaler` is sensitive to outliers, and the features may scale differently from each other in the presence of outliers. For an example visualization, refer to :ref:`Compare StandardScaler with other scalers <plot_all_scaling_standard_scaler_section>`.  This scaler can also be applied to sparse CSR or CSC matrices by passing `with_mean=False` to avoid breaking the sparsity structure of the data.  Read more in the :ref:`User Guide <preprocessing_scaler>`.  Parameters ---------- copy : bool, default=True     If False, try to avoid a copy and do inplace scaling instead.     This is not guaranteed to always work inplace; e.g. if the data is     not a NumPy array or scipy.sparse CSR matrix, a copy may still be     returned.  with_mean : bool, default=True     If True, center the data before scaling.     This does not work (and will raise an exception) when attempted on     sparse matrices, because centering them entails building a dense     matrix which in common use cases is likely to be too large to fit in     memory.  with_std : bool, default=True     If True, scale the data to unit variance (or equivalently,     unit standard deviation).  Attributes ---------- scale_ : ndarray of shape (n_features,) or None     Per feature relative scaling of the data to achieve zero mean and unit     variance. Generally this is calculated using `np.sqrt(var_)`. If a     variance is zero, we can't achieve unit variance, and the data is left     as-is, giving a scaling factor of 1. `scale_` is equal to `None`     when `with_std=False`.      .. versionadded:: 0.17        *scale_*  mean_ : ndarray of shape (n_features,) or None     The mean value for each feature in the training set.     Equal to ``None`` when ``with_mean=False`` and ``with_std=False``.  var_ : ndarray of shape (n_features,) or None     The variance for each feature in the training set. Used to compute     `scale_`. Equal to ``None`` when ``with_mean=False`` and     ``with_std=False``.  n_features_in_ : int     Number of features seen during :term:`fit`.      .. versionadded:: 0.24  feature_names_in_ : ndarray of shape (`n_features_in_`,)     Names of features seen during :term:`fit`. Defined only when `X`     has feature names that are all strings.      .. versionadded:: 1.0  n_samples_seen_ : int or ndarray of shape (n_features,)     The number of samples processed by the estimator for each feature.     If there are no missing samples, the ``n_samples_seen`` will be an     integer, otherwise it will be an array of dtype int. If     `sample_weights` are used it will be a float (if no missing data)     or an array of dtype float that sums the weights seen so far.     Will be reset on new calls to fit, but increments across     ``partial_fit`` calls.  See Also -------- scale : Equivalent function without the estimator API.  :class:`~sklearn.decomposition.PCA` : Further removes the linear     correlation across features with 'whiten=True'.  Notes ----- NaNs are treated as missing values: disregarded in fit, and maintained in transform.  We use a biased estimator for the standard deviation, equivalent to `numpy.std(x, ddof=0)`. Note that the choice of `ddof` is unlikely to affect model performance.  Examples -------- >>> from sklearn.preprocessing import StandardScaler >>> data = [[0, 0], [0, 0], [1, 1], [1, 1]] >>> scaler = StandardScaler() >>> print(scaler.fit(data)) StandardScaler() >>> print(scaler.mean_) [0.5 0.5] >>> print(scaler.transform(data)) [[-1. -1.]  [-1. -1.]  [ 1.  1.]  [ 1.  1.]] >>> print(scaler.transform([[2, 2]])) [[3. 3.]]"
LogisticRegression,"Logistic Regression (aka logit, MaxEnt) classifier.  This class implements regularized logistic regression using the 'liblinear' library, 'newton-cg', 'sag', 'saga' and 'lbfgs' solvers. **Note that regularization is applied by default**. It can handle both dense and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit floats for optimal performance; any other input format will be converted (and copied).  The 'newton-cg', 'sag', and 'lbfgs' solvers support only L2 regularization with primal formulation, or no regularization. The 'liblinear' solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty. The Elastic-Net regularization is only supported by the 'saga' solver.  For :term:`multiclass` problems, only 'newton-cg', 'sag', 'saga' and 'lbfgs' handle multinomial loss. 'liblinear' and 'newton-cholesky' only handle binary classification but can be extended to handle multiclass by using :class:`~sklearn.multiclass.OneVsRestClassifier`.  Read more in the :ref:`User Guide <logistic_regression>`.  Parameters ---------- penalty : {'l1', 'l2', 'elasticnet', None}, default='l2'     Specify the norm of the penalty:      - `None`: no penalty is added;     - `'l2'`: add a L2 penalty term and it is the default choice;     - `'l1'`: add a L1 penalty term;     - `'elasticnet'`: both L1 and L2 penalty terms are added.      .. warning::        Some penalties may not work with some solvers. See the parameter        `solver` below, to know the compatibility between the penalty and        solver.      .. versionadded:: 0.19        l1 penalty with SAGA solver (allowing 'multinomial' + L1)  dual : bool, default=False     Dual (constrained) or primal (regularized, see also     :ref:`this equation <regularized-logistic-loss>`) formulation. Dual formulation     is only implemented for l2 penalty with liblinear solver. Prefer dual=False when     n_samples > n_features.  tol : float, default=1e-4     Tolerance for stopping criteria.  C : float, default=1.0     Inverse of regularization strength; must be a positive float.     Like in support vector machines, smaller values specify stronger     regularization.  fit_intercept : bool, default=True     Specifies if a constant (a.k.a. bias or intercept) should be     added to the decision function.  intercept_scaling : float, default=1     Useful only when the solver 'liblinear' is used     and self.fit_intercept is set to True. In this case, x becomes     [x, self.intercept_scaling],     i.e. a ""synthetic"" feature with constant value equal to     intercept_scaling is appended to the instance vector.     The intercept becomes ``intercept_scaling * synthetic_feature_weight``.      Note! the synthetic feature weight is subject to l1/l2 regularization     as all other features.     To lessen the effect of regularization on synthetic feature weight     (and therefore on the intercept) intercept_scaling has to be increased.  class_weight : dict or 'balanced', default=None     Weights associated with classes in the form ``{class_label: weight}``.     If not given, all classes are supposed to have weight one.      The ""balanced"" mode uses the values of y to automatically adjust     weights inversely proportional to class frequencies in the input data     as ``n_samples / (n_classes * np.bincount(y))``.      Note that these weights will be multiplied with sample_weight (passed     through the fit method) if sample_weight is specified.      .. versionadded:: 0.17        *class_weight='balanced'*  random_state : int, RandomState instance, default=None     Used when ``solver`` == 'sag', 'saga' or 'liblinear' to shuffle the     data. See :term:`Glossary <random_state>` for details.  solver : {'lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga'},             default='lbfgs'      Algorithm to use in the optimization problem. Default is 'lbfgs'.     To choose a solver, you might want to consider the following aspects:      - For small datasets, 'liblinear' is a good choice, whereas 'sag'       and 'saga' are faster for large ones;     - For :term:`multiclass` problems, all solvers except 'liblinear' minimize the       full multinomial loss;     - 'liblinear' can only handle binary classification by default. To apply a       one-versus-rest scheme for the multiclass setting one can wrap it with the       :class:`~sklearn.multiclass.OneVsRestClassifier`.     - 'newton-cholesky' is a good choice for       `n_samples` >> `n_features * n_classes`, especially with one-hot encoded       categorical features with rare categories. Be aware that the memory usage       of this solver has a quadratic dependency on `n_features * n_classes`       because it explicitly computes the full Hessian matrix.      .. warning::        The choice of the algorithm depends on the penalty chosen and on        (multinomial) multiclass support:         ================= ============================== ======================        solver            penalty                        multinomial multiclass        ================= ============================== ======================        'lbfgs'           'l2', None                     yes        'liblinear'       'l1', 'l2'                     no        'newton-cg'       'l2', None                     yes        'newton-cholesky' 'l2', None                     no        'sag'             'l2', None                     yes        'saga'            'elasticnet', 'l1', 'l2', None yes        ================= ============================== ======================      .. note::        'sag' and 'saga' fast convergence is only guaranteed on features        with approximately the same scale. You can preprocess the data with        a scaler from :mod:`sklearn.preprocessing`.      .. seealso::        Refer to the :ref:`User Guide <Logistic_regression>` for more        information regarding :class:`LogisticRegression` and more specifically the        :ref:`Table <logistic_regression_solvers>`        summarizing solver/penalty supports.      .. versionadded:: 0.17        Stochastic Average Gradient descent solver.     .. versionadded:: 0.19        SAGA solver.     .. versionchanged:: 0.22         The default solver changed from 'liblinear' to 'lbfgs' in 0.22.     .. versionadded:: 1.2        newton-cholesky solver.  max_iter : int, default=100     Maximum number of iterations taken for the solvers to converge.  multi_class : {'auto', 'ovr', 'multinomial'}, default='auto'     If the option chosen is 'ovr', then a binary problem is fit for each     label. For 'multinomial' the loss minimised is the multinomial loss fit     across the entire probability distribution, *even when the data is     binary*. 'multinomial' is unavailable when solver='liblinear'.     'auto' selects 'ovr' if the data is binary, or if solver='liblinear',     and otherwise selects 'multinomial'.      .. versionadded:: 0.18        Stochastic Average Gradient descent solver for 'multinomial' case.     .. versionchanged:: 0.22         Default changed from 'ovr' to 'auto' in 0.22.     .. deprecated:: 1.5        ``multi_class`` was deprecated in version 1.5 and will be removed in 1.7.        From then on, the recommended 'multinomial' will always be used for        `n_classes >= 3`.        Solvers that do not support 'multinomial' will raise an error.        Use `sklearn.multiclass.OneVsRestClassifier(LogisticRegression())` if you        still want to use OvR.  verbose : int, default=0     For the liblinear and lbfgs solvers set verbose to any positive     number for verbosity.  warm_start : bool, default=False     When set to True, reuse the solution of the previous call to fit as     initialization, otherwise, just erase the previous solution.     Useless for liblinear solver. See :term:`the Glossary <warm_start>`.      .. versionadded:: 0.17        *warm_start* to support *lbfgs*, *newton-cg*, *sag*, *saga* solvers.  n_jobs : int, default=None     Number of CPU cores used when parallelizing over classes if     multi_class='ovr'"". This parameter is ignored when the ``solver`` is     set to 'liblinear' regardless of whether 'multi_class' is specified or     not. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`     context. ``-1`` means using all processors.     See :term:`Glossary <n_jobs>` for more details.  l1_ratio : float, default=None     The Elastic-Net mixing parameter, with ``0 <= l1_ratio <= 1``. Only     used if ``penalty='elasticnet'``. Setting ``l1_ratio=0`` is equivalent     to using ``penalty='l2'``, while setting ``l1_ratio=1`` is equivalent     to using ``penalty='l1'``. For ``0 < l1_ratio <1``, the penalty is a     combination of L1 and L2.  Attributes ----------  classes_ : ndarray of shape (n_classes, )     A list of class labels known to the classifier.  coef_ : ndarray of shape (1, n_features) or (n_classes, n_features)     Coefficient of the features in the decision function.      `coef_` is of shape (1, n_features) when the given problem is binary.     In particular, when `multi_class='multinomial'`, `coef_` corresponds     to outcome 1 (True) and `-coef_` corresponds to outcome 0 (False).  intercept_ : ndarray of shape (1,) or (n_classes,)     Intercept (a.k.a. bias) added to the decision function.      If `fit_intercept` is set to False, the intercept is set to zero.     `intercept_` is of shape (1,) when the given problem is binary.     In particular, when `multi_class='multinomial'`, `intercept_`     corresponds to outcome 1 (True) and `-intercept_` corresponds to     outcome 0 (False).  n_features_in_ : int     Number of features seen during :term:`fit`.      .. versionadded:: 0.24  feature_names_in_ : ndarray of shape (`n_features_in_`,)     Names of features seen during :term:`fit`. Defined only when `X`     has feature names that are all strings.      .. versionadded:: 1.0  n_iter_ : ndarray of shape (n_classes,) or (1, )     Actual number of iterations for all classes. If binary or multinomial,     it returns only 1 element. For liblinear solver, only the maximum     number of iteration across all classes is given.      .. versionchanged:: 0.20          In SciPy <= 1.0.0 the number of lbfgs iterations may exceed         ``max_iter``. ``n_iter_`` will now report at most ``max_iter``.  See Also -------- SGDClassifier : Incrementally trained logistic regression (when given     the parameter ``loss=""log_loss""``). LogisticRegressionCV : Logistic regression with built-in cross validation.  Notes ----- The underlying C implementation uses a random number generator to select features when fitting the model. It is thus not uncommon, to have slightly different results for the same input data. If that happens, try with a smaller tol parameter.  Predict output may not match that of standalone liblinear in certain cases. See :ref:`differences from liblinear <liblinear_differences>` in the narrative documentation.  References ----------  L-BFGS-B -- Software for Large-scale Bound-constrained Optimization     Ciyou Zhu, Richard Byrd, Jorge Nocedal and Jose Luis Morales.     http://users.iems.northwestern.edu/~nocedal/lbfgsb.html  LIBLINEAR -- A Library for Large Linear Classification     https://www.csie.ntu.edu.tw/~cjlin/liblinear/  SAG -- Mark Schmidt, Nicolas Le Roux, and Francis Bach     Minimizing Finite Sums with the Stochastic Average Gradient     https://hal.inria.fr/hal-00860051/document  SAGA -- Defazio, A., Bach F. & Lacoste-Julien S. (2014).         :arxiv:`""SAGA: A Fast Incremental Gradient Method With Support         for Non-Strongly Convex Composite Objectives"" <1407.0202>`  Hsiang-Fu Yu, Fang-Lan Huang, Chih-Jen Lin (2011). Dual coordinate descent     methods for logistic regression and maximum entropy models.     Machine Learning 85(1-2):41-75.     https://www.csie.ntu.edu.tw/~cjlin/papers/maxent_dual.pdf  Examples -------- >>> from sklearn.datasets import load_iris >>> from sklearn.linear_model import LogisticRegression >>> X, y = load_iris(return_X_y=True) >>> clf = LogisticRegression(random_state=0).fit(X, y) >>> clf.predict(X[:2, :]) array([0, 0]) >>> clf.predict_proba(X[:2, :]) array([[9.8...e-01, 1.8...e-02, 1.4...e-08],        [9.7...e-01, 2.8...e-02, ...e-08]]) >>> clf.score(X, y) 0.97...  For a comaprison of the LogisticRegression with other classifiers see: :ref:`sphx_glr_auto_examples_classification_plot_classification_probability.py`."
make_pipeline,"Construct a :class:`Pipeline` from the given estimators.  This is a shorthand for the :class:`Pipeline` constructor; it does not require, and does not permit, naming the estimators. Instead, their names will be set to the lowercase of their types automatically.  Parameters ---------- *steps : list of Estimator objects     List of the scikit-learn estimators that are chained together.  memory : str or object with the joblib.Memory interface, default=None     Used to cache the fitted transformers of the pipeline. The last step     will never be cached, even if it is a transformer. By default, no     caching is performed. If a string is given, it is the path to the     caching directory. Enabling caching triggers a clone of the transformers     before fitting. Therefore, the transformer instance given to the     pipeline cannot be inspected directly. Use the attribute ``named_steps``     or ``steps`` to inspect estimators within the pipeline. Caching the     transformers is advantageous when fitting is time consuming.  transform_input : list of str, default=None     This enables transforming some input arguments to ``fit`` (other than ``X``)     to be transformed by the steps of the pipeline up to the step which requires     them. Requirement is defined via :ref:`metadata routing <metadata_routing>`.     This can be used to pass a validation set through the pipeline for instance.      You can only set this if metadata routing is enabled, which you     can enable using ``sklearn.set_config(enable_metadata_routing=True)``.      .. versionadded:: 1.6  verbose : bool, default=False     If True, the time elapsed while fitting each step will be printed as it     is completed.  Returns ------- p : Pipeline     Returns a scikit-learn :class:`Pipeline` object.  See Also -------- Pipeline : Class for creating a pipeline of transforms with a final     estimator.  Examples -------- >>> from sklearn.naive_bayes import GaussianNB >>> from sklearn.preprocessing import StandardScaler >>> from sklearn.pipeline import make_pipeline >>> make_pipeline(StandardScaler(), GaussianNB(priors=None)) Pipeline(steps=[('standardscaler', StandardScaler()),                 ('gaussiannb', GaussianNB())])"
PCA,"Principal component analysis (PCA).  Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. The input data is centered but not scaled for each feature before applying the SVD.  It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.  With sparse inputs, the ARPACK implementation of the truncated SVD can be used (i.e. through :func:`scipy.sparse.linalg.svds`). Alternatively, one may consider :class:`TruncatedSVD` where the data are not centered.  Notice that this class only supports sparse inputs for some solvers such as ""arpack"" and ""covariance_eigh"". See :class:`TruncatedSVD` for an alternative with sparse data.  For a usage example, see :ref:`sphx_glr_auto_examples_decomposition_plot_pca_iris.py`  Read more in the :ref:`User Guide <PCA>`.  Parameters ---------- n_components : int, float or 'mle', default=None     Number of components to keep.     if n_components is not set all components are kept::          n_components == min(n_samples, n_features)      If ``n_components == 'mle'`` and ``svd_solver == 'full'``, Minka's     MLE is used to guess the dimension. Use of ``n_components == 'mle'``     will interpret ``svd_solver == 'auto'`` as ``svd_solver == 'full'``.      If ``0 < n_components < 1`` and ``svd_solver == 'full'``, select the     number of components such that the amount of variance that needs to be     explained is greater than the percentage specified by n_components.      If ``svd_solver == 'arpack'``, the number of components must be     strictly less than the minimum of n_features and n_samples.      Hence, the None case results in::          n_components == min(n_samples, n_features) - 1  copy : bool, default=True     If False, data passed to fit are overwritten and running     fit(X).transform(X) will not yield the expected results,     use fit_transform(X) instead.  whiten : bool, default=False     When True (False by default) the `components_` vectors are multiplied     by the square root of n_samples and then divided by the singular values     to ensure uncorrelated outputs with unit component-wise variances.      Whitening will remove some information from the transformed signal     (the relative variance scales of the components) but can sometime     improve the predictive accuracy of the downstream estimators by     making their data respect some hard-wired assumptions.  svd_solver : {'auto', 'full', 'covariance_eigh', 'arpack', 'randomized'},            default='auto'     ""auto"" :         The solver is selected by a default 'auto' policy is based on `X.shape` and         `n_components`: if the input data has fewer than 1000 features and         more than 10 times as many samples, then the ""covariance_eigh""         solver is used. Otherwise, if the input data is larger than 500x500         and the number of components to extract is lower than 80% of the         smallest dimension of the data, then the more efficient         ""randomized"" method is selected. Otherwise the exact ""full"" SVD is         computed and optionally truncated afterwards.     ""full"" :         Run exact full SVD calling the standard LAPACK solver via         `scipy.linalg.svd` and select the components by postprocessing     ""covariance_eigh"" :         Precompute the covariance matrix (on centered data), run a         classical eigenvalue decomposition on the covariance matrix         typically using LAPACK and select the components by postprocessing.         This solver is very efficient for n_samples >> n_features and small         n_features. It is, however, not tractable otherwise for large         n_features (large memory footprint required to materialize the         covariance matrix). Also note that compared to the ""full"" solver,         this solver effectively doubles the condition number and is         therefore less numerical stable (e.g. on input data with a large         range of singular values).     ""arpack"" :         Run SVD truncated to `n_components` calling ARPACK solver via         `scipy.sparse.linalg.svds`. It requires strictly         `0 < n_components < min(X.shape)`     ""randomized"" :         Run randomized SVD by the method of Halko et al.      .. versionadded:: 0.18.0      .. versionchanged:: 1.5         Added the 'covariance_eigh' solver.  tol : float, default=0.0     Tolerance for singular values computed by svd_solver == 'arpack'.     Must be of range [0.0, infinity).      .. versionadded:: 0.18.0  iterated_power : int or 'auto', default='auto'     Number of iterations for the power method computed by     svd_solver == 'randomized'.     Must be of range [0, infinity).      .. versionadded:: 0.18.0  n_oversamples : int, default=10     This parameter is only relevant when `svd_solver=""randomized""`.     It corresponds to the additional number of random vectors to sample the     range of `X` so as to ensure proper conditioning. See     :func:`~sklearn.utils.extmath.randomized_svd` for more details.      .. versionadded:: 1.1  power_iteration_normalizer : {'auto', 'QR', 'LU', 'none'}, default='auto'     Power iteration normalizer for randomized SVD solver.     Not used by ARPACK. See :func:`~sklearn.utils.extmath.randomized_svd`     for more details.      .. versionadded:: 1.1  random_state : int, RandomState instance or None, default=None     Used when the 'arpack' or 'randomized' solvers are used. Pass an int     for reproducible results across multiple function calls.     See :term:`Glossary <random_state>`.      .. versionadded:: 0.18.0  Attributes ---------- components_ : ndarray of shape (n_components, n_features)     Principal axes in feature space, representing the directions of     maximum variance in the data. Equivalently, the right singular     vectors of the centered input data, parallel to its eigenvectors.     The components are sorted by decreasing ``explained_variance_``.  explained_variance_ : ndarray of shape (n_components,)     The amount of variance explained by each of the selected components.     The variance estimation uses `n_samples - 1` degrees of freedom.      Equal to n_components largest eigenvalues     of the covariance matrix of X.      .. versionadded:: 0.18  explained_variance_ratio_ : ndarray of shape (n_components,)     Percentage of variance explained by each of the selected components.      If ``n_components`` is not set then all components are stored and the     sum of the ratios is equal to 1.0.  singular_values_ : ndarray of shape (n_components,)     The singular values corresponding to each of the selected components.     The singular values are equal to the 2-norms of the ``n_components``     variables in the lower-dimensional space.      .. versionadded:: 0.19  mean_ : ndarray of shape (n_features,)     Per-feature empirical mean, estimated from the training set.      Equal to `X.mean(axis=0)`.  n_components_ : int     The estimated number of components. When n_components is set     to 'mle' or a number between 0 and 1 (with svd_solver == 'full') this     number is estimated from input data. Otherwise it equals the parameter     n_components, or the lesser value of n_features and n_samples     if n_components is None.  n_samples_ : int     Number of samples in the training data.  noise_variance_ : float     The estimated noise covariance following the Probabilistic PCA model     from Tipping and Bishop 1999. See ""Pattern Recognition and     Machine Learning"" by C. Bishop, 12.2.1 p. 574 or     http://www.miketipping.com/papers/met-mppca.pdf. It is required to     compute the estimated data covariance and score samples.      Equal to the average of (min(n_features, n_samples) - n_components)     smallest eigenvalues of the covariance matrix of X.  n_features_in_ : int     Number of features seen during :term:`fit`.      .. versionadded:: 0.24  feature_names_in_ : ndarray of shape (`n_features_in_`,)     Names of features seen during :term:`fit`. Defined only when `X`     has feature names that are all strings.      .. versionadded:: 1.0  See Also -------- KernelPCA : Kernel Principal Component Analysis. SparsePCA : Sparse Principal Component Analysis. TruncatedSVD : Dimensionality reduction using truncated SVD. IncrementalPCA : Incremental Principal Component Analysis.  References ---------- For n_components == 'mle', this class uses the method from: `Minka, T. P.. ""Automatic choice of dimensionality for PCA"". In NIPS, pp. 598-604 <https://tminka.github.io/papers/pca/minka-pca.pdf>`_  Implements the probabilistic PCA model from: `Tipping, M. E., and Bishop, C. M. (1999). ""Probabilistic principal component analysis"". Journal of the Royal Statistical Society: Series B (Statistical Methodology), 61(3), 611-622. <http://www.miketipping.com/papers/met-mppca.pdf>`_ via the score and score_samples methods.  For svd_solver == 'arpack', refer to `scipy.sparse.linalg.svds`.  For svd_solver == 'randomized', see: :doi:`Halko, N., Martinsson, P. G., and Tropp, J. A. (2011). ""Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions"". SIAM review, 53(2), 217-288. <10.1137/090771806>` and also :doi:`Martinsson, P. G., Rokhlin, V., and Tygert, M. (2011). ""A randomized algorithm for the decomposition of matrices"". Applied and Computational Harmonic Analysis, 30(1), 47-68. <10.1016/j.acha.2010.02.003>`  Examples -------- >>> import numpy as np >>> from sklearn.decomposition import PCA >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]]) >>> pca = PCA(n_components=2) >>> pca.fit(X) PCA(n_components=2) >>> print(pca.explained_variance_ratio_) [0.9924... 0.0075...] >>> print(pca.singular_values_) [6.30061... 0.54980...]  >>> pca = PCA(n_components=2, svd_solver='full') >>> pca.fit(X) PCA(n_components=2, svd_solver='full') >>> print(pca.explained_variance_ratio_) [0.9924... 0.00755...] >>> print(pca.singular_values_) [6.30061... 0.54980...]  >>> pca = PCA(n_components=1, svd_solver='arpack') >>> pca.fit(X) PCA(n_components=1, svd_solver='arpack') >>> print(pca.explained_variance_ratio_) [0.99244...] >>> print(pca.singular_values_) [6.30061...]"
torch,"The torch package contains data structures for multi-dimensional tensors and defines mathematical operations over these tensors. Additionally, it provides many utilities for efficient serialization of Tensors and arbitrary types, and other useful utilities.  It has a CUDA counterpart, that enables you to run your tensor computations on an NVIDIA GPU with compute capability >= 3.0."
torch.nn,No documentation found.
torch.optim,":mod:`torch.optim` is a package implementing various optimization algorithms.  Most commonly used methods are already supported, and the interface is general enough, so that more sophisticated ones can also be easily integrated in the future."
torchvision,No documentation found.
torchvision.transforms,No documentation found.
torch.nn.functional,Functional interface.
Dataset,"An abstract class representing a :class:`Dataset`.  All datasets that represent a map from keys to data samples should subclass it. All subclasses should overwrite :meth:`__getitem__`, supporting fetching a data sample for a given key. Subclasses could also optionally overwrite :meth:`__len__`, which is expected to return the size of the dataset by many :class:`~torch.utils.data.Sampler` implementations and the default options of :class:`~torch.utils.data.DataLoader`. Subclasses could also optionally implement :meth:`__getitems__`, for speedup batched samples loading. This method accepts list of indices of samples of batch and returns list of samples.  .. note::   :class:`~torch.utils.data.DataLoader` by default constructs an index   sampler that yields integral indices.  To make it work with a map-style   dataset with non-integral indices/keys, a custom sampler must be provided."
DataLoader,"Data loader combines a dataset and a sampler, and provides an iterable over the given dataset.  The :class:`~torch.utils.data.DataLoader` supports both map-style and iterable-style datasets with single- or multi-process loading, customizing loading order and optional automatic batching (collation) and memory pinning.  See :py:mod:`torch.utils.data` documentation page for more details.  Args:     dataset (Dataset): dataset from which to load the data.     batch_size (int, optional): how many samples per batch to load         (default: ``1``).     shuffle (bool, optional): set to ``True`` to have the data reshuffled         at every epoch (default: ``False``).     sampler (Sampler or Iterable, optional): defines the strategy to draw         samples from the dataset. Can be any ``Iterable`` with ``__len__``         implemented. If specified, :attr:`shuffle` must not be specified.     batch_sampler (Sampler or Iterable, optional): like :attr:`sampler`, but         returns a batch of indices at a time. Mutually exclusive with         :attr:`batch_size`, :attr:`shuffle`, :attr:`sampler`,         and :attr:`drop_last`.     num_workers (int, optional): how many subprocesses to use for data         loading. ``0`` means that the data will be loaded in the main process.         (default: ``0``)     collate_fn (Callable, optional): merges a list of samples to form a         mini-batch of Tensor(s).  Used when using batched loading from a         map-style dataset.     pin_memory (bool, optional): If ``True``, the data loader will copy Tensors         into device/CUDA pinned memory before returning them.  If your data elements         are a custom type, or your :attr:`collate_fn` returns a batch that is a custom type,         see the example below.     drop_last (bool, optional): set to ``True`` to drop the last incomplete batch,         if the dataset size is not divisible by the batch size. If ``False`` and         the size of dataset is not divisible by the batch size, then the last batch         will be smaller. (default: ``False``)     timeout (numeric, optional): if positive, the timeout value for collecting a batch         from workers. Should always be non-negative. (default: ``0``)     worker_init_fn (Callable, optional): If not ``None``, this will be called on each         worker subprocess with the worker id (an int in ``[0, num_workers - 1]``) as         input, after seeding and before data loading. (default: ``None``)     multiprocessing_context (str or multiprocessing.context.BaseContext, optional): If         ``None``, the default `multiprocessing context`_ of your operating system will         be used. (default: ``None``)     generator (torch.Generator, optional): If not ``None``, this RNG will be used         by RandomSampler to generate random indexes and multiprocessing to generate         ``base_seed`` for workers. (default: ``None``)     prefetch_factor (int, optional, keyword-only arg): Number of batches loaded         in advance by each worker. ``2`` means there will be a total of         2 * num_workers batches prefetched across all workers. (default value depends         on the set value for num_workers. If value of num_workers=0 default is ``None``.         Otherwise, if value of ``num_workers > 0`` default is ``2``).     persistent_workers (bool, optional): If ``True``, the data loader will not shut down         the worker processes after a dataset has been consumed once. This allows to         maintain the workers `Dataset` instances alive. (default: ``False``)     pin_memory_device (str, optional): the device to :attr:`pin_memory` on if ``pin_memory`` is         ``True``. If not given, the current :ref:`accelerator<accelerators>` will be the         default. This argument is discouraged and subject to deprecated.     in_order (bool, optional): If ``False``, the data loader will not enforce that batches         are returned in a first-in, first-out order. Only applies when ``num_workers > 0``. (default: ``True``)   .. warning:: If the ``spawn`` start method is used, :attr:`worker_init_fn`              cannot be an unpicklable object, e.g., a lambda function. See              :ref:`multiprocessing-best-practices` on more details related              to multiprocessing in PyTorch.  .. warning:: ``len(dataloader)`` heuristic is based on the length of the sampler used.              When :attr:`dataset` is an :class:`~torch.utils.data.IterableDataset`,              it instead returns an estimate based on ``len(dataset) / batch_size``, with proper              rounding depending on :attr:`drop_last`, regardless of multi-process loading              configurations. This represents the best guess PyTorch can make because PyTorch              trusts user :attr:`dataset` code in correctly handling multi-process              loading to avoid duplicate data.               However, if sharding results in multiple workers having incomplete last batches,              this estimate can still be inaccurate, because (1) an otherwise complete batch can              be broken into multiple ones and (2) more than one batch worth of samples can be              dropped when :attr:`drop_last` is set. Unfortunately, PyTorch can not detect such              cases in general.               See `Dataset Types`_ for more details on these two types of datasets and how              :class:`~torch.utils.data.IterableDataset` interacts with              `Multi-process data loading`_.  .. warning:: See :ref:`reproducibility`, and :ref:`dataloader-workers-random-seed`, and              :ref:`data-loading-randomness` notes for random seed related questions.  .. warning:: Setting `in_order` to `False` can harm reproducibility and may lead to a skewed data              distribution being fed to the trainer in cases with imbalanced data.  .. _multiprocessing context:     https://docs.python.org/3/library/multiprocessing.html#contexts-and-start-methods"
torchvision.datasets,No documentation found.
ToTensor,"Convert a PIL Image or ndarray to tensor and scale the values accordingly.  This transform does not support torchscript.  Converts a PIL Image or numpy.ndarray (H x W x C) in the range [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0] if the PIL Image belongs to one of the modes (L, LA, P, I, F, RGB, YCbCr, RGBA, CMYK, 1) or if the numpy.ndarray has dtype = np.uint8  In the other cases, tensors are returned without scaling.  .. note::     Because the input image is scaled to [0.0, 1.0], this transformation should not be used when     transforming target image masks. See the `references`_ for implementing the transforms for image masks.  .. _references: https://github.com/pytorch/vision/tree/main/references/segmentation"
numpy,"NumPy =====  Provides   1. An array object of arbitrary homogeneous items   2. Fast mathematical operations over arrays   3. Linear Algebra, Fourier Transforms, Random Number Generation  How to use the documentation ---------------------------- Documentation is available in two forms: docstrings provided with the code, and a loose standing reference guide, available from `the NumPy homepage <https://numpy.org>`_.  We recommend exploring the docstrings using `IPython <https://ipython.org>`_, an advanced Python shell with TAB-completion and introspection capabilities.  See below for further instructions.  The docstring examples assume that `numpy` has been imported as ``np``::    >>> import numpy as np  Code snippets are indicated by three greater-than signs::    >>> x = 42   >>> x = x + 1  Use the built-in ``help`` function to view a function's docstring::    >>> help(np.sort)   ... # doctest: +SKIP  For some objects, ``np.info(obj)`` may provide additional help.  This is particularly true if you see the line ""Help on ufunc object:"" at the top of the help() page.  Ufuncs are implemented in C, not Python, for speed. The native Python help() does not know how to view their help, but our np.info() function does.  Available subpackages --------------------- lib     Basic functions used by several sub-packages. random     Core Random Tools linalg     Core Linear Algebra Tools fft     Core FFT routines polynomial     Polynomial tools testing     NumPy testing tools distutils     Enhancements to distutils with support for     Fortran compilers support and more (for Python <= 3.11)  Utilities --------- test     Run numpy unittests show_config     Show numpy build configuration __version__     NumPy version string  Viewing documentation using IPython -----------------------------------  Start IPython and import `numpy` usually under the alias ``np``: `import numpy as np`.  Then, directly past or use the ``%cpaste`` magic to paste examples into the shell.  To see which functions are available in `numpy`, type ``np.<TAB>`` (where ``<TAB>`` refers to the TAB key), or use ``np.*cos*?<ENTER>`` (where ``<ENTER>`` refers to the ENTER key) to narrow down the list.  To view the docstring for a function, use ``np.cos?<ENTER>`` (to view the docstring) and ``np.cos??<ENTER>`` (to view the source code).  Copies vs. in-place operation ----------------------------- Most of the functions in `numpy` return a copy of the array argument (e.g., `np.sort`).  In-place versions of these functions are often available as array methods, i.e. ``x = np.array([1,2,3]); x.sort()``. Exceptions to this rule are documented."
pandas,"pandas - a powerful data analysis and manipulation library for Python =====================================================================  **pandas** is a Python package providing fast, flexible, and expressive data structures designed to make working with ""relational"" or ""labeled"" data both easy and intuitive. It aims to be the fundamental high-level building block for doing practical, **real world** data analysis in Python. Additionally, it has the broader goal of becoming **the most powerful and flexible open source data analysis / manipulation tool available in any language**. It is already well on its way toward this goal.  Main Features ------------- Here are just a few of the things that pandas does well:    - Easy handling of missing data in floating point as well as non-floating     point data.   - Size mutability: columns can be inserted and deleted from DataFrame and     higher dimensional objects   - Automatic and explicit data alignment: objects can be explicitly aligned     to a set of labels, or the user can simply ignore the labels and let     `Series`, `DataFrame`, etc. automatically align the data for you in     computations.   - Powerful, flexible group by functionality to perform split-apply-combine     operations on data sets, for both aggregating and transforming data.   - Make it easy to convert ragged, differently-indexed data in other Python     and NumPy data structures into DataFrame objects.   - Intelligent label-based slicing, fancy indexing, and subsetting of large     data sets.   - Intuitive merging and joining data sets.   - Flexible reshaping and pivoting of data sets.   - Hierarchical labeling of axes (possible to have multiple labels per tick).   - Robust IO tools for loading data from flat files (CSV and delimited),     Excel files, databases, and saving/loading data from the ultrafast HDF5     format.   - Time series-specific functionality: date range generation and frequency     conversion, moving window statistics, date shifting and lagging."
matplotlib.pyplot,"`matplotlib.pyplot` is a state-based interface to matplotlib. It provides an implicit,  MATLAB-like, way of plotting.  It also opens figures on your screen, and acts as the figure GUI manager.  pyplot is mainly intended for interactive plots and simple cases of programmatic plot generation::      import numpy as np     import matplotlib.pyplot as plt      x = np.arange(0, 5, 0.1)     y = np.sin(x)     plt.plot(x, y)     plt.show()  The explicit object-oriented API is recommended for complex plots, though pyplot is still usually used to create the figure and often the Axes in the figure. See `.pyplot.figure`, `.pyplot.subplots`, and `.pyplot.subplot_mosaic` to create figures, and :doc:`Axes API </api/axes_api>` for the plotting methods on an Axes::      import numpy as np     import matplotlib.pyplot as plt      x = np.arange(0, 5, 0.1)     y = np.sin(x)     fig, ax = plt.subplots()     ax.plot(x, y)     plt.show()   See :ref:`api_interfaces` for an explanation of the tradeoffs between the implicit and explicit interfaces."
AutoTokenizer,This is a generic tokenizer class that will be instantiated as one of the tokenizer classes of the library when created with the [`AutoTokenizer.from_pretrained`] class method.  This class cannot be instantiated directly using `__init__()` (throws an error).
DataCollatorWithPadding,"Data collator that will dynamically pad the inputs received.  Args:     tokenizer ([`PreTrainedTokenizer`] or [`PreTrainedTokenizerFast`]):         The tokenizer used for encoding the data.     padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `True`):         Select a strategy to pad the returned sequences (according to the model's padding side and padding index)         among:          - `True` or `'longest'` (default): Pad to the longest sequence in the batch (or no padding if only a single           sequence is provided).         - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum           acceptable input length for the model if that argument is not provided.         - `False` or `'do_not_pad'`: No padding (i.e., can output a batch with sequences of different lengths).     max_length (`int`, *optional*):         Maximum length of the returned list and optionally padding length (see above).     pad_to_multiple_of (`int`, *optional*):         If set will pad the sequence to a multiple of the provided value.          This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=         7.0 (Volta).     return_tensors (`str`, *optional*, defaults to `""pt""`):         The type of Tensor to return. Allowable values are ""np"", ""pt"" and ""tf""."
AutoModelForSequenceClassification,This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created with the [`~AutoModelForSequenceClassification.from_pretrained`] class method or the [`~AutoModelForSequenceClassification.from_config`] class method.  This class cannot be instantiated directly using `__init__()` (throws an error).
TrainingArguments,"TrainingArguments is the subset of the arguments we use in our example scripts **which relate to the training loop itself**.  Using [`HfArgumentParser`] we can turn this class into [argparse](https://docs.python.org/3/library/argparse#module-argparse) arguments that can be specified on the command line.  Parameters:     output_dir (`str`, *optional*, defaults to `""trainer_output""`):         The output directory where the model predictions and checkpoints will be written.     overwrite_output_dir (`bool`, *optional*, defaults to `False`):         If `True`, overwrite the content of the output directory. Use this to continue training if `output_dir`         points to a checkpoint directory.     do_train (`bool`, *optional*, defaults to `False`):         Whether to run training or not. This argument is not directly used by [`Trainer`], it's intended to be used         by your training/evaluation scripts instead. See the [example         scripts](https://github.com/huggingface/transformers/tree/main/examples) for more details.     do_eval (`bool`, *optional*):         Whether to run evaluation on the validation set or not. Will be set to `True` if `eval_strategy` is         different from `""no""`. This argument is not directly used by [`Trainer`], it's intended to be used by your         training/evaluation scripts instead. See the [example         scripts](https://github.com/huggingface/transformers/tree/main/examples) for more details.     do_predict (`bool`, *optional*, defaults to `False`):         Whether to run predictions on the test set or not. This argument is not directly used by [`Trainer`], it's         intended to be used by your training/evaluation scripts instead. See the [example         scripts](https://github.com/huggingface/transformers/tree/main/examples) for more details.     eval_strategy (`str` or [`~trainer_utils.IntervalStrategy`], *optional*, defaults to `""no""`):         The evaluation strategy to adopt during training. Possible values are:              - `""no""`: No evaluation is done during training.             - `""steps""`: Evaluation is done (and logged) every `eval_steps`.             - `""epoch""`: Evaluation is done at the end of each epoch.      prediction_loss_only (`bool`, *optional*, defaults to `False`):         When performing evaluation and generating predictions, only returns the loss.     per_device_train_batch_size (`int`, *optional*, defaults to 8):         The batch size per device accelerator core/CPU for training.     per_device_eval_batch_size (`int`, *optional*, defaults to 8):         The batch size per device accelerator core/CPU for evaluation.     gradient_accumulation_steps (`int`, *optional*, defaults to 1):         Number of updates steps to accumulate the gradients for, before performing a backward/update pass.          <Tip warning={true}>          When using gradient accumulation, one step is counted as one step with backward pass. Therefore, logging,         evaluation, save will be conducted every `gradient_accumulation_steps * xxx_step` training examples.          </Tip>      eval_accumulation_steps (`int`, *optional*):         Number of predictions steps to accumulate the output tensors for, before moving the results to the CPU. If         left unset, the whole predictions are accumulated on the device accelerator before being moved to the CPU (faster but         requires more memory).     eval_delay (`float`, *optional*):         Number of epochs or steps to wait for before the first evaluation can be performed, depending on the         eval_strategy.     torch_empty_cache_steps (`int`, *optional*):         Number of steps to wait before calling `torch.<device>.empty_cache()`. If left unset or set to None, cache will not be emptied.          <Tip>          This can help avoid CUDA out-of-memory errors by lowering peak VRAM usage at a cost of about [10% slower performance](https://github.com/huggingface/transformers/issues/31372).          </Tip>      learning_rate (`float`, *optional*, defaults to 5e-5):         The initial learning rate for [`AdamW`] optimizer.     weight_decay (`float`, *optional*, defaults to 0):         The weight decay to apply (if not zero) to all layers except all bias and LayerNorm weights in [`AdamW`]         optimizer.     adam_beta1 (`float`, *optional*, defaults to 0.9):         The beta1 hyperparameter for the [`AdamW`] optimizer.     adam_beta2 (`float`, *optional*, defaults to 0.999):         The beta2 hyperparameter for the [`AdamW`] optimizer.     adam_epsilon (`float`, *optional*, defaults to 1e-8):         The epsilon hyperparameter for the [`AdamW`] optimizer.     max_grad_norm (`float`, *optional*, defaults to 1.0):         Maximum gradient norm (for gradient clipping).     num_train_epochs(`float`, *optional*, defaults to 3.0):         Total number of training epochs to perform (if not an integer, will perform the decimal part percents of         the last epoch before stopping training).     max_steps (`int`, *optional*, defaults to -1):         If set to a positive number, the total number of training steps to perform. Overrides `num_train_epochs`.         For a finite dataset, training is reiterated through the dataset (if all data is exhausted) until         `max_steps` is reached.     lr_scheduler_type (`str` or [`SchedulerType`], *optional*, defaults to `""linear""`):         The scheduler type to use. See the documentation of [`SchedulerType`] for all possible values.     lr_scheduler_kwargs ('dict', *optional*, defaults to {}):         The extra arguments for the lr_scheduler. See the documentation of each scheduler for possible values.     warmup_ratio (`float`, *optional*, defaults to 0.0):         Ratio of total training steps used for a linear warmup from 0 to `learning_rate`.     warmup_steps (`int`, *optional*, defaults to 0):         Number of steps used for a linear warmup from 0 to `learning_rate`. Overrides any effect of `warmup_ratio`.     log_level (`str`, *optional*, defaults to `passive`):         Logger log level to use on the main process. Possible choices are the log levels as strings: 'debug',         'info', 'warning', 'error' and 'critical', plus a 'passive' level which doesn't set anything and keeps the         current log level for the Transformers library (which will be `""warning""` by default).     log_level_replica (`str`, *optional*, defaults to `""warning""`):         Logger log level to use on replicas. Same choices as `log_level`""     log_on_each_node (`bool`, *optional*, defaults to `True`):         In multinode distributed training, whether to log using `log_level` once per node, or only on the main         node.     logging_dir (`str`, *optional*):         [TensorBoard](https://www.tensorflow.org/tensorboard) log directory. Will default to         *output_dir/runs/**CURRENT_DATETIME_HOSTNAME***.     logging_strategy (`str` or [`~trainer_utils.IntervalStrategy`], *optional*, defaults to `""steps""`):         The logging strategy to adopt during training. Possible values are:              - `""no""`: No logging is done during training.             - `""epoch""`: Logging is done at the end of each epoch.             - `""steps""`: Logging is done every `logging_steps`.      logging_first_step (`bool`, *optional*, defaults to `False`):         Whether to log the first `global_step` or not.     logging_steps (`int` or `float`, *optional*, defaults to 500):         Number of update steps between two logs if `logging_strategy=""steps""`. Should be an integer or a float in         range `[0,1)`. If smaller than 1, will be interpreted as ratio of total training steps.     logging_nan_inf_filter (`bool`, *optional*, defaults to `True`):         Whether to filter `nan` and `inf` losses for logging. If set to `True` the loss of every step that is `nan`         or `inf` is filtered and the average loss of the current logging window is taken instead.          <Tip>          `logging_nan_inf_filter` only influences the logging of loss values, it does not change the behavior the         gradient is computed or applied to the model.          </Tip>      save_strategy (`str` or [`~trainer_utils.SaveStrategy`], *optional*, defaults to `""steps""`):         The checkpoint save strategy to adopt during training. Possible values are:              - `""no""`: No save is done during training.             - `""epoch""`: Save is done at the end of each epoch.             - `""steps""`: Save is done every `save_steps`.             - `""best""`: Save is done whenever a new `best_metric` is achieved.              If `""epoch""` or `""steps""` is chosen, saving will also be performed at the             very end of training, always.     save_steps (`int` or `float`, *optional*, defaults to 500):         Number of updates steps before two checkpoint saves if `save_strategy=""steps""`. Should be an integer or a         float in range `[0,1)`. If smaller than 1, will be interpreted as ratio of total training steps.     save_total_limit (`int`, *optional*):         If a value is passed, will limit the total amount of checkpoints. Deletes the older checkpoints in         `output_dir`. When `load_best_model_at_end` is enabled, the ""best"" checkpoint according to         `metric_for_best_model` will always be retained in addition to the most recent ones. For example, for         `save_total_limit=5` and `load_best_model_at_end`, the four last checkpoints will always be retained         alongside the best model. When `save_total_limit=1` and `load_best_model_at_end`, it is possible that two         checkpoints are saved: the last one and the best one (if they are different).     save_safetensors (`bool`, *optional*, defaults to `True`):         Use [safetensors](https://huggingface.co/docs/safetensors) saving and loading for state dicts instead of         default `torch.load` and `torch.save`.     save_on_each_node (`bool`, *optional*, defaults to `False`):         When doing multi-node distributed training, whether to save models and checkpoints on each node, or only on         the main one.          This should not be activated when the different nodes use the same storage as the files will be saved with         the same names for each node.     save_only_model (`bool`, *optional*, defaults to `False`):         When checkpointing, whether to only save the model, or also the optimizer, scheduler & rng state.         Note that when this is true, you won't be able to resume training from checkpoint.         This enables you to save storage by not storing the optimizer, scheduler & rng state.         You can only load the model using `from_pretrained` with this option set to `True`.     restore_callback_states_from_checkpoint (`bool`, *optional*, defaults to `False`):         Whether to restore the callback states from the checkpoint. If `True`, will override         callbacks passed to the `Trainer` if they exist in the checkpoint.""     use_cpu (`bool`, *optional*, defaults to `False`):         Whether or not to use cpu. If set to False, we will use cuda or mps device if available.     seed (`int`, *optional*, defaults to 42):         Random seed that will be set at the beginning of training. To ensure reproducibility across runs, use the         [`~Trainer.model_init`] function to instantiate the model if it has some randomly initialized parameters.     data_seed (`int`, *optional*):         Random seed to be used with data samplers. If not set, random generators for data sampling will use the         same seed as `seed`. This can be used to ensure reproducibility of data sampling, independent of the model         seed.     jit_mode_eval (`bool`, *optional*, defaults to `False`):         Whether or not to use PyTorch jit trace for inference.     use_ipex (`bool`, *optional*, defaults to `False`):         Use Intel extension for PyTorch when it is available. [IPEX         installation](https://github.com/intel/intel-extension-for-pytorch).     bf16 (`bool`, *optional*, defaults to `False`):         Whether to use bf16 16-bit (mixed) precision training instead of 32-bit training. Requires Ampere or higher         NVIDIA architecture or using CPU (use_cpu) or Ascend NPU. This is an experimental API and it may change.     fp16 (`bool`, *optional*, defaults to `False`):         Whether to use fp16 16-bit (mixed) precision training instead of 32-bit training.     fp16_opt_level (`str`, *optional*, defaults to 'O1'):         For `fp16` training, Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3']. See details on         the [Apex documentation](https://nvidia.github.io/apex/amp).     fp16_backend (`str`, *optional*, defaults to `""auto""`):         This argument is deprecated. Use `half_precision_backend` instead.     half_precision_backend (`str`, *optional*, defaults to `""auto""`):         The backend to use for mixed precision training. Must be one of `""auto"", ""apex"", ""cpu_amp""`. `""auto""` will         use CPU/CUDA AMP or APEX depending on the PyTorch version detected, while the other choices will force the         requested backend.     bf16_full_eval (`bool`, *optional*, defaults to `False`):         Whether to use full bfloat16 evaluation instead of 32-bit. This will be faster and save memory but can harm         metric values. This is an experimental API and it may change.     fp16_full_eval (`bool`, *optional*, defaults to `False`):         Whether to use full float16 evaluation instead of 32-bit. This will be faster and save memory but can harm         metric values.     tf32 (`bool`, *optional*):         Whether to enable the TF32 mode, available in Ampere and newer GPU architectures. The default value depends         on PyTorch's version default of `torch.backends.cuda.matmul.allow_tf32`. For more details please refer to         the [TF32](https://huggingface.co/docs/transformers/perf_train_gpu_one#tf32) documentation. This is an         experimental API and it may change.     local_rank (`int`, *optional*, defaults to -1):         Rank of the process during distributed training.     ddp_backend (`str`, *optional*):         The backend to use for distributed training. Must be one of `""nccl""`, `""mpi""`, `""ccl""`, `""gloo""`, `""hccl""`.     tpu_num_cores (`int`, *optional*):         When training on TPU, the number of TPU cores (automatically passed by launcher script).     dataloader_drop_last (`bool`, *optional*, defaults to `False`):         Whether to drop the last incomplete batch (if the length of the dataset is not divisible by the batch size)         or not.     eval_steps (`int` or `float`, *optional*):         Number of update steps between two evaluations if `eval_strategy=""steps""`. Will default to the same         value as `logging_steps` if not set. Should be an integer or a float in range `[0,1)`. If smaller than 1,         will be interpreted as ratio of total training steps.     dataloader_num_workers (`int`, *optional*, defaults to 0):         Number of subprocesses to use for data loading (PyTorch only). 0 means that the data will be loaded in the         main process.     past_index (`int`, *optional*, defaults to -1):         Some models like [TransformerXL](../model_doc/transformerxl) or [XLNet](../model_doc/xlnet) can make use of         the past hidden states for their predictions. If this argument is set to a positive int, the `Trainer` will         use the corresponding output (usually index 2) as the past state and feed it to the model at the next         training step under the keyword argument `mems`.     run_name (`str`, *optional*, defaults to `output_dir`):         A descriptor for the run. Typically used for [wandb](https://www.wandb.com/),         [mlflow](https://www.mlflow.org/), [comet](https://www.comet.com/site) and [swanlab](https://swanlab.cn)         logging. If not specified, will be the same as `output_dir`.     disable_tqdm (`bool`, *optional*):         Whether or not to disable the tqdm progress bars and table of metrics produced by         [`~notebook.NotebookTrainingTracker`] in Jupyter Notebooks. Will default to `True` if the logging level is         set to warn or lower (default), `False` otherwise.     remove_unused_columns (`bool`, *optional*, defaults to `True`):         Whether or not to automatically remove the columns unused by the model forward method.     label_names (`List[str]`, *optional*):         The list of keys in your dictionary of inputs that correspond to the labels.          Will eventually default to the list of argument names accepted by the model that contain the word ""label"",         except if the model used is one of the `XxxForQuestionAnswering` in which case it will also include the         `[""start_positions"", ""end_positions""]` keys.     load_best_model_at_end (`bool`, *optional*, defaults to `False`):         Whether or not to load the best model found during training at the end of training. When this option is         enabled, the best checkpoint will always be saved. See         [`save_total_limit`](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments.save_total_limit)         for more.          <Tip>          When set to `True`, the parameters `save_strategy` needs to be the same as `eval_strategy`, and in         the case it is ""steps"", `save_steps` must be a round multiple of `eval_steps`.          </Tip>      metric_for_best_model (`str`, *optional*):         Use in conjunction with `load_best_model_at_end` to specify the metric to use to compare two different         models. Must be the name of a metric returned by the evaluation with or without the prefix `""eval_""`.          If not specified, this will default to `""loss""` when either `load_best_model_at_end == True`         or `lr_scheduler_type == SchedulerType.REDUCE_ON_PLATEAU` (to use the evaluation loss).          If you set this value, `greater_is_better` will default to `True` unless the name ends with ""loss"".         Don't forget to set it to `False` if your metric is better when lower.     greater_is_better (`bool`, *optional*):         Use in conjunction with `load_best_model_at_end` and `metric_for_best_model` to specify if better models         should have a greater metric or not. Will default to:          - `True` if `metric_for_best_model` is set to a value that doesn't end in `""loss""`.         - `False` if `metric_for_best_model` is not set, or set to a value that ends in `""loss""`.     ignore_data_skip (`bool`, *optional*, defaults to `False`):         When resuming training, whether or not to skip the epochs and batches to get the data loading at the same         stage as in the previous training. If set to `True`, the training will begin faster (as that skipping step         can take a long time) but will not yield the same results as the interrupted training would have.     fsdp (`bool`, `str` or list of [`~trainer_utils.FSDPOption`], *optional*, defaults to `''`):         Use PyTorch Distributed Parallel Training (in distributed training only).          A list of options along the following:          - `""full_shard""`: Shard parameters, gradients and optimizer states.         - `""shard_grad_op""`: Shard optimizer states and gradients.         - `""hybrid_shard""`: Apply `FULL_SHARD` within a node, and replicate parameters across nodes.         - `""hybrid_shard_zero2""`: Apply `SHARD_GRAD_OP` within a node, and replicate parameters across nodes.         - `""offload""`: Offload parameters and gradients to CPUs (only compatible with `""full_shard""` and           `""shard_grad_op""`).         - `""auto_wrap""`: Automatically recursively wrap layers with FSDP using `default_auto_wrap_policy`.     fsdp_config (`str` or `dict`, *optional*):         Config to be used with fsdp (Pytorch Distributed Parallel Training). The value is either a location of         fsdp json config file (e.g., `fsdp_config.json`) or an already loaded json file as `dict`.          A List of config and its options:             - min_num_params (`int`, *optional*, defaults to `0`):                 FSDP's minimum number of parameters for Default Auto Wrapping. (useful only when `fsdp` field is                 passed).             - transformer_layer_cls_to_wrap (`List[str]`, *optional*):                 List of transformer layer class names (case-sensitive) to wrap, e.g, `BertLayer`, `GPTJBlock`,                 `T5Block` .... (useful only when `fsdp` flag is passed).             - backward_prefetch (`str`, *optional*)                 FSDP's backward prefetch mode. Controls when to prefetch next set of parameters (useful only when                 `fsdp` field is passed).                  A list of options along the following:                  - `""backward_pre""` : Prefetches the next set of parameters before the current set of parameter's                   gradient                     computation.                 - `""backward_post""` : This prefetches the next set of parameters after the current set of                   parameters                     gradient computation.             - forward_prefetch (`bool`, *optional*, defaults to `False`)                 FSDP's forward prefetch mode (useful only when `fsdp` field is passed).                  If `""True""`, then FSDP explicitly prefetches the next upcoming all-gather while executing in the                  forward pass.             - limit_all_gathers (`bool`, *optional*, defaults to `False`)                 FSDP's limit_all_gathers (useful only when `fsdp` field is passed).                  If `""True""`, FSDP explicitly synchronizes the CPU thread to prevent too many in-flight                  all-gathers.             - use_orig_params (`bool`, *optional*, defaults to `True`)                 If `""True""`, allows non-uniform `requires_grad` during init, which means support for interspersed                 frozen and trainable parameters. Useful in cases such as parameter-efficient fine-tuning. Please                 refer this                 [blog](https://dev-discuss.pytorch.org/t/rethinking-pytorch-fully-sharded-data-parallel-fsdp-from-first-principles/1019             - sync_module_states (`bool`, *optional*, defaults to `True`)                 If `""True""`, each individually wrapped FSDP unit will broadcast module parameters from rank 0 to                 ensure they are the same across all ranks after initialization             - cpu_ram_efficient_loading (`bool`, *optional*, defaults to `False`)                 If `""True""`, only the first process loads the pretrained model checkpoint while all other processes                 have empty weights.  When this setting as `""True""`, `sync_module_states` also must to be `""True""`,                 otherwise all the processes except the main process would have random weights leading to unexpected                 behaviour during training.             - activation_checkpointing (`bool`, *optional*, defaults to `False`):                 If `""True""`, activation checkpointing is a technique to reduce memory usage by clearing activations of                 certain layers and recomputing them during a backward pass. Effectively, this trades extra                 computation time for reduced memory usage.             - xla (`bool`, *optional*, defaults to `False`):                 Whether to use PyTorch/XLA Fully Sharded Data Parallel Training. This is an experimental feature                 and its API may evolve in the future.             - xla_fsdp_settings (`dict`, *optional*)                 The value is a dictionary which stores the XLA FSDP wrapping parameters.                  For a complete list of options, please see [here](                 https://github.com/pytorch/xla/blob/master/torch_xla/distributed/fsdp/xla_fully_sharded_data_parallel.py).             - xla_fsdp_grad_ckpt (`bool`, *optional*, defaults to `False`):                 Will use gradient checkpointing over each nested XLA FSDP wrapped layer. This setting can only be                 used when the xla flag is set to true, and an auto wrapping policy is specified through                 fsdp_min_num_params or fsdp_transformer_layer_cls_to_wrap.     deepspeed (`str` or `dict`, *optional*):         Use [Deepspeed](https://github.com/deepspeedai/DeepSpeed). This is an experimental feature and its API may         evolve in the future. The value is either the location of DeepSpeed json config file (e.g.,         `ds_config.json`) or an already loaded json file as a `dict`""          <Tip warning={true}>             If enabling any Zero-init, make sure that your model is not initialized until             *after* initializing the `TrainingArguments`, else it will not be applied.         </Tip>      accelerator_config (`str`, `dict`, or `AcceleratorConfig`, *optional*):         Config to be used with the internal `Accelerator` implementation. The value is either a location of         accelerator json config file (e.g., `accelerator_config.json`), an already loaded json file as `dict`,         or an instance of [`~trainer_pt_utils.AcceleratorConfig`].          A list of config and its options:             - split_batches (`bool`, *optional*, defaults to `False`):                 Whether or not the accelerator should split the batches yielded by the dataloaders across the devices. If                 `True` the actual batch size used will be the same on any kind of distributed processes, but it must be a                 round multiple of the `num_processes` you are using. If `False`, actual batch size used will be the one set                 in your script multiplied by the number of processes.             - dispatch_batches (`bool`, *optional*):                 If set to `True`, the dataloader prepared by the Accelerator is only iterated through on the main process                 and then the batches are split and broadcast to each process. Will default to `True` for `DataLoader` whose                 underlying dataset is an `IterableDataset`, `False` otherwise.             - even_batches (`bool`, *optional*, defaults to `True`):                 If set to `True`, in cases where the total batch size across all processes does not exactly divide the                 dataset, samples at the start of the dataset will be duplicated so the batch can be divided equally among                 all workers.             - use_seedable_sampler (`bool`, *optional*, defaults to `True`):                 Whether or not use a fully seedable random sampler ([`accelerate.data_loader.SeedableRandomSampler`]). Ensures                 training results are fully reproducible using a different sampling technique. While seed-to-seed results                 may differ, on average the differences are negligible when using multiple different seeds to compare. Should                 also be ran with [`~utils.set_seed`] for the best results.             - use_configured_state (`bool`, *optional*, defaults to `False`):                 Whether or not to use a pre-configured `AcceleratorState` or `PartialState` defined before calling `TrainingArguments`.                 If `True`, an `Accelerator` or `PartialState` must be initialized. Note that by doing so, this could lead to issues                 with hyperparameter tuning.      label_smoothing_factor (`float`, *optional*, defaults to 0.0):         The label smoothing factor to use. Zero means no label smoothing, otherwise the underlying onehot-encoded         labels are changed from 0s and 1s to `label_smoothing_factor/num_labels` and `1 - label_smoothing_factor +         label_smoothing_factor/num_labels` respectively.     debug (`str` or list of [`~debug_utils.DebugOption`], *optional*, defaults to `""""`):         Enable one or more debug features. This is an experimental feature.          Possible options are:          - `""underflow_overflow""`: detects overflow in model's input/outputs and reports the last frames that led to           the event         - `""tpu_metrics_debug""`: print debug metrics on TPU          The options should be separated by whitespaces.     optim (`str` or [`training_args.OptimizerNames`], *optional*, defaults to `""adamw_torch""`):         The optimizer to use, such as ""adamw_torch"", ""adamw_torch_fused"", ""adamw_apex_fused"", ""adamw_anyprecision"",         ""adafactor"". See `OptimizerNames` in [training_args.py](https://github.com/huggingface/transformers/blob/main/src/transformers/training_args.py)         for a full list of optimizers.     optim_args (`str`, *optional*):         Optional arguments that are supplied to optimizers such as AnyPrecisionAdamW, AdEMAMix, and GaLore.     group_by_length (`bool`, *optional*, defaults to `False`):         Whether or not to group together samples of roughly the same length in the training dataset (to minimize         padding applied and be more efficient). Only useful if applying dynamic padding.     length_column_name (`str`, *optional*, defaults to `""length""`):         Column name for precomputed lengths. If the column exists, grouping by length will use these values rather         than computing them on train startup. Ignored unless `group_by_length` is `True` and the dataset is an         instance of `Dataset`.     report_to (`str` or `List[str]`, *optional*, defaults to `""all""`):         The list of integrations to report the results and logs to. Supported platforms are `""azure_ml""`,         `""clearml""`, `""codecarbon""`, `""comet_ml""`, `""dagshub""`, `""dvclive""`, `""flyte""`, `""mlflow""`, `""neptune""`,         `""swanlab""`, `""tensorboard""`, and `""wandb""`. Use `""all""` to report to all integrations installed, `""none""`         for no integrations.     ddp_find_unused_parameters (`bool`, *optional*):         When using distributed training, the value of the flag `find_unused_parameters` passed to         `DistributedDataParallel`. Will default to `False` if gradient checkpointing is used, `True` otherwise.     ddp_bucket_cap_mb (`int`, *optional*):         When using distributed training, the value of the flag `bucket_cap_mb` passed to `DistributedDataParallel`.     ddp_broadcast_buffers (`bool`, *optional*):         When using distributed training, the value of the flag `broadcast_buffers` passed to         `DistributedDataParallel`. Will default to `False` if gradient checkpointing is used, `True` otherwise.     dataloader_pin_memory (`bool`, *optional*, defaults to `True`):         Whether you want to pin memory in data loaders or not. Will default to `True`.     dataloader_persistent_workers (`bool`, *optional*, defaults to `False`):         If True, the data loader will not shut down the worker processes after a dataset has been consumed once.         This allows to maintain the workers Dataset instances alive. Can potentially speed up training, but will         increase RAM usage. Will default to `False`.     dataloader_prefetch_factor (`int`, *optional*):         Number of batches loaded in advance by each worker.         2 means there will be a total of 2 * num_workers batches prefetched across all workers.     skip_memory_metrics (`bool`, *optional*, defaults to `True`):         Whether to skip adding of memory profiler reports to metrics. This is skipped by default because it slows         down the training and evaluation speed.     push_to_hub (`bool`, *optional*, defaults to `False`):         Whether or not to push the model to the Hub every time the model is saved. If this is activated,         `output_dir` will begin a git directory synced with the repo (determined by `hub_model_id`) and the content         will be pushed each time a save is triggered (depending on your `save_strategy`). Calling         [`~Trainer.save_model`] will also trigger a push.          <Tip warning={true}>          If `output_dir` exists, it needs to be a local clone of the repository to which the [`Trainer`] will be         pushed.          </Tip>      resume_from_checkpoint (`str`, *optional*):         The path to a folder with a valid checkpoint for your model. This argument is not directly used by         [`Trainer`], it's intended to be used by your training/evaluation scripts instead. See the [example         scripts](https://github.com/huggingface/transformers/tree/main/examples) for more details.     hub_model_id (`str`, *optional*):         The name of the repository to keep in sync with the local *output_dir*. It can be a simple model ID in         which case the model will be pushed in your namespace. Otherwise it should be the whole repository name,         for instance `""user_name/model""`, which allows you to push to an organization you are a member of with         `""organization_name/model""`. Will default to `user_name/output_dir_name` with *output_dir_name* being the         name of `output_dir`.          Will default to the name of `output_dir`.     hub_strategy (`str` or [`~trainer_utils.HubStrategy`], *optional*, defaults to `""every_save""`):         Defines the scope of what is pushed to the Hub and when. Possible values are:          - `""end""`: push the model, its configuration, the processing class e.g. tokenizer (if passed along to the [`Trainer`]) and a           draft of a model card when the [`~Trainer.save_model`] method is called.         - `""every_save""`: push the model, its configuration, the processing class e.g. tokenizer (if passed along to the [`Trainer`]) and           a draft of a model card each time there is a model save. The pushes are asynchronous to not block           training, and in case the save are very frequent, a new push is only attempted if the previous one is           finished. A last push is made with the final model at the end of training.         - `""checkpoint""`: like `""every_save""` but the latest checkpoint is also pushed in a subfolder named           last-checkpoint, allowing you to resume training easily with           `trainer.train(resume_from_checkpoint=""last-checkpoint"")`.         - `""all_checkpoints""`: like `""checkpoint""` but all checkpoints are pushed like they appear in the output           folder (so you will get one checkpoint folder per folder in your final repository)      hub_token (`str`, *optional*):         The token to use to push the model to the Hub. Will default to the token in the cache folder obtained with         `huggingface-cli login`.     hub_private_repo (`bool`, *optional*):         Whether to make the repo private. If `None` (default), the repo will be public unless the organization's default is private. This value is ignored if the repo already exists.     hub_always_push (`bool`, *optional*, defaults to `False`):         Unless this is `True`, the `Trainer` will skip pushing a checkpoint when the previous push is not finished.     gradient_checkpointing (`bool`, *optional*, defaults to `False`):         If True, use gradient checkpointing to save memory at the expense of slower backward pass.     gradient_checkpointing_kwargs (`dict`, *optional*, defaults to `None`):         Key word arguments to be passed to the `gradient_checkpointing_enable` method.     include_inputs_for_metrics (`bool`, *optional*, defaults to `False`):         This argument is deprecated. Use `include_for_metrics` instead, e.g, `include_for_metrics = [""inputs""]`.     include_for_metrics (`List[str]`, *optional*, defaults to `[]`):         Include additional data in the `compute_metrics` function if needed for metrics computation.         Possible options to add to `include_for_metrics` list:         - `""inputs""`: Input data passed to the model, intended for calculating input dependent metrics.         - `""loss""`: Loss values computed during evaluation, intended for calculating loss dependent metrics.     eval_do_concat_batches (`bool`, *optional*, defaults to `True`):         Whether to recursively concat inputs/losses/labels/predictions across batches. If `False`,         will instead store them as lists, with each batch kept separate.     auto_find_batch_size (`bool`, *optional*, defaults to `False`)         Whether to find a batch size that will fit into memory automatically through exponential decay, avoiding         CUDA Out-of-Memory errors. Requires accelerate to be installed (`pip install accelerate`)     full_determinism (`bool`, *optional*, defaults to `False`)         If `True`, [`enable_full_determinism`] is called instead of [`set_seed`] to ensure reproducible results in         distributed training. Important: this will negatively impact the performance, so only use it for debugging.     torchdynamo (`str`, *optional*):         If set, the backend compiler for TorchDynamo. Possible choices are `""eager""`, `""aot_eager""`, `""inductor""`,         `""nvfuser""`, `""aot_nvfuser""`, `""aot_cudagraphs""`, `""ofi""`, `""fx2trt""`, `""onnxrt""` and `""ipex""`.     ray_scope (`str`, *optional*, defaults to `""last""`):         The scope to use when doing hyperparameter search with Ray. By default, `""last""` will be used. Ray will         then use the last checkpoint of all trials, compare those, and select the best one. However, other options         are also available. See the [Ray documentation](         https://docs.ray.io/en/latest/tune/api_docs/analysis.html#ray.tune.ExperimentAnalysis.get_best_trial) for         more options.     ddp_timeout (`int`, *optional*, defaults to 1800):         The timeout for `torch.distributed.init_process_group` calls, used to avoid GPU socket timeouts when         performing slow operations in distributed runnings. Please refer the [PyTorch documentation]         (https://pytorch.org/docs/stable/distributed.html#torch.distributed.init_process_group) for more         information.     use_mps_device (`bool`, *optional*, defaults to `False`):         This argument is deprecated.`mps` device will be used if it is available similar to `cuda` device.     torch_compile (`bool`, *optional*, defaults to `False`):         Whether or not to compile the model using PyTorch 2.0         [`torch.compile`](https://pytorch.org/get-started/pytorch-2.0/).          This will use the best defaults for the [`torch.compile`         API](https://pytorch.org/docs/stable/generated/torch.compile.html?highlight=torch+compile#torch.compile).         You can customize the defaults with the argument `torch_compile_backend` and `torch_compile_mode` but we         don't guarantee any of them will work as the support is progressively rolled in in PyTorch.          This flag and the whole compile API is experimental and subject to change in future releases.     torch_compile_backend (`str`, *optional*):         The backend to use in `torch.compile`. If set to any value, `torch_compile` will be set to `True`.          Refer to the PyTorch doc for possible values and note that they may change across PyTorch versions.          This flag is experimental and subject to change in future releases.     torch_compile_mode (`str`, *optional*):         The mode to use in `torch.compile`. If set to any value, `torch_compile` will be set to `True`.          Refer to the PyTorch doc for possible values and note that they may change across PyTorch versions.          This flag is experimental and subject to change in future releases.     include_tokens_per_second (`bool`, *optional*):         Whether or not to compute the number of tokens per second per device for training speed metrics.          This will iterate over the entire training dataloader once beforehand,          and will slow down the entire process.      include_num_input_tokens_seen (`bool`, *optional*):         Whether or not to track the number of input tokens seen throughout training.          May be slower in distributed training as gather operations must be called.      neftune_noise_alpha (`Optional[float]`):         If not `None`, this will activate NEFTune noise embeddings. This can drastically improve model performance         for instruction fine-tuning. Check out the [original paper](https://arxiv.org/abs/2310.05914) and the         [original code](https://github.com/neelsjain/NEFTune). Support transformers `PreTrainedModel` and also         `PeftModel` from peft. The original paper used values in the range [5.0, 15.0].     optim_target_modules (`Union[str, List[str]]`, *optional*):         The target modules to optimize, i.e. the module names that you would like to train.         Currently used for the GaLore algorithm (https://arxiv.org/abs/2403.03507) and APOLLO algorithm (https://arxiv.org/abs/2412.05270).         See GaLore implementation (https://github.com/jiaweizzhao/GaLore) and APOLLO implementation (https://github.com/zhuhanqing/APOLLO) for more details.         You need to make sure to pass a valid GaLore or APOLLO optimizer, e.g., one of: ""apollo_adamw"", ""galore_adamw"", ""galore_adamw_8bit"", ""galore_adafactor"" and make sure that the target modules are `nn.Linear` modules only.      batch_eval_metrics (`Optional[bool]`, defaults to `False`):         If set to `True`, evaluation will call compute_metrics at the end of each batch to accumulate statistics         rather than saving all eval logits in memory. When set to `True`, you must pass a compute_metrics function         that takes a boolean argument `compute_result`, which when passed `True`, will trigger the final global         summary statistics from the batch-level summary statistics you've accumulated over the evaluation set.      eval_on_start (`bool`, *optional*, defaults to `False`):         Whether to perform a evaluation step (sanity check) before the training to ensure the validation steps works correctly.      eval_use_gather_object (`bool`, *optional*, defaults to `False`):         Whether to run recursively gather object in a nested list/tuple/dictionary of objects from all devices. This should only be enabled if users are not just returning tensors, and this is actively discouraged by PyTorch.      use_liger_kernel (`bool`, *optional*, defaults to `False`):         Whether enable [Liger](https://github.com/linkedin/Liger-Kernel) Kernel for LLM model training.         It can effectively increase multi-GPU training throughput by ~20% and reduces memory usage by ~60%, works out of the box with         flash attention, PyTorch FSDP, and Microsoft DeepSpeed. Currently, it supports llama, mistral, mixtral and gemma models.      average_tokens_across_devices (`bool`, *optional*, defaults to `False`):         Whether or not to average tokens across devices. If enabled, will use all_reduce to synchronize         num_tokens_in_batch for precise loss calculation. Reference:         https://github.com/huggingface/transformers/issues/34242"
Trainer,"Trainer is a simple but feature-complete training and eval loop for PyTorch, optimized for  Transformers.  Args:     model ([`PreTrainedModel`] or `torch.nn.Module`, *optional*):         The model to train, evaluate or use for predictions. If not provided, a `model_init` must be passed.          <Tip>          [`Trainer`] is optimized to work with the [`PreTrainedModel`] provided by the library. You can still use         your own models defined as `torch.nn.Module` as long as they work the same way as the  Transformers         models.          </Tip>      args ([`TrainingArguments`], *optional*):         The arguments to tweak for training. Will default to a basic instance of [`TrainingArguments`] with the         `output_dir` set to a directory named *tmp_trainer* in the current directory if not provided.     data_collator (`DataCollator`, *optional*):         The function to use to form a batch from a list of elements of `train_dataset` or `eval_dataset`. Will         default to [`default_data_collator`] if no `processing_class` is provided, an instance of         [`DataCollatorWithPadding`] otherwise if the processing_class is a feature extractor or tokenizer.     train_dataset (Union[`torch.utils.data.Dataset`, `torch.utils.data.IterableDataset`, `datasets.Dataset`], *optional*):         The dataset to use for training. If it is a [`~datasets.Dataset`], columns not accepted by the         `model.forward()` method are automatically removed.          Note that if it's a `torch.utils.data.IterableDataset` with some randomization and you are training in a         distributed fashion, your iterable dataset should either use a internal attribute `generator` that is a         `torch.Generator` for the randomization that must be identical on all processes (and the Trainer will         manually set the seed of this `generator` at each epoch) or have a `set_epoch()` method that internally         sets the seed of the RNGs used.     eval_dataset (Union[`torch.utils.data.Dataset`, Dict[str, `torch.utils.data.Dataset`, `datasets.Dataset`]), *optional*):          The dataset to use for evaluation. If it is a [`~datasets.Dataset`], columns not accepted by the          `model.forward()` method are automatically removed. If it is a dictionary, it will evaluate on each          dataset prepending the dictionary key to the metric name.     processing_class (`PreTrainedTokenizerBase` or `BaseImageProcessor` or `FeatureExtractionMixin` or `ProcessorMixin`, *optional*):         Processing class used to process the data. If provided, will be used to automatically process the inputs         for the model, and it will be saved along the model to make it easier to rerun an interrupted training or         reuse the fine-tuned model.         This supersedes the `tokenizer` argument, which is now deprecated.     model_init (`Callable[[], PreTrainedModel]`, *optional*):         A function that instantiates the model to be used. If provided, each call to [`~Trainer.train`] will start         from a new instance of the model as given by this function.          The function may have zero argument, or a single one containing the optuna/Ray Tune/SigOpt trial object, to         be able to choose different architectures according to hyper parameters (such as layer count, sizes of         inner layers, dropout probabilities etc).     compute_loss_func (`Callable`, *optional*):         A function that accepts the raw model outputs, labels, and the number of items in the entire accumulated         batch (batch_size * gradient_accumulation_steps) and returns the loss. For example, see the default [loss function](https://github.com/huggingface/transformers/blob/052e652d6d53c2b26ffde87e039b723949a53493/src/transformers/trainer.py#L3618) used by [`Trainer`].     compute_metrics (`Callable[[EvalPrediction], Dict]`, *optional*):         The function that will be used to compute metrics at evaluation. Must take a [`EvalPrediction`] and return         a dictionary string to metric values. *Note* When passing TrainingArgs with `batch_eval_metrics` set to         `True`, your compute_metrics function must take a boolean `compute_result` argument. This will be triggered         after the last eval batch to signal that the function needs to calculate and return the global summary         statistics rather than accumulating the batch-level statistics     callbacks (List of [`TrainerCallback`], *optional*):         A list of callbacks to customize the training loop. Will add those to the list of default callbacks         detailed in [here](callback).          If you want to remove one of the default callbacks used, use the [`Trainer.remove_callback`] method.     optimizers (`Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR]`, *optional*, defaults to `(None, None)`):         A tuple containing the optimizer and the scheduler to use. Will default to an instance of [`AdamW`] on your         model and a scheduler given by [`get_linear_schedule_with_warmup`] controlled by `args`.     optimizer_cls_and_kwargs (`Tuple[Type[torch.optim.Optimizer], Dict[str, Any]]`, *optional*):         A tuple containing the optimizer class and keyword arguments to use.         Overrides `optim` and `optim_args` in `args`. Incompatible with the `optimizers` argument.          Unlike `optimizers`, this argument avoids the need to place model parameters on the correct devices before initializing the Trainer.     preprocess_logits_for_metrics (`Callable[[torch.Tensor, torch.Tensor], torch.Tensor]`, *optional*):         A function that preprocess the logits right before caching them at each evaluation step. Must take two         tensors, the logits and the labels, and return the logits once processed as desired. The modifications made         by this function will be reflected in the predictions received by `compute_metrics`.          Note that the labels (second parameter) will be `None` if the dataset does not have them.  Important attributes:      - **model** -- Always points to the core model. If using a transformers model, it will be a [`PreTrainedModel`]       subclass.     - **model_wrapped** -- Always points to the most external model in case one or more other modules wrap the       original model. This is the model that should be used for the forward pass. For example, under `DeepSpeed`,       the inner model is wrapped in `DeepSpeed` and then again in `torch.nn.DistributedDataParallel`. If the inner       model hasn't been wrapped, then `self.model_wrapped` is the same as `self.model`.     - **is_model_parallel** -- Whether or not a model has been switched to a model parallel mode (different from       data parallelism, this means some of the model layers are split on different GPUs).     - **place_model_on_device** -- Whether or not to automatically place the model on the device - it will be set       to `False` if model parallel or deepspeed is used, or if the default       `TrainingArguments.place_model_on_device` is overridden to return `False` .     - **is_in_train** -- Whether or not a model is currently running `train` (e.g. when `evaluate` is called while       in `train`)"
notebook_login,"Displays a widget to log in to the HF website and store the token.  This is equivalent to [`login`] without passing a token when run in a notebook. [`notebook_login`] is useful if you want to force the use of the notebook widget instead of a prompt in the terminal.  For more details, see [`login`].  Args:     new_session (`bool`, defaults to `True`):         If `True`, will request a token even if one is already saved on the machine.     write_permission (`bool`):         Ignored and deprecated argument."
load_dataset,"Load a dataset from the Hugging Face Hub, or a local dataset.  You can find the list of datasets on the [Hub](https://huggingface.co/datasets) or with [`huggingface_hub.list_datasets`].  A dataset is a directory that contains:  - some data files in generic formats (JSON, CSV, Parquet, text, etc.). - and optionally a dataset script, if it requires some code to read the data files. This is used to load any kind of formats or structures.  Note that dataset scripts can also download and read data files from anywhere - in case your data files already exist online.  This function does the following under the hood:      1. Download and import in the library the dataset script from `path` if it's not already cached inside the library.          If the dataset has no dataset script, then a generic dataset script is imported instead (JSON, CSV, Parquet, text, etc.)          Dataset scripts are small python scripts that define dataset builders. They define the citation, info and format of the dataset,         contain the path or URL to the original data files and the code to load examples from the original data files.          You can find the complete list of datasets in the Datasets [Hub](https://huggingface.co/datasets).      2. Run the dataset script which will:          * Download the dataset file from the original URL (see the script) if it's not already available locally or cached.         * Process and cache the dataset in typed Arrow tables for caching.              Arrow table are arbitrarily long, typed tables which can store nested objects and be mapped to numpy/pandas/python generic types.             They can be directly accessed from disk, loaded in RAM or even streamed over the web.      3. Return a dataset built from the requested splits in `split` (default: all).  It also allows to load a dataset from a local directory or a dataset repository on the Hugging Face Hub without dataset script. In this case, it automatically loads all the data files from the directory or the dataset repository.  Args:      path (`str`):         Path or name of the dataset.         Depending on `path`, the dataset builder that is used comes from a generic dataset script (JSON, CSV, Parquet, text etc.) or from the dataset script (a python file) inside the dataset directory.          For local datasets:          - if `path` is a local directory (containing data files only)           -> load a generic dataset builder (csv, json, text etc.) based on the content of the directory           e.g. `'./path/to/directory/with/my/csv/data'`.         - if `path` is a local dataset script or a directory containing a local dataset script (if the script has the same name as the directory)           -> load the dataset builder from the dataset script           e.g. `'./dataset/squad'` or `'./dataset/squad/squad.py'`.          For datasets on the Hugging Face Hub (list all available datasets with [`huggingface_hub.list_datasets`])          - if `path` is a dataset repository on the HF hub (containing data files only)           -> load a generic dataset builder (csv, text etc.) based on the content of the repository           e.g. `'username/dataset_name'`, a dataset repository on the HF hub containing your data files.         - if `path` is a dataset repository on the HF hub with a dataset script (if the script has the same name as the directory)           -> load the dataset builder from the dataset script in the dataset repository           e.g. `glue`, `squad`, `'username/dataset_name'`, a dataset repository on the HF hub containing a dataset script `'dataset_name.py'`.      name (`str`, *optional*):         Defining the name of the dataset configuration.     data_dir (`str`, *optional*):         Defining the `data_dir` of the dataset configuration. If specified for the generic builders (csv, text etc.) or the Hub datasets and `data_files` is `None`,         the behavior is equal to passing `os.path.join(data_dir, **)` as `data_files` to reference all the files in a directory.     data_files (`str` or `Sequence` or `Mapping`, *optional*):         Path(s) to source data file(s).     split (`Split` or `str`):         Which split of the data to load.         If `None`, will return a `dict` with all splits (typically `datasets.Split.TRAIN` and `datasets.Split.TEST`).         If given, will return a single Dataset.         Splits can be combined and specified like in tensorflow-datasets.     cache_dir (`str`, *optional*):         Directory to read/write data. Defaults to `""~/.cache/huggingface/datasets""`.     features (`Features`, *optional*):         Set the features type to use for this dataset.     download_config ([`DownloadConfig`], *optional*):         Specific download configuration parameters.     download_mode ([`DownloadMode`] or `str`, defaults to `REUSE_DATASET_IF_EXISTS`):         Download/generate mode.     verification_mode ([`VerificationMode`] or `str`, defaults to `BASIC_CHECKS`):         Verification mode determining the checks to run on the downloaded/processed dataset information (checksums/size/splits/...).          <Added version=""2.9.1""/>     ignore_verifications (`bool`, defaults to `False`):         Ignore the verifications of the downloaded/processed dataset information (checksums/size/splits/...).          <Deprecated version=""2.9.1"">          `ignore_verifications` was deprecated in version 2.9.1 and will be removed in 3.0.0.         Please use `verification_mode` instead.          </Deprecated>     keep_in_memory (`bool`, defaults to `None`):         Whether to copy the dataset in-memory. If `None`, the dataset         will not be copied in-memory unless explicitly enabled by setting `datasets.config.IN_MEMORY_MAX_SIZE` to         nonzero. See more details in the [improve performance](../cache#improve-performance) section.     save_infos (`bool`, defaults to `False`):         Save the dataset information (checksums/size/splits/...).     revision ([`Version`] or `str`, *optional*):         Version of the dataset script to load.         As datasets have their own git repository on the Datasets Hub, the default version ""main"" corresponds to their ""main"" branch.         You can specify a different version than the default ""main"" by using a commit SHA or a git tag of the dataset repository.     token (`str` or `bool`, *optional*):         Optional string or boolean to use as Bearer token for remote files on the Datasets Hub.         If `True`, or not specified, will get token from `""~/.huggingface""`.     use_auth_token (`str` or `bool`, *optional*):         Optional string or boolean to use as Bearer token for remote files on the Datasets Hub.         If `True`, or not specified, will get token from `""~/.huggingface""`.          <Deprecated version=""2.14.0"">          `use_auth_token` was deprecated in favor of `token` in version 2.14.0 and will be removed in 3.0.0.          </Deprecated>     task (`str`):         The task to prepare the dataset for during training and evaluation. Casts the dataset's [`Features`] to standardized column names and types as detailed in `datasets.tasks`.          <Deprecated version=""2.13.0"">          `task` was deprecated in version 2.13.0 and will be removed in 3.0.0.          </Deprecated>     streaming (`bool`, defaults to `False`):         If set to `True`, don't download the data files. Instead, it streams the data progressively while         iterating on the dataset. An [`IterableDataset`] or [`IterableDatasetDict`] is returned instead in this case.          Note that streaming works for datasets that use data formats that support being iterated over like txt, csv, jsonl for example.         Json files may be downloaded completely. Also streaming from remote zip or gzip files is supported but other compressed formats         like rar and xz are not yet supported. The tgz format doesn't allow streaming.     num_proc (`int`, *optional*, defaults to `None`):         Number of processes when downloading and generating the dataset locally.         Multiprocessing is disabled by default.          <Added version=""2.7.0""/>     storage_options (`dict`, *optional*, defaults to `None`):         **Experimental**. Key/value pairs to be passed on to the dataset file-system backend, if any.          <Added version=""2.11.0""/>     **config_kwargs (additional keyword arguments):         Keyword arguments to be passed to the `BuilderConfig`         and used in the [`DatasetBuilder`].  Returns:     [`Dataset`] or [`DatasetDict`]:     - if `split` is not `None`: the dataset requested,     - if `split` is `None`, a [`~datasets.DatasetDict`] with each split.      or [`IterableDataset`] or [`IterableDatasetDict`]: if `streaming=True`      - if `split` is not `None`, the dataset is requested     - if `split` is `None`, a [`~datasets.streaming.IterableDatasetDict`] with each split.  Example:  Load a dataset from the Hugging Face Hub:  ```py >>> from datasets import load_dataset >>> ds = load_dataset('rotten_tomatoes', split='train')  # Map data files to splits >>> data_files = {'train': 'train.csv', 'test': 'test.csv'} >>> ds = load_dataset('namespace/your_dataset_name', data_files=data_files) ```  Load a local dataset:  ```py # Load a CSV file >>> from datasets import load_dataset >>> ds = load_dataset('csv', data_files='path/to/local/my_dataset.csv')  # Load a JSON file >>> from datasets import load_dataset >>> ds = load_dataset('json', data_files='path/to/local/my_dataset.json')  # Load from a local loading script >>> from datasets import load_dataset >>> ds = load_dataset('path/to/local/loading_script/loading_script.py', split='train') ```  Load an [`~datasets.IterableDataset`]:  ```py >>> from datasets import load_dataset >>> ds = load_dataset('rotten_tomatoes', split='train', streaming=True) ```  Load an image dataset with the `ImageFolder` dataset builder:  ```py >>> from datasets import load_dataset >>> ds = load_dataset('imagefolder', data_dir='/path/to/images', split='train') ```"
