{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11773,"status":"ok","timestamp":1749072913451,"user":{"displayName":"Neeharika Gupta","userId":"13434636909198909500"},"user_tz":420},"id":"DJWlP8rrbIuz","outputId":"30922d80-8525-47ed-ef14-1264522f2b9f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: google-generativeai in /usr/local/lib/python3.11/dist-packages (0.8.5)\n","Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (0.6.15)\n","Requirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.24.2)\n","Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.170.0)\n","Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.38.0)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (5.29.5)\n","Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.11.5)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.67.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.13.2)\n","Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n","Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (1.70.0)\n","Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (2.32.3)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n","Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n","Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n","Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (4.1.1)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.4.1)\n","Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.0)\n","Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.0)\n","Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.3)\n","Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2025.4.26)\n"]}],"source":["!pip install google-generativeai"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BoTHSQodcv01"},"outputs":[],"source":["import os\n","import google.generativeai as genai\n","import pandas as pd\n","import time\n","from google.api_core.exceptions import TooManyRequests"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HwiFdq_kc5Yc"},"outputs":[],"source":["api_key = \"\"\n","genai.configure(api_key=api_key)\n","model = genai.GenerativeModel('models/gemini-2.5-flash-preview-04-17')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qygU4u92IEpW"},"outputs":[],"source":["REQUESTS_PER_MINUTE = 10\n","DELAY_BETWEEN_REQUESTS = 60 / REQUESTS_PER_MINUTE\n","\n","def make_request(prompt, model, model_name=model):\n","    response = model.generate_content(prompt)\n","    return response.text"]},{"cell_type":"markdown","metadata":{"id":"17H6y--W2UYV"},"source":["##Direct and COT prompting"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":318},"executionInfo":{"elapsed":8712,"status":"ok","timestamp":1748732248463,"user":{"displayName":"Neeharika Gupta","userId":"13434636909198909500"},"user_tz":420},"id":"-Bpi8HeMeU0K","outputId":"e42413a4-2ab2-4a28-beb8-838e3e10960b"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"summary":"{\n  \"name\": \"df\",\n  \"rows\": 8,\n  \"fields\": [\n    {\n      \"column\": \"Task\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"Write a Python script using PyTorch to train a convolutional neural network (CNN) on the Cifar 10 dataset. Include data loading, model definition, training loop, and evaluation\",\n          \"Write a PyTorch class that subclasses torch.utils.data.Dataset to load image files and labels from a folder. Use this class with a DataLoader for batching. Use FashionMNIST dataset.\",\n          \"Write a Python function using TensorFlow to create and compile a simple linear regression model with one dense layer. Train it on the classic California housing dataset.\\n\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"direct prompt\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"Write a Python script using PyTorch to train a convolutional neural network (CNN) on the Cifar 10 dataset. Include data loading, model definition, training loop, and evaluation. Just write the code, no explanations necessary.\",\n          \"Write a PyTorch class that subclasses torch.utils.data.Dataset to load image files and labels from a folder. Use this class with a DataLoader for batching. Use FashionMNIST dataset. Just write the code, no explanations necessary.\",\n          \"Write a Python function using TensorFlow to create and compile a simple linear regression model with one dense layer. Train it on the classic California housing dataset. Just write the code, no explanations necessary.\\n\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cot\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"Write a Python script using PyTorch to train a convolutional neural network (CNN) on the Cifar 10 dataset. Include data loading, model definition, training loop, and evaluation. Please explain your reasoning step-by-step while generating code.\",\n          \"Write a PyTorch class that subclasses torch.utils.data.Dataset to load image files and labels from a folder. Use this class with a DataLoader for batching. Use FashionMNIST dataset. Please explain your reasoning step-by-step while generating code.\",\n          \"Write a Python function using TensorFlow to create and compile a simple linear regression model with one dense layer. Train it on the classic California housing dataset. Please explain your reasoning step-by-step while generating code.\\n\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}","type":"dataframe","variable_name":"df"},"text/html":["\n","  <div id=\"df-882d9d80-7e60-491d-9c6d-6c5e370f0bca\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Task</th>\n","      <th>direct prompt</th>\n","      <th>cot</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Write a Python function using TensorFlow to cr...</td>\n","      <td>Write a Python function using TensorFlow to cr...</td>\n","      <td>Write a Python function using TensorFlow to cr...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Write a Python script using PyTorch to train a...</td>\n","      <td>Write a Python script using PyTorch to train a...</td>\n","      <td>Write a Python script using PyTorch to train a...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Create a scikit-learn pipeline that generates ...</td>\n","      <td>Create a scikit-learn pipeline that generates ...</td>\n","      <td>Create a scikit-learn pipeline that generates ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Write a Python function that uses Hugging Face...</td>\n","      <td>Write a Python function that uses Hugging Face...</td>\n","      <td>Write a Python function that uses Hugging Face...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Build a TensorFlow model for image classificat...</td>\n","      <td>Build a TensorFlow model for image classificat...</td>\n","      <td>Build a TensorFlow model for image classificat...</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>Write a PyTorch class that subclasses torch.ut...</td>\n","      <td>Write a PyTorch class that subclasses torch.ut...</td>\n","      <td>Write a PyTorch class that subclasses torch.ut...</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>Write a Python script that loads the Iris data...</td>\n","      <td>Write a Python script that loads the Iris data...</td>\n","      <td>Write a Python script that loads the Iris data...</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>Write a Java program using the Hadoop MapReduc...</td>\n","      <td>Write a Java program using the Hadoop MapReduc...</td>\n","      <td>Write a Java program using the Hadoop MapReduc...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-882d9d80-7e60-491d-9c6d-6c5e370f0bca')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-882d9d80-7e60-491d-9c6d-6c5e370f0bca button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-882d9d80-7e60-491d-9c6d-6c5e370f0bca');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    <div id=\"df-74dbef7d-f85c-491e-a7dc-3a93f0648be1\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-74dbef7d-f85c-491e-a7dc-3a93f0648be1')\"\n","                title=\"Suggest charts\"\n","                style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","      <script>\n","        async function quickchart(key) {\n","          const quickchartButtonEl =\n","            document.querySelector('#' + key + ' button');\n","          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","          quickchartButtonEl.classList.add('colab-df-spinner');\n","          try {\n","            const charts = await google.colab.kernel.invokeFunction(\n","                'suggestCharts', [key], {});\n","          } catch (error) {\n","            console.error('Error during call to suggestCharts:', error);\n","          }\n","          quickchartButtonEl.classList.remove('colab-df-spinner');\n","          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","        }\n","        (() => {\n","          let quickchartButtonEl =\n","            document.querySelector('#df-74dbef7d-f85c-491e-a7dc-3a93f0648be1 button');\n","          quickchartButtonEl.style.display =\n","            google.colab.kernel.accessAllowed ? 'block' : 'none';\n","        })();\n","      </script>\n","    </div>\n","\n","  <div id=\"id_45784e74-1936-47db-8993-e86c026be1bb\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_45784e74-1936-47db-8993-e86c026be1bb button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('df');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"],"text/plain":["                                                Task  \\\n","0  Write a Python function using TensorFlow to cr...   \n","1  Write a Python script using PyTorch to train a...   \n","2  Create a scikit-learn pipeline that generates ...   \n","3  Write a Python function that uses Hugging Face...   \n","4  Build a TensorFlow model for image classificat...   \n","5  Write a PyTorch class that subclasses torch.ut...   \n","6  Write a Python script that loads the Iris data...   \n","7  Write a Java program using the Hadoop MapReduc...   \n","\n","                                       direct prompt  \\\n","0  Write a Python function using TensorFlow to cr...   \n","1  Write a Python script using PyTorch to train a...   \n","2  Create a scikit-learn pipeline that generates ...   \n","3  Write a Python function that uses Hugging Face...   \n","4  Build a TensorFlow model for image classificat...   \n","5  Write a PyTorch class that subclasses torch.ut...   \n","6  Write a Python script that loads the Iris data...   \n","7  Write a Java program using the Hadoop MapReduc...   \n","\n","                                                 cot  \n","0  Write a Python function using TensorFlow to cr...  \n","1  Write a Python script using PyTorch to train a...  \n","2  Create a scikit-learn pipeline that generates ...  \n","3  Write a Python function that uses Hugging Face...  \n","4  Build a TensorFlow model for image classificat...  \n","5  Write a PyTorch class that subclasses torch.ut...  \n","6  Write a Python script that loads the Iris data...  \n","7  Write a Java program using the Hadoop MapReduc...  "]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["df = pd.read_csv(\"/content/drive/MyDrive/Stanford NLP/prompts-step2 - prompts.csv\")\n","df"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":415445,"status":"ok","timestamp":1748732663929,"user":{"displayName":"Neeharika Gupta","userId":"13434636909198909500"},"user_tz":420},"id":"ukUYaIrbc9Fy","outputId":"513786bd-47cf-4687-de20-7b251adab9e2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Processing row 1/8...\n","```python\n","import tensorflow as tf\n","import numpy as np\n","from sklearn.datasets import fetch_california_housing\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","\n","def train_california_housing_linear_model():\n","    # Load the dataset\n","    housing = fetch_california_housing()\n","    X, y = housing.data, housing.target\n","\n","    # Preprocess the data (scaling)\n","    scaler = StandardScaler()\n","    X_scaled = scaler.fit_transform(X)\n","    y_scaled = y # For simplicity, we won't scale the target here, model will learn direct mapping\n","\n","    # Split data (optional but good practice)\n","    # X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=42)\n","    # Using all data for training as requested implicitly\n","\n","    # Define the model\n","    model = tf.keras.Sequential([\n","        tf.keras.layers.Dense(units=1, input_shape=(X_scaled.shape[1],))\n","    ])\n","\n","    # Compile the model\n","    model.compile(optimizer='adam', loss='mean_squared_error')\n","\n","    # Train the model\n","    model.fit(X_scaled, y_scaled, epochs=50, batch_size=32, verbose=0)\n","\n","    return model\n","\n","if __name__ == '__main__':\n","    # Example usage:\n","    trained_model = train_california_housing_linear_model()\n","    print(\"Model training complete.\")\n","    # You can now use the trained_model for predictions\n","    # For example: trained_model.predict(some_scaled_data)\n","```\n","Okay, let's create a Python function using TensorFlow to train a simple linear regression model on the California Housing dataset. We'll go through the steps: loading data, preparing it, defining the model, compiling it, and training it.\n","\n","**Step-by-step Reasoning and Code Generation:**\n","\n","1.  **Import necessary libraries:** We need TensorFlow for the model, `tensorflow_datasets` to load the dataset easily, and scikit-learn for data splitting and scaling (common preprocessing steps).\n","\n","    ```python\n","    import tensorflow as tf\n","    import tensorflow_datasets as tfds\n","    from sklearn.model_selection import train_test_split\n","    from sklearn.preprocessing import StandardScaler\n","    import numpy as np\n","    ```\n","    *   `tensorflow`: The core library.\n","    *   `tensorflow_datasets`: Provides convenient access to many standard datasets like California Housing.\n","    *   `train_test_split`: Essential for dividing our data into a set for training the model and a separate set for evaluating it. This prevents overfitting.\n","    *   `StandardScaler`: A standard preprocessing step for numerical data. It scales features to have zero mean and unit variance, which often helps optimization algorithms (like the one used in training) converge faster and perform better.\n","    *   `numpy`: Useful for handling numerical arrays, which the data will be converted into.\n","\n","2.  **Define the function:** We'll put all the logic inside a function for reusability.\n","\n","    ```python\n","    def train_linear_regression_california_housing(epochs=50, batch_size=32):\n","        # ... rest of the code will go here\n","        pass # Placeholder\n","    ```\n","    *   The function is named descriptively.\n","    *   We add parameters for `epochs` and `batch_size` as these are common hyperparameters for training. Default values are provided.\n","\n","3.  **Load the dataset:** We use `tensorflow_datasets` to load the California Housing dataset. We'll load the 'train' split initially and then manually split it further using scikit-learn, which is often more straightforward for applying preprocessing like scaling.\n","\n","    ```python\n","        # 1. Load the dataset\n","        print(\"Loading California Housing dataset...\")\n","        ds = tfds.load('california_housing', split='train', as_supervised=False)\n","        print(\"Dataset loaded.\")\n","    ```\n","    *   `tfds.load('california_housing', ...)`: Loads the dataset by name.\n","    *   `split='train'`: We load the official 'train' split provided by TFDS. Note that the California Housing dataset in TFDS only has a 'train' split, so we'll split this further ourselves.\n","    *   `as_supervised=False`: By default, `as_supervised=True` would return tuples of (features, label). Since the California Housing dataset has multiple features under a single key ('features') and the target ('median_house_value') is separate, `as_supervised=False` returns a dictionary for each example, which is easier to work with here.\n","\n","4.  **Prepare the data (Extract, Convert, Split, Scale):**\n","    *   Convert the `tf.data.Dataset` into NumPy arrays.\n","    *   Separate features (X) and target (y).\n","    *   Split into training and testing sets.\n","    *   Scale the features.\n","\n","    ```python\n","        # 2. Prepare the data\n","        print(\"Preparing data (Extracting, Splitting, Scaling)...\")\n","        # Convert dataset to numpy arrays\n","        data = list(ds.as_numpy_iterator())\n","        features = np.array([item['features'] for item in data])\n","        labels = np.array([item['median_house_value'] for item in data])\n","\n","        # Split data into training and testing sets\n","        # We'll use 80% for training, 20% for testing\n","        X_train, X_test, y_train, y_test = train_test_split(\n","            features, labels, test_size=0.2, random_state=42\n","        )\n","\n","        # Scale the features\n","        # Scaling is important for many optimization algorithms\n","        scaler = StandardScaler()\n","        X_train_scaled = scaler.fit_transform(X_train)\n","        X_test_scaled = scaler.transform(X_test)\n","\n","        # The target variable (house value) is also numerical,\n","        # and while not strictly necessary for basic linear regression correctness,\n","        # scaling the target can sometimes help training stability or interpretation\n","        # depending on the loss function and metrics used.\n","        # However, for standard MSE, scaling features is usually sufficient\n","        # and scaling the target requires inverse scaling for predictions.\n","        # Let's keep it simple and only scale features for this example.\n","        print(f\"Data shapes: X_train={X_train_scaled.shape}, y_train={y_train.shape}, X_test={X_test_scaled.shape}, y_test={y_test.shape}\")\n","        print(\"Data preparation complete.\")\n","\n","    ```\n","    *   `list(ds.as_numpy_iterator())`: Converts the `tf.data.Dataset` into a list of dictionaries, then `np.array([...])` converts the values from those dictionaries into NumPy arrays.\n","    *   `features = np.array([item['features'] for item in data])`: Extracts the feature arrays.\n","    *   `labels = np.array([item['median_house_value'] for item in data])`: Extracts the target variable.\n","    *   `train_test_split(...)`: Shuffles and splits the data. `test_size=0.2` means 20% goes to the test set. `random_state=42` ensures reproducibility.\n","    *   `StandardScaler()`: Creates a scaler object.\n","    *   `scaler.fit_transform(X_train)`: Calculates the mean and standard deviation *only* on the training data (`fit`) and then applies the scaling to the training data (`transform`).\n","    *   `scaler.transform(X_test)`: Applies the *same* scaling (using the mean/std from the training data) to the test data. It's crucial *not* to fit the scaler on the test data to avoid data leakage.\n","\n","5.  **Define the model:** A simple linear regression model can be represented by a single `Dense` layer with one unit and no activation function (or linear activation, which is the default).\n","\n","    ```python\n","        # 3. Define the model\n","        print(\"Defining the model...\")\n","        # A linear regression model is essentially a single neuron (dense layer with 1 unit)\n","        # with a linear activation function (which is the default).\n","        # The number of input features must match the input data shape.\n","        model = tf.keras.Sequential([\n","            tf.keras.layers.Dense(\n","                units=1,                  # One output unit for the single regression target\n","                input_shape=(X_train_scaled.shape[1],) # Input shape is the number of features\n","            )\n","        ])\n","        print(\"Model defined.\")\n","        model.summary()\n","    ```\n","    *   `tf.keras.Sequential([...])`: Creates a sequential model, which is a stack of layers.\n","    *   `tf.keras.layers.Dense(...)`: A fully connected layer.\n","    *   `units=1`: This layer has one neuron, which will produce a single output (our predicted house value).\n","    *   `input_shape=(X_train_scaled.shape[1],)`: Tells the first layer the expected shape of the input data. `X_train_scaled.shape[1]` gives the number of features. The comma is important to indicate it's a tuple representing the shape of *one* input sample (e.g., `(8,)` for 8 features).\n","    *   `model.summary()`: Prints a summary of the model's layers and parameters. For this model, the parameters will be `num_features * 1` (weights) + `1` (bias).\n","\n","6.  **Compile the model:** We need to specify the optimizer, the loss function, and any metrics to monitor.\n","\n","    ```python\n","        # 4. Compile the model\n","        print(\"Compiling the model...\")\n","        # For linear regression:\n","        # Optimizer: Adam is a common and effective choice. SGD or others could also work.\n","        # Loss: Mean Squared Error (MSE) is the standard loss function for regression.\n","        # Metrics: MSE and Mean Absolute Error (MAE) are useful for evaluation.\n","        model.compile(\n","            optimizer='adam', # Or tf.keras.optimizers.Adam()\n","            loss='mse',       # Mean Squared Error\n","            metrics=['mae', 'mse'] # Mean Absolute Error and Mean Squared Error\n","        )\n","        print(\"Model compiled.\")\n","    ```\n","    *   `model.compile(...)`: Configures the model for training.\n","    *   `optimizer='adam'`: Specifies the Adam optimization algorithm. This algorithm adapts the learning rate during training and often converges quickly.\n","    *   `loss='mse'`: Specifies Mean Squared Error as the loss function. The model will try to minimize this value during training. MSE is calculated as the average of the squared differences between the predicted and actual values.\n","    *   `metrics=['mae', 'mse']`: Specifies additional metrics to calculate and report during training and evaluation. MAE (Mean Absolute Error) is the average of the absolute differences. It's often easier to interpret than MSE as it's in the same units as the target variable. We include MSE again here to see its value directly during training.\n","\n","7.  **Train the model:** Use the `fit` method to train the model on the training data.\n","\n","    ```python\n","        # 5. Train the model\n","        print(f\"Training the model for {epochs} epochs with batch size {batch_size}...\")\n","        history = model.fit(\n","            X_train_scaled, y_train,\n","            validation_data=(X_test_scaled, y_test), # Monitor performance on the test set during training\n","            epochs=epochs,\n","            batch_size=batch_size,\n","            verbose=1 # Show training progress\n","        )\n","        print(\"Model training complete.\")\n","    ```\n","    *   `model.fit(...)`: Starts the training process.\n","    *   `X_train_scaled, y_train`: The input features and target values for training.\n","    *   `validation_data=(X_test_scaled, y_test)`: Provides data to evaluate the model on after each epoch. This helps monitor for overfitting (if validation loss starts increasing while training loss decreases).\n","    *   `epochs`: The number of times the model will iterate over the entire training dataset.\n","    *   `batch_size`: The number of samples processed before the model's weights are updated.\n","    *   `verbose=1`: Displays a progress bar and metrics for each epoch.\n","\n","8.  **Return results:** The function can return the trained model and/or the training history. The history object contains the loss and metric values recorded during training.\n","\n","    ```python\n","        # 6. Return the trained model and history\n","        return model, history\n","    ```\n","\n","9.  **Add execution block:** Include a main block to call the function and potentially print some results or evaluate the model.\n","\n","    ```python\n","    if __name__ == \"__main__\":\n","        # Set a random seed for reproducibility\n","        tf.random.set_seed(42)\n","        np.random.seed(42)\n","\n","        trained_model, training_history = train_linear_regression_california_housing(epochs=100, batch_size=64)\n","\n","        print(\"\\n--- Training Finished ---\")\n","        print(\"Trained Model Summary:\")\n","        trained_model.summary()\n","\n","        # Optional: Evaluate the model on the test set\n","        print(\"\\nEvaluating the model on the test set...\")\n","        # Need to reload/re-prepare test data for evaluation outside the function call\n","        # (or pass it back from the function, which is cleaner)\n","        # Let's modify the function to return the test data as well for evaluation\n","        # OR just use the history object which contains validation results.\n","        # History contains validation metrics per epoch.\n","        # To get final performance, we can evaluate the model directly.\n","        # Let's re-split/scale outside just for the final evaluation step demonstration.\n","\n","        # --- Re-prepare test data for final evaluation (demonstration) ---\n","        ds_eval = tfds.load('california_housing', split='train', as_supervised=False)\n","        data_eval = list(ds_eval.as_numpy_iterator())\n","        features_eval = np.array([item['features'] for item in data_eval])\n","        labels_eval = np.array([item['median_house_value'] for item in data_eval])\n","        # Use the same split and random_state as inside the function\n","        _, X_test_eval, _, y_test_eval = train_test_split(\n","            features_eval, labels_eval, test_size=0.2, random_state=42\n","        )\n","        # Use the *same* scaler fitted *only* on the training data (if scaler was returned)\n","        # Since the scaler wasn't returned, we need to refit it on training data again,\n","        # which is okay *only* for demonstrating post-training evaluation if we can't\n","        # reuse the scaler object directly. A better design might pass the scaler back.\n","        # For simplicity here, let's just fit a *new* scaler on the original *full* dataset\n","        # for evaluation features - this isn't ideal but works for demo if scaler isn't returned.\n","        # A *better* approach is to return the scaler from the function or evaluate within.\n","        # Let's stick to evaluating *within* the function before returning, or use history.\n","        # History has validation results, which is sufficient for this example.\n","        # Let's print the last validation metrics from history.\n","\n","        print(\"\\nValidation results from last epoch:\")\n","        # history.history is a dictionary containing lists of metric values per epoch\n","        last_epoch_metrics = {metric: values[-1] for metric, values in training_history.history.items()}\n","        print(last_epoch_metrics)\n","\n","        # Example prediction\n","        # Predict on the first few test samples\n","        print(\"\\nMaking predictions on the first 5 test samples:\")\n","        predictions = trained_model.predict(X_test_scaled[:5])\n","        print(\"Predicted values:\", predictions.flatten())\n","        print(\"Actual values:   \", y_test[:5])\n","\n","    ```\n","    *   `if __name__ == \"__main__\":`: Ensures this code runs only when the script is executed directly.\n","    *   `tf.random.set_seed(42)` and `np.random.seed(42)`: Sets seeds for reproducibility of TensorFlow operations and NumPy operations (like splitting).\n","    *   Calls the function.\n","    *   Prints the model summary again.\n","    *   Accesses the `history` object to print the final validation metrics.\n","    *   Demonstrates making predictions on a few samples from the scaled test set.\n","\n","10. **Assemble the final code:** Combine all the pieces into one script.\n","\n","This detailed process breaks down the problem into manageable steps: data handling, model definition, configuration, training, and evaluation, explaining the purpose of each code segment along the way.\n","\n","```python\n","import tensorflow as tf\n","import tensorflow_datasets as tfds\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","import numpy as np\n","\n","def train_linear_regression_california_housing(epochs=100, batch_size=64):\n","    \"\"\"\n","    Loads, prepares, and trains a simple linear regression model\n","    on the California Housing dataset using TensorFlow/Keras.\n","\n","    Args:\n","        epochs (int): Number of epochs to train for.\n","        batch_size (int): Batch size for training.\n","\n","    Returns:\n","        tuple: A tuple containing the trained Keras model and the training history object.\n","    \"\"\"\n","\n","    # --- Step-by-step Implementation ---\n","\n","    # 1. Load the dataset\n","    # Reasoning: We need the data to train and evaluate the model.\n","    # tfds provides easy access to standard datasets.\n","    print(\"Loading California Housing dataset...\")\n","    # Load the 'train' split. as_supervised=False gives a dict for each example.\n","    ds = tfds.load('california_housing', split='train', as_supervised=False)\n","    print(\"Dataset loaded.\")\n","\n","    # 2. Prepare the data (Extract, Convert, Split, Scale)\n","    # Reasoning:\n","    # - We need features (X) and targets (y) separately.\n","    # - Keras .fit() method typically expects numpy arrays or tf.data.Dataset.\n","    # - Splitting into train/test is crucial to evaluate model performance on unseen data.\n","    # - Scaling features helps the optimizer converge faster.\n","    print(\"Preparing data (Extracting, Splitting, Scaling)...\")\n","    # Convert dataset to numpy arrays for easier manipulation with sklearn\n","    data = list(ds.as_numpy_iterator())\n","    features = np.array([item['features'] for item in data])\n","    labels = np.array([item['median_house_value'] for item in data])\n","\n","    # Split data into training and testing sets (e.g., 80% train, 20% test)\n","    X_train, X_test, y_train, y_test = train_test_split(\n","        features, labels, test_size=0.2, random_state=42 # random_state for reproducibility\n","    )\n","\n","    # Scale the features using StandardScaler\n","    # Fit on training data, transform both training and testing data.\n","    scaler = StandardScaler()\n","    X_train_scaled = scaler.fit_transform(X_train)\n","    X_test_scaled = scaler.transform(X_test)\n","\n","    # Optional: Inspect shapes\n","    print(f\"Data shapes: X_train={X_train_scaled.shape}, y_train={y_train.shape}, X_test={X_test_scaled.shape}, y_test={y_test.shape}\")\n","    print(\"Data preparation complete.\")\n","\n","    # 3. Define the model\n","    # Reasoning:\n","    # - A simple linear regression model predicts y as a linear combination of X: y = w*X + b\n","    # - In Keras, a Dense layer with units=1 and linear activation (the default)\n","    #   performs exactly this calculation.\n","    # - Sequential model is used for a simple stack of layers.\n","    print(\"Defining the model (Simple Linear Regression with one Dense layer)...\")\n","    model = tf.keras.Sequential([\n","        # The first layer needs input_shape to know the number of features\n","        tf.keras.layers.Dense(\n","            units=1,                  # One output unit for the single regression target\n","            input_shape=(X_train_scaled.shape[1],) # Input shape is the number of features\n","        )\n","    ])\n","    print(\"Model defined.\")\n","    model.summary() # Print model architecture\n","\n","    # 4. Compile the model\n","    # Reasoning:\n","    # - Compilation configures the model for training.\n","    # - Optimizer: Algorithm to minimize the loss (e.g., Adam).\n","    # - Loss: Function to measure how far predictions are from actual values (MSE for regression).\n","    # - Metrics: Additional metrics to evaluate during training (MAE is often more interpretable than MSE).\n","    print(\"Compiling the model...\")\n","    model.compile(\n","        optimizer='adam',       # Adam optimizer is a good default\n","        loss='mse',             # Mean Squared Error, standard for regression\n","        metrics=['mae', 'mse']  # Mean Absolute Error and MSE to monitor\n","    )\n","    print(\"Model compiled.\")\n","\n","    # 5. Train the model\n","    # Reasoning:\n","    # - The .fit() method trains the model using the specified data and parameters.\n","    # - X_train_scaled, y_train: The training data.\n","    # - validation_data: Data to evaluate model performance *during* training (after each epoch).\n","    # - epochs: Number of passes over the entire training dataset.\n","    # - batch_size: Number of samples processed before a weight update.\n","    print(f\"Training the model for {epochs} epochs with batch size {batch_size}...\")\n","    history = model.fit(\n","        X_train_scaled, y_train,\n","        validation_data=(X_test_scaled, y_test), # Evaluate on test set after each epoch\n","        epochs=epochs,\n","        batch_size=batch_size,\n","        verbose=1 # Show training progress per epoch\n","    )\n","    print(\"Model training complete.\")\n","\n","    # 6. Return the trained model and history\n","    # Reasoning: The caller might want to use the model for predictions or analyze training progress.\n","    return model, history\n","\n","# --- Example Usage ---\n","if __name__ == \"__main__\":\n","    # Set seeds for reproducibility\n","    tf.random.set_seed(42)\n","    np.random.seed(42)\n","\n","    # Train the model by calling the function\n","    trained_model, training_history = train_linear_regression_california_housing(epochs=100, batch_size=64)\n","\n","    print(\"\\n--- Training Summary ---\")\n","    print(\"Trained Model Summary:\")\n","    trained_model.summary()\n","\n","    # Print final validation metrics from the training history\n","    print(\"\\nValidation results from the last epoch:\")\n","    # history.history is a dictionary containing lists of metric values per epoch\n","    last_epoch_metrics = {metric: values[-1] for metric, values in training_history.history.items()}\n","    print(last_epoch_metrics)\n","\n","    # --- Demonstrate Prediction ---\n","    # To make predictions, we need the test data and the scaler used during training.\n","    # A robust function would return the scaler, but for this simple example,\n","    # we'll just show the process assuming we have access to X_test_scaled.\n","    # In a real application, save the scaler or return it from the function.\n","\n","    # Let's reload/resplit the data just to get X_test and y_test again outside the function scope\n","    # (This is not ideal for general use, but demonstrates prediction flow)\n","    ds_eval = tfds.load('california_housing', split='train', as_supervised=False)\n","    data_eval = list(ds_eval.as_numpy_iterator())\n","    features_eval = np.array([item['features'] for item in data_eval])\n","    labels_eval = np.array([item['median_house_value'] for item in data_eval])\n","    _, X_test_raw, _, y_test_raw = train_test_split(\n","        features_eval, labels_eval, test_size=0.2, random_state=42 # Same split\n","    )\n","    # IMPORTANT: You MUST use the SAME scaler fitted on the training data!\n","    # Since we didn't return the scaler, we'd ideally reload it if saved, or refit on original training data here.\n","    # For demonstration, let's just create and fit a new scaler on the *full* data for prediction features.\n","    # ***WARNING: Fitting scaler on full data for evaluation/prediction is technically data leakage.\n","    # A proper flow returns the fitted scaler or saves/loads it.***\n","    # Re-creating the scaler fitted ONLY on original training data:\n","    temp_scaler = StandardScaler()\n","    temp_scaler.fit(train_test_split(features_eval, labels_eval, test_size=0.2, random_state=42)[0]) # Fit only on training part\n","\n","    X_test_scaled_for_pred = temp_scaler.transform(X_test_raw)\n","\n","\n","    print(\"\\nMaking predictions on the first 5 test samples:\")\n","    # Predict expects scaled input features\n","    predictions = trained_model.predict(X_test_scaled_for_pred[:5])\n","    print(\"Predicted values:\", predictions.flatten())\n","    print(\"Actual values:   \", y_test_raw[:5])\n","    # Note: Predictions are in the original scale of y because y was not scaled.\n","```\n","\n","**Explanation of Key Steps and Reasoning:**\n","\n","1.  **Import Libraries:** We gather the necessary tools from TensorFlow (for model building), TensorFlow Datasets (for easy data access), and Scikit-learn (for data splitting and scaling).\n","2.  **Load Dataset:** We use `tfds.load` to get the California Housing data. We load the 'train' split and keep it in a dictionary format (`as_supervised=False`) initially, as the features and target are separate keys.\n","3.  **Prepare Data:**\n","    *   We convert the TF dataset into NumPy arrays. This is convenient for using Scikit-learn's data preprocessing utilities.\n","    *   We separate the features (`X`) and the target (`y`). The goal of linear regression is to predict `y` from `X`.\n","    *   `train_test_split` divides the data into two subsets: one for training the model (`X_train`, `y_train`) and one for evaluating its performance (`X_test`, `y_test`). This prevents the model from simply memorizing the training data (overfitting). A `random_state` is used to ensure the split is the same every time the code runs.\n","    *   `StandardScaler` is applied to the features. Scaling brings all features to a similar range (mean 0, variance 1). This is important for gradient-based optimizers like Adam, as it can help them converge faster and more stably, especially when features have different scales. It's fitted *only* on the training data (`fit_transform`) to learn the scaling parameters, and then these parameters are applied to both training and test data (`transform`) to avoid data leakage from the test set into the training process.\n","4.  **Define Model:**\n","    *   We use `tf.keras.Sequential` because our model is a simple linear stack of layers.\n","    *   The model consists of a single `tf.keras.layers.Dense` layer.\n","    *   `units=1`: A `Dense` layer with one unit calculates a weighted sum of its inputs plus a bias term (`output = w1*x1 + w2*x2 + ... + wn*xn + b`). This is exactly the form of a simple linear regression equation.\n","    *   `input_shape=(X_train_scaled.shape[1],)`: This tells the layer how many input features to expect for each sample. `X_train_scaled.shape[1]` is the number of columns (features) in our scaled training data.\n","    *   The `activation` is by default 'linear', which is correct for linear regression (we don't want to apply a non-linear transformation to the output).\n","5.  **Compile Model:**\n","    *   `model.compile` prepares the model for the training process.\n","    *   `optimizer='adam'`: We choose the Adam optimizer. It's an efficient and widely used algorithm for updating the model's weights based on the training data and the calculated loss.\n","    *   `loss='mse'`: We select Mean Squared Error (MSE) as the loss function. This is the standard loss for regression problems. The training process aims to minimize the average of the squared differences between the model's predictions and the actual target values.\n","    *   `metrics=['mae', 'mse']`: We specify Mean Absolute Error (MAE) and MSE as metrics to monitor. Metrics are displayed during training but *not* used to optimize the model (only the loss function is used for optimization). MAE is often easier to interpret than MSE as it's in the same units as the target variable.\n","6.  **Train Model:**\n","    *   `model.fit` starts the training loop.\n","    *   `X_train_scaled` and `y_train` are the input and output data used to train the model.\n","    *   `validation_data=(X_test_scaled, y_test)` provides the test set to evaluate the model's performance after each training epoch. This helps monitor if the model is learning well or starting to overfit (validation loss increases).\n","    *   `epochs` is the number of times the training algorithm iterates through the entire training dataset.\n","    *   `batch_size` determines how many samples are processed at a time within each epoch before the model's weights are updated.\n","    *   `verbose=1` displays training progress.\n","7.  **Return Results:** The function returns the trained `model` object, which can be used for making predictions, and the `history` object, which contains the recorded loss and metric values per epoch for both training and validation sets.\n","\n","This function provides a clear and well-structured way to perform a simple linear regression task using TensorFlow on a common dataset, following standard machine learning practices like data splitting and scaling.\n","Processing row 2/8...\n","```python\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision\n","import torchvision.transforms as transforms\n","from torch.utils.data import DataLoader\n","from tqdm import tqdm\n","\n","# Device configuration\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# Hyperparameters\n","num_epochs = 10\n","batch_size = 64\n","learning_rate = 0.001\n","\n","# CIFAR10 dataset\n","transform_train = transforms.Compose([\n","    transforms.RandomCrop(32, padding=4),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n","])\n","\n","transform_test = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n","])\n","\n","train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True,\n","                                             download=True, transform=transform_train)\n","\n","test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False,\n","                                            download=True, transform=transform_test)\n","\n","train_loader = DataLoader(train_dataset, batch_size=batch_size,\n","                          shuffle=True, num_workers=2)\n","\n","test_loader = DataLoader(test_dataset, batch_size=batch_size,\n","                         shuffle=False, num_workers=2)\n","\n","# Convolutional neural network (simple architecture)\n","class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n","        self.relu = nn.ReLU()\n","        self.pool = nn.MaxPool2d(2, 2)\n","        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n","        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n","        self.fc1 = nn.Linear(128 * 4 * 4, 512)\n","        self.fc2 = nn.Linear(512, 10)\n","\n","    def forward(self, x):\n","        x = self.pool(self.relu(self.conv1(x)))\n","        x = self.pool(self.relu(self.conv2(x)))\n","        x = self.pool(self.relu(self.conv3(x)))\n","        x = x.view(-1, 128 * 4 * 4) # Flatten the tensor\n","        x = self.relu(self.fc1(x))\n","        x = self.fc2(x)\n","        return x\n","\n","model = Net().to(device)\n","\n","# Loss and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# Training loop\n","print(\"Starting training...\")\n","for epoch in range(num_epochs):\n","    model.train()\n","    running_loss = 0.0\n","    for i, (inputs, labels) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")):\n","        inputs, labels = inputs.to(device), labels.to(device)\n","\n","        # Forward pass\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","\n","        # Backward and optimize\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item()\n","\n","    avg_loss = running_loss / len(train_loader)\n","    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n","\n","print(\"Finished Training\")\n","\n","# Evaluation\n","print(\"Starting evaluation...\")\n","model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for inputs, labels in tqdm(test_loader, desc=\"Evaluating\"):\n","        inputs, labels = inputs.to(device), labels.to(device)\n","        outputs = model(inputs)\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","accuracy = 100 * correct / total\n","print(f'Accuracy of the network on the 10000 test images: {accuracy:.2f} %')\n","\n","```\n","Okay, let's build a Python script using PyTorch to train a Convolutional Neural Network (CNN) on the CIFAR-10 dataset. I will explain each step as we go.\n","\n","CIFAR-10 is a dataset of 60,000 32x32 colour images in 10 classes, with 6,000 images per class. There are 50,000 training images and 10,000 test images.\n","\n","Here's the plan:\n","1.  **Import necessary libraries:** PyTorch core, neural network module, optimizer module, data utilities, and torchvision for dataset and transforms.\n","2.  **Define Hyperparameters:** Set values like batch size, learning rate, number of epochs, etc.\n","3.  **Set up Device:** Check if a GPU is available and use it, otherwise use the CPU.\n","4.  **Load and Preprocess Data:** Download the CIFAR-10 dataset, apply necessary transformations (like converting to tensor and normalization), and create DataLoaders for training and testing.\n","5.  **Define the CNN Model:** Create a PyTorch `nn.Module` class for our CNN architecture.\n","6.  **Instantiate Model, Loss Function, and Optimizer:** Create an instance of the model, define the loss criterion (CrossEntropyLoss for classification), and choose an optimizer (e.g., Adam or SGD).\n","7.  **Define Training Loop:** Write a function or code block that iterates over the epochs, processing data in batches, calculating loss, performing backpropagation, and updating model weights.\n","8.  **Define Evaluation Function:** Write a function to evaluate the model's performance on the test dataset (calculate accuracy).\n","9.  **Run Training and Evaluation:** Execute the training loop and periodically evaluate the model.\n","10. **Optional: Plot Results:** Visualize training loss and test accuracy over epochs.\n","\n","Let's start coding!\n","\n","```python\n","# 1. Import necessary libraries\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision\n","import torchvision.transforms as transforms\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import os # To create directory for saving model\n","from tqdm import tqdm # For progress bar\n","\n","# 2. Define Hyperparameters\n","num_epochs = 10         # Number of training epochs\n","batch_size = 100        # Number of images per batch\n","learning_rate = 0.001   # Learning rate for the optimizer\n","data_path = './data'    # Directory to download the dataset\n","model_path = './models' # Directory to save the trained model\n","model_save_name = 'cifar10_cnn.pth' # Name for the saved model file\n","\n","# 3. Set up Device\n","# This checks if a CUDA-enabled GPU is available. If yes, it uses the GPU; otherwise, it uses the CPU.\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(f'Using device: {device}')\n","\n","# 4. Load and Preprocess Data\n","\n","# Data augmentation and normalization for training\n","# We define a sequence of transformations to apply to the images.\n","# transforms.ToTensor(): Converts a PIL Image or NumPy array (H x W x C)\n","#                        to a PyTorch Tensor (C x H x W) in the range [0.0, 1.0].\n","# transforms.Normalize(mean, std): Normalizes a tensor image with mean and standard deviation.\n","# We use the standard mean and std for CIFAR-10 across its channels.\n","transform_train = transforms.Compose([\n","    transforms.RandomCrop(32, padding=4), # Pad and randomly crop to 32x32\n","    transforms.RandomHorizontalFlip(), # Randomly flip the image horizontally\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n","])\n","\n","# Normalization for testing (no augmentation)\n","transform_test = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n","])\n","\n","# Load the CIFAR-10 training dataset\n","# root: directory where the dataset will be saved or loaded from.\n","# train=True: specifies the training set.\n","# download=True: downloads the dataset if it's not found in the root directory.\n","# transform: applies the specified transformations.\n","trainset = torchvision.datasets.CIFAR10(root=data_path, train=True,\n","                                        download=True, transform=transform_train)\n","\n","# Create a DataLoader for the training set\n","# DataLoader is an iterator that provides batches of data.\n","# dataset: the dataset to load data from.\n","# batch_size: how many samples per batch to load.\n","# shuffle=True: shuffles the data at the beginning of each epoch (important for training).\n","# num_workers: how many subprocesses to use for data loading (higher values can speed up loading).\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n","                                          shuffle=True, num_workers=2)\n","\n","# Load the CIFAR-10 test dataset\n","testset = torchvision.datasets.CIFAR10(root=data_path, train=False,\n","                                       download=True, transform=transform_test)\n","\n","# Create a DataLoader for the test set\n","# shuffle=False: shuffling is not necessary for testing/evaluation.\n","testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n","                                         shuffle=False, num_workers=2)\n","\n","# Define the 10 class names for CIFAR-10\n","classes = ('plane', 'car', 'bird', 'cat', 'deer',\n","           'dog', 'frog', 'horse', 'ship', 'truck')\n","\n","print(f'Training data size: {len(trainset)}')\n","print(f'Test data size: {len(testset)}')\n","\n","# Optional: Show a few images (for debugging/visualization)\n","# def imshow(img):\n","#     # Unnormalize the image\n","#     img = img / 2 + 0.5     # Assuming the normalization was done around mean 0.5, std 0.5 for simplicity here,\n","#                           # though we used CIFAR10 specific mean/std above.\n","#                           # For correct visualization with CIFAR10 norm:\n","#     mean = torch.tensor((0.4914, 0.4822, 0.4465)).reshape(3, 1, 1)\n","#     std = torch.tensor((0.2023, 0.1994, 0.2010)).reshape(3, 1, 1)\n","#     img = img * std + mean\n","#\n","#     npimg = img.numpy()\n","#     plt.imshow(np.transpose(npimg, (1, 2, 0))) # Convert from Tensor (C, H, W) to NumPy array (H, W, C)\n","#     plt.show()\n","\n","# Get some random training images\n","# dataiter = iter(trainloader)\n","# images, labels = next(dataiter)\n","# print(' '.join(f'{classes[labels[j]]:5s}' for j in range(4)))\n","# imshow(torchvision.utils.make_grid(images))\n","\n","\n","# 5. Define the CNN Model\n","# We create a class MyCNN that inherits from nn.Module.\n","# This is the standard way to define models in PyTorch.\n","class MyCNN(nn.Module):\n","    def __init__(self):\n","        # Call the constructor of the parent class (nn.Module)\n","        super(MyCNN, self).__init__()\n","\n","        # Define the layers of the network\n","\n","        # Convolutional Layer 1\n","        # nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n","        # Input image size: 3x32x32 (3 channels, 32x32 pixels)\n","        # Output size after conv1: (32 + 2*1 - 3)/1 + 1 = 32 -> 32x32x32\n","        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1)\n","        # Batch Normalization 1: Applied after convolution, before activation, helps stabilize training.\n","        self.bn1 = nn.BatchNorm2d(32)\n","        # ReLU Activation: Applies the rectified linear unit function element-wise (max(0, x)).\n","        self.relu = nn.ReLU()\n","        # Max Pooling Layer 1: Reduces spatial dimensions.\n","        # nn.MaxPool2d(kernel_size, stride)\n","        # Output size after pool1: 32/2 x 32/2 = 16x16 -> 32x16x16\n","        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n","\n","        # Convolutional Layer 2\n","        # Input size: 32x16x16\n","        # Output size after conv2: (64 + 2*1 - 3)/1 + 1 = 16 -> 64x16x16\n","        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n","        self.bn2 = nn.BatchNorm2d(64)\n","        # Output size after pool: 16/2 x 16/2 = 8x8 -> 64x8x8\n","\n","        # Convolutional Layer 3\n","        # Input size: 64x8x8\n","        # Output size after conv3: (128 + 2*1 - 3)/1 + 1 = 8 -> 128x8x8\n","        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n","        self.bn3 = nn.BatchNorm2d(128)\n","        # Output size after pool: 8/2 x 8/2 = 4x4 -> 128x4x4\n","\n","        # Fully Connected Layers (also called Dense layers)\n","        # Input size for the first fully connected layer: Flat output from the last pooling layer\n","        # The output tensor from the last pooling layer is 128 channels of size 4x4.\n","        # Flattened size: 128 * 4 * 4 = 2048\n","        self.fc1 = nn.Linear(in_features=128 * 4 * 4, out_features=512)\n","        # Dropout Layer: Randomly sets a fraction of input units to 0 at each update during training. Helps prevent overfitting.\n","        self.dropout = nn.Dropout(p=0.5) # 50% probability of dropping an activation\n","        self.fc2 = nn.Linear(in_features=512, out_features=10) # Output layer: 10 features for 10 classes\n","\n","    # Define the forward pass (how data flows through the network)\n","    def forward(self, x):\n","        # Apply Conv -> BN -> ReLU -> Pool\n","        x = self.pool(self.relu(self.bn1(self.conv1(x))))\n","        x = self.pool(self.relu(self.bn2(self.conv2(x))))\n","        x = self.pool(self.relu(self.bn3(self.conv3(x))))\n","\n","        # Flatten the output from the convolutional layers for the fully connected layers\n","        # x.size(0) is the batch size\n","        x = x.view(-1, 128 * 4 * 4) # -1 lets PyTorch figure out the batch size\n","\n","        # Apply Fully Connected layers\n","        x = self.relu(self.fc1(x))\n","        x = self.dropout(x) # Apply dropout after the first FC layer\n","        x = self.fc2(x)\n","\n","        return x # The output layer doesn't typically have an activation for CrossEntropyLoss,\n","                 # as it includes LogSoftmax internally.\n","\n","# 6. Instantiate Model, Loss Function, and Optimizer\n","model = MyCNN().to(device) # Create an instance of the model and move it to the selected device (GPU/CPU)\n","\n","# Loss function: Cross-Entropy Loss is suitable for multi-class classification.\n","criterion = nn.CrossEntropyLoss()\n","\n","# Optimizer: Adam is an adaptive learning rate optimization algorithm.\n","# model.parameters(): Provides all the learnable parameters of the model.\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# Optional: Learning Rate Scheduler\n","# Decays the learning rate by a factor of 0.1 every 30 epochs\n","# scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n","\n","# 7. Define Training Loop\n","# This function trains the model for one epoch\n","def train_epoch(model, train_loader, criterion, optimizer, device):\n","    model.train() # Set the model to training mode (enables dropout and batch normalization updates)\n","    running_loss = 0.0\n","    # Wrap the DataLoader with tqdm for a progress bar\n","    for i, data in enumerate(tqdm(train_loader, desc='Training', leave=False)):\n","        # Get the inputs; data is a list of [inputs, labels]\n","        inputs, labels = data\n","        # Move data to the appropriate device\n","        inputs, labels = inputs.to(device), labels.to(device)\n","\n","        # Zero the parameter gradients\n","        # It's important to zero the gradients before each backpropagation step,\n","        # as gradients are accumulated by default in PyTorch.\n","        optimizer.zero_grad()\n","\n","        # Forward pass: Compute the predicted outputs by passing inputs through the model\n","        outputs = model(inputs)\n","        # Compute the loss: Compare the predicted outputs with the true labels\n","        loss = criterion(outputs, labels)\n","\n","        # Backward pass: Compute gradients of the loss with respect to model parameters\n","        loss.backward()\n","        # Optimizer step: Update the model parameters using the computed gradients\n","        optimizer.step()\n","\n","        # Print statistics\n","        running_loss += loss.item() # .item() gets the scalar value from a tensor\n","    return running_loss / len(train_loader) # Return average loss for the epoch\n","\n","# 8. Define Evaluation Function\n","# This function evaluates the model on a given data loader (usually the test set)\n","def evaluate(model, data_loader, device):\n","    model.eval() # Set the model to evaluation mode (disables dropout and batch normalization tracking/updates)\n","    correct = 0\n","    total = 0\n","    # Disable gradient calculations during evaluation\n","    # This reduces memory consumption and speeds up computation as we don't need gradients for evaluation.\n","    with torch.no_grad():\n","        for data in tqdm(data_loader, desc='Evaluating', leave=False):\n","            images, labels = data\n","            # Move data to the appropriate device\n","            images, labels = images.to(device), labels.to(device)\n","            # Forward pass: Get predictions\n","            outputs = model(images)\n","            # Get the predicted class with the highest score\n","            # torch.max returns (values, indices). We need the indices (the class predictions).\n","            _, predicted = torch.max(outputs.data, 1)\n","            # Update total number of images and correct predictions\n","            total += labels.size(0) # Add batch size to total count\n","            correct += (predicted == labels).sum().item() # Count correct predictions in the batch\n","\n","    accuracy = 100 * correct / total\n","    return accuracy\n","\n","# 9. Run Training and Evaluation\n","train_losses = []\n","test_accuracies = []\n","\n","# Ensure model save directory exists\n","os.makedirs(model_path, exist_ok=True)\n","best_accuracy = 0.0 # To keep track of the best test accuracy achieved\n","\n","print(\"Starting Training...\")\n","for epoch in range(num_epochs):\n","    print(f'\\nEpoch {epoch + 1}/{num_epochs}')\n","\n","    # Train for one epoch\n","    avg_train_loss = train_epoch(model, trainloader, criterion, optimizer, device)\n","    train_losses.append(avg_train_loss)\n","\n","    # Evaluate on the test set\n","    test_accuracy = evaluate(model, testloader, device)\n","    test_accuracies.append(test_accuracy)\n","\n","    print(f'Epoch {epoch + 1} finished.')\n","    print(f'Train Loss: {avg_train_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')\n","\n","    # Optional: Save the model checkpoint if it's the best performing so far\n","    if test_accuracy > best_accuracy:\n","        best_accuracy = test_accuracy\n","        save_path = os.path.join(model_path, model_save_name)\n","        # Save the model's state dictionary\n","        # Saving state_dict is recommended as it's more flexible for loading later\n","        torch.save(model.state_dict(), save_path)\n","        print(f'Model saved to {save_path} with improved accuracy: {best_accuracy:.2f}%')\n","\n","    # Optional: Step the learning rate scheduler\n","    # if 'scheduler' in locals():\n","    #     scheduler.step()\n","\n","print('Finished Training')\n","\n","# 10. Plot Results (Optional)\n","plt.figure(figsize=(10, 5))\n","plt.subplot(1, 2, 1)\n","plt.plot(range(1, num_epochs + 1), train_losses)\n","plt.xlabel('Epoch')\n","plt.ylabel('Training Loss')\n","plt.title('Training Loss per Epoch')\n","\n","plt.subplot(1, 2, 2)\n","plt.plot(range(1, num_epochs + 1), test_accuracies, color='orange')\n","plt.xlabel('Epoch')\n","plt.ylabel('Test Accuracy (%)')\n","plt.title('Test Accuracy per Epoch')\n","\n","plt.tight_layout()\n","plt.show()\n","\n","# Optional: Load the best model and evaluate one last time\n","try:\n","    loaded_model = MyCNN().to(device)\n","    load_path = os.path.join(model_path, model_save_name)\n","    loaded_model.load_state_dict(torch.load(load_path))\n","    print(f'\\nLoaded model from {load_path}')\n","    final_test_accuracy = evaluate(loaded_model, testloader, device)\n","    print(f'Final Test Accuracy (loaded model): {final_test_accuracy:.2f}%')\n","except FileNotFoundError:\n","    print(f\"\\nModel file not found at {load_path}. Skipping final evaluation of loaded model.\")\n","except Exception as e:\n","    print(f\"\\nError loading or evaluating model: {e}\")\n","\n","```\n","\n","**Explanation of Steps and Reasoning:**\n","\n","1.  **Imports:** We import `torch` for the core framework, `torch.nn` for neural network modules (layers), `torch.optim` for optimizers, `torchvision` for datasets and common transforms, `torchvision.transforms` for data augmentation/preprocessing, `matplotlib.pyplot` for plotting, `numpy` for potential numerical operations (used implicitly by matplotlib), `os` to handle directories, and `tqdm` for progress bars.\n","2.  **Hyperparameters:** These are settings that control the training process. Defining them at the top makes it easy to experiment with different values without changing the core logic.\n","3.  **Device Setup:** Training neural networks is computationally intensive. Using a GPU (`cuda`) if available is crucial for performance. This snippet automatically detects and uses the GPU or falls back to the CPU.\n","4.  **Data Loading and Preprocessing:**\n","    *   We use `torchvision.datasets.CIFAR10` which handles downloading and loading the dataset.\n","    *   `transforms.Compose` chains together multiple transformations.\n","    *   `transforms.RandomCrop` and `RandomHorizontalFlip` are common data *augmentation* techniques. They artificially increase the size and variability of the training data, making the model more robust and reducing overfitting.\n","    *   `transforms.ToTensor()` converts images to PyTorch tensors and scales pixel values from \\[0, 255] to \\[0.0, 1.0].\n","    *   `transforms.Normalize()` scales pixel values based on the channel-wise mean and standard deviation of the dataset. This helps in speeding up convergence and improving performance.\n","    *   `torch.utils.data.DataLoader` is essential. It wraps the dataset and provides an iterable that yields data in batches. This is necessary because training is typically done using mini-batches, which makes gradient estimation more stable than training on individual samples and is more memory-efficient than training on the entire dataset at once. `shuffle=True` for training is important to prevent the model from learning the order of data.\n","5.  **Define the CNN Model:**\n","    *   We create a class `MyCNN` inheriting from `nn.Module`. The `__init__` method defines the layers, and the `forward` method defines how data passes through these layers.\n","    *   **Convolutional Layers (`nn.Conv2d`):** These are the core of the CNN. They apply filters (kernels) to the input image to extract features. `in_channels` is the number of input feature maps (3 for RGB images), `out_channels` is the number of filters (and thus output feature maps), `kernel_size` is the size of the filter (e.g., 3x3), `stride` is how many pixels the filter moves at each step, and `padding` adds zeros around the input borders (useful for controlling output size).\n","    *   **Batch Normalization (`nn.BatchNorm2d`):** Normalizes the output of a convolutional layer across the batch dimension. This helps in training deeper networks by making the training process more stable and allowing for higher learning rates. It's typically applied after the convolution and before the activation function.\n","    *   **Activation Function (`nn.ReLU`):** Introduces non-linearity into the network. Without non-linearities, a CNN would just be a sequence of linear operations, which cannot model complex patterns.\n","    *   **Pooling Layers (`nn.MaxPool2d`):** Reduces the spatial dimensions (height and width) of the feature maps. This reduces the number of parameters and computation in the network, and helps the network become more invariant to small translations in the input. Max pooling selects the maximum value in a given window.\n","    *   **Flattening:** After the convolutional and pooling layers, the 2D feature maps are flattened into a 1D vector to be fed into the fully connected layers. `x.view(x.size(0), -1)` reshapes the tensor while keeping the batch size the same and automatically calculating the required size for the flattened dimension.\n","    *   **Fully Connected Layers (`nn.Linear`):** These are standard feedforward layers. They take the flattened features and perform classification based on them. The last layer has `out_features=10` corresponding to the 10 classes of CIFAR-10.\n","    *   **Dropout (`nn.Dropout`):** A regularization technique where random connections are temporarily ignored during training. This prevents units from becoming too dependent on each other and helps reduce overfitting.\n","6.  **Instantiate Model, Loss, and Optimizer:**\n","    *   `model = MyCNN().to(device)` creates an instance of our defined network and moves all its parameters to the specified device (GPU or CPU).\n","    *   `nn.CrossEntropyLoss()` is the standard loss function for multi-class classification problems. It measures the difference between the model's output logits and the true class labels.\n","    *   `optim.Adam()` is a popular optimization algorithm that updates the model's weights based on the gradients computed during backpropagation. It's often a good default choice. We pass `model.parameters()` so the optimizer knows which parameters to update.\n","7.  **Training Loop (`train_epoch`):**\n","    *   `model.train()` sets the model to training mode. This is important because layers like Dropout and BatchNorm behave differently during training (e.g., Dropout is active, BatchNorm tracks running mean/variance and uses batch statistics) vs. evaluation.\n","    *   We iterate through the `train_loader`, getting batches of `inputs` and `labels`.\n","    *   `optimizer.zero_grad()` clears the gradients from the *previous* training step. Gradients are accumulated by default, so this reset is crucial for calculating gradients correctly for the *current* batch.\n","    *   `outputs = model(inputs)` performs the forward pass, feeding the inputs through the network to get predictions.\n","    *   `loss = criterion(outputs, labels)` calculates the loss between the predicted outputs and the ground truth labels.\n","    *   `loss.backward()` calculates the gradients of the loss with respect to every trainable parameter in the model using backpropagation.\n","    *   `optimizer.step()` updates the model's parameters using the computed gradients and the chosen optimization algorithm (Adam in this case).\n","    *   `loss.item()` gets the scalar value of the loss tensor.\n","8.  **Evaluation Function (`evaluate`):**\n","    *   `model.eval()` sets the model to evaluation mode. Dropout is disabled, and BatchNorm layers use their saved running statistics instead of batch statistics.\n","    *   `with torch.no_grad():` is a context manager that disables gradient calculation. This is important during evaluation because we don't need to compute gradients, saving memory and computation.\n","    *   We iterate through the `test_loader`.\n","    *   `torch.max(outputs.data, 1)` finds the index (class prediction) with the highest probability/logit score along dimension 1 (the class dimension).\n","    *   We compare the `predicted` classes with the true `labels` to count correct predictions and calculate the accuracy.\n","9.  **Run Training and Evaluation:** The main loop iterates for the specified number of `num_epochs`. In each epoch, it calls the `train_epoch` function and then the `evaluate` function on the test set to track performance. It prints the loss and accuracy for each epoch. Saving the model based on the best test accuracy is a common practice.\n","10. **Plot Results:** `matplotlib` is used to visualize the training loss and test accuracy over the epochs, providing insight into the training process (e.g., if the model is overfitting, if the loss is decreasing).\n","11. **Load and Evaluate Final Model:** After training, we load the `state_dict` of the model that achieved the best accuracy during training and perform a final evaluation on the test set to confirm its performance. Loading just the `state_dict` is standard practice as it's more portable than saving the entire model object.\n","\n","This script provides a solid foundation for training a CNN on CIFAR-10. You can extend it by adding more complex model architectures, different optimizers or schedulers, more advanced data augmentations, logging, etc.\n","Processing row 3/8...\n","```python\n","import numpy as np\n","from sklearn.datasets import make_classification\n","from sklearn.model_selection import train_test_split\n","from sklearn.pipeline import Pipeline\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.linear_model import LogisticRegression\n","\n","# 1. Generate a random n-class classification problem\n","n_classes = 3  # Example: 3 classes\n","X, y = make_classification(n_samples=1000,\n","                           n_features=20,\n","                           n_informative=10,\n","                           n_redundant=5,\n","                           n_classes=n_classes,\n","                           n_clusters_per_class=2,\n","                           weights=None,\n","                           flip_y=0.01,\n","                           class_sep=1.0,\n","                           hypercube=True,\n","                           shift=0.0,\n","                           scale=1.0,\n","                           shuffle=True,\n","                           random_state=42)\n","\n","# Split data into training and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n","\n","# 2. Create a pipeline to compute mean/std on training set and apply it\n","# along with a classifier\n","pipeline = Pipeline([\n","    ('scaler', StandardScaler()),  # Computes mean and std on train, applies transform\n","    ('classifier', LogisticRegression(multi_class='auto', solver='liblinear', random_state=42)) # Example classifier\n","])\n","\n","# 3. Train the model Fit the dataset using the same pipeline\n","pipeline.fit(X_train, y_train)\n","\n","# 4. Compute score on the test set\n","score = pipeline.score(X_test, y_test)\n","\n","# Print the score\n","print(f\"Test set score: {score:.4f}\")\n","```\n","Okay, let's build this scikit-learn pipeline step by step. The goal is to simulate a common machine learning workflow: generate data, split it, preprocess the training data (specifically, calculate mean and standard deviation), apply that *same* preprocessing to both training and testing data, train a model, and evaluate it, all within a single pipeline.\n","\n","Using a pipeline is crucial here because it ensures that the data preprocessing steps (like calculating mean and standard deviation) are fitted *only* on the training data and then applied consistently to both training and testing data. This prevents **data leakage**, where information from the test set could inadvertently influence the preprocessing steps, leading to overly optimistic performance estimates.\n","\n","Here's the code and the detailed explanation:\n","\n","```python\n","import numpy as np\n","from sklearn.datasets import make_classification\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.linear_model import LogisticRegression # We'll use Logistic Regression as a simple classifier\n","from sklearn.pipeline import Pipeline\n","from sklearn.metrics import accuracy_score # To explicitly show the score calculation\n","\n","# --- Step 1: Generate a Random N-Class Classification Problem ---\n","print(\"Step 1: Generating synthetic dataset...\")\n","\n","# Parameters for the dataset\n","n_samples = 1000      # Total number of samples\n","n_features = 25       # Total number of features\n","n_informative = 20    # Number of informative features\n","n_redundant = 3       # Number of redundant features\n","n_classes = 5         # Number of classes\n","random_state = 42     # For reproducibility\n","\n","X, y = make_classification(\n","    n_samples=n_samples,\n","    n_features=n_features,\n","    n_informative=n_informative,\n","    n_redundant=n_redundant,\n","    n_repeated=0,\n","    n_classes=n_classes,\n","    n_clusters_per_class=1,\n","    weights=None,       # Uniform weights for classes\n","    flip_y=0.01,        # Some noise in labels\n","    class_sep=1.0,      # How distinct the classes are\n","    hypercube=True,\n","    shift=0.0,\n","    scale=1.0,\n","    random_state=random_state # Ensure the same dataset is generated each time\n",")\n","\n","print(f\"Dataset generated: X shape {X.shape}, y shape {y.shape}\")\n","print(f\"Number of unique classes: {len(np.unique(y))}\") # Verify the number of classes\n","\n","# --- Step 2: Split Data into Training and Testing Sets ---\n","print(\"\\nStep 2: Splitting data into training and testing sets...\")\n","\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.25, random_state=random_state, stratify=y # stratify ensures class distribution is similar\n",")\n","\n","print(f\"Training data shape: {X_train.shape}\")\n","print(f\"Testing data shape: {X_test.shape}\")\n","\n","# --- Step 3: Define Preprocessing and Model Steps ---\n","print(\"\\nStep 3: Defining preprocessing (scaling) and model steps...\")\n","\n","# Preprocessing: StandardScaler\n","# This transformer calculates the mean and standard deviation on the data it is fitted on\n","# and then transforms the data by subtracting the mean and dividing by the standard deviation.\n","# This is exactly what we need to calculate mean/std on train and apply to both.\n","scaler = StandardScaler()\n","\n","# Model: Logistic Regression Classifier\n","# A simple linear model for multi-class classification.\n","# We use 'lbfgs' solver which works well with StandardScaler and multi_class='auto'.\n","# Increased max_iter to avoid potential convergence warnings after scaling.\n","classifier = LogisticRegression(multi_class='auto', solver='lbfgs', max_iter=1000, random_state=random_state)\n","\n","print(\"Preprocessing step defined: StandardScaler\")\n","print(\"Model step defined: LogisticRegression\")\n","\n","# --- Step 4: Create the Scikit-learn Pipeline ---\n","print(\"\\nStep 4: Creating the pipeline...\")\n","\n","# The pipeline chains the steps in order: first the scaler, then the classifier.\n","# The output of the first step (scaled data) becomes the input for the second step (classifier).\n","pipeline = Pipeline([\n","    ('scaler', scaler),         # First step: Apply the scaler\n","    ('classifier', classifier)  # Second step: Apply the classifier\n","])\n","\n","print(\"Pipeline created:\")\n","print(pipeline)\n","\n","# --- Step 5: Train the Pipeline (Fit on Training Data) ---\n","print(\"\\nStep 5: Training the pipeline on the training data...\")\n","\n","# When you call .fit() on the pipeline:\n","# 1. The 'scaler' step's .fit_transform(X_train, y_train) method is called.\n","#    - StandardScaler calculates the mean and standard deviation of X_train.\n","#    - It then transforms X_train using these calculated values.\n","# 2. The transformed X_train (let's call it X_train_scaled) is passed to the 'classifier' step.\n","# 3. The 'classifier' step's .fit(X_train_scaled, y_train) method is called.\n","#    - The Logistic Regression model is trained on the scaled training data.\n","# This ensures the scaler learns *only* from the training data, preventing data leakage.\n","pipeline.fit(X_train, y_train)\n","\n","print(\"Pipeline training complete.\")\n","\n","# You can optionally access the fitted components inside the pipeline\n","# print(\"\\nMean calculated by scaler during fit:\", pipeline.named_steps['scaler'].mean_)\n","# print(\"Std dev calculated by scaler during fit:\", pipeline.named_steps['scaler'].scale_)\n","\n","\n","# --- Step 6: Compute Score on the Test Set using the Pipeline ---\n","print(\"\\nStep 6: Evaluating the pipeline on the test data...\")\n","\n","# When you call .score() on the pipeline:\n","# 1. The 'scaler' step's .transform(X_test) method is called.\n","#    - StandardScaler uses the mean and standard deviation *learned during the .fit() on X_train*\n","#      to scale X_test. It does *not* recalculate mean/std from X_test.\n","# 2. The transformed X_test (X_test_scaled) is passed to the 'classifier' step.\n","# 3. The 'classifier' step's .score(X_test_scaled, y_test) method is called.\n","#    - The trained Logistic Regression model makes predictions on the scaled test data.\n","#    - The score (accuracy for classification by default) is computed by comparing predictions to y_test.\n","test_score = pipeline.score(X_test, y_test)\n","\n","print(f\"Pipeline test set score (Accuracy): {test_score:.4f}\")\n","\n","# Alternatively, predict and calculate accuracy explicitly\n","# y_pred = pipeline.predict(X_test)\n","# explicit_accuracy = accuracy_score(y_test, y_pred)\n","# print(f\"Explicitly calculated test set accuracy: {explicit_accuracy:.4f}\")\n","```\n","\n","### Explanation of Reasoning:\n","\n","1.  **Generating the Dataset (`make_classification`):** We use `make_classification` because it's a convenient function in scikit-learn to create synthetic data that mimics a real classification problem. We can control parameters like the number of samples, features, and classes (`n_samples`, `n_features`, `n_classes`) to simulate a dataset of a desired size and complexity. `random_state` is set to ensure that running the code multiple times produces the exact same dataset, making the results reproducible.\n","\n","2.  **Splitting Data (`train_test_split`):** It's crucial to split the data into distinct training and testing sets *before* any preprocessing or model training begins. The training set is used to train the model and learn any necessary parameters (like the mean and standard deviation for scaling). The test set is held out and used *only* for final evaluation to estimate how the model will perform on unseen data. `test_size` specifies the proportion of data for the test set (here, 25%). `stratify=y` is important for classification tasks, especially with multiple classes or unbalanced classes; it ensures that the proportion of each class is approximately the same in both the training and testing splits as it is in the original dataset. `random_state` makes the split reproducible.\n","\n","3.  **Preprocessing (`StandardScaler`):** Many machine learning algorithms perform better when features are on a similar scale. `StandardScaler` is a common preprocessing step that standardizes features by removing the mean and scaling to unit variance (i.e., `(X - mean) / standard_deviation`). The specific requirement was to calculate mean and std on the *training* set and apply this to *both* sets. `StandardScaler` is designed precisely for this.\n","    *   When its `.fit()` method is called, it calculates the mean and standard deviation of the data it receives.\n","    *   When its `.transform()` method is called, it applies the scaling using the mean and standard deviation it learned during the last `.fit()` call.\n","\n","4.  **Model (`LogisticRegression`):** We chose `LogisticRegression` as a simple, interpretable multi-class classifier. It works well with standardized data. In a real scenario, you would choose a model appropriate for your problem and data, such as `RandomForestClassifier`, `SVC`, `KNeighborsClassifier`, etc.\n","\n","5.  **Pipeline (`Pipeline`):** The `Pipeline` object sequentially applies a list of transformers and a final estimator. This is the key to preventing data leakage and simplifying the workflow.\n","    *   We define the sequence as `('scaler', StandardScaler())` followed by `('classifier', LogisticRegression(...))`.\n","    *   The names ('scaler', 'classifier') are arbitrary strings used to access the steps within the pipeline.\n","\n","6.  **Training the Pipeline (`pipeline.fit(X_train, y_train)`):** When you call `.fit()` on the pipeline with the training data (`X_train`, `y_train`):\n","    *   The first step (`scaler`) automatically calls its `.fit_transform()` method on `X_train`. This is where `StandardScaler` computes the mean and standard deviation *specifically from* `X_train`. It then applies this scaling to `X_train`, producing a scaled version of the training data.\n","    *   The output of `fit_transform` (the scaled `X_train`) is then passed as input to the next step (`classifier`).\n","    *   The final step (`classifier`) automatically calls its `.fit()` method with the scaled training data and `y_train`. The classifier is trained using the preprocessed training features.\n","    *   **This ensures that the mean and standard deviation used for scaling are derived *only* from the training data, and the model is trained on data scaled using these parameters.**\n","\n","7.  **Evaluating the Pipeline (`pipeline.score(X_test, y_test)`):** When you call `.score()` on the pipeline with the test data (`X_test`, `y_test`):\n","    *   The first step (`scaler`) automatically calls its `.transform()` method on `X_test`. *Crucially*, it uses the mean and standard deviation that were learned *during the `pipeline.fit()` call on `X_train`*. It does *not* recalculate mean/std from `X_test`. This correctly simulates applying the *trained* preprocessing step to new, unseen data.\n","    *   The output of `transform` (the scaled `X_test`) is passed to the next step (`classifier`).\n","    *   The final step (`classifier`) automatically calls its `.score()` method with the scaled test data and `y_test`. The trained model makes predictions on the correctly scaled test data, and the accuracy score is computed.\n","    *   **This provides a realistic estimate of the model's performance on unseen data, as the test data is preprocessed using parameters learned *only* from the training data, mimicking a real-world deployment scenario.**\n","\n","By using the pipeline, we encapsulate the entire workflow (preprocessing + model training/prediction) into a single object. This makes the code cleaner, less error-prone (especially regarding data leakage), and easier to manage.\n","Processing row 4/8...\n","```python\n","import torch\n","from datasets import load_dataset\n","from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding\n","\n","def train_imdb_sentiment():\n","    \"\"\"\n","    Trains a DistilBERT model on the IMDb dataset for binary sentiment classification.\n","\n","    Returns:\n","        Trainer: The trained Hugging Face Trainer object.\n","    \"\"\"\n","    # 1. Load the dataset\n","    dataset = load_dataset(\"imdb\")\n","\n","    # 2. Load the tokenizer\n","    tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n","\n","    # 3. Preprocess the dataset\n","    def tokenize_function(examples):\n","        # Tokenize the text and ensure padding and truncation\n","        return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n","\n","    # Apply the tokenization function to the dataset\n","    tokenized_datasets = dataset.map(tokenize_function, batched=True)\n","\n","    # Rename the label column to 'labels' as expected by the Trainer\n","    tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n","\n","    # Remove the original text column as it's no longer needed after tokenization\n","    tokenized_datasets = tokenized_datasets.remove_columns([\"text\"])\n","\n","    # Set format to PyTorch tensors\n","    tokenized_datasets.set_format(\"torch\")\n","\n","    # Split dataset into training and validation sets\n","    train_dataset = tokenized_datasets[\"train\"]\n","    eval_dataset = tokenized_datasets[\"test\"]\n","\n","    # 4. Load the model\n","    model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n","\n","    # 5. Define Training Arguments\n","    training_args = TrainingArguments(\n","        output_dir=\"./results\",  # Output directory for checkpoints and predictions\n","        num_train_epochs=3,      # Total number of training epochs\n","        per_device_train_batch_size=16,  # Batch size per device during training\n","        per_device_eval_batch_size=16,   # Batch size for evaluation\n","        warmup_steps=500,        # Number of warmup steps for learning rate scheduler\n","        weight_decay=0.01,       # Strength of weight decay\n","        logging_dir=\"./logs\",    # Directory for storing logs\n","        logging_steps=10,        # Log every X updates steps\n","        evaluation_strategy=\"epoch\", # Evaluate every epoch\n","        save_strategy=\"epoch\",   # Save checkpoint every epoch\n","        load_best_model_at_end=True, # Load the best model when finished training\n","        metric_for_best_model=\"eval_loss\", # Metric to use to compare models\n","        report_to=\"none\",        # Don't report to any platform\n","    )\n","\n","    # 6. Define Data Collator\n","    # Data collator will dynamically pad the sentences to the longest length in a batch\n","    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n","\n","    # 7. Initialize the Trainer\n","    trainer = Trainer(\n","        model=model,                         # The instantiated 🤗 Transformers model to be trained\n","        args=training_args,                  # Training arguments, defined above\n","        train_dataset=train_dataset,         # Training dataset\n","        eval_dataset=eval_dataset,           # Evaluation dataset\n","        tokenizer=tokenizer,                 # Tokenizer to use for padding\n","        data_collator=data_collator          # Data collator\n","    )\n","\n","    # 8. Train the model\n","    trainer.train()\n","\n","    return trainer\n","\n","if __name__ == \"__main__\":\n","    # Example usage:\n","    print(\"Starting training...\")\n","    trainer = train_imdb_sentiment()\n","    print(\"Training finished.\")\n","\n","    # You can optionally save the model and tokenizer\n","    # output_dir = \"./fine_tuned_distilbert_imdb\"\n","    # trainer.save_model(output_dir)\n","    # tokenizer.save_pretrained(output_dir)\n","    # print(f\"Model saved to {output_dir}\")\n","\n","    # You can also evaluate the trained model\n","    # eval_results = trainer.evaluate()\n","    # print(\"Evaluation results:\")\n","    # print(eval_results)\n","```\n","Okay, let's create a Python function using Hugging Face libraries to train a DistilBERT model on the IMDb dataset for binary sentiment classification.\n","\n","We'll go through the necessary steps:\n","1.  Loading the dataset.\n","2.  Loading the tokenizer for the chosen model.\n","3.  Preprocessing the text data (tokenization).\n","4.  Loading the pre-trained model configured for sequence classification.\n","5.  Setting up training arguments.\n","6.  Defining metrics for evaluation.\n","7.  Creating and using the Hugging Face `Trainer`.\n","\n","Here's the code with step-by-step explanations:\n","\n","```python\n","import os\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n","from datasets import load_dataset\n","import evaluate\n","import numpy as np\n","import torch\n","\n","# Define the model name and dataset name\n","MODEL_NAME = \"distilbert/distilbert-base-uncased\"\n","DATASET_NAME = \"imdb\"\n","# Define the output directory for saving checkpoints and results\n","OUTPUT_DIR = \"./distilbert_imdb_sentiment\"\n","\n","def train_distilbert_imdb(model_name: str = MODEL_NAME,\n","                          dataset_name: str = DATASET_NAME,\n","                          output_dir: str = OUTPUT_DIR,\n","                          num_train_epochs: int = 3,\n","                          per_device_train_batch_size: int = 16,\n","                          per_device_eval_batch_size: int = 16,\n","                          learning_rate: float = 2e-5,\n","                          weight_decay: float = 0.01,\n","                          logging_steps: int = 500,\n","                          eval_strategy: str = \"epoch\"):\n","    \"\"\"\n","    Trains a DistilBERT model on the IMDb dataset for binary sentiment classification.\n","\n","    Args:\n","        model_name (str): The name of the pre-trained model from Hugging Face Hub.\n","                          Defaults to 'distilbert/distilbert-base-uncased'.\n","        dataset_name (str): The name of the dataset from Hugging Face Hub.\n","                            Defaults to 'imdb'.\n","        output_dir (str): Directory to save model checkpoints and results.\n","        num_train_epochs (int): Number of training epochs.\n","        per_device_train_batch_size (int): Batch size per device for training.\n","        per_device_eval_batch_size (int): Batch size per device for evaluation.\n","        learning_rate (float): Learning rate for the optimizer.\n","        weight_decay (float): Weight decay for the optimizer.\n","        logging_steps (int): Number of steps between logging.\n","        eval_strategy (str): Evaluation strategy ('no', 'steps', 'epoch').\n","    \"\"\"\n","\n","    print(f\"--- Starting training for {model_name} on {dataset_name} ---\")\n","    print(f\"Output directory: {os.path.abspath(output_dir)}\")\n","\n","    # Step 1: Load the dataset\n","    # We load the train and test splits of the IMDb dataset from the Hugging Face datasets library.\n","    # The dataset structure for IMDb is typically {'text': ..., 'label': ...} where label is 0 for negative and 1 for positive.\n","    print(\"Loading dataset...\")\n","    dataset = load_dataset(dataset_name)\n","    print(\"Dataset loaded successfully.\")\n","    print(f\"Train dataset size: {len(dataset['train'])}\")\n","    print(f\"Test dataset size: {len(dataset['test'])}\")\n","    print(\"Example from dataset:\", dataset['train'][0])\n","\n","    # Step 2: Load the tokenizer\n","    # The tokenizer is responsible for converting text into numerical IDs that the model understands.\n","    # We load the tokenizer corresponding to the chosen pre-trained model.\n","    # use_fast=True provides faster tokenization if available for the model.\n","    print(f\"Loading tokenizer for {model_name}...\")\n","    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n","    print(\"Tokenizer loaded successfully.\")\n","\n","    # Step 3: Preprocess the data (Tokenization)\n","    # Models like BERT/DistilBERT require input in a specific format: sequences of token IDs.\n","    # This function takes a batch of examples, tokenizes the 'text' field, and returns the results.\n","    # 'truncation=True' ensures that sequences longer than the model's max length are cut off.\n","    # 'padding=True' pads shorter sequences to the maximum length within the batch (or to the model's max length if max_length is specified).\n","    # 'padding=True' defaults to 'longest' which pads to the longest sequence in the batch. This is efficient.\n","    # The 'label' column is automatically handled by the Trainer if it exists.\n","    def tokenize_function(examples):\n","        return tokenizer(examples[\"text\"], truncation=True, padding=True)\n","\n","    print(\"Tokenizing dataset...\")\n","    # We apply the tokenization function to the entire dataset using the map method.\n","    # batched=True allows the function to process multiple examples at once, which is much faster.\n","    tokenized_dataset = dataset.map(tokenize_function, batched=True)\n","    print(\"Dataset tokenization complete.\")\n","\n","    # Step 4: Load the model\n","    # We load the pre-trained DistilBERT model.\n","    # AutoModelForSequenceClassification adds a classification head on top of the base model.\n","    # num_labels=2 specifies that this is a binary classification task (positive/negative).\n","    # This function automatically downloads the pre-trained weights.\n","    print(f\"Loading model {model_name} for sequence classification...\")\n","    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n","    print(\"Model loaded successfully.\")\n","\n","    # Step 5: Define training arguments\n","    # TrainingArguments specifies all the parameters for the training process.\n","    # output_dir: Where to save checkpoints and the final model.\n","    # num_train_epochs: How many times to iterate over the training data.\n","    # per_device_*: Batch size for training and evaluation.\n","    # learning_rate: Step size for optimizer updates.\n","    # weight_decay: Regularization parameter.\n","    # evaluation_strategy: When to run evaluation (e.g., after every epoch).\n","    # logging_dir: Where to save logs (for TensorBoard).\n","    # logging_steps: How often to log training metrics.\n","    print(\"Defining training arguments...\")\n","    training_args = TrainingArguments(\n","        output_dir=output_dir,\n","        num_train_epochs=num_train_epochs,\n","        per_device_train_batch_size=per_device_train_batch_size,\n","        per_device_eval_batch_size=per_device_eval_batch_size,\n","        learning_rate=learning_rate,\n","        weight_decay=weight_decay,\n","        evaluation_strategy=eval_strategy, # Evaluate at the end of each epoch\n","        logging_dir=f\"{output_dir}/logs\",\n","        logging_steps=logging_steps,\n","        save_strategy=eval_strategy, # Save checkpoint at the end of each epoch\n","        load_best_model_at_end=True, # Load the best model based on evaluation metric at the end\n","        metric_for_best_model=\"accuracy\", # Metric to monitor for best model\n","        greater_is_better=True, # For accuracy, higher is better\n","        report_to=\"tensorboard\", # Report metrics to TensorBoard\n","        # auto_find_batch_size=True # Uncomment this if you want the trainer to find a good batch size\n","                                   # Potentially useful if you get CUDA out of memory errors\n","    )\n","    print(\"Training arguments defined.\")\n","    print(f\"Evaluation strategy: {training_args.evaluation_strategy}\")\n","\n","    # Step 6: Define metrics for evaluation\n","    # We need a function to compute metrics like accuracy, precision, recall, F1-score.\n","    # The evaluate library provides easy access to standard metrics.\n","    # This function will be called by the Trainer during evaluation.\n","    print(\"Loading evaluation metric...\")\n","    accuracy_metric = evaluate.load(\"accuracy\")\n","    # We could load others like precision, recall, f1 simultaneously if needed\n","    # precision_metric = evaluate.load(\"precision\")\n","    # recall_metric = evaluate.load(\"recall\")\n","    # f1_metric = evaluate.load(\"f1\")\n","\n","    def compute_metrics(eval_pred):\n","        # eval_pred is a tuple containing predictions and labels\n","        predictions, labels = eval_pred\n","        # For classification, the predictions are logits. We take the argmax to get the predicted class ID.\n","        predictions = np.argmax(predictions, axis=1)\n","        # compute() method calculates the metric score\n","        accuracy = accuracy_metric.compute(predictions=predictions, references=labels)\n","        # We can compute other metrics here as well\n","        # precision = precision_metric.compute(predictions=predictions, references=labels, average=\"binary\") # average=\"binary\" for binary classification\n","        # recall = recall_metric.compute(predictions=predictions, references=labels, average=\"binary\")\n","        # f1 = f1_metric.compute(predictions=predictions, references=labels, average=\"binary\")\n","        # Return a dictionary of metrics\n","        return {\n","            \"accuracy\": accuracy[\"accuracy\"],\n","            # \"precision\": precision[\"precision\"],\n","            # \"recall\": recall[\"recall\"],\n","            # \"f1\": f1[\"f1\"],\n","        }\n","\n","    print(\"Compute metrics function defined.\")\n","\n","    # Step 7: Create and use the Trainer\n","    # The Trainer class simplifies the training loop significantly.\n","    # It handles optimizer, scheduler, device placement, logging, evaluation, saving, etc.\n","    # We pass the model, training arguments, tokenized datasets, tokenizer, and compute_metrics function.\n","    print(\"Initializing Trainer...\")\n","    trainer = Trainer(\n","        model=model,                              # The loaded model\n","        args=training_args,                       # The training arguments\n","        train_dataset=tokenized_dataset[\"train\"], # The training dataset\n","        eval_dataset=tokenized_dataset[\"test\"],   # The evaluation dataset\n","        tokenizer=tokenizer,                      # The tokenizer (used for padding/truncation within Trainer if needed)\n","        compute_metrics=compute_metrics           # The function to compute evaluation metrics\n","    )\n","    print(\"Trainer initialized.\")\n","\n","    # Check if GPU is available\n","    if torch.cuda.is_available():\n","        print(\"GPU available. Using GPU for training.\")\n","    else:\n","        print(\"GPU not available. Using CPU for training. Training will be slower.\")\n","\n","    # Start the training process\n","    print(\"Starting training...\")\n","    train_result = trainer.train()\n","    print(\"Training finished.\")\n","\n","    # Evaluate the model after training\n","    print(\"Evaluating the model on the test set...\")\n","    eval_results = trainer.evaluate()\n","    print(\"Evaluation results:\", eval_results)\n","\n","    # Save the final model and tokenizer\n","    print(f\"Saving final model and tokenizer to {output_dir}...\")\n","    trainer.save_model(output_dir)\n","    tokenizer.save_pretrained(output_dir)\n","    print(\"Model and tokenizer saved.\")\n","\n","    # Optional: Print some summary of the training\n","    metrics = train_result.metrics\n","    print(\"Training Metrics:\", metrics)\n","    trainer.log_metrics(\"train\", metrics)\n","    trainer.save_metrics(\"train\", metrics)\n","    trainer.save_state()\n","\n","    print(\"--- Training complete ---\")\n","    return trainer, eval_results\n","\n","# Example usage:\n","if __name__ == \"__main__\":\n","    # Ensure output directory exists\n","    os.makedirs(OUTPUT_DIR, exist_ok=True)\n","\n","    # You can call the function with default arguments or customize them\n","    trainer, eval_results = train_distilbert_imdb(\n","        num_train_epochs=3, # Train for 3 epochs\n","        per_device_train_batch_size=16, # Smaller batch size if memory is an issue\n","        per_device_eval_batch_size=16,\n","        learning_rate=2e-5,\n","        output_dir=\"./distilbert_imdb_sentiment_run\" # Use a specific run directory\n","    )\n","\n","    print(\"\\nTraining and evaluation process completed.\")\n","    print(f\"Final evaluation results: {eval_results}\")\n","    print(f\"Model saved to: {os.path.abspath('./distilbert_imdb_sentiment_run')}\")\n","\n","    # Example of loading the trained model for inference:\n","    # from transformers import pipeline\n","    # # Need to map label IDs to names if you want text output\n","    # id2label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n","    # label2id = {\"NEGATIVE\": 0, \"POSITIVE\": 1}\n","    # classifier = pipeline(\n","    #     \"sentiment-analysis\",\n","    #     model=\"./distilbert_imdb_sentiment_run\",\n","    #     tokenizer=\"./distilbert_imdb_sentiment_run\",\n","    #     id2label=id2label,\n","    #     device=0 if torch.cuda.is_available() else -1 # Use GPU if available\n","    # )\n","    # print(\"\\nExample inference:\")\n","    # result = classifier(\"This movie was absolutely fantastic and I loved every minute of it!\")\n","    # print(result)\n","    # result = classifier(\"This movie was terrible and boring.\")\n","    # print(result)\n","```\n","\n","### Explanation of Steps:\n","\n","1.  **Import necessary libraries:** We import `AutoTokenizer`, `AutoModelForSequenceClassification`, `Trainer`, `TrainingArguments` from `transformers`, `load_dataset` from `datasets`, `evaluate` for metrics, `numpy` for processing predictions, and `torch` to check for GPU.\n","2.  **Define Constants:** `MODEL_NAME` and `DATASET_NAME` are set for clarity. `OUTPUT_DIR` specifies where the trained model and training artifacts will be saved.\n","3.  **Define `train_distilbert_imdb` Function:** This encapsulates the entire training process. It takes parameters to make it flexible (e.g., changing epochs, batch size, output directory).\n","4.  **Load Dataset (`load_dataset`):** This is the easiest way to get standard datasets. `load_dataset(\"imdb\")` downloads and loads the IMDb dataset, which is conveniently split into 'train' and 'test'. It has 'text' and 'label' columns, perfect for our task.\n","5.  **Load Tokenizer (`AutoTokenizer.from_pretrained`):** We load the tokenizer corresponding to `distilbert-base-uncased`. The tokenizer's job is to convert raw text into token IDs that the model can understand, handling things like special tokens (`[CLS]`, `[SEP]`), subword splitting, and mapping tokens to integers. `use_fast=True` is generally recommended.\n","6.  **Define and Apply Preprocessing (`tokenize_function`, `dataset.map`):**\n","    *   We define a function `tokenize_function` that takes a batch of examples.\n","    *   Inside `tokenize_function`, `tokenizer(examples[\"text\"], truncation=True, padding=True)` processes the text.\n","        *   `truncation=True` cuts off sequences longer than the model's maximum input size (usually 512 tokens for BERT-like models).\n","        *   `padding=True` adds special padding tokens (`[PAD]`) to shorter sequences to make them all the same length within a batch. This is necessary because models process fixed-size tensors.\n","    *   `dataset.map(tokenize_function, batched=True)` applies this function efficiently across the entire dataset. `batched=True` means the function receives multiple examples at once, speeding up processing. The results (input IDs, attention masks, etc.) are added as new columns to the dataset. The original 'label' column remains.\n","7.  **Load Model (`AutoModelForSequenceClassification.from_pretrained`):** We load the pre-trained `distilbert-base-uncased` model. `AutoModelForSequenceClassification` automatically adds a classification layer on top of the base model. `num_labels=2` configures this layer for binary classification. The model is initialized with the pre-trained weights from the base model, and the new classification head is randomly initialized.\n","8.  **Define Training Arguments (`TrainingArguments`):** This class holds all hyperparameter settings for training (learning rate, number of epochs, batch size, logging frequency, evaluation strategy, saving strategy, etc.). `output_dir` is crucial as the trainer saves checkpoints and the final model there. We set `evaluation_strategy=\"epoch\"` and `save_strategy=\"epoch\"` to evaluate and save a checkpoint after each training epoch. `load_best_model_at_end=True` is useful to ensure the best performing model on the evaluation set is loaded back at the end of training.\n","9.  **Define Metrics (`compute_metrics`, `evaluate.load`):**\n","    *   We load the standard \"accuracy\" metric using the `evaluate` library.\n","    *   `compute_metrics` is a function required by the `Trainer`. It takes the model's predictions and the true labels.\n","    *   Model predictions are typically raw scores (logits). We use `np.argmax(predictions, axis=1)` to get the predicted class ID (0 or 1).\n","    *   We use the loaded metric objects (`accuracy_metric.compute()`) to calculate the desired scores and return them in a dictionary.\n","10. **Create Trainer (`Trainer`)**: This is the core class for training. We instantiate it by passing the model, training arguments, the tokenized train and test datasets, the tokenizer (often used internally by the trainer for things like dynamic padding), and the `compute_metrics` function.\n","11. **Start Training (`trainer.train()`):** Calling this method starts the training loop. The `Trainer` handles moving data to the GPU (if available), optimizing the model parameters, running evaluation periodically based on `evaluation_strategy`, logging, and saving checkpoints.\n","12. **Evaluate After Training (`trainer.evaluate()`):** Although evaluation runs during training, calling `evaluate()` after `train()` gives you the final performance metrics on the evaluation set.\n","13. **Save Model (`trainer.save_model()`):** Saves the final trained model weights and configuration to the specified `output_dir`.\n","14. **Save Tokenizer (`tokenizer.save_pretrained()`):** Saves the tokenizer's configuration and vocabulary to the same directory. This is important so that when you load the model later, you use the *exact* same tokenizer that was used during training.\n","15. **Example Usage (`if __name__ == \"__main__\":`)**: This block demonstrates how to call the training function. It also includes commented-out code showing how you could load the trained model using a `pipeline` for inference.\n","\n","To run this code, you need to install the required libraries:\n","```bash\n","pip install transformers datasets evaluate accelerate torch matplotlib\n","```\n","(`accelerate` is recommended for faster training, `matplotlib` is needed by `report_to=\"tensorboard\"`)\n","Processing row 5/8...\n","```python\n","import tensorflow as tf\n","from tensorflow import keras\n","import numpy as np\n","import os\n","\n","# Load the Fashion MNIST dataset\n","fashion_mnist = keras.datasets.fashion_mnist\n","(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n","\n","# Preprocess the data\n","train_images = train_images / 255.0\n","test_images = test_images / 255.0\n","\n","# Reshape images for convolutional layers (add channel dimension)\n","# Assuming channels last format\n","train_images = train_images.reshape((train_images.shape[0], 28, 28, 1))\n","test_images = test_images.reshape((test_images.shape[0], 28, 28, 1))\n","\n","# Define the model architecture\n","model = keras.Sequential([\n","    keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n","    keras.layers.MaxPooling2D((2, 2)),\n","    keras.layers.Conv2D(64, (3, 3), activation='relu'),\n","    keras.layers.MaxPooling2D((2, 2)),\n","    keras.layers.Flatten(),\n","    keras.layers.Dense(128, activation='relu'),\n","    keras.layers.Dense(10, activation='softmax')\n","])\n","\n","# Compile the model\n","model.compile(optimizer='adam',\n","              loss='sparse_categorical_crossentropy',\n","              metrics=['accuracy'])\n","\n","# Define Callbacks\n","# Early Stopping: Stop training when validation loss has not improved for 5 epochs\n","early_stopping = keras.callbacks.EarlyStopping(\n","    monitor='val_loss',\n","    patience=5,\n","    restore_best_weights=True\n",")\n","\n","# Model Checkpoint: Save the best model based on validation loss\n","checkpoint_path = \"fashion_mnist_checkpoint/cp.ckpt\"\n","checkpoint_dir = os.path.dirname(checkpoint_path)\n","\n","# Create checkpoint directory if it does not exist\n","if not os.path.exists(checkpoint_dir):\n","    os.makedirs(checkpoint_dir)\n","\n","model_checkpoint = keras.callbacks.ModelCheckpoint(\n","    filepath=checkpoint_path,\n","    save_weights_only=False, # Set to True if you only want to save weights\n","    monitor='val_loss',\n","    save_best_only=True\n",")\n","\n","# Train the model with callbacks\n","history = model.fit(\n","    train_images,\n","    train_labels,\n","    epochs=50, # Set a reasonably large number, EarlyStopping will stop it\n","    validation_split=0.2, # Use part of training data for validation\n","    callbacks=[early_stopping, model_checkpoint]\n",")\n","\n","# Evaluate the model on the test set\n","test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)\n","print(f'\\nTest accuracy: {test_acc}')\n","\n","# Optionally, load the best model weights manually if save_weights_only=True\n","# model.load_weights(checkpoint_path)\n","\n","# You can now use the trained model for predictions\n","# predictions = model.predict(test_images)\n","```\n","Okay, let's build a TensorFlow model for Fashion MNIST classification, incorporating Early Stopping and Model Checkpointing. We'll go step-by-step, explaining each part of the code and the reasoning behind it.\n","\n","**Goal:** Train an image classification model on Fashion MNIST using TensorFlow/Keras, stopping training early if validation performance plateaus and saving the model with the best validation performance.\n","\n","**Steps:**\n","\n","1.  **Import Libraries:** Get the necessary tools from TensorFlow and Keras.\n","2.  **Load and Prepare Data:** Download the Fashion MNIST dataset and preprocess it for model training.\n","3.  **Build the Model:** Define the neural network architecture.\n","4.  **Compile the Model:** Configure the training process (optimizer, loss, metrics).\n","5.  **Define Callbacks:** Set up Early Stopping and Model Checkpointing.\n","6.  **Train the Model:** Start the training process with the data and callbacks.\n","7.  **Evaluate the Model:** Check the performance on the test set.\n","8.  **Load and Use the Best Model:** Demonstrate loading the saved best model.\n","\n","Let's start coding and explaining!\n","\n","```python\n","# Step 1: Import Libraries\n","import tensorflow as tf\n","from tensorflow import keras\n","import numpy as np\n","import os # To create directories for checkpoints\n","\n","# Print TensorFlow version to ensure it's installed\n","print(\"TensorFlow version:\", tf.__version__)\n","```\n","\n","*   **Explanation:** We import `tensorflow` as `tf` and `tensorflow.keras` as `keras`, which provides the high-level API for building and training models. `numpy` is useful for data manipulation, and `os` will help us manage the directory for saving model checkpoints. We also print the TensorFlow version for confirmation.\n","\n","```python\n","# Step 2: Load and Prepare Data\n","\n","# Load the Fashion MNIST dataset\n","fashion_mnist = keras.datasets.fashion_mnist\n","(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n","\n","# Define class names (for later use, not strictly needed for training)\n","class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n","               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n","\n","# Data Preprocessing\n","\n","# Normalize pixel values to be between 0 and 1\n","# The images are 28x28 arrays with pixel values from 0 to 255\n","train_images = train_images / 255.0\n","test_images = test_images / 255.0\n","\n","# Add a channel dimension to the images\n","# CNNs typically expect input shape (height, width, channels)\n","# For grayscale, the channel is 1. (28, 28) -> (28, 28, 1)\n","train_images = train_images[..., tf.newaxis]\n","test_images = test_images[..., tf.newaxis]\n","\n","# --- Data Splitting for Validation ---\n","# Callbacks like EarlyStopping/ModelCheckpoint monitor a validation set.\n","# We'll split the original training data into training and validation sets.\n","# A common split is 80/20 or 90/10. Let's use 5000 images for validation.\n","num_validation_samples = 5000\n","\n","val_images = train_images[:num_validation_samples]\n","val_labels = train_labels[:num_validation_samples]\n","\n","train_images = train_images[num_validation_samples:]\n","train_labels = train_labels[num_validation_samples:]\n","\n","print(\"Training images shape:\", train_images.shape)\n","print(\"Training labels shape:\", train_labels.shape)\n","print(\"Validation images shape:\", val_images.shape)\n","print(\"Validation labels shape:\", val_labels.shape)\n","print(\"Test images shape:\", test_images.shape)\n","print(\"Test labels shape:\", test_labels.shape)\n","```\n","\n","*   **Explanation:**\n","    *   `fashion_mnist.load_data()`: Downloads the dataset and splits it into training and testing sets. Each set contains images and their corresponding labels.\n","    *   `train_images / 255.0`, `test_images / 255.0`: Neural networks perform better when input features are on a similar scale. We normalize the pixel values from the range [0, 255] to [0, 1] by dividing by the maximum possible value (255).\n","    *   `[..., tf.newaxis]`: Convolutional layers in Keras expect an input shape with a channel dimension. For grayscale images, this channel dimension is 1. The original images are shape `(num_samples, height, width)`. We reshape them to `(num_samples, height, width, 1)`.\n","    *   **Validation Split:** It's crucial to have a separate validation set that the model *does not train on directly* but whose performance is monitored. This gives an unbiased estimate of how well the model generalizes *during* training, which is what Early Stopping and Model Checkpointing use. We reserve the first `num_validation_samples` from the original training data for validation.\n","\n","```python\n","# Step 3: Build the Model\n","\n","# We'll use a simple Convolutional Neural Network (CNN) as it's effective for images.\n","model = keras.Sequential([\n","    # Convolutional Layer 1\n","    # Filters: Number of output filters (features). 32 is a common starting point.\n","    # Kernel Size: Dimensions of the convolution window (3x3 here).\n","    # Activation: ReLU (Rectified Linear Unit) introduces non-linearity.\n","    # Input Shape: Required for the first layer to know the input dimensions.\n","    keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n","\n","    # Max Pooling Layer 1\n","    # Pool Size: Downsamples the input by taking the maximum value over a window (2x2 here).\n","    # Reduces spatial dimensions, helps with translation invariance.\n","    keras.layers.MaxPooling2D((2, 2)),\n","\n","    # Convolutional Layer 2\n","    # More filters to potentially learn more complex features.\n","    keras.layers.Conv2D(64, (3, 3), activation='relu'),\n","\n","    # Max Pooling Layer 2\n","    keras.layers.MaxPooling2D((2, 2)),\n","\n","    # Flatten Layer\n","    # Flattens the 2D feature maps into a 1D vector to feed into the dense layers.\n","    keras.layers.Flatten(),\n","\n","    # Dense (Fully Connected) Layer\n","    # Standard dense layer with ReLU activation.\n","    keras.layers.Dense(128, activation='relu'),\n","\n","    # Output Layer\n","    # 10 units because there are 10 classes in Fashion MNIST.\n","    # Softmax activation ensures the output is a probability distribution over the classes.\n","    keras.layers.Dense(10, activation='softmax')\n","])\n","\n","# Print the model summary\n","model.summary()\n","```\n","\n","*   **Explanation:**\n","    *   `keras.Sequential`: A simple way to build a model as a stack of layers.\n","    *   `Conv2D`: Applies convolutional filters to the input. This layer learns spatial hierarchies of features (e.g., edges, textures).\n","    *   `MaxPooling2D`: Downsamples the feature maps, reducing computational cost and making the network more robust to small translations in the input.\n","    *   `Flatten`: Converts the 2D output of the convolutional/pooling layers into a 1D vector, which is needed for the standard dense layers.\n","    *   `Dense`: A standard fully connected neural network layer.\n","    *   `activation='relu'`: The Rectified Linear Unit activation is common in hidden layers. It helps the model learn non-linear relationships.\n","    *   `Dense(10, activation='softmax')`: The final layer has 10 units (one for each class). `softmax` activation outputs a probability distribution, where the sum of probabilities for all classes is 1. The class with the highest probability is the model's prediction.\n","    *   `model.summary()`: Prints a helpful table showing the layers, their output shapes, and the number of parameters.\n","\n","```python\n","# Step 4: Compile the Model\n","\n","# Configure the model for training\n","model.compile(optimizer='adam', # Optimizer: How the model updates weights based on loss\n","              loss='sparse_categorical_crossentropy', # Loss function: Measures how accurate the model is during training\n","                                                       # Use sparse_categorical_crossentropy when labels are integers (0-9)\n","                                                       # Use categorical_crossentropy if labels are one-hot encoded\n","              metrics=['accuracy']) # Metrics: Used to monitor the training and testing steps (e.g., accuracy)\n","```\n","\n","*   **Explanation:**\n","    *   `model.compile()`: Configures the learning process before training.\n","    *   `optimizer='adam'`: A popular and generally effective optimization algorithm. It adapts the learning rate during training.\n","    *   `loss='sparse_categorical_crossentropy'`: This is the standard loss function for multi-class classification where the labels are integers (like 0, 1, 2...). It calculates the difference between the predicted probability distribution (from the softmax layer) and the true class label.\n","    *   `metrics=['accuracy']`: We want to track the classification accuracy during training and evaluation.\n","\n","```python\n","# Step 5: Define Callbacks\n","\n","# --- Early Stopping Callback ---\n","# Stop training when the monitored metric has stopped improving.\n","early_stopping_callback = keras.callbacks.EarlyStopping(\n","    monitor='val_loss', # Metric to monitor (validation loss)\n","    patience=5,         # Number of epochs with no improvement after which training will be stopped.\n","                        # If val_loss doesn't decrease for 5 consecutive epochs, stop.\n","    mode='min',         # 'min' means stop when the monitored quantity stops decreasing.\n","    verbose=1           # Print message when stopping\n",")\n","\n","# --- Model Checkpoint Callback ---\n","# Save the model (or weights) during training.\n","# We'll save the model with the best validation accuracy.\n","\n","# Create a directory to save checkpoints\n","checkpoint_dir = './training_checkpoints'\n","os.makedirs(checkpoint_dir, exist_ok=True) # Create directory if it doesn't exist\n","\n","# Define the checkpoint file path\n","# Use '{epoch:02d}-{val_loss:.2f}' to include epoch number and validation loss in the filename\n","checkpoint_filepath = os.path.join(checkpoint_dir, 'best_model_epoch_{epoch:02d}_val_loss_{val_loss:.2f}')\n","\n","\n","model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n","    filepath=checkpoint_filepath,\n","    monitor='val_loss',      # Metric to monitor for saving (validation loss)\n","    save_best_only=True,     # Only save the model when the monitored metric improves.\n","                             # This saves the model with the lowest validation loss found during training.\n","    mode='min',              # 'min' means save when the monitored quantity decreases.\n","    save_weights_only=False, # If False, saves the entire model (structure + weights).\n","                             # If True, only saves the weights. Saving the entire model is often easier.\n","    verbose=1                # Print message when saving\n",")\n","\n","# Create a list of callbacks to pass to model.fit()\n","callbacks_list = [early_stopping_callback, model_checkpoint_callback]\n","```\n","\n","*   **Explanation:**\n","    *   **EarlyStopping:** This callback monitors a specified metric (`val_loss` in this case) on the validation data. If this metric does not improve (decrease) for `patience` number of epochs, training is stopped. This prevents overfitting and saves computation time once the model has learned as much as it can effectively from the training data.\n","    *   **ModelCheckpoint:** This callback saves the model during training.\n","        *   `filepath`: Specifies where to save the model files. We include `{epoch:02d}` and `{val_loss:.2f}` as format strings, which will be replaced by the current epoch number and validation loss when the file is saved.\n","        *   `monitor='val_loss'`: We monitor the validation loss.\n","        *   `save_best_only=True`: This is crucial when combined with early stopping. It ensures that the checkpoint file is only updated when the `val_loss` *improves* (decreases). So, even if early stopping stops training after 100 epochs, the saved checkpoint will be the model from the epoch (maybe epoch 95) that had the *best* validation loss, not necessarily the final epoch.\n","        *   `mode='min'`: Because we're monitoring loss, we want the *minimum* value, so we set mode to 'min'.\n","        *   `save_weights_only=False`: We choose to save the entire model (architecture, weights, and optimizer state) rather than just the weights. This makes it easier to load and use the model later.\n","\n","```python\n","# Step 6: Train the Model\n","\n","# Define the number of epochs to train for.\n","# Since we have Early Stopping, we can set this to a reasonably large number.\n","# The model will stop training automatically when needed.\n","EPOCHS = 50 # Start with a value larger than patience, but not excessively large\n","\n","# Define the batch size\n","BATCH_SIZE = 32\n","\n","print(\"\\nStarting training...\")\n","history = model.fit(train_images, train_labels,\n","                    epochs=EPOCHS,\n","                    batch_size=BATCH_SIZE,\n","                    validation_data=(val_images, val_labels), # Provide validation data\n","                    callbacks=callbacks_list)                  # Pass the list of callbacks\n","\n","print(\"\\nTraining finished.\")\n","```\n","\n","*   **Explanation:**\n","    *   `model.fit()`: This is where the actual training happens.\n","    *   `train_images`, `train_labels`: The training data.\n","    *   `epochs=EPOCHS`: The maximum number of times the model will iterate over the entire training dataset. Early Stopping will likely stop it before reaching this if performance plateaus.\n","    *   `batch_size=BATCH_SIZE`: The number of samples per gradient update.\n","    *   `validation_data=(val_images, val_labels)`: **Crucially**, we provide the validation data here. Keras will automatically evaluate the model on this data at the end of each epoch and calculate `val_loss` and `val_accuracy`, which are needed by our callbacks.\n","    *   `callbacks=callbacks_list`: We pass the list containing our Early Stopping and Model Checkpoint callbacks. Keras automatically calls these at appropriate times during training (e.g., at the end of each epoch).\n","    *   `history`: The `fit` method returns a `History` object, which contains a record of loss and metric values during training.\n","\n","```python\n","# Step 7: Evaluate the Model on the Test Set\n","\n","print(\"\\nEvaluating the model on the test set...\")\n","test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)\n","\n","print('\\nTest accuracy:', test_acc)\n","print('Test loss:', test_loss)\n","```\n","\n","*   **Explanation:**\n","    *   `model.evaluate()`: Calculates the loss and metrics on the provided test data. This is the final, unbiased evaluation of the model's performance on data it has never seen during training *or* validation.\n","    *   We use the *currently trained* model (which might have been stopped early). While this is a valid evaluation, it's often better practice to load the `save_best_only` model saved by the checkpoint callback, as that model had the best performance on the validation set encountered during the entire run.\n","\n","```python\n","# Step 8: Load and Use the Best Model\n","\n","# Find the path to the best model saved by the checkpoint\n","# ModelCheckpoint saves files like 'best_model_epoch_XX_val_loss_Y.YY'\n","# We need to find the file with the lowest validation loss among those saved.\n","# Since save_best_only=True, there should ideally be only one such file at the end,\n","# corresponding to the minimum val_loss achieved.\n","# Let's list files and find the one that matches the pattern\n","\n","import glob\n","\n","list_of_files = glob.glob(os.path.join(checkpoint_dir, 'best_model_epoch_*'))\n","# Sort by modification time to get the latest file saved, assuming save_best_only\n","# is effectively overwriting (though with the epoch in the name, it creates new ones)\n","# A more robust way is to track the best val_loss ourselves or parse filenames.\n","# However, with save_best_only and unique filenames per epoch, sorting by time or just taking the list\n","# of files saved should give us the relevant checkpoint(s).\n","# Let's assume the file with the lowest loss *name* is the best one among those saved.\n","# A simpler way is to just load from the *latest* checkpoint if save_best_only\n","# was combined with a non-unique filename, but here filenames are unique.\n","# Let's load the first file found that matches the pattern, assuming save_best_only\n","# made sure this is the best one saved.\n","\n","# A better approach: explicitly define the best file path pattern if not using epoch numbers\n","# For simplicity with epoch numbers, let's just load one of the saved checkpoints.\n","# If save_best_only=True, any saved checkpoint should represent an improvement.\n","# The *last* saved file might not be the *best* because Early Stopping kicked in.\n","# The filename format includes the loss, we could parse that.\n","# Or, if save_best_only was used with a constant filename (e.g., 'best_model.h5'),\n","# that file would *be* the best one automatically.\n","# Let's modify the checkpoint filepath to be constant when saving the best.\n","\n","# New checkpoint filepath for the single best model file\n","best_checkpoint_filepath = os.path.join(checkpoint_dir, 'best_model.h5')\n","\n","# Redefine the ModelCheckpoint callback to save to a single file\n","model_checkpoint_callback_best = keras.callbacks.ModelCheckpoint(\n","    filepath=best_checkpoint_filepath,\n","    monitor='val_loss',      # Metric to monitor\n","    save_best_only=True,     # ONLY save the model when the monitored metric improves\n","    mode='min',              # Minimize validation loss\n","    save_weights_only=False, # Save the entire model\n","    verbose=1\n",")\n","\n","# Rerun training with the simplified checkpoint path (optional, just demonstrating)\n","# In a real scenario, you'd likely just use the simpler path from the start.\n","# Or load the best one from the epoch-based names saved earlier.\n","# For demonstration, let's show loading *a* saved model.\n","# Let's assume the last checkpoint saved *before* early stopping was the best.\n","# This is not guaranteed if save_best_only was used with epoch names and early stopping triggered.\n","# Let's just load one of the existing checkpoints saved in the first run.\n","\n","# Let's find the path of the checkpoint with the LOWEST validation loss from the first run\n","# This requires parsing the filenames.\n","import re\n","\n","best_loss_so_far = float('inf')\n","best_model_path = None\n","\n","for filepath in glob.glob(os.path.join(checkpoint_dir, 'best_model_epoch_*_val_loss_*.h5')):\n","    # Extract val_loss from filename using regex\n","    match = re.search(r'_val_loss_(\\d+\\.\\d+)', filepath)\n","    if match:\n","        current_loss = float(match.group(1))\n","        if current_loss < best_loss_so_far:\n","            best_loss_so_far = current_loss\n","            best_model_path = filepath\n","\n","print(f\"\\nIdentified best model path based on filename: {best_model_path}\")\n","\n","# Load the best model found during training\n","if best_model_path:\n","    print(f\"Loading model from: {best_model_path}\")\n","    loaded_model = keras.models.load_model(best_model_path)\n","\n","    # Evaluate the loaded model on the test set\n","    print(\"\\nEvaluating the LOADED best model on the test set...\")\n","    test_loss_loaded, test_acc_loaded = loaded_model.evaluate(test_images, test_labels, verbose=2)\n","\n","    print('\\nLoaded Model Test accuracy:', test_acc_loaded)\n","    print('Loaded Model Test loss:', test_loss_loaded)\n","else:\n","    print(\"No checkpoint files found.\")\n","\n","# Example prediction with the loaded model\n","if loaded_model:\n","    print(\"\\nMaking predictions with the loaded model...\")\n","    predictions = loaded_model.predict(test_images[:5]) # Predict on the first 5 test images\n","    print(\"Predictions shape:\", predictions.shape)\n","\n","    # Show the predicted class for the first 5 images\n","    predicted_classes = np.argmax(predictions, axis=1)\n","    print(\"Predicted classes (first 5):\", predicted_classes)\n","    print(\"True classes (first 5):\", test_labels[:5])\n","\n","    # Map predicted class indices to names\n","    predicted_class_names = [class_names[i] for i in predicted_classes]\n","    true_class_names = [class_names[i] for i in test_labels[:5]]\n","\n","    print(\"Predicted class names (first 5):\", predicted_class_names)\n","    print(\"True class names (first 5):\", true_class_names)\n","\n","```\n","\n","*   **Explanation:**\n","    *   Loading the `save_best_only` model is crucial because Early Stopping stops training based on validation performance. The epoch right before stopping might not have had the absolute best validation performance; the best might have occurred a few epochs earlier. Loading the saved 'best' model ensures you are using the weights from the epoch that performed best on the validation set.\n","    *   We use `glob` to find all files matching our checkpoint pattern in the directory.\n","    *   Since our filename pattern includes the loss, we can parse the filenames to find the one with the minimum validation loss saved. This is a slightly more robust way to find the \"best\" one when using epoch numbers in the filename and `save_best_only`. *A simpler strategy for `save_best_only` is to use a constant filename like `best_model.h5`; in that case, the `best_model.h5` file will always contain the weights corresponding to the lowest validation loss found so far.*\n","    *   `keras.models.load_model()`: Loads the entire model (architecture and weights) from the saved file.\n","    *   We then evaluate this loaded model on the test set again to confirm its performance. The test accuracy of the loaded model should ideally be close to or slightly better than the accuracy of the model from the end of the `model.fit` call, as it represents the best generalization capability found during the training process on the validation set.\n","    *   Finally, we demonstrate how to use the loaded model to make predictions on new data. `model.predict()` returns the output of the softmax layer (probabilities for each class). `np.argmax` finds the index of the highest probability, which corresponds to the predicted class.\n","\n","This comprehensive script covers loading data, building and compiling a CNN, implementing Early Stopping and Model Checkpointing callbacks, training the model with these callbacks, evaluating the final model, and demonstrating how to load the best model checkpoint and use it for predictions.\n","Processing row 6/8...\n","```python\n","import torch\n","import os\n","from PIL import Image\n","from torch.utils.data import Dataset, DataLoader\n","import torchvision.transforms as transforms\n","\n","# Assume the FashionMNIST dataset has been organized into folders\n","# like this:\n","# root_dir/\n","#   train/\n","#     0/\n","#       image1.png\n","#       image2.png\n","#       ...\n","#     1/\n","#       imageX.png\n","#       ...\n","#     ...\n","#     9/\n","#   test/\n","#     0/\n","#       ...\n","#     ...\n","#     9/\n","\n","class FashionMNISTFolderDataset(Dataset):\n","    \"\"\"\n","    A custom PyTorch Dataset for loading image files from a folder structure.\n","    Assumes the data is organized into subfolders where each subfolder\n","    represents a class.\n","    \"\"\"\n","    def __init__(self, root_dir, transform=None):\n","        \"\"\"\n","        Args:\n","            root_dir (string): Directory with all the class folders.\n","            transform (callable, optional): Optional transform to be applied\n","                on a sample.\n","        \"\"\"\n","        self.root_dir = root_dir\n","        self.transform = transform\n","        self.image_files = []\n","        self.labels = []\n","        self.classes = sorted(entry.name for entry in os.scandir(root_dir) if entry.is_dir())\n","        self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.classes)}\n","\n","        if not self.classes:\n","             raise FileNotFoundError(f\"No subdirectories found in {root_dir}. \"\n","                                     f\"Please organize images into class folders.\")\n","\n","        for class_name in self.classes:\n","            class_dir = os.path.join(root_dir, class_name)\n","            if not os.path.isdir(class_dir):\n","                continue # Skip if it's not a directory\n","\n","            for entry in os.scandir(class_dir):\n","                 if entry.is_file() and entry.name.lower().endswith(('.png', '.jpg', '.jpeg')):\n","                    self.image_files.append(entry.path)\n","                    self.labels.append(self.class_to_idx[class_name])\n","\n","        if not self.image_files:\n","             raise FileNotFoundError(f\"No image files found in subdirectories of {root_dir}. \"\n","                                     f\"Supported formats: .png, .jpg, .jpeg.\")\n","\n","\n","    def __len__(self):\n","        return len(self.image_files)\n","\n","    def __getitem__(self, idx):\n","        if torch.is_tensor(idx):\n","            idx = idx.tolist()\n","\n","        img_path = self.image_files[idx]\n","        image = Image.open(img_path).convert('L') # Convert to grayscale as FashionMNIST is grayscale\n","        label = self.labels[idx]\n","\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        return image, label\n","\n","# --- Example Usage ---\n","\n","# Define transformations (example: resize, to tensor, normalize)\n","# FashionMNIST images are 28x28\n","data_transform = transforms.Compose([\n","    transforms.Resize((32, 32)), # Example: resize to 32x32\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5,), (0.5,)) # FashionMNIST is grayscale, use single mean/std\n","])\n","\n","# Define dummy root directories assuming the data is structured as described\n","# Replace with actual paths if you have the data organized this way\n","train_data_root = \"./dummy_fashion_mnist_folder/train\"\n","test_data_root = \"./dummy_fashion_mnist_folder/test\"\n","\n","# --- Create Dummy Folders and Files for Demonstration ---\n","# This part is just to make the code runnable without a real dataset\n","# In a real scenario, you would already have the data organized.\n","if not os.path.exists(train_data_root):\n","    print(f\"Creating dummy data structure in {os.path.abspath('./dummy_fashion_mnist_folder')}\")\n","    os.makedirs(train_data_root, exist_ok=True)\n","    os.makedirs(test_data_root, exist_ok=True)\n","    # Create a few dummy files for 2 classes\n","    for class_id in [0, 1]:\n","        os.makedirs(os.path.join(train_data_root, str(class_id)), exist_ok=True)\n","        os.makedirs(os.path.join(test_data_root, str(class_id)), exist_ok=True)\n","        # Create dummy images (e.g., blank images)\n","        for i in range(5): # 5 dummy images per class\n","            dummy_img_train = Image.new('L', (28, 28), color='white')\n","            dummy_img_train.save(os.path.join(train_data_root, str(class_id), f'img_{i}.png'))\n","            dummy_img_test = Image.new('L', (28, 28), color='white')\n","            dummy_img_test.save(os.path.join(test_data_root, str(class_id), f'test_img_{i}.png'))\n","    print(\"Dummy data structure created.\")\n","else:\n","    print(f\"Dummy data structure already exists in {os.path.abspath('./dummy_fashion_mnist_folder')}\")\n","# --- End of Dummy Data Creation ---\n","\n","\n","# Instantiate the custom Dataset\n","try:\n","    fashion_mnist_dataset = FashionMNISTFolderDataset(root_dir=train_data_root, transform=data_transform)\n","\n","    # Check dataset size and classes\n","    print(f\"Number of samples in dataset: {len(fashion_mnist_dataset)}\")\n","    print(f\"Classes found: {fashion_mnist_dataset.classes}\")\n","    print(f\"Class to index mapping: {fashion_mnist_dataset.class_to_idx}\")\n","\n","    # Create a DataLoader\n","    batch_size = 32\n","    dataloader = DataLoader(fashion_mnist_dataset, batch_size=batch_size, shuffle=True)\n","\n","    # Iterate through the DataLoader (example)\n","    print(f\"\\nIterating through DataLoader with batch size {batch_size}...\")\n","    for i, (images, labels) in enumerate(dataloader):\n","        print(f\"Batch {i+1}:\")\n","        print(f\"  Image batch shape: {images.shape}\")\n","        print(f\"  Label batch shape: {labels.shape}\")\n","        # Stop after 2 batches for demonstration\n","        if i >= 1:\n","            break\n","\n","    print(\"\\nDataLoader usage demonstrated.\")\n","\n","except FileNotFoundError as e:\n","    print(f\"Error: {e}\")\n","    print(\"Please ensure the root_dir contains subdirectories for each class with image files inside.\")\n","    print(f\"The dummy data creation code created a structure in {os.path.abspath('./dummy_fashion_mnist_folder')}.\")\n","    print(\"You can point the `root_dir` to this location.\")\n","\n","```\n","Okay, let's create a PyTorch `Dataset` subclass to load FashionMNIST images and labels from a folder structure, and then use it with a `DataLoader`.\n","\n","Since FashionMNIST doesn't *natively* come as individual image files in class subfolders, we first need a pre-processing step to download the standard dataset and save it in the desired folder structure. Then, we'll build the custom `Dataset` class that reads from this structure.\n","\n","**Step 1: Understand the Goal and Data Structure**\n","\n","We want to load data organized like this:\n","\n","```\n","fashion_mnist_folder/\n","├── train/\n","│   ├── T-shirt_top/\n","│   │   ├── img_00001.png\n","│   │   ├── img_00002.png\n","│   │   └── ...\n","│   ├── Trouser/\n","│   │   ├── img_01001.png\n","│   │   └── ...\n","│   └── ... (other class folders)\n","└── test/\n","    ├── T-shirt_top/\n","    │   ├── img_60001.png\n","    │   └── ...\n","    └── ... (other class folders)\n","```\n","\n","Our custom `Dataset` will need to traverse this structure, find all images, associate them with their labels based on the folder name, and provide a way to load individual images and labels when indexed.\n","\n","**Step 2: Pre-processing - Saving FashionMNIST to Folders**\n","\n","This part is *not* the custom `Dataset` itself, but a necessary setup step before our custom `Dataset` can be used. We will use `torchvision.datasets.FashionMNIST` to easily download the data and then save it into the target folder structure.\n","\n","*   We'll need to map the integer labels (0-9) from the original dataset to class names (e.g., 0 -> 'T-shirt/top').\n","*   We'll iterate through the training and test sets.\n","*   For each image and label, we'll create the appropriate directory (`root_dir/split/class_name/`) if it doesn't exist and save the image (which is a PIL Image) as a file (e.g., PNG).\n","\n","```python\n","import torch\n","import torchvision\n","import torchvision.transforms as transforms\n","import os\n","from PIL import Image\n","import shutil\n","\n","# --- Pre-processing: Save FashionMNIST to Folder Structure ---\n","# This part downloads the data and organizes it into folders.\n","# You only need to run this once.\n","\n","def save_fashion_mnist_to_folder(root_output_dir='./fashion_mnist_folder'):\n","    \"\"\"\n","    Downloads FashionMNIST and saves images into a folder structure\n","    like root_output_dir/split/class_name/image.png\n","    \"\"\"\n","    print(f\"Saving FashionMNIST to folders in {root_output_dir}...\")\n","\n","    # Define class names for FashionMNIST\n","    classes = [\n","        'T-shirt_top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n","        'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle_boot'\n","    ]\n","\n","    # Ensure the root output directory is clean or created\n","    if os.path.exists(root_output_dir):\n","        print(f\"Removing existing directory: {root_output_dir}\")\n","        shutil.rmtree(root_output_dir)\n","    os.makedirs(root_output_dir)\n","\n","    # Download and process training data\n","    train_dataset_original = torchvision.datasets.FashionMNIST(\n","        root='./data', train=True, download=True\n","    )\n","    save_split(train_dataset_original, 'train', root_output_dir, classes)\n","\n","    # Download and process test data\n","    test_dataset_original = torchvision.datasets.FashionMNIST(\n","        root='./data', train=False, download=True\n","    )\n","    save_split(test_dataset_original, 'test', root_output_dir, classes)\n","\n","    print(\"FashionMNIST data saved successfully.\")\n","\n","def save_split(dataset, split_name, root_output_dir, classes):\n","    \"\"\"Saves images from a single split (train/test) to folders.\"\"\"\n","    split_dir = os.path.join(root_output_dir, split_name)\n","    print(f\"Processing {split_name} split...\")\n","\n","    for class_name in classes:\n","        os.makedirs(os.path.join(split_dir, class_name), exist_ok=True)\n","\n","    for i, (img, label) in enumerate(dataset):\n","        class_name = classes[label]\n","        # FashionMNIST images are 28x28 grayscale PIL Images\n","        img_path = os.path.join(split_dir, class_name, f'img_{i:05d}.png')\n","        img.save(img_path) # PIL Image save\n","\n","# --- Run the pre-processing step ---\n","# Uncomment the line below to generate the folder structure.\n","# save_fashion_mnist_to_folder()\n","# print(\"Please run the save_fashion_mnist_to_folder() function to create the data structure.\")\n","\n","```\n","\n","**Reasoning for Pre-processing:**\n","\n","*   `torchvision.datasets.FashionMNIST` is the easiest way to get the data initially.\n","*   We need to convert the dataset object (which holds images and labels in memory or arrays) into a *file system* representation.\n","*   Saving as individual image files (like PNG) is a common practice and allows our custom `Dataset` to simulate loading data from scratch, as if it wasn't downloaded via `torchvision`.\n","*   Organizing by `split/class_name/` makes it straightforward for our `Dataset` to determine the label from the file path.\n","*   Using `PIL.Image.save()` is standard for saving image objects loaded or generated by PIL.\n","\n","**Step 3: Create the Custom `Dataset` Class**\n","\n","Now, we define `FashionMNISTFolderDataset` which inherits from `torch.utils.data.Dataset`.\n","\n","*   **`__init__(self, root_dir, split='train', transform=None)`:**\n","    *   Takes the `root_dir` (e.g., `./fashion_mnist_folder`), the `split` ('train' or 'test'), and an optional `transform`.\n","    *   It will build a list of all image file paths and their corresponding integer labels.\n","    *   It needs to map folder names (class names) to integer labels (0-9).\n","*   **`__len__(self)`:**\n","    *   Returns the total number of samples (images) in the dataset. This is simply the length of the list of (path, label) pairs created in `__init__`.\n","*   **`__getitem__(self, idx)`:**\n","    *   Takes an index `idx`.\n","    *   Retrieves the image path and label from the stored list at that index.\n","    *   Loads the image file using `PIL.Image.open()`.\n","    *   Applies the `transform` if provided.\n","    *   Returns the transformed image tensor and the label.\n","\n","```python\n","import torch\n","from torch.utils.data import Dataset\n","import torchvision.transforms as transforms\n","import os\n","from PIL import Image\n","\n","# --- Custom Dataset Class ---\n","\n","class FashionMNISTFolderDataset(Dataset):\n","    \"\"\"\n","    Custom Dataset to load FashionMNIST images from a folder structure\n","    like root_dir/split/class_name/image.png\n","    \"\"\"\n","    def __init__(self, root_dir, split='train', transform=None):\n","        \"\"\"\n","        Args:\n","            root_dir (string): Directory with all the split folders (e.g. 'fashion_mnist_folder/').\n","            split (string): 'train' or 'test' indicating the split to load.\n","            transform (callable, optional): Optional transform to be applied\n","                on a sample.\n","        \"\"\"\n","        self.root_dir = root_dir\n","        self.split = split\n","        self.transform = transform\n","        self.split_dir = os.path.join(root_dir, split)\n","\n","        if not os.path.exists(self.split_dir):\n","            raise FileNotFoundError(f\"Split directory not found: {self.split_dir}\")\n","\n","        # Store class names and create mapping from class name to index\n","        # FashionMNIST classes (sorted alphabetically would match the folder structure)\n","        self.classes = sorted(entry.name for entry in os.scandir(self.split_dir) if entry.is_dir())\n","        self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.classes)}\n","\n","        if not self.classes:\n","             raise RuntimeError(f\"No class directories found in {self.split_dir}\")\n","\n","\n","        # Build the list of (image_path, label) pairs\n","        self.samples = []\n","        for class_name in self.classes:\n","            class_dir = os.path.join(self.split_dir, class_name)\n","            if not os.path.isdir(class_dir):\n","                continue # Skip if it's not actually a directory\n","\n","            label = self.class_to_idx[class_name]\n","\n","            for img_file in os.listdir(class_dir):\n","                # Basic check for image files (can be expanded)\n","                if img_file.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif')):\n","                    img_path = os.path.join(class_dir, img_file)\n","                    self.samples.append((img_path, label))\n","\n","        if not self.samples:\n","            raise RuntimeError(f\"No images found in {self.split_dir}. Make sure images are saved in class subfolders.\")\n","\n","        print(f\"Found {len(self.samples)} images in the '{split}' split.\")\n","        print(f\"Classes: {self.classes}\")\n","\n","\n","    def __len__(self):\n","        \"\"\"Returns the total number of samples.\"\"\"\n","        return len(self.samples)\n","\n","    def __getitem__(self, idx):\n","        \"\"\"Loads and returns a sample from the dataset at the given index.\"\"\"\n","        if not (0 <= idx < len(self)):\n","            raise IndexError(f\"Index {idx} is out of bounds for dataset of size {len(self)}\")\n","\n","        img_path, label = self.samples[idx]\n","\n","        # Load image using Pillow (PIL)\n","        # FashionMNIST is grayscale, so convert to 'L'\n","        try:\n","            img = Image.open(img_path).convert('L')\n","        except Exception as e:\n","            print(f\"Error loading image {img_path}: {e}\")\n","            # Return a dummy or handle error appropriately, e.g., skip sample\n","            # For simplicity here, we'll raise, but a real system might skip\n","            raise\n","\n","        # Apply transforms\n","        if self.transform:\n","            img = self.transform(img)\n","\n","        return img, label # Return the image tensor and the label\n","\n","```\n","\n","**Reasoning for Custom Dataset:**\n","\n","*   **Inheritance:** Subclassing `torch.utils.data.Dataset` is the contract PyTorch expects. It requires implementing `__len__` and `__getitem__`.\n","*   **`__init__`:**\n","    *   It finds all relevant image files and stores their paths along with their corresponding labels. This is done upfront to avoid repeatedly scanning the file system during training/inference.\n","    *   It relies on the folder structure: the parent directory of the image file *is* the class name.\n","    *   It creates `self.classes` and `self.class_to_idx` which are standard attributes for image datasets, providing the mapping between integer labels and human-readable names.\n","    *   `self.samples` stores the list of `(path, label)` tuples that `__getitem__` will use.\n","*   **`__len__`:** Simple lookup of the total number of samples stored in `self.samples`.\n","*   **`__getitem__`:**\n","    *   Takes the index `idx`.\n","    *   Retrieves the pre-calculated `img_path` and `label`.\n","    *   Uses `PIL.Image.open()` to load the image file from disk. `.convert('L')` is used because FashionMNIST is grayscale. This ensures the image is in a consistent format before transformations.\n","    *   Applies the `self.transform`. This is crucial because transforms handle conversion to PyTorch tensors (`ToTensor`) and often include resizing, augmentation, or normalization needed for model input.\n","    *   Returns the processed image tensor and the integer label. This is the format `DataLoader` expects: a tensor for the data and a number for the label.\n","\n","**Step 4: Using the Custom Dataset with `DataLoader`**\n","\n","Now we can instantiate our `FashionMNISTFolderDataset` and pass it to a `torch.utils.data.DataLoader`. The `DataLoader` handles batching, shuffling, and loading data in parallel processes (`num_workers`).\n","\n","```python\n","import torch\n","from torch.utils.data import DataLoader\n","import torchvision.transforms as transforms\n","import matplotlib.pyplot as plt # Optional: for visualization\n","import numpy as np # Optional: for visualization\n","\n","# --- Usage with DataLoader ---\n","\n","# Define the directory where data was saved (modify if you changed it)\n","DATA_FOLDER_ROOT = './fashion_mnist_folder'\n","\n","# Define transformations\n","# Common transforms for image data:\n","# 1. ToTensor: Converts PIL Image (H x W) to PyTorch Tensor (C x H x W)\n","#    and scales pixel values from [0, 255] to [0.0, 1.0].\n","# 2. Normalize: Standardizes pixel values using mean and standard deviation.\n","#    FashionMNIST mean and std for grayscale: mean=0.286, std=0.353 (approx)\n","transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.286,), (0.353,)) # Mean and Std for grayscale FashionMNIST\n","])\n","\n","# --- Instantiate the Dataset ---\n","# Make sure you have run save_fashion_mnist_to_folder() at least once\n","# to create the ./fashion_mnist_folder directory.\n","try:\n","    train_dataset = FashionMNISTFolderDataset(\n","        root_dir=DATA_FOLDER_ROOT,\n","        split='train',\n","        transform=transform\n","    )\n","\n","    test_dataset = FashionMNISTFolderDataset(\n","        root_dir=DATA_FOLDER_ROOT,\n","        split='test',\n","        transform=transform\n","    )\n","\n","except FileNotFoundError as e:\n","    print(f\"Data folder not found: {e}\")\n","    print(\"Please run the save_fashion_mnist_to_folder() function first.\")\n","    exit() # Exit if data isn't ready\n","\n","# --- Instantiate the DataLoader ---\n","BATCH_SIZE = 64\n","NUM_WORKERS = 2 # Number of subprocesses for data loading (set to 0 for debugging)\n","\n","train_loader = DataLoader(\n","    train_dataset,\n","    batch_size=BATCH_SIZE,\n","    shuffle=True,      # Shuffle training data\n","    num_workers=NUM_WORKERS\n",")\n","\n","test_loader = DataLoader(\n","    test_dataset,\n","    batch_size=BATCH_SIZE,\n","    shuffle=False,     # No need to shuffle test data\n","    num_workers=NUM_WORKERS\n",")\n","\n","# --- Example: Iterate through a batch ---\n","print(\"\\nIterating through a batch from the DataLoader...\")\n","\n","# Get one batch\n","images, labels = next(iter(train_loader))\n","\n","print(f\"Batch of images shape: {images.shape}\") # Expected: [batch_size, channels, height, width] -> [64, 1, 28, 28]\n","print(f\"Batch of labels shape: {labels.shape}\") # Expected: [batch_size]\n","print(f\"Labels in batch: {labels}\")\n","\n","# Optional: Visualize a few images from the batch\n","print(\"\\nDisplaying first 4 images from the batch:\")\n","fig = plt.figure(figsize=(8, 8))\n","for i in range(4):\n","    ax = fig.add_subplot(2, 2, i + 1)\n","    ax.set_title(f\"Label: {train_dataset.classes[labels[i]]}\")\n","    ax.axis('off')\n","    # Undo normalization and permute dimensions for plotting (C, H, W) -> (H, W, C) or (H, W) for grayscale\n","    img_display = images[i] * 0.353 + 0.286 # Undo normalization\n","    img_display = img_display.squeeze() # Remove channel dimension if grayscale\n","    plt.imshow(img_display.numpy(), cmap='gray')\n","plt.tight_layout()\n","plt.show()\n","\n","\n","print(\"\\nDataLoader setup complete. You can now use train_loader and test_loader.\")\n","```\n","\n","**Reasoning for DataLoader Usage:**\n","\n","*   **Instantiation:** You create instances of your custom `FashionMNISTFolderDataset` for training and testing, applying the desired transformations.\n","*   **`DataLoader(dataset, ...)`:** The `DataLoader` takes the dataset object as its primary argument.\n","*   **`batch_size`:** Specifies how many samples are grouped together in each batch returned by the loader.\n","*   **`shuffle=True`:** Important for training data to randomize the order of samples across epochs, helping the model generalize better. `shuffle=False` is typical for validation/testing.\n","*   **`num_workers`:** Controls how many subprocesses are used for data loading. `0` means the data loading is done in the main process. Using a small number (like 2 or 4) can speed up loading by fetching the next batch while the current one is being processed on the GPU, but setting it too high can consume excessive CPU/memory.\n","*   **Iteration:** You typically iterate through the `DataLoader` in your training loop (`for images, labels in train_loader:`). Each iteration yields a batch of image tensors and a batch of label tensors, ready to be passed to your model.\n","*   **`next(iter(train_loader))`:** A common way to get just *one* batch for inspection, as shown in the example.\n","\n","**Summary of the Full Process:**\n","\n","1.  **(One-time Setup)** Run the `save_fashion_mnist_to_folder()` function to download the original FashionMNIST data and save it as individual image files within a class-based folder structure.\n","2.  Define the `FashionMNISTFolderDataset` class, inheriting from `torch.utils.data.Dataset`. Implement `__init__`, `__len__`, and `__getitem__` to read from the created folder structure.\n","3.  Define any necessary `torchvision.transforms`.\n","4.  Instantiate the `FashionMNISTFolderDataset`, providing the root folder path, split (train/test), and transforms.\n","5.  Instantiate `torch.utils.data.DataLoader`, passing the dataset object, desired batch size, shuffle option, and number of workers.\n","6.  Use the `DataLoader` instances (e.g., `train_loader`, `test_loader`) in your training and evaluation loops.\n","\n","This setup effectively demonstrates how to handle datasets that are stored as loose files in a directory structure, providing a flexible pattern applicable to many real-world image datasets.\n","Processing row 7/8...\n","```python\n","from sklearn.datasets import load_iris\n","from sklearn.decomposition import PCA\n","from sklearn.neighbors import KNeighborsClassifier\n","\n","# Load the Iris dataset\n","iris = load_iris()\n","X, y = iris.data, iris.target\n","\n","# Reduce dimensionality using PCA\n","# Reducing to 2 principal components as an example\n","pca = PCA(n_components=2)\n","X_pca = pca.fit_transform(X)\n","\n","# Fit a k-nearest neighbors classifier\n","# Using default n_neighbors=5\n","knn = KNeighborsClassifier()\n","knn.fit(X_pca, y)\n","\n","# The KNN classifier is now trained on the PCA-reduced data\n","```\n","Okay, let's write a Python script using scikit-learn to load the Iris dataset, apply PCA for dimensionality reduction, and train a K-Nearest Neighbors (KNN) classifier. We'll explain each step.\n","\n","```python\n","# Step 1: Import necessary libraries\n","# We need modules for loading the dataset, PCA, splitting data, KNN, and evaluating the model.\n","from sklearn.datasets import load_iris\n","from sklearn.decomposition import PCA\n","from sklearn.model_selection import train_test_split\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.metrics import accuracy_score\n","import numpy as np # Often useful, though not strictly needed for this basic script\n","\n","print(\"Step 1: Imported necessary libraries.\")\n","\n","# Step 2: Load the Iris dataset\n","# scikit-learn provides a convenient function to load this classic dataset.\n","# The dataset is returned as a Bunch object, containing data (features) and target (labels).\n","iris = load_iris()\n","X = iris.data  # Features (sepal length, sepal width, petal length, petal width)\n","y = iris.target # Target labels (0: setosa, 1: versicolor, 2: virginica)\n","\n","print(f\"Step 2: Loaded Iris dataset. Original feature shape: {X.shape}\")\n","print(f\"Number of samples: {X.shape[0]}\")\n","print(f\"Number of original features: {X.shape[1]}\")\n","\n","\n","# Step 3: Split the data into training and testing sets\n","# It's crucial to split the data *before* applying any transformations like PCA or scaling\n","# to prevent data leakage from the test set into the training process.\n","# train_test_split shuffles the data by default before splitting.\n","# We use random_state for reproducibility so that the split is the same every time the script runs.\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n","\n","print(f\"Step 3: Split data into training (75%) and testing (25%) sets.\")\n","print(f\"Training feature shape: {X_train.shape}\")\n","print(f\"Testing feature shape: {X_test.shape}\")\n","\n","# Step 4: Apply PCA for dimensionality reduction\n","# We instantiate a PCA object, specifying the desired number of components.\n","# The Iris dataset has 4 features. A common choice for visualization is 2 components,\n","# which we'll use here. You could choose more components (up to the original number of features)\n","# or choose a number based on explained variance ratio.\n","pca = PCA(n_components=2)\n","\n","# Fit PCA *only* on the training data. This step calculates the principal components\n","# based on the training data's variance.\n","pca.fit(X_train)\n","\n","# Transform both the training and testing data using the fitted PCA model.\n","# This projects the original data onto the lower-dimensional principal components.\n","X_train_pca = pca.transform(X_train)\n","X_test_pca = pca.transform(X_test)\n","\n","print(f\"Step 4: Applied PCA to reduce dimensionality to 2 components.\")\n","print(f\"PCA-transformed training shape: {X_train_pca.shape}\")\n","print(f\"PCA-transformed testing shape: {X_test_pca.shape}\")\n","# Optional: Print explained variance ratio\n","print(f\"Explained variance ratio by components: {pca.explained_variance_ratio_}\")\n","print(f\"Total explained variance: {np.sum(pca.explained_variance_ratio_):.4f}\")\n","\n","\n","# Step 5: Fit a K-Nearest Neighbors (KNN) classifier\n","# We instantiate the KNeighborsClassifier. 'n_neighbors' is the 'k' value -\n","# the number of nearest neighbors to consider for classification. A common default is 5.\n","knn = KNeighborsClassifier(n_neighbors=5)\n","\n","# Train the KNN model using the PCA-transformed training data and the corresponding labels.\n","knn.fit(X_train_pca, y_train)\n","\n","print(f\"Step 5: Trained a KNN classifier with n_neighbors={knn.n_neighbors} on the PCA-transformed training data.\")\n","\n","# Step 6: Evaluate the model on the PCA-transformed test data\n","# Make predictions on the unseen, PCA-transformed test data.\n","y_pred = knn.predict(X_test_pca)\n","\n","# Calculate the accuracy by comparing the predicted labels to the actual test labels.\n","accuracy = accuracy_score(y_test, y_pred)\n","\n","print(f\"Step 6: Evaluated the KNN model on the PCA-transformed test data.\")\n","print(f\"Accuracy: {accuracy:.4f}\")\n","\n","# --- Summary and optional further steps ---\n","print(\"\\n--- Summary ---\")\n","print(\"Script completed successfully.\")\n","print(f\"Original data shape: {X.shape}\")\n","print(f\"Reduced data shape after PCA: {X_train_pca.shape}\")\n","print(f\"KNN Model trained on PCA-reduced data.\")\n","print(f\"Accuracy on test set: {accuracy:.4f}\")\n","\n","# Optional: You could visualize the PCA-reduced data and decision boundary if needed\n","# import matplotlib.pyplot as plt\n","# # (Code for visualization would go here, plotting X_test_pca and coloring by y_test or y_pred)\n","```\n","\n","**Explanation of Steps:**\n","\n","1.  **Import Libraries:** We start by importing all the necessary classes and functions from `sklearn` and `numpy`.\n","2.  **Load Dataset:** `load_iris()` fetches the Iris dataset. `X` becomes a NumPy array of the features (measurements), and `y` is a NumPy array of the corresponding species labels (0, 1, or 2).\n","3.  **Split Data:** `train_test_split` divides the data into two sets: one for training the model (`X_train`, `y_train`) and one for testing its performance on unseen data (`X_test`, `y_test`). We use `test_size=0.25` to allocate 25% of the data to the test set and `random_state=42` to ensure the split is the same every time we run the script, making the results reproducible.\n","4.  **Apply PCA:**\n","    *   We create a `PCA` object and specify `n_components=2`. This tells PCA to find the two principal components that capture the most variance in the data.\n","    *   `pca.fit(X_train)` analyzes the training data (`X_train`) to learn the directions (components) of maximum variance. **It's crucial to fit only on the training data** to avoid the PCA algorithm from \"seeing\" the test data.\n","    *   `pca.transform(X_train)` and `pca.transform(X_test)` project the original 4-dimensional training and testing data onto the 2-dimensional space defined by the principal components learned in the previous step. The test data is transformed using the *same* components learned from the training data.\n","    *   We print the `explained_variance_ratio_` to see how much of the original data's variance is captured by each of the chosen components.\n","5.  **Fit KNN Classifier:**\n","    *   We create a `KNeighborsClassifier` object. `n_neighbors=5` means that for any point we want to classify, the algorithm will look at its 5 nearest neighbors in the feature space and assign the class that is most common among those 5 neighbors.\n","    *   `knn.fit(X_train_pca, y_train)` trains the KNN model. It essentially just stores the `X_train_pca` and `y_train` data because KNN is a lazy learning algorithm – the \"training\" is minimal; most work happens during prediction.\n","6.  **Evaluate Model:**\n","    *   `knn.predict(X_test_pca)` uses the trained KNN model to predict the labels for the samples in the PCA-transformed test set.\n","    *   `accuracy_score(y_test, y_pred)` compares the predicted labels (`y_pred`) with the actual labels (`y_test`) and calculates the proportion of correctly classified samples. This gives us a measure of the model's performance on data it hasn't seen during training.\n","\n","This script demonstrates a common machine learning workflow: load data, split data, preprocess data (PCA in this case), train a model, and evaluate the model.\n","Processing row 8/8...\n","```java\n","import java.io.IOException;\n","import java.util.StringTokenizer;\n","\n","import org.apache.hadoop.conf.Configuration;\n","import org.apache.hadoop.conf.Configured;\n","import org.apache.hadoop.fs.Path;\n","import org.apache.hadoop.io.IntWritable;\n","import org.apache.hadoop.io.LongWritable;\n","import org.apache.hadoop.io.Text;\n","import org.apache.hadoop.mapreduce.Job;\n","import org.apache.hadoop.mapreduce.Mapper;\n","import org.apache.hadoop.mapreduce.Reducer;\n","import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\n","import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n","import org.apache.hadoop.util.Tool;\n","import org.apache.hadoop.util.ToolRunner;\n","\n","public class WordCount extends Configured implements Tool {\n","\n","    public static class TokenizerMapper extends Mapper<LongWritable, Text, Text, IntWritable> {\n","\n","        private final static IntWritable one = new IntWritable(1);\n","        private Text word = new Text();\n","\n","        public void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {\n","            String line = value.toString();\n","            // Basic tokenization and cleaning\n","            String[] tokens = line.split(\"\\\\s+\");\n","            for (String token : tokens) {\n","                String cleanedToken = token.toLowerCase().replaceAll(\"[^a-z]\", \"\"); // Keep only lowercase letters\n","                if (!cleanedToken.isEmpty()) {\n","                    word.set(cleanedToken);\n","                    context.write(word, one);\n","                }\n","            }\n","        }\n","    }\n","\n","    public static class IntSumReducer extends Reducer<Text, IntWritable, Text, IntWritable> {\n","        private IntWritable result = new IntWritable();\n","\n","        public void reduce(Text key, Iterable<IntWritable> values, Context context)\n","                throws IOException, InterruptedException {\n","            int sum = 0;\n","            for (IntWritable val : values) {\n","                sum += val.get();\n","            }\n","            result.set(sum);\n","            context.write(key, result);\n","        }\n","    }\n","\n","    public static void main(String[] args) throws Exception {\n","        int res = ToolRunner.run(new Configuration(), new WordCount(), args);\n","        System.exit(res);\n","    }\n","\n","    @Override\n","    public int run(String[] args) throws Exception {\n","        if (args.length != 2) {\n","            System.err.println(\"Usage: WordCount <input path> <output path>\");\n","            System.exit(-1);\n","        }\n","\n","        Configuration conf = this.getConf();\n","        Job job = Job.getInstance(conf, \"word count\");\n","\n","        job.setJarByClass(WordCount.class);\n","        job.setMapperClass(TokenizerMapper.class);\n","        job.setCombinerClass(IntSumReducer.class); // Optional: use reducer as combiner for local aggregation\n","        job.setReducerClass(IntSumReducer.class);\n","\n","        job.setOutputKeyClass(Text.class);\n","        job.setOutputValueClass(IntWritable.class);\n","\n","        FileInputFormat.addInputPath(job, new Path(args[0]));\n","        FileOutputFormat.setOutputPath(job, new Path(args[1]));\n","\n","        return job.waitForCompletion(true) ? 0 : 1;\n","    }\n","}\n","```\n","Okay, let's break down how to write a Word Count program using Hadoop MapReduce. This is the \"Hello World\" of Hadoop, illustrating the core concepts beautifully.\n","\n","**Goal:** Count the frequency of every word in a large text file.\n","\n","**MapReduce Approach:**\n","\n","1.  **Mapping:** Read the input text line by line. For each line, split it into words. For *every* word encountered, emit a key-value pair where the key is the word itself, and the value is `1` (representing one occurrence).\n","2.  **Shuffling and Sorting:** Hadoop automatically groups all emitted pairs with the *same key* (the same word). It also sorts the values associated with each key.\n","3.  **Reducing:** For each unique word (key), the Reducer receives the word and an iterable list of all the `1`s emitted for that word by the Mappers. The Reducer sums these `1`s to get the total count for that word. It then emits the final key-value pair: the word and its total count.\n","\n","**Components of a MapReduce Program:**\n","\n","1.  **Mapper Class:** Extends `Mapper`, defines the map logic.\n","2.  **Reducer Class:** Extends `Reducer`, defines the reduce logic.\n","3.  **Driver Class:** Contains the `main` method, configures the job, sets input/output paths, specifies Mapper and Reducer classes, and submits the job.\n","\n","Let's write the code step by step.\n","\n","---\n","\n","**Step 1: Set up Project and Dependencies**\n","\n","You'll need a Java project (e.g., using Maven or Gradle) and include the necessary Hadoop client dependencies.\n","\n","For Maven, add this to your `pom.xml`:\n","\n","```xml\n","<dependency>\n","    <groupId>org.apache.hadoop</groupId>\n","    <artifactId>hadoop-client</artifactId>\n","    <version>3.3.1</version> <!-- Use your Hadoop version -->\n","    <scope>provided</scope>\n","</dependency>\n","<dependency>\n","    <groupId>org.apache.hadoop</groupId>\n","    <artifactId>hadoop-common</artifactId>\n","    <version>3.3.1</version> <!-- Use your Hadoop version -->\n","    <scope>provided</scope>\n","</dependency>\n","```\n","*(Note: `provided` scope means the Hadoop libraries are expected to be present on the cluster where the job runs)*\n","\n","---\n","\n","**Step 2: Create the Mapper (`WordCountMapper.java`)**\n","\n","*   **Purpose:** To take each line of text, split it into words, and emit `(word, 1)` for each word.\n","*   **Input:** `LongWritable` (line offset, not used here), `Text` (the line content).\n","*   **Output:** `Text` (the word), `IntWritable` (the count, always 1 in the map phase).\n","\n","```java\n","package com.yourcompany.hadoop; // Use your own package name\n","\n","import java.io.IOException;\n","import java.util.StringTokenizer;\n","\n","import org.apache.hadoop.io.IntWritable;\n","import org.apache.hadoop.io.LongWritable;\n","import org.apache.hadoop.io.Text;\n","import org.apache.hadoop.mapreduce.Mapper;\n","\n","public class WordCountMapper extends Mapper<LongWritable, Text, Text, IntWritable> {\n","\n","    // Output value is always 1 for a single occurrence\n","    private final static IntWritable one = new IntWritable(1);\n","    // Reusable Text object for the word\n","    private Text word = new Text();\n","\n","    @Override\n","    public void map(LongWritable key, Text value, Context context)\n","            throws IOException, InterruptedException {\n","\n","        // 1. Get the line of text as a Java String\n","        String line = value.toString();\n","\n","        // 2. Tokenize the line into words\n","        // StringTokenizer is a simple way; for more robust parsing,\n","        // you might use split() with regex or other libraries.\n","        // Here, we split by whitespace characters.\n","        StringTokenizer tokenizer = new StringTokenizer(line);\n","\n","        // 3. Iterate through the tokens (words)\n","        while (tokenizer.hasMoreTokens()) {\n","            // 4. Get the next token\n","            String nextToken = tokenizer.nextToken();\n","\n","            // Optional: Clean up the word (e.g., remove punctuation, lowercase)\n","            // A simple approach is to convert to lowercase and remove non-alphanumeric\n","            String cleanWord = nextToken.toLowerCase().replaceAll(\"[^a-z0-9]\", \"\");\n","\n","            // 5. Set the Text object to the current word\n","            // Only process non-empty cleaned words\n","            if (!cleanWord.isEmpty()) {\n","                word.set(cleanWord);\n","\n","                // 6. Emit the word and a count of 1\n","                // The key is the word (Text), the value is 1 (IntWritable)\n","                context.write(word, one);\n","            }\n","        }\n","    }\n","}\n","```\n","\n","**Reasoning for Mapper:**\n","\n","*   We extend `Mapper` with the specified input and output types: `<LongWritable, Text, Text, IntWritable>`.\n","*   The `map` method receives the line number (`LongWritable key`), the line content (`Text value`), and a `Context` object.\n","*   We convert the `Text` value to a standard Java `String` for easier processing.\n","*   `StringTokenizer` (or `String.split()`) is used to break the line into potential words based on delimiters (here, whitespace). We added `replaceAll` and `toLowerCase` for basic cleaning to treat \"Word\", \"word.\", and \"word\" as the same.\n","*   For each token (word), we set our reusable `Text word` object and use `context.write(word, one)` to emit the intermediate key-value pair `(word, 1)`. The `Context` object is how the Mapper communicates its output to the Hadoop framework.\n","\n","---\n","\n","**Step 3: Create the Reducer (`WordCountReducer.java`)**\n","\n","*   **Purpose:** To take a word and the list of `1`s emitted for it by the Mappers, and sum them up.\n","*   **Input:** `Text` (the word), `Iterable<IntWritable>` (the list of `1`s for that word).\n","*   **Output:** `Text` (the word), `IntWritable` (the total count).\n","\n","```java\n","package com.yourcompany.hadoop; // Use your own package name\n","\n","import java.io.IOException;\n","\n","import org.apache.hadoop.io.IntWritable;\n","import org.apache.hadoop.io.Text;\n","import org.apache.hadoop.mapreduce.Reducer;\n","\n","public class WordCountReducer extends Reducer<Text, IntWritable, Text, IntWritable> {\n","\n","    // Reusable IntWritable object for the final count\n","    private IntWritable result = new IntWritable();\n","\n","    @Override\n","    public void reduce(Text key, Iterable<IntWritable> values, Context context)\n","            throws IOException, InterruptedException {\n","\n","        // 1. The key is the word itself\n","        // 2. The values is an iterable collection of all the '1's emitted for this word\n","        int sum = 0;\n","\n","        // 3. Iterate through the list of values (all are IntWritable(1))\n","        for (IntWritable val : values) {\n","            // 4. Sum up the values\n","            sum += val.get();\n","        }\n","\n","        // 5. Set the final count in the result object\n","        result.set(sum);\n","\n","        // 6. Emit the word (key) and its total count (result)\n","        context.write(key, result);\n","    }\n","}\n","```\n","\n","**Reasoning for Reducer:**\n","\n","*   We extend `Reducer` with input and output types: `<Text, IntWritable, Text, IntWritable>`. The input types match the Mapper's output types. The output types are the final desired result types.\n","*   The `reduce` method receives a `Text key` (the word) and an `Iterable<IntWritable> values` (the collection of `1`s for that word).\n","*   We initialize a `sum` variable.\n","*   We iterate through the `values` collection. For each `IntWritable` in the list, we get its integer value using `val.get()` and add it to the `sum`.\n","*   After the loop, `sum` holds the total count for the word.\n","*   We set the `IntWritable result` object with the final `sum`.\n","*   We use `context.write(key, result)` to emit the final pair `(word, total_count)`.\n","\n","---\n","\n","**Step 4: Create the Driver Program (`WordCount.java`)**\n","\n","*   **Purpose:** To configure and run the MapReduce job. This includes setting the Mapper and Reducer classes, specifying input/output paths, and launching the job.\n","*   It's standard practice in Hadoop 2+ to implement `Tool` and use `ToolRunner` for configuration and command-line handling.\n","\n","```java\n","package com.yourcompany.hadoop; // Use your own package name\n","\n","import org.apache.hadoop.conf.Configuration;\n","import org.apache.hadoop.conf.Configured;\n","import org.apache.hadoop.fs.Path;\n","import org.apache.hadoop.io.IntWritable;\n","import org.apache.hadoop.io.Text;\n","import org.apache.hadoop.mapreduce.Job;\n","import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\n","import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n","import org.apache.hadoop.util.Tool;\n","import org.apache.hadoop.util.ToolRunner;\n","import org.apache.hadoop.fs.FileSystem;\n","import java.net.URI;\n","\n","public class WordCount extends Configured implements Tool {\n","\n","    @Override\n","    public int run(String[] args) throws Exception {\n","\n","        // 1. Get configuration object\n","        Configuration conf = this.getConf();\n","\n","        // 2. Create a Job object\n","        Job job = Job.getInstance(conf, \"Word Count\");\n","\n","        // 3. Specify the JAR file by class\n","        // This tells Hadoop where to find your Mapper and Reducer classes\n","        job.setJarByClass(WordCount.class);\n","\n","        // 4. Specify the Mapper class\n","        job.setMapperClass(WordCountMapper.class);\n","\n","        // 5. Specify the Combiner class (Optional, but good for performance)\n","        // The Combiner runs on the mapper side and performs local aggregation\n","        // It uses the same logic as the Reducer for Word Count\n","        job.setCombinerClass(WordCountReducer.class);\n","\n","        // 6. Specify the Reducer class\n","        job.setReducerClass(WordCountReducer.class);\n","\n","        // 7. Specify the output key and value types for the job (Reducer output)\n","        job.setOutputKeyClass(Text.class);\n","        job.setOutputValueClass(IntWritable.class);\n","\n","        // Optional: Specify the output key and value types for the Mapper (intermediate output)\n","        // This is often inferred, but can be explicit if desired or necessary\n","        // job.setMapOutputKeyClass(Text.class);\n","        // job.setMapOutputValueClass(IntWritable.class);\n","\n","\n","        // 8. Handle input and output paths\n","        if (args.length != 2) {\n","            System.err.println(\"Usage: WordCount <input path> <output path>\");\n","            System.exit(1); // Exit if arguments are incorrect\n","        }\n","\n","        Path inputPath = new Path(args[0]);\n","        Path outputPath = new Path(args[1]);\n","\n","        // Delete output directory if it already exists (useful for testing)\n","        FileSystem fs = FileSystem.get(URI.create(outputPath.toString()), conf);\n","        if (fs.exists(outputPath)) {\n","            fs.delete(outputPath, true); // delete recursively\n","        }\n","\n","        FileInputFormat.addInputPath(job, inputPath);\n","        FileOutputFormat.setOutputPath(job, outputPath);\n","\n","\n","        // 9. Submit the job and wait for it to complete\n","        // job.waitForCompletion(true) submits the job and returns true if successful, false otherwise\n","        return job.waitForCompletion(true) ? 0 : 1;\n","    }\n","\n","    // Main method to run the job using ToolRunner\n","    public static void main(String[] args) throws Exception {\n","        // ToolRunner efficiently runs Tool implementations, handling configuration\n","        int res = ToolRunner.run(new Configuration(), new WordCount(), args);\n","        System.exit(res);\n","    }\n","}\n","```\n","\n","**Reasoning for Driver:**\n","\n","*   We implement `Tool` and extend `Configured` to leverage `ToolRunner`.\n","*   The `run` method contains the main logic for setting up the job.\n","*   `Job.getInstance(conf, \"Word Count\")` creates a job instance with a descriptive name.\n","*   `job.setJarByClass(WordCount.class)` is crucial. It tells Hadoop which JAR file contains the job code, allowing the framework to distribute it to the cluster nodes.\n","*   `job.setMapperClass()` and `job.setReducerClass()` tell Hadoop which classes to use for the map and reduce phases.\n","*   `job.setCombinerClass()` is set to `WordCountReducer.class`. This is a performance optimization. The Reducer logic (summing 1s) can be applied locally on the Mapper nodes before the data is shuffled, reducing the amount of data sent over the network. Since the operation is associative and commutative (summing 1s), the Reducer logic works correctly as a Combiner.\n","*   `job.setOutputKeyClass()` and `job.setOutputValueClass()` specify the final output types written by the Reducer.\n","*   `FileInputFormat.addInputPath()` and `FileOutputFormat.setOutputPath()` define the input and output locations in HDFS (or another compatible file system).\n","*   We added code to check for and delete the output directory. Hadoop requires the output directory *not* to exist when the job starts.\n","*   `job.waitForCompletion(true)` submits the job to the cluster and waits for it to finish, printing progress details to the console.\n","*   The `main` method simply calls `ToolRunner.run`, passing the configuration, the `WordCount` instance, and the command-line arguments. `ToolRunner` manages the job execution based on the configuration.\n","\n","---\n","\n","**Step 5: Package and Run**\n","\n","1.  **Package:** Compile your Java code and package it into a JAR file. If you used Maven, you can typically do this with `mvn clean package`. Make sure your `pom.xml` is configured to create an executable JAR or a JAR containing your classes.\n","2.  **Prepare Input Data:** Upload your large text file (e.g., from Project Gutenberg) to your Hadoop Distributed File System (HDFS).\n","    ```bash\n","    hdfs dfs -put your_text_file.txt /user/yourname/input/\n","    ```\n","3.  **Run the Job:** Use the `hadoop jar` command to submit your JAR to the Hadoop cluster.\n","    ```bash\n","    hadoop jar your_wordcount.jar com.yourcompany.hadoop.WordCount /user/yourname/input /user/yourname/output_wordcount\n","    ```\n","    *   `your_wordcount.jar`: The path to the JAR file you created.\n","    *   `com.yourcompany.hadoop.WordCount`: The fully qualified name of your Driver class (the one with the `main` method and implementing `Tool`).\n","    *   `/user/yourname/input`: The HDFS path to your input file or directory (where you put `your_text_file.txt`).\n","    *   `/user/yourname/output_wordcount`: The HDFS path where the results will be written. This directory must *not* exist before running the job.\n","\n","4.  **View Results:** After the job completes (it might take some time for large files), the output will be in the specified HDFS output directory, typically in files named `part-r-xxxxx`.\n","    ```bash\n","    hdfs dfs -ls /user/yourname/output_wordcount\n","    hdfs dfs -cat /user/yourname/output_wordcount/part-r-00000\n","    ```\n","\n","The output files (`part-r-xxxxx`) will contain lines like:\n","\n","```\n","a\t12345\n","the\t67890\n","word\t567\n","...\n","```\n","Each line represents a word and its total count across the entire input file.\n","\n","This program demonstrates the fundamental MapReduce pattern for data processing: transforming input records (lines) into intermediate key-value pairs (word, 1) and then aggregating those pairs by key (word) in the Reducer.\n"]}],"source":["\n","if \"direct_response\" not in df.columns:\n","    df[\"direct_response\"] = \"\"\n","if \"cot_response\" not in df.columns:\n","    df[\"cot_response\"] = \"\"\n","\n","for index, row in df.iterrows():\n","    print(f\"Processing row {index+1}/{len(df)}...\")\n","\n","    try:\n","        prompt_direct = row['direct prompt']\n","        response_direct = make_request(prompt_direct, model)\n","        print(response_direct)\n","        df.at[index, \"direct_response\"] = response_direct\n","    except TooManyRequests:\n","        print(\"Rate limit hit during direct prompt. Skipping.\")\n","        df.at[index, \"direct_response\"] = \"Error: Rate limit exceeded.\"\n","    except Exception as e:\n","        print(f\"Error generating direct response: {e}\")\n","        df.at[index, \"direct_response\"] = f\"Error: {e}\"\n","\n","    time.sleep(DELAY_BETWEEN_REQUESTS)\n","\n","    try:\n","        prompt_cot = row['cot']\n","        response_cot = make_request(prompt_cot, model)\n","        print(response_cot)\n","        df.at[index, \"cot_response\"] = response_cot\n","    except TooManyRequests:\n","        print(\"Rate limit hit during CoT prompt. Skipping.\")\n","        df.at[index, \"cot_response\"] = \"Error: Rate limit exceeded.\"\n","    except Exception as e:\n","        print(f\"Error generating CoT response: {e}\")\n","        df.at[index, \"cot_response\"] = f\"Error: {e}\"\n","\n","    time.sleep(DELAY_BETWEEN_REQUESTS)\n","\n","    df.to_csv(\"/content/drive/MyDrive/Stanford NLP/prompts-step2 - responses.csv\", index=False)\n"]},{"cell_type":"markdown","metadata":{"id":"nZp79Gv1-Htc"},"source":["##Few shot"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":300},"executionInfo":{"elapsed":1566,"status":"ok","timestamp":1748742097726,"user":{"displayName":"Neeharika Gupta","userId":"13434636909198909500"},"user_tz":420},"id":"ihXDd9fOGkLF","outputId":"b122d8ad-cab0-4473-d563-336108f4a91f"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"summary":"{\n  \"name\": \"df_fs\",\n  \"rows\": 8,\n  \"fields\": [\n    {\n      \"column\": \"prompt\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"Write a Python script using PyTorch to train a convolutional neural network (CNN) on the MNIST dataset. Include data loading, model definition, training loop, and evaluation\\nimport numpy as np\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\n\\n\\nimport torch\\nfrom torch import optim\\nfrom torch import nn\\nfrom torch.utils.data import DataLoader\\nfrom tqdm import tqdm\\n\\n# !pip install torchvision\\nimport torchvision\\n\\nimport torch.nn.functional as F\\nimport torchvision.datasets as datasets\\nimport torchvision.transforms as transforms\\n\\n# !pip install torchmetrics\\nimport torchmetrics\\n\\nbatch_size = 60\\n\\ntrain_dataset = datasets.MNIST(root=\\\"dataset/\\\", download=True, train=True, transform=transforms.ToTensor())\\n\\ntrain_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\\n\\ntest_dataset = datasets.MNIST(root=\\\"dataset/\\\", download=True, train=False, transform=transforms.ToTensor())\\n\\ntest_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)\\n\\nclass CNN(nn.Module):\\n   def __init__(self, in_channels, num_classes):\\n\\n       \\\"\\\"\\\"\\n       Building blocks of convolutional neural network.\\n\\n       Parameters:\\n           * in_channels: Number of channels in the input image (for grayscale images, 1)\\n           * num_classes: Number of classes to predict. In our problem, 10 (i.e digits from  0 to 9).\\n       \\\"\\\"\\\"\\n       super(CNN, self).__init__()\\n\\n       # 1st convolutional layer\\n       self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=8, kernel_size=3, padding=1)\\n       # Max pooling layer\\n       self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\\n       # 2nd convolutional layer\\n       self.conv2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, padding=1)\\n       # Fully connected layer\\n       self.fc1 = nn.Linear(16 * 7 * 7, num_classes)\\n\\n   def forward(self, x):\\n       \\\"\\\"\\\"\\n       Define the forward pass of the neural network.\\n\\n       Parameters:\\n           x: Input tensor.\\n\\n       Returns:\\n           torch.Tensor\\n               The output tensor after passing through the network.\\n       \\\"\\\"\\\"\\n       x = F.relu(self.conv1(x))  # Apply first convolution and ReLU activation\\n       x = self.pool(x)           # Apply max pooling\\n       x = F.relu(self.conv2(x))  # Apply second convolution and ReLU activation\\n       x = self.pool(x)           # Apply max pooling\\n       x = x.reshape(x.shape[0], -1)  # Flatten the tensor\\n       x = self.fc1(x)            # Apply fully connected layer\\n       return x\\n       x = x.reshape(x.shape[0], -1)  # Flatten the tensor\\n       x = self.fc1(x)            # Apply fully connected layer\\n       return x\\n\\n# Define the loss function\\ncriterion = nn.CrossEntropyLoss()\\n\\n# Define the optimizer\\noptimizer = optim.Adam(model.parameters(), lr=0.001)\\n\\nnum_epochs=10\\nfor epoch in range(num_epochs):\\n # Iterate over training batches\\n   print(f\\\"Epoch [{epoch + 1}/{num_epochs}]\\\")\\n\\n   for batch_index, (data, targets) in enumerate(tqdm(dataloader_train)):\\n       data = data.to(device)\\n       targets = targets.to(device)\\n       scores = model(data)\\n       loss = criterion(scores, targets)\\n       optimizer.zero_grad()\\n       loss.backward()\\n       optimizer.step()\\n# Set up of multiclass accuracy metric\\nacc = Accuracy(task=\\\"multiclass\\\",num_classes=10)\\n\\n# Iterate over the dataset batches\\nmodel.eval()\\nwith torch.no_grad():\\n   for images, labels in dataloader_test:\\n       # Get predicted probabilities for test data batch\\n       outputs = model(images)\\n       _, preds = torch.max(outputs, 1)\\n       acc(preds, labels)\\n       precision(preds, labels)\\n       recall(preds, labels)\\n\\n#Compute total test accuracy\\ntest_accuracy = acc.compute()\\nprint(f\\\"Test accuracy: {test_accuracy}\\\")\\n\\nWrite a Python script using PyTorch to train a convolutional neural network (CNN) on the KMNIST dataset. Include data loading, model definition, training loop, and evaluation\\nfrom torch.nn import Module\\nfrom torch.nn import Conv2d\\nfrom torch.nn import Linear\\nfrom torch.nn import MaxPool2d\\nfrom torch.nn import ReLU\\nfrom torch.nn import LogSoftmax\\nfrom torch import flatten\\nimport matplotlib\\nmatplotlib.use(\\\"Agg\\\")\\nfrom pyimagesearch.lenet import LeNet\\nfrom sklearn.metrics import classification_report\\nfrom torch.utils.data import random_split\\nfrom torch.utils.data import DataLoader\\nfrom torchvision.transforms import ToTensor\\nfrom torchvision.datasets import KMNIST\\nfrom torch.optim import Adam\\nfrom torch import nn\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\nimport argparse\\nimport torch\\nimport time\\n\\nclass LeNet(Module):\\n        def __init__(self, numChannels, classes):\\n                # call the parent constructor\\n                super(LeNet, self).__init__()\\n                # initialize first set of CONV => RELU => POOL layers\\n                self.conv1 = Conv2d(in_channels=numChannels, out_channels=20,\\n                        kernel_size=(5, 5))\\n                self.relu1 = ReLU()\\n                self.maxpool1 = MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\\n                # initialize second set of CONV => RELU => POOL layers\\n                self.conv2 = Conv2d(in_channels=20, out_channels=50,\\n                        kernel_size=(5, 5))\\n                self.relu2 = ReLU()\\n                self.maxpool2 = MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\\n                # initialize first (and only) set of FC => RELU layers\\n                self.fc1 = Linear(in_features=800, out_features=500)\\n                self.relu3 = ReLU()\\n                # initialize our softmax classifier\\n                self.fc2 = Linear(in_features=500, out_features=classes)\\n                self.logSoftmax = LogSoftmax(dim=1)\\n\\ndef forward(self, x):\\n                # pass the input through our first set of CONV => RELU =>\\n                # POOL layers\\n                x = self.conv1(x)\\n                x = self.relu1(x)\\n                x = self.maxpool1(x)\\n                # pass the output from the previous layer through the second\\n                # set of CONV => RELU => POOL layers\\n                x = self.conv2(x)\\n                x = self.relu2(x)\\n                x = self.maxpool2(x)\\n                # flatten the output from the previous layer and pass it\\n                # through our only set of FC => RELU layers\\n                x = flatten(x, 1)\\n                x = self.fc1(x)\\n                x = self.relu3(x)\\n                # pass the output to our softmax classifier to get our output\\n                # predictions\\n                x = self.fc2(x)\\n                output = self.logSoftmax(x)\\n                # return the output predictions\\n                return output\\n# construct the argument parser and parse the arguments\\nap = argparse.ArgumentParser()\\nap.add_argument(\\\"-m\\\", \\\"--model\\\", type=str, required=True,\\n        help=\\\"path to output trained model\\\")\\nap.add_argument(\\\"-p\\\", \\\"--plot\\\", type=str, required=True,\\n        help=\\\"path to output loss/accuracy plot\\\")\\nargs = vars(ap.parse_args())\\n\\nPyTorch: Training your first Convolutional Neural Network (CNN)\\n# define training hyperparameters\\nINIT_LR = 1e-3\\nBATCH_SIZE = 64\\nEPOCHS = 10\\n# define the train and val splits\\nTRAIN_SPLIT = 0.75\\nVAL_SPLIT = 1 - TRAIN_SPLIT\\n# set the device we will be using to train the model\\ndevice = torch.device(\\\"cuda\\\" if torch.cuda.is_available() else \\\"cpu\\\")\\n\\n# load the KMNIST dataset\\nprint(\\\"[INFO] loading the KMNIST dataset...\\\")\\ntrainData = KMNIST(root=\\\"data\\\", train=True, download=True,\\n        transform=ToTensor())\\ntestData = KMNIST(root=\\\"data\\\", train=False, download=True,\\n        transform=ToTensor())\\n# calculate the train/validation split\\nprint(\\\"[INFO] generating the train/validation split...\\\")\\nnumTrainSamples = int(len(trainData) * TRAIN_SPLIT)\\nnumValSamples = int(len(trainData) * VAL_SPLIT)\\n(trainData, valData) = random_split(trainData,\\n        [numTrainSamples, numValSamples],\\n        generator=torch.Generator().manual_seed(42))\\n\\n# initialize the train, validation, and test data loaders\\ntrainDataLoader = DataLoader(trainData, shuffle=True,\\n        batch_size=BATCH_SIZE)\\nvalDataLoader = DataLoader(valData, batch_size=BATCH_SIZE)\\ntestDataLoader = DataLoader(testData, batch_size=BATCH_SIZE)\\n# calculate steps per epoch for training and validation set\\ntrainSteps = len(trainDataLoader.dataset) // BATCH_SIZE\\nvalSteps = len(valDataLoader.dataset) // BATCH_SIZE\\n\\n# initialize the LeNet model\\nprint(\\\"[INFO] initializing the LeNet model...\\\")\\nmodel = LeNet(\\n        numChannels=1,\\n        classes=len(trainData.dataset.classes)).to(device)\\n# initialize our optimizer and loss function\\nopt = Adam(model.parameters(), lr=INIT_LR)\\nlossFn = nn.NLLLoss()\\n# initialize a dictionary to store training history\\nH = {\\n        \\\"train_loss\\\": [],\\n        \\\"train_acc\\\": [],\\n        \\\"val_loss\\\": [],\\n        \\\"val_acc\\\": []\\n}\\n# measure how long training is going to take\\nprint(\\\"[INFO] training the network...\\\")\\n\\n# loop over our epochs\\nfor e in range(0, EPOCHS):\\n        # set the model in training mode\\n        model.train()\\n        # initialize the total training and validation loss\\n        totalTrainLoss = 0\\n        totalValLoss = 0\\n        # initialize the number of correct predictions in the training\\n        # and validation step\\n        trainCorrect = 0\\n        valCorrect = 0\\n        # loop over the training set\\n        for (x, y) in trainDataLoader:\\n                # send the input to the device\\n                (x, y) = (x.to(device), y.to(device))\\n                # perform a forward pass and calculate the training loss\\n                pred = model(x)\\n                loss = lossFn(pred, y)\\n                # zero out the gradients, perform the backpropagation step,\\n                # and update the weights\\n                opt.zero_grad()\\n                loss.backward()\\n                opt.step()\\n                # add the loss to the total training loss so far and\\n                # calculate the number of correct predictions\\n                totalTrainLoss += loss\\n                trainCorrect += (pred.argmax(1) == y).type(\\n                        torch.float).sum().item()\\n\\n        # switch off autograd for evaluation\\n        with torch.no_grad():\\n                # set the model in evaluation mode\\n                model.eval()\\n                # loop over the validation set\\n                for (x, y) in valDataLoader:\\n                        # send the input to the device\\n                        (x, y) = (x.to(device), y.to(device))\\n                        # make the predictions and calculate the validation loss\\n                        pred = model(x)\\n                        totalValLoss += lossFn(pred, y)\\n                        # calculate the number of correct predictions\\n                        valCorrect += (pred.argmax(1) == y).type(\\n                                torch.float).sum().item()\\n\\n        # calculate the average training and validation loss\\n        avgTrainLoss = totalTrainLoss / trainSteps\\n        avgValLoss = totalValLoss / valSteps\\n        # calculate the training and validation accuracy\\n        trainCorrect = trainCorrect / len(trainDataLoader.dataset)\\n        valCorrect = valCorrect / len(valDataLoader.dataset)\\n        # update our training history\\n        H[\\\"train_loss\\\"].append(avgTrainLoss.cpu().detach().numpy())\\n        H[\\\"train_acc\\\"].append(trainCorrect)\\n        H[\\\"val_loss\\\"].append(avgValLoss.cpu().detach().numpy())\\n        H[\\\"val_acc\\\"].append(valCorrect)\\n        # print the model training and validation information\\n        print(\\\"[INFO] EPOCH: {}/{}\\\".format(e + 1, EPOCHS))\\n        print(\\\"Train loss: {:.6f}, Train accuracy: {:.4f}\\\".format(\\n                avgTrainLoss, trainCorrect))\\n        print(\\\"Val loss: {:.6f}, Val accuracy: {:.4f}\\\\n\\\".format(\\n                avgValLoss, valCorrect))\\n\\nWrite a Python script using PyTorch to train a convolutional neural network (CNN) on the Cifar 10 dataset. Include data loading, model definition, training loop, and evaluation\\n\",\n          \"Write a PyTorch class that creates custom dataset. It takes in input of img dir and annotations csv file.\\nimport os\\nimport pandas as pd\\nfrom torchvision.io import read_image\\n\\nclass CustomImageDataset(Dataset):\\n    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\\n        self.img_labels = pd.read_csv(annotations_file)\\n        self.img_dir = img_dir\\n        self.transform = transform\\n        self.target_transform = target_transform\\n\\n    def __len__(self):\\n        return len(self.img_labels)\\n\\n    def __getitem__(self, idx):\\n        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\\n        image = read_image(img_path)\\n        label = self.img_labels.iloc[idx, 1]\\n        if self.transform:\\n            image = self.transform(image)\\n        if self.target_transform:\\n            label = self.target_transform(label)\\n        return image, label\\n\\nWrite a PyTorch class that subclasses torch.utils.data.Dataset to load 100 image files and labels from a folder. The data would be randomly created and the class should also be defined such that its a custom class.Use this class with a DataLoader for batching.\\nimport torch\\nfrom torch.utils.data import Dataset, DataLoader\\nimport torch.nn as nn\\nimport torch.optim as optim\\nimport numpy as np\\n\\n# Custom Dataset class\\nclass CustomDataset(Dataset):\\n    def __init__(self, data, labels):\\n        self.data = data\\n        self.labels = labels\\n\\n    def __len__(self):\\n        return len(self.data)\\n\\n    def __getitem__(self, idx):\\n        sample = self.data[idx]\\n        label = self.labels[idx]\\n        return sample, label\\n\\n# Prepare data\\ndata = np.random.randn(100, 3, 32, 32)  # 100 samples of 3x32x32 images\\nlabels = np.random.randint(0, 10, size=(100,))  # 100 labels in the range 0-9\\n\\n# Create Dataset\\ndataset = CustomDataset(data, labels)\\n\\n# Create DataLoader\\ndataloader = DataLoader(dataset, batch_size=4, shuffle=True, num_workers=2)\\n\\nWrite a PyTorch class that subclasses torch.utils.data.Dataset to load image files and labels from a folder. Use this class with a DataLoader for batching. Use FashionMNIST dataset.\\n\",\n          \"Create a python script that uses tensorflow to model a linear regression problem using randomly genenrated 50 datat points. You need to train the model and also get the predictions.\\nimport numpy as np\\nimport tensorflow as tf\\nimport matplotlib.pyplot as plt\\n\\nnp.random.seed(101)\\n\\n# Generating random linear data\\n# There will be 50 data points ranging from 0 to 50\\nx = np.linspace(0, 50, 50)\\ny = np.linspace(0, 50, 50)\\n\\n# Adding noise to the random linear data\\nx += np.random.uniform(-4, 4, 50)\\ny += np.random.uniform(-4, 4, 50)\\n\\nn = len(x) # Number of data points\\n\\n# Plot of Training Data\\nplt.scatter(x, y)\\nplt.xlabel('x')\\nplt.ylabel('y')\\nplt.title(\\\"Training Data\\\")\\nplt.show()\\n\\nX = tf.placeholder(\\\"float\\\")\\nY = tf.placeholder(\\\"float\\\")\\n\\nW = tf.Variable(np.random.randn(), name = \\\"W\\\")\\nb = tf.Variable(np.random.randn(), name = \\\"b\\\")\\n\\nlearning_rate = 0.01\\ntraining_epochs = 1000\\n# Hypothesis\\ny_pred = tf.add(tf.multiply(X, W), b)\\n\\n# Mean Squared Error Cost Function\\ncost = tf.reduce_sum(tf.pow(y_pred-Y, 2)) / (2 * n)\\n\\n# Gradient Descent Optimizer\\noptimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\\n\\n# Global Variables Initializer\\ninit = tf.global_variables_initializer()\\n\\n# Starting the Tensorflow Session\\nwith tf.Session() as sess:\\n    \\n    # Initializing the Variables\\n    sess.run(init)\\n    \\n    # Iterating through all the epochs\\n    for epoch in range(training_epochs):\\n        \\n        # Feeding each data point into the optimizer using Feed Dictionary\\n        for (_x, _y) in zip(x, y):\\n            sess.run(optimizer, feed_dict = {X : _x, Y : _y})\\n        \\n        # Displaying the result after every 50 epochs\\n        if (epoch + 1) % 50 == 0:\\n            # Calculating the cost a every epoch\\n            c = sess.run(cost, feed_dict = {X : x, Y : y})\\n            print(\\\"Epoch\\\", (epoch + 1), \\\": cost =\\\", c, \\\"W =\\\", sess.run(W), \\\"b =\\\", sess.run(b))\\n    \\n    # Storing necessary values to be used outside the Session\\n    training_cost = sess.run(cost, feed_dict ={X: x, Y: y})\\n    weight = sess.run(W)\\n    bias = sess.run(b)\\n\\n# Calculating the predictions\\npredictions = weight * x + bias\\nprint(\\\"Training cost =\\\", training_cost, \\\"Weight =\\\", weight, \\\"bias =\\\", bias, '\\\\n')\\n\\nWrite a Python function using TensorFlow to create and compile a simple linear regression model which has a normalizer layer and a dense layer. Train it on the fuel efficiency dataset.\\npip install -q seaborn\\n\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\nimport pandas as pd\\nimport seaborn as sns\\n\\n# Make NumPy printouts easier to read.\\nnp.set_printoptions(precision=3, suppress=True)\\nimport tensorflow as tf\\n\\nfrom tensorflow import keras\\nfrom tensorflow.keras import layers\\n\\nprint(tf.__version__)\\n\\nurl = 'http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data'\\ncolumn_names = ['MPG', 'Cylinders', 'Displacement', 'Horsepower', 'Weight',\\n                'Acceleration', 'Model Year', 'Origin']\\n\\nraw_dataset = pd.read_csv(url, names=column_names,\\n                          na_values='?', comment='\\\\t',\\n                          sep=' ', skipinitialspace=True)\\n\\ndataset = raw_dataset.copy()\\ndataset.tail()\\n\\ndataset.isna().sum()\\n\\ndataset = dataset.dropna()\\n\\ndataset['Origin'] = dataset['Origin'].map({1: 'USA', 2: 'Europe', 3: 'Japan'})\\n\\ndataset = pd.get_dummies(dataset, columns=['Origin'], prefix='', prefix_sep='')\\ndataset.tail()\\n\\ntrain_dataset = dataset.sample(frac=0.8, random_state=0)\\ntest_dataset = dataset.drop(train_dataset.index)\\n\\ntrain_features = train_dataset.copy()\\ntest_features = test_dataset.copy()\\n\\ntrain_labels = train_features.pop('MPG')\\ntest_labels = test_features.pop('MPG')\\n\\nhorsepower = np.array(train_features['Horsepower'])\\n\\nhorsepower_normalizer = layers.Normalization(input_shape=[1,], axis=None)\\nhorsepower_normalizer.adapt(horsepower)\\n\\nhorsepower_model = tf.keras.Sequential([\\n    horsepower_normalizer,\\n    layers.Dense(units=1)\\n])\\n\\nhorsepower_model.summary()\\n\\nhorsepower_model.compile(\\n    optimizer=tf.keras.optimizers.Adam(learning_rate=0.1),\\n    loss='mean_absolute_error')\\n\\n\\nhistory = horsepower_model.fit(\\n    train_features['Horsepower'],\\n    train_labels,\\n    epochs=100,\\n    # Suppress logging.\\n    verbose=0,\\n    # Calculate validation results on 20% of the training data.\\n    validation_split = 0.2)\\n\\nWrite a Python function using TensorFlow to create and compile a simple linear regression model with one dense layer. Train it on the classic California housing dataset.\\n\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}","type":"dataframe","variable_name":"df_fs"},"text/html":["\n","  <div id=\"df-19ba0c00-7604-436b-acb4-2aa7f99c3dd1\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>prompt</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Create a python script that uses tensorflow to...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Write a Python script using PyTorch to train a...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Create a scikit learn pipeline that builds sta...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>In this example we learn to identify past vs p...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Build an image recognition tensorflow model to...</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>Write a PyTorch class that creates custom data...</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>Write a Python script that loads the UCI handw...</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>Write a Java program using the Hadoop MapReduc...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-19ba0c00-7604-436b-acb4-2aa7f99c3dd1')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-19ba0c00-7604-436b-acb4-2aa7f99c3dd1 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-19ba0c00-7604-436b-acb4-2aa7f99c3dd1');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    <div id=\"df-8909fff1-8d1a-4228-9b08-14a359ba919b\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-8909fff1-8d1a-4228-9b08-14a359ba919b')\"\n","                title=\"Suggest charts\"\n","                style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","      <script>\n","        async function quickchart(key) {\n","          const quickchartButtonEl =\n","            document.querySelector('#' + key + ' button');\n","          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","          quickchartButtonEl.classList.add('colab-df-spinner');\n","          try {\n","            const charts = await google.colab.kernel.invokeFunction(\n","                'suggestCharts', [key], {});\n","          } catch (error) {\n","            console.error('Error during call to suggestCharts:', error);\n","          }\n","          quickchartButtonEl.classList.remove('colab-df-spinner');\n","          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","        }\n","        (() => {\n","          let quickchartButtonEl =\n","            document.querySelector('#df-8909fff1-8d1a-4228-9b08-14a359ba919b button');\n","          quickchartButtonEl.style.display =\n","            google.colab.kernel.accessAllowed ? 'block' : 'none';\n","        })();\n","      </script>\n","    </div>\n","\n","  <div id=\"id_d195b491-6f17-4ef8-8eee-fbb37bd328fc\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_fs')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_d195b491-6f17-4ef8-8eee-fbb37bd328fc button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('df_fs');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"],"text/plain":["                                              prompt\n","0  Create a python script that uses tensorflow to...\n","1  Write a Python script using PyTorch to train a...\n","2  Create a scikit learn pipeline that builds sta...\n","3  In this example we learn to identify past vs p...\n","4  Build an image recognition tensorflow model to...\n","5  Write a PyTorch class that creates custom data...\n","6  Write a Python script that loads the UCI handw...\n","7  Write a Java program using the Hadoop MapReduc..."]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["df_fs = pd.read_csv(\"/content/drive/MyDrive/Stanford NLP/few_shot_prompts.csv\")\n","df_fs"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":160},"executionInfo":{"elapsed":219836,"status":"ok","timestamp":1748742407570,"user":{"displayName":"Neeharika Gupta","userId":"13434636909198909500"},"user_tz":420},"id":"1shjirNIHVS7","outputId":"07c31357-785c-46b0-ad2c-82e351b8e611"},"outputs":[{"name":"stdout","output_type":"stream","text":["Processing row 1/8...\n","Processing row 2/8...\n","Processing row 3/8...\n","Processing row 4/8...\n","Processing row 5/8...\n","Processing row 6/8...\n","Processing row 7/8...\n","Processing row 8/8...\n"]}],"source":["if \"few_shot\" not in df_fs.columns:\n","    df_fs[\"few_shot\"] = \"\"\n","\n","\n","for index, row in df_fs.iterrows():\n","    print(f\"Processing row {index+1}/{len(df_fs)}...\")\n","\n","    try:\n","        prompt_fs = row['prompt']\n","        response_fs = make_request(prompt_fs,model)\n","        df_fs.at[index, \"few_shot\"] = response_fs\n","    except Exception as e:\n","        print(f\"Error generating direct response: {e}\")\n","        df_fs.at[index, \"few_shot\"] = f\"Error: {e}\"\n","\n","    time.sleep(DELAY_BETWEEN_REQUESTS)\n","\n","    df_fs.to_csv(\"/content/drive/MyDrive/Stanford NLP/prompts-step2 - responses_few_Shot_gemini.csv\", index=False)"]},{"cell_type":"markdown","metadata":{"id":"X1mzJKOA1qrz"},"source":["##API extraction"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":8436,"status":"ok","timestamp":1748756535571,"user":{"displayName":"Neeharika Gupta","userId":"13434636909198909500"},"user_tz":420},"id":"UzDHqVfUJ_aM","outputId":"e4eb442c-a211-4204-90fc-6b67364e0e30"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"summary":"{\n  \"name\": \"df_api\",\n  \"rows\": 33,\n  \"fields\": [\n    {\n      \"column\": \"API Component\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 33,\n        \"samples\": [\n          \"notebook_login\",\n          \"torch.optim\",\n          \"AutoTokenizer\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Documentation\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 30,\n        \"samples\": [\n          \"Trainer is a simple but feature-complete training and eval loop for PyTorch, optimized for \\ud83e\\udd17 Transformers.  Args:     model ([`PreTrainedModel`] or `torch.nn.Module`, *optional*):         The model to train, evaluate or use for predictions. If not provided, a `model_init` must be passed.          <Tip>          [`Trainer`] is optimized to work with the [`PreTrainedModel`] provided by the library. You can still use         your own models defined as `torch.nn.Module` as long as they work the same way as the \\ud83e\\udd17 Transformers         models.          </Tip>      args ([`TrainingArguments`], *optional*):         The arguments to tweak for training. Will default to a basic instance of [`TrainingArguments`] with the         `output_dir` set to a directory named *tmp_trainer* in the current directory if not provided.     data_collator (`DataCollator`, *optional*):         The function to use to form a batch from a list of elements of `train_dataset` or `eval_dataset`. Will         default to [`default_data_collator`] if no `processing_class` is provided, an instance of         [`DataCollatorWithPadding`] otherwise if the processing_class is a feature extractor or tokenizer.     train_dataset (Union[`torch.utils.data.Dataset`, `torch.utils.data.IterableDataset`, `datasets.Dataset`], *optional*):         The dataset to use for training. If it is a [`~datasets.Dataset`], columns not accepted by the         `model.forward()` method are automatically removed.          Note that if it's a `torch.utils.data.IterableDataset` with some randomization and you are training in a         distributed fashion, your iterable dataset should either use a internal attribute `generator` that is a         `torch.Generator` for the randomization that must be identical on all processes (and the Trainer will         manually set the seed of this `generator` at each epoch) or have a `set_epoch()` method that internally         sets the seed of the RNGs used.     eval_dataset (Union[`torch.utils.data.Dataset`, Dict[str, `torch.utils.data.Dataset`, `datasets.Dataset`]), *optional*):          The dataset to use for evaluation. If it is a [`~datasets.Dataset`], columns not accepted by the          `model.forward()` method are automatically removed. If it is a dictionary, it will evaluate on each          dataset prepending the dictionary key to the metric name.     processing_class (`PreTrainedTokenizerBase` or `BaseImageProcessor` or `FeatureExtractionMixin` or `ProcessorMixin`, *optional*):         Processing class used to process the data. If provided, will be used to automatically process the inputs         for the model, and it will be saved along the model to make it easier to rerun an interrupted training or         reuse the fine-tuned model.         This supersedes the `tokenizer` argument, which is now deprecated.     model_init (`Callable[[], PreTrainedModel]`, *optional*):         A function that instantiates the model to be used. If provided, each call to [`~Trainer.train`] will start         from a new instance of the model as given by this function.          The function may have zero argument, or a single one containing the optuna/Ray Tune/SigOpt trial object, to         be able to choose different architectures according to hyper parameters (such as layer count, sizes of         inner layers, dropout probabilities etc).     compute_loss_func (`Callable`, *optional*):         A function that accepts the raw model outputs, labels, and the number of items in the entire accumulated         batch (batch_size * gradient_accumulation_steps) and returns the loss. For example, see the default [loss function](https://github.com/huggingface/transformers/blob/052e652d6d53c2b26ffde87e039b723949a53493/src/transformers/trainer.py#L3618) used by [`Trainer`].     compute_metrics (`Callable[[EvalPrediction], Dict]`, *optional*):         The function that will be used to compute metrics at evaluation. Must take a [`EvalPrediction`] and return         a dictionary string to metric values. *Note* When passing TrainingArgs with `batch_eval_metrics` set to         `True`, your compute_metrics function must take a boolean `compute_result` argument. This will be triggered         after the last eval batch to signal that the function needs to calculate and return the global summary         statistics rather than accumulating the batch-level statistics     callbacks (List of [`TrainerCallback`], *optional*):         A list of callbacks to customize the training loop. Will add those to the list of default callbacks         detailed in [here](callback).          If you want to remove one of the default callbacks used, use the [`Trainer.remove_callback`] method.     optimizers (`Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR]`, *optional*, defaults to `(None, None)`):         A tuple containing the optimizer and the scheduler to use. Will default to an instance of [`AdamW`] on your         model and a scheduler given by [`get_linear_schedule_with_warmup`] controlled by `args`.     optimizer_cls_and_kwargs (`Tuple[Type[torch.optim.Optimizer], Dict[str, Any]]`, *optional*):         A tuple containing the optimizer class and keyword arguments to use.         Overrides `optim` and `optim_args` in `args`. Incompatible with the `optimizers` argument.          Unlike `optimizers`, this argument avoids the need to place model parameters on the correct devices before initializing the Trainer.     preprocess_logits_for_metrics (`Callable[[torch.Tensor, torch.Tensor], torch.Tensor]`, *optional*):         A function that preprocess the logits right before caching them at each evaluation step. Must take two         tensors, the logits and the labels, and return the logits once processed as desired. The modifications made         by this function will be reflected in the predictions received by `compute_metrics`.          Note that the labels (second parameter) will be `None` if the dataset does not have them.  Important attributes:      - **model** -- Always points to the core model. If using a transformers model, it will be a [`PreTrainedModel`]       subclass.     - **model_wrapped** -- Always points to the most external model in case one or more other modules wrap the       original model. This is the model that should be used for the forward pass. For example, under `DeepSpeed`,       the inner model is wrapped in `DeepSpeed` and then again in `torch.nn.DistributedDataParallel`. If the inner       model hasn't been wrapped, then `self.model_wrapped` is the same as `self.model`.     - **is_model_parallel** -- Whether or not a model has been switched to a model parallel mode (different from       data parallelism, this means some of the model layers are split on different GPUs).     - **place_model_on_device** -- Whether or not to automatically place the model on the device - it will be set       to `False` if model parallel or deepspeed is used, or if the default       `TrainingArguments.place_model_on_device` is overridden to return `False` .     - **is_in_train** -- Whether or not a model is currently running `train` (e.g. when `evaluate` is called while       in `train`)\",\n          \":mod:`torch.optim` is a package implementing various optimization algorithms.  Most commonly used methods are already supported, and the interface is general enough, so that more sophisticated ones can also be easily integrated in the future.\",\n          \"This is a generic tokenizer class that will be instantiated as one of the tokenizer classes of the library when created with the [`AutoTokenizer.from_pretrained`] class method.  This class cannot be instantiated directly using `__init__()` (throws an error).\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}","type":"dataframe","variable_name":"df_api"},"text/html":["\n","  <div id=\"df-524fc460-64e1-40a9-9ce9-2a2445edcbdc\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>API Component</th>\n","      <th>Documentation</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>tensorflow</td>\n","      <td>Top-level module of TensorFlow. By convention,...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>tensorflow.keras</td>\n","      <td>DO NOT EDIT.  This file was autogenerated. Do ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Sequential</td>\n","      <td>`Sequential` groups a linear stack of layers i...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Dense</td>\n","      <td>Just your regular densely-connected NN layer. ...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Input</td>\n","      <td>Used to instantiate a Keras tensor.  A Keras t...</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>Concatenate</td>\n","      <td>Concatenates a list of inputs.  It takes as in...</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>fetch_california_housing</td>\n","      <td>Load the California housing dataset (regressio...</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>make_classification</td>\n","      <td>Generate a random n-class classification probl...</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>train_test_split</td>\n","      <td>Split arrays or matrices into random train and...</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>StandardScaler</td>\n","      <td>Standardize features by removing the mean and ...</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>LogisticRegression</td>\n","      <td>Logistic Regression (aka logit, MaxEnt) classi...</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>make_pipeline</td>\n","      <td>Construct a :class:`Pipeline` from the given e...</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>PCA</td>\n","      <td>Principal component analysis (PCA).  Linear di...</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>torch</td>\n","      <td>The torch package contains data structures for...</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>torch.nn</td>\n","      <td>No documentation found.</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>torch.optim</td>\n","      <td>:mod:`torch.optim` is a package implementing v...</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>torchvision</td>\n","      <td>No documentation found.</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>torchvision.transforms</td>\n","      <td>No documentation found.</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>torch.nn.functional</td>\n","      <td>Functional interface.</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>Dataset</td>\n","      <td>An abstract class representing a :class:`Datas...</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>DataLoader</td>\n","      <td>Data loader combines a dataset and a sampler, ...</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>torchvision.datasets</td>\n","      <td>No documentation found.</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>ToTensor</td>\n","      <td>Convert a PIL Image or ndarray to tensor and s...</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>numpy</td>\n","      <td>NumPy =====  Provides   1. An array object of ...</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>pandas</td>\n","      <td>pandas - a powerful data analysis and manipula...</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>matplotlib.pyplot</td>\n","      <td>`matplotlib.pyplot` is a state-based interface...</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>AutoTokenizer</td>\n","      <td>This is a generic tokenizer class that will be...</td>\n","    </tr>\n","    <tr>\n","      <th>27</th>\n","      <td>DataCollatorWithPadding</td>\n","      <td>Data collator that will dynamically pad the in...</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>AutoModelForSequenceClassification</td>\n","      <td>This is a generic model class that will be ins...</td>\n","    </tr>\n","    <tr>\n","      <th>29</th>\n","      <td>TrainingArguments</td>\n","      <td>TrainingArguments is the subset of the argumen...</td>\n","    </tr>\n","    <tr>\n","      <th>30</th>\n","      <td>Trainer</td>\n","      <td>Trainer is a simple but feature-complete train...</td>\n","    </tr>\n","    <tr>\n","      <th>31</th>\n","      <td>notebook_login</td>\n","      <td>Displays a widget to log in to the HF website ...</td>\n","    </tr>\n","    <tr>\n","      <th>32</th>\n","      <td>load_dataset</td>\n","      <td>Load a dataset from the Hugging Face Hub, or a...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-524fc460-64e1-40a9-9ce9-2a2445edcbdc')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-524fc460-64e1-40a9-9ce9-2a2445edcbdc button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-524fc460-64e1-40a9-9ce9-2a2445edcbdc');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    <div id=\"df-5a5f35f5-350e-4783-9563-7d9bf9a3d973\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5a5f35f5-350e-4783-9563-7d9bf9a3d973')\"\n","                title=\"Suggest charts\"\n","                style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","      <script>\n","        async function quickchart(key) {\n","          const quickchartButtonEl =\n","            document.querySelector('#' + key + ' button');\n","          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","          quickchartButtonEl.classList.add('colab-df-spinner');\n","          try {\n","            const charts = await google.colab.kernel.invokeFunction(\n","                'suggestCharts', [key], {});\n","          } catch (error) {\n","            console.error('Error during call to suggestCharts:', error);\n","          }\n","          quickchartButtonEl.classList.remove('colab-df-spinner');\n","          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","        }\n","        (() => {\n","          let quickchartButtonEl =\n","            document.querySelector('#df-5a5f35f5-350e-4783-9563-7d9bf9a3d973 button');\n","          quickchartButtonEl.style.display =\n","            google.colab.kernel.accessAllowed ? 'block' : 'none';\n","        })();\n","      </script>\n","    </div>\n","\n","  <div id=\"id_afa38904-c12f-4b40-91f3-9a85d4398163\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_api')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_afa38904-c12f-4b40-91f3-9a85d4398163 button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('df_api');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"],"text/plain":["                         API Component  \\\n","0                           tensorflow   \n","1                     tensorflow.keras   \n","2                           Sequential   \n","3                                Dense   \n","4                                Input   \n","5                          Concatenate   \n","6             fetch_california_housing   \n","7                  make_classification   \n","8                     train_test_split   \n","9                       StandardScaler   \n","10                  LogisticRegression   \n","11                       make_pipeline   \n","12                                 PCA   \n","13                               torch   \n","14                            torch.nn   \n","15                         torch.optim   \n","16                         torchvision   \n","17              torchvision.transforms   \n","18                 torch.nn.functional   \n","19                             Dataset   \n","20                          DataLoader   \n","21                torchvision.datasets   \n","22                            ToTensor   \n","23                               numpy   \n","24                              pandas   \n","25                   matplotlib.pyplot   \n","26                       AutoTokenizer   \n","27             DataCollatorWithPadding   \n","28  AutoModelForSequenceClassification   \n","29                   TrainingArguments   \n","30                             Trainer   \n","31                      notebook_login   \n","32                        load_dataset   \n","\n","                                        Documentation  \n","0   Top-level module of TensorFlow. By convention,...  \n","1   DO NOT EDIT.  This file was autogenerated. Do ...  \n","2   `Sequential` groups a linear stack of layers i...  \n","3   Just your regular densely-connected NN layer. ...  \n","4   Used to instantiate a Keras tensor.  A Keras t...  \n","5   Concatenates a list of inputs.  It takes as in...  \n","6   Load the California housing dataset (regressio...  \n","7   Generate a random n-class classification probl...  \n","8   Split arrays or matrices into random train and...  \n","9   Standardize features by removing the mean and ...  \n","10  Logistic Regression (aka logit, MaxEnt) classi...  \n","11  Construct a :class:`Pipeline` from the given e...  \n","12  Principal component analysis (PCA).  Linear di...  \n","13  The torch package contains data structures for...  \n","14                            No documentation found.  \n","15  :mod:`torch.optim` is a package implementing v...  \n","16                            No documentation found.  \n","17                            No documentation found.  \n","18                              Functional interface.  \n","19  An abstract class representing a :class:`Datas...  \n","20  Data loader combines a dataset and a sampler, ...  \n","21                            No documentation found.  \n","22  Convert a PIL Image or ndarray to tensor and s...  \n","23  NumPy =====  Provides   1. An array object of ...  \n","24  pandas - a powerful data analysis and manipula...  \n","25  `matplotlib.pyplot` is a state-based interface...  \n","26  This is a generic tokenizer class that will be...  \n","27  Data collator that will dynamically pad the in...  \n","28  This is a generic model class that will be ins...  \n","29  TrainingArguments is the subset of the argumen...  \n","30  Trainer is a simple but feature-complete train...  \n","31  Displays a widget to log in to the HF website ...  \n","32  Load a dataset from the Hugging Face Hub, or a...  "]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["df_api = pd.read_csv(\"/content/drive/MyDrive/Stanford NLP/api_documentation.csv\")\n","df_api"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HMjh3fXY2Z8P"},"outputs":[],"source":["prompt=\"Given the following API documentation, extract a structured representation of the API, including types, functions, and their relationships: [API documentation] Provide the structured representation in JSON format.\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":606},"executionInfo":{"elapsed":723751,"status":"ok","timestamp":1748757259320,"user":{"displayName":"Neeharika Gupta","userId":"13434636909198909500"},"user_tz":420},"id":"OMtOaFNA2p5p","outputId":"f3e6b699-90f5-4194-c61a-2c4106d329f5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Processing row 1/33...\n","Processing row 2/33...\n","Processing row 3/33...\n","Processing row 4/33...\n","Processing row 5/33...\n","Processing row 6/33...\n","Processing row 7/33...\n","Processing row 8/33...\n","Processing row 9/33...\n","Processing row 10/33...\n","Processing row 11/33...\n","Processing row 12/33...\n","Processing row 13/33...\n","Processing row 14/33...\n","Processing row 15/33...\n","Processing row 16/33...\n","Processing row 17/33...\n","Processing row 18/33...\n","Processing row 19/33...\n","Processing row 20/33...\n","Processing row 21/33...\n","Processing row 22/33...\n","Processing row 23/33...\n","Processing row 24/33...\n","Processing row 25/33...\n","Processing row 26/33...\n","Processing row 27/33...\n","Processing row 28/33...\n","Processing row 29/33...\n","Processing row 30/33...\n","Processing row 31/33...\n","Processing row 32/33...\n","Processing row 33/33...\n"]}],"source":["json_outputs = []\n","\n","for index, row in df_api.iterrows():\n","    print(f\"Processing row {index+1}/{len(df_api)}...\")\n","\n","    try:\n","        prompt = (\n","            f\"Given the following API documentation, extract a structured representation of the API, including types, functions, and their relationships:\\n\"\n","            f\"{row['Documentation'].strip()}\\n\\n\"\n","            f\"Provide the structured representation in JSON format.\"\n","        )\n","\n","        response = make_request(prompt, model)\n","    except Exception as e:\n","        print(f\"Error generating response: {e}\")\n","        response = f\"Error: {e}\"\n","\n","    json_outputs.append(response)\n","\n","    time.sleep(DELAY_BETWEEN_REQUESTS)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NUY2W_LY5iA3"},"outputs":[],"source":["df_results = pd.DataFrame({'api_extract': json_outputs})\n","\n","output_path = \"/content/drive/MyDrive/Stanford NLP/api_extraction_results.csv\"\n","df_results.to_csv(output_path, index=False)"]},{"cell_type":"markdown","metadata":{"id":"FFULkvKhBfnJ"},"source":["##Neurosymbolic Generation"]},{"cell_type":"markdown","metadata":{"id":"RNrJLGsnQQ5V"},"source":["###Neural suggestion"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"collapsed":true,"executionInfo":{"elapsed":123,"status":"ok","timestamp":1749074188461,"user":{"displayName":"Neeharika Gupta","userId":"13434636909198909500"},"user_tz":420},"id":"VKq3mE1Wm5vx","outputId":"70625edd-71fd-42f8-82c9-779c465cc80d"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"summary":"{\n  \"name\": \"df_api\",\n  \"rows\": 33,\n  \"fields\": [\n    {\n      \"column\": \"API Component\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 33,\n        \"samples\": [\n          \"notebook_login\",\n          \"torch.optim\",\n          \"AutoTokenizer\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Documentation\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 30,\n        \"samples\": [\n          \"Trainer is a simple but feature-complete training and eval loop for PyTorch, optimized for \\ud83e\\udd17 Transformers.  Args:     model ([`PreTrainedModel`] or `torch.nn.Module`, *optional*):         The model to train, evaluate or use for predictions. If not provided, a `model_init` must be passed.          <Tip>          [`Trainer`] is optimized to work with the [`PreTrainedModel`] provided by the library. You can still use         your own models defined as `torch.nn.Module` as long as they work the same way as the \\ud83e\\udd17 Transformers         models.          </Tip>      args ([`TrainingArguments`], *optional*):         The arguments to tweak for training. Will default to a basic instance of [`TrainingArguments`] with the         `output_dir` set to a directory named *tmp_trainer* in the current directory if not provided.     data_collator (`DataCollator`, *optional*):         The function to use to form a batch from a list of elements of `train_dataset` or `eval_dataset`. Will         default to [`default_data_collator`] if no `processing_class` is provided, an instance of         [`DataCollatorWithPadding`] otherwise if the processing_class is a feature extractor or tokenizer.     train_dataset (Union[`torch.utils.data.Dataset`, `torch.utils.data.IterableDataset`, `datasets.Dataset`], *optional*):         The dataset to use for training. If it is a [`~datasets.Dataset`], columns not accepted by the         `model.forward()` method are automatically removed.          Note that if it's a `torch.utils.data.IterableDataset` with some randomization and you are training in a         distributed fashion, your iterable dataset should either use a internal attribute `generator` that is a         `torch.Generator` for the randomization that must be identical on all processes (and the Trainer will         manually set the seed of this `generator` at each epoch) or have a `set_epoch()` method that internally         sets the seed of the RNGs used.     eval_dataset (Union[`torch.utils.data.Dataset`, Dict[str, `torch.utils.data.Dataset`, `datasets.Dataset`]), *optional*):          The dataset to use for evaluation. If it is a [`~datasets.Dataset`], columns not accepted by the          `model.forward()` method are automatically removed. If it is a dictionary, it will evaluate on each          dataset prepending the dictionary key to the metric name.     processing_class (`PreTrainedTokenizerBase` or `BaseImageProcessor` or `FeatureExtractionMixin` or `ProcessorMixin`, *optional*):         Processing class used to process the data. If provided, will be used to automatically process the inputs         for the model, and it will be saved along the model to make it easier to rerun an interrupted training or         reuse the fine-tuned model.         This supersedes the `tokenizer` argument, which is now deprecated.     model_init (`Callable[[], PreTrainedModel]`, *optional*):         A function that instantiates the model to be used. If provided, each call to [`~Trainer.train`] will start         from a new instance of the model as given by this function.          The function may have zero argument, or a single one containing the optuna/Ray Tune/SigOpt trial object, to         be able to choose different architectures according to hyper parameters (such as layer count, sizes of         inner layers, dropout probabilities etc).     compute_loss_func (`Callable`, *optional*):         A function that accepts the raw model outputs, labels, and the number of items in the entire accumulated         batch (batch_size * gradient_accumulation_steps) and returns the loss. For example, see the default [loss function](https://github.com/huggingface/transformers/blob/052e652d6d53c2b26ffde87e039b723949a53493/src/transformers/trainer.py#L3618) used by [`Trainer`].     compute_metrics (`Callable[[EvalPrediction], Dict]`, *optional*):         The function that will be used to compute metrics at evaluation. Must take a [`EvalPrediction`] and return         a dictionary string to metric values. *Note* When passing TrainingArgs with `batch_eval_metrics` set to         `True`, your compute_metrics function must take a boolean `compute_result` argument. This will be triggered         after the last eval batch to signal that the function needs to calculate and return the global summary         statistics rather than accumulating the batch-level statistics     callbacks (List of [`TrainerCallback`], *optional*):         A list of callbacks to customize the training loop. Will add those to the list of default callbacks         detailed in [here](callback).          If you want to remove one of the default callbacks used, use the [`Trainer.remove_callback`] method.     optimizers (`Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR]`, *optional*, defaults to `(None, None)`):         A tuple containing the optimizer and the scheduler to use. Will default to an instance of [`AdamW`] on your         model and a scheduler given by [`get_linear_schedule_with_warmup`] controlled by `args`.     optimizer_cls_and_kwargs (`Tuple[Type[torch.optim.Optimizer], Dict[str, Any]]`, *optional*):         A tuple containing the optimizer class and keyword arguments to use.         Overrides `optim` and `optim_args` in `args`. Incompatible with the `optimizers` argument.          Unlike `optimizers`, this argument avoids the need to place model parameters on the correct devices before initializing the Trainer.     preprocess_logits_for_metrics (`Callable[[torch.Tensor, torch.Tensor], torch.Tensor]`, *optional*):         A function that preprocess the logits right before caching them at each evaluation step. Must take two         tensors, the logits and the labels, and return the logits once processed as desired. The modifications made         by this function will be reflected in the predictions received by `compute_metrics`.          Note that the labels (second parameter) will be `None` if the dataset does not have them.  Important attributes:      - **model** -- Always points to the core model. If using a transformers model, it will be a [`PreTrainedModel`]       subclass.     - **model_wrapped** -- Always points to the most external model in case one or more other modules wrap the       original model. This is the model that should be used for the forward pass. For example, under `DeepSpeed`,       the inner model is wrapped in `DeepSpeed` and then again in `torch.nn.DistributedDataParallel`. If the inner       model hasn't been wrapped, then `self.model_wrapped` is the same as `self.model`.     - **is_model_parallel** -- Whether or not a model has been switched to a model parallel mode (different from       data parallelism, this means some of the model layers are split on different GPUs).     - **place_model_on_device** -- Whether or not to automatically place the model on the device - it will be set       to `False` if model parallel or deepspeed is used, or if the default       `TrainingArguments.place_model_on_device` is overridden to return `False` .     - **is_in_train** -- Whether or not a model is currently running `train` (e.g. when `evaluate` is called while       in `train`)\",\n          \":mod:`torch.optim` is a package implementing various optimization algorithms.  Most commonly used methods are already supported, and the interface is general enough, so that more sophisticated ones can also be easily integrated in the future.\",\n          \"This is a generic tokenizer class that will be instantiated as one of the tokenizer classes of the library when created with the [`AutoTokenizer.from_pretrained`] class method.  This class cannot be instantiated directly using `__init__()` (throws an error).\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"api_extract_gemini\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 33,\n        \"samples\": [\n          \"```json\\n{\\n  \\\"api\\\": {\\n    \\\"name\\\": \\\"Hugging Face Login API (Partial)\\\",\\n    \\\"description\\\": \\\"API functions related to user authentication and token management.\\\",\\n    \\\"functions\\\": [\\n      {\\n        \\\"name\\\": \\\"notebook_login\\\",\\n        \\\"description\\\": \\\"Displays a widget to log in to the HF website and store the token. Equivalent to `login` without passing a token when run in a notebook. Useful if you want to force the use of the notebook widget instead of a prompt in the terminal. For more details, see `login`.\\\",\\n        \\\"parameters\\\": [\\n          {\\n            \\\"name\\\": \\\"new_session\\\",\\n            \\\"type\\\": \\\"bool\\\",\\n            \\\"description\\\": \\\"If `True`, will request a token even if one is already saved on the machine.\\\",\\n            \\\"default_value\\\": true,\\n            \\\"required\\\": false\\n          },\\n          {\\n            \\\"name\\\": \\\"write_permission\\\",\\n            \\\"type\\\": \\\"bool\\\",\\n            \\\"description\\\": \\\"Ignored and deprecated argument.\\\",\\n            \\\"required\\\": false,\\n            \\\"notes\\\": [\\\"ignored\\\", \\\"deprecated\\\"]\\n          }\\n        ],\\n        \\\"returns\\\": null,\\n        \\\"relationships\\\": [\\n          {\\n            \\\"type\\\": \\\"related_to\\\",\\n            \\\"target\\\": \\\"login\\\",\\n            \\\"description\\\": \\\"Equivalent to `login` without a token in a notebook context, and provides a widget UI. See `login` for more details.\\\"\\n          }\\n        ],\\n        \\\"notes\\\": \\\"Specifically designed for notebook environments to provide a GUI login experience.\\\"\\n      }\\n    ],\\n    \\\"types\\\": [\\n      {\\n        \\\"name\\\": \\\"bool\\\",\\n        \\\"description\\\": \\\"A boolean value, either true or false.\\\"\\n      }\\n      // The type of the \\\"token\\\" is not explicitly defined as an input/output type\\n      // for this specific function's signature, so it's not listed here.\\n    ]\\n  }\\n}\\n```\",\n          \"```json\\n{\\n  \\\"package\\\": {\\n    \\\"name\\\": \\\"torch.optim\\\",\\n    \\\"description\\\": \\\"A package implementing various optimization algorithms. Most commonly used methods are already supported, and the interface is general enough, so that more sophisticated ones can also be easily integrated in the future.\\\",\\n    \\\"contents\\\": {\\n      \\\"types\\\": [\\n        {\\n          \\\"category\\\": \\\"Optimization Algorithm\\\",\\n          \\\"description\\\": \\\"Classes that implement specific optimization algorithms (e.g., SGD, Adam). The exact names and specific details of these types are not provided in the source documentation snippet.\\\",\\n          \\\"nature\\\": \\\"Class (inferred, as optimizers typically maintain state and have methods like step())\\\",\\n          \\\"count\\\": \\\"Multiple (implied by 'various', 'most commonly used')\\\"\\n        }\\n      ],\\n      \\\"functions\\\": [\\n        {\\n          \\\"category\\\": \\\"Interface Functionality\\\",\\n          \\\"description\\\": \\\"While specific functions within the package namespace are not named, the description implies a general interface, which would involve methods on the optimizer objects (e.g., step(), zero_grad()). The package itself might expose utility functions, but none are specified.\\\",\\n          \\\"details_available\\\": false\\n        }\\n      ]\\n    },\\n    \\\"relationships\\\": [\\n      {\\n        \\\"type\\\": \\\"Adherence to Interface\\\",\\n        \\\"description\\\": \\\"The optimization algorithm types/classes within this package adhere to a general interface.\\\",\\n        \\\"scope\\\": \\\"Internal package components (optimizer types)\\\",\\n        \\\"purpose\\\": \\\"Ensures compatibility, ease of use, and allows for integration of new optimizers.\\\"\\n      },\\n      {\\n        \\\"type\\\": \\\"Implementation Provider\\\",\\n        \\\"description\\\": \\\"The package provides concrete implementations of optimization algorithms.\\\",\\n        \\\"from\\\": \\\"torch.optim (package)\\\",\\n        \\\"to\\\": \\\"Optimization algorithms (conceptual/types)\\\"\\n      }\\n    ]\\n  }\\n}\\n```\",\n          \"```json\\n{\\n  \\\"classes\\\": [\\n    {\\n      \\\"name\\\": \\\"AutoTokenizer\\\",\\n      \\\"description\\\": \\\"A generic tokenizer class that will be instantiated as one of the specific tokenizer classes from the library.\\\",\\n      \\\"instantiation_notes\\\": \\\"Cannot be instantiated directly using `__init__()` (throws an error). Must be created using the `from_pretrained` class method.\\\",\\n      \\\"methods\\\": [\\n        {\\n          \\\"name\\\": \\\"from_pretrained\\\",\\n          \\\"type\\\": \\\"class method\\\",\\n          \\\"description\\\": \\\"Method used to instantiate the generic AutoTokenizer as a specific tokenizer class.\\\"\\n        }\\n      ]\\n    }\\n  ],\\n  \\\"types\\\": [],\\n  \\\"functions\\\": []\\n}\\n```\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"api_extract_gpt\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 33,\n        \"samples\": [\n          \"```json\\n{\\n  \\\"API\\\": {\\n    \\\"function\\\": \\\"notebook_login\\\",\\n    \\\"description\\\": \\\"Displays a widget to log in to the HF website and store the token. Equivalent to the login function without passing a token when run in a notebook. Useful to force the use of the notebook widget instead of a prompt in the terminal.\\\",\\n    \\\"arguments\\\": {\\n      \\\"new_session\\\": {\\n        \\\"type\\\": \\\"bool\\\",\\n        \\\"default\\\": true,\\n        \\\"description\\\": \\\"If True, will request a token even if one is already saved on the machine.\\\"\\n      },\\n      \\\"write_permission\\\": {\\n        \\\"type\\\": \\\"bool\\\",\\n        \\\"description\\\": \\\"Ignored and deprecated argument.\\\"\\n      }\\n    },\\n    \\\"related_functions\\\": [\\\"login\\\"]\\n  }\\n}\\n```\",\n          \"To extract a structured representation for the `torch.optim` API described, we will focus on the main components and relationships specified in the documentation. Since only a brief description is provided, the structured representation will be based on typical content expected in such an API, with placeholder values indicative of the expected content structure. If more concrete content was provided, it would enable a more accurate extraction. For now, the representation is general.\\n\\n```json\\n{\\n  \\\"module\\\": \\\"torch.optim\\\",\\n  \\\"description\\\": \\\"A package implementing various optimization algorithms with a general interface.\\\",\\n  \\\"types\\\": [],\\n  \\\"functions\\\": [],\\n  \\\"classes\\\": [\\n    {\\n      \\\"name\\\": \\\"<Optimizer Name>\\\",\\n      \\\"description\\\": \\\"A class implementing a specific optimization algorithm.\\\",\\n      \\\"methods\\\": [],\\n      \\\"properties\\\": [],\\n      \\\"relationships\\\": []\\n    }\\n  ]\\n}\\n```\\n\\nThis structured representation outlines the basic structure based on the given brief description. Typically, this would be expanded to include actual optimizer names and descriptions along with methods, properties, and relationships, depending on the full content of the module.\",\n          \"Certainly! Based on the provided API documentation, here's a structured JSON representation of the API, capturing the relevant types, functions, and their relationships:\\n\\n```json\\n{\\n  \\\"classes\\\": [\\n    {\\n      \\\"name\\\": \\\"Tokenizer\\\",\\n      \\\"description\\\": \\\"A generic tokenizer class that cannot be instantiated directly.\\\",\\n      \\\"constructor\\\": {\\n        \\\"method\\\": \\\"__init__\\\",\\n        \\\"instantiable\\\": false,\\n        \\\"error_message\\\": \\\"This class cannot be instantiated directly using __init__()\\\"\\n      },\\n      \\\"methods\\\": [],\\n      \\\"instantiation\\\": {\\n        \\\"method\\\": \\\"AutoTokenizer.from_pretrained\\\",\\n        \\\"description\\\": \\\"Instantiates the tokenizer as one of the library's tokenizer classes.\\\"\\n      }\\n    }\\n  ],\\n  \\\"methods\\\": [\\n    {\\n      \\\"class\\\": \\\"AutoTokenizer\\\",\\n      \\\"name\\\": \\\"from_pretrained\\\",\\n      \\\"description\\\": \\\"Class method to create a tokenizer object\\\",\\n      \\\"type\\\": \\\"class_method\\\"\\n    }\\n  ]\\n}\\n```\\n\\nThis JSON structure captures the key concepts from the API specification: the `Tokenizer` class, the fact that it cannot be instantiated directly, and how it can be instantiated through a class method of `AutoTokenizer`.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}","type":"dataframe","variable_name":"df_api"},"text/html":["\n","  <div id=\"df-e4c549e3-5ef7-452d-bec6-f003c7665fe7\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>API Component</th>\n","      <th>Documentation</th>\n","      <th>api_extract_gemini</th>\n","      <th>api_extract_gpt</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>tensorflow</td>\n","      <td>Top-level module of TensorFlow. By convention,...</td>\n","      <td>```json\\n{\\n  \"module\": {\\n    \"name\": \"tf\",\\n...</td>\n","      <td>```json\\n{\\n  \"module\": \"tensorflow\",\\n  \"alia...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>tensorflow.keras</td>\n","      <td>DO NOT EDIT.  This file was autogenerated. Do ...</td>\n","      <td>```json\\n{\\n  \"types\": {\\n    \"SomeType\": {\\n ...</td>\n","      <td>Of course! Please provide the API documentatio...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Sequential</td>\n","      <td>`Sequential` groups a linear stack of layers i...</td>\n","      <td>```json\\n{\\n  \"description\": \"API representati...</td>\n","      <td>Certainly! Here's a structured representation ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Dense</td>\n","      <td>Just your regular densely-connected NN layer. ...</td>\n","      <td>```json\\n{\\n  \"name\": \"Dense\",\\n  \"type\": \"Lay...</td>\n","      <td>Here is a structured representation of the API...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Input</td>\n","      <td>Used to instantiate a Keras tensor.  A Keras t...</td>\n","      <td>```json\\n{\\n  \"types\": [\\n    {\\n      \"name\":...</td>\n","      <td>To extract the structured representation of th...</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>Concatenate</td>\n","      <td>Concatenates a list of inputs.  It takes as in...</td>\n","      <td>```json\\n{\\n  \"name\": \"Concatenate\",\\n  \"type\"...</td>\n","      <td>```json\\n{\\n  \"API\": \"Concatenate\",\\n  \"Type\":...</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>fetch_california_housing</td>\n","      <td>Load the California housing dataset (regressio...</td>\n","      <td>```json\\n{\\n  \"module\": \"sklearn.datasets\",\\n ...</td>\n","      <td>Here's a structured representation of the API ...</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>make_classification</td>\n","      <td>Generate a random n-class classification probl...</td>\n","      <td>```json\\n{\\n  \"function\": {\\n    \"name\": \"make...</td>\n","      <td>Here's a structured representation of the API ...</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>train_test_split</td>\n","      <td>Split arrays or matrices into random train and...</td>\n","      <td>```json\\n{\\n  \"function\": {\\n    \"name\": \"trai...</td>\n","      <td>Below is the structured representation of the ...</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>StandardScaler</td>\n","      <td>Standardize features by removing the mean and ...</td>\n","      <td>```json\\n{\\n  \"classes\": [\\n    {\\n      \"name...</td>\n","      <td>```json\\n{\\n  \"class\": \"StandardScaler\",\\n  \"d...</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>LogisticRegression</td>\n","      <td>Logistic Regression (aka logit, MaxEnt) classi...</td>\n","      <td>```json\\n{\\n  \"name\": \"LogisticRegression\",\\n ...</td>\n","      <td>```json\\n{\\n  \"class\": \"LogisticRegression\",\\n...</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>make_pipeline</td>\n","      <td>Construct a :class:`Pipeline` from the given e...</td>\n","      <td>```json\\n{\\n  \"api_element\": {\\n    \"type\": \"f...</td>\n","      <td>Here is the structured representation of the A...</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>PCA</td>\n","      <td>Principal component analysis (PCA).  Linear di...</td>\n","      <td>```json\\n{\\n  \"class\": \"PCA\",\\n  \"description\"...</td>\n","      <td>Here is a structured representation of the giv...</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>torch</td>\n","      <td>The torch package contains data structures for...</td>\n","      <td>```json\\n{\\n  \"package\": {\\n    \"name\": \"torch...</td>\n","      <td>Based on the provided description of the `torc...</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>torch.nn</td>\n","      <td>No documentation found.</td>\n","      <td>Based on the input provided (\"No documentation...</td>\n","      <td>Unfortunately, without any documentation or de...</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>torch.optim</td>\n","      <td>:mod:`torch.optim` is a package implementing v...</td>\n","      <td>```json\\n{\\n  \"package\": {\\n    \"name\": \"torch...</td>\n","      <td>To extract a structured representation for the...</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>torchvision</td>\n","      <td>No documentation found.</td>\n","      <td>```json\\n{\\n  \"status\": \"error\",\\n  \"message\":...</td>\n","      <td>To create a structured representation of an AP...</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>torchvision.transforms</td>\n","      <td>No documentation found.</td>\n","      <td>Based on the input \"No documentation found,\" I...</td>\n","      <td>To create a structured representation of an AP...</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>torch.nn.functional</td>\n","      <td>Functional interface.</td>\n","      <td>```json\\n{\\n  \"api_description\": \"Structured r...</td>\n","      <td>To create a structured representation of the A...</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>Dataset</td>\n","      <td>An abstract class representing a :class:`Datas...</td>\n","      <td>```json\\n{\\n  \"types\": [\\n    {\\n      \"name\":...</td>\n","      <td>```json\\n{\\n  \"class\": \"Dataset\",\\n  \"descript...</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>DataLoader</td>\n","      <td>Data loader combines a dataset and a sampler, ...</td>\n","      <td>```json\\n{\\n  \"api_description\": \"Data loader ...</td>\n","      <td>Here's a structured representation of the API ...</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>torchvision.datasets</td>\n","      <td>No documentation found.</td>\n","      <td>```json\\n{\\n  \"types\": [],\\n  \"functions\": [],...</td>\n","      <td>Since there is no documentation provided, it's...</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>ToTensor</td>\n","      <td>Convert a PIL Image or ndarray to tensor and s...</td>\n","      <td>```json\\n{\\n  \"api_component\": \"Transform\",\\n ...</td>\n","      <td>```json\\n{\\n  \"API\": {\\n    \"name\": \"ConvertTo...</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>numpy</td>\n","      <td>NumPy =====  Provides   1. An array object of ...</td>\n","      <td>```json\\n{\\n  \"name\": \"NumPy\",\\n  \"description...</td>\n","      <td>Here's a structured representation of the NumP...</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>pandas</td>\n","      <td>pandas - a powerful data analysis and manipula...</td>\n","      <td>```json\\n{\\n  \"library\": \"pandas\",\\n  \"descrip...</td>\n","      <td>Certainly! Below is a structured representatio...</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>matplotlib.pyplot</td>\n","      <td>`matplotlib.pyplot` is a state-based interface...</td>\n","      <td>```json\\n{\\n  \"module\": {\\n    \"name\": \"matplo...</td>\n","      <td>```json\\n{\\n  \"module\": \"matplotlib.pyplot\",\\n...</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>AutoTokenizer</td>\n","      <td>This is a generic tokenizer class that will be...</td>\n","      <td>```json\\n{\\n  \"classes\": [\\n    {\\n      \"name...</td>\n","      <td>Certainly! Based on the provided API documenta...</td>\n","    </tr>\n","    <tr>\n","      <th>27</th>\n","      <td>DataCollatorWithPadding</td>\n","      <td>Data collator that will dynamically pad the in...</td>\n","      <td>```json\\n[\\n  {\\n    \"name\": \"DataCollatorWith...</td>\n","      <td>```json\\n{\\n  \"DataCollator\": {\\n    \"descript...</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>AutoModelForSequenceClassification</td>\n","      <td>This is a generic model class that will be ins...</td>\n","      <td>```json\\n{\\n  \"types\": [\\n    {\\n      \"name\":...</td>\n","      <td>Based on the provided API documentation, the s...</td>\n","    </tr>\n","    <tr>\n","      <th>29</th>\n","      <td>TrainingArguments</td>\n","      <td>TrainingArguments is the subset of the argumen...</td>\n","      <td>```json\\n{\\n  \"class_name\": \"TrainingArguments...</td>\n","      <td>Below is a structured JSON representation of t...</td>\n","    </tr>\n","    <tr>\n","      <th>30</th>\n","      <td>Trainer</td>\n","      <td>Trainer is a simple but feature-complete train...</td>\n","      <td>```json\\n{\\n  \"api\": {\\n    \"name\": \"Hugging F...</td>\n","      <td>Here is a structured representation of the API...</td>\n","    </tr>\n","    <tr>\n","      <th>31</th>\n","      <td>notebook_login</td>\n","      <td>Displays a widget to log in to the HF website ...</td>\n","      <td>```json\\n{\\n  \"api\": {\\n    \"name\": \"Hugging F...</td>\n","      <td>```json\\n{\\n  \"API\": {\\n    \"function\": \"noteb...</td>\n","    </tr>\n","    <tr>\n","      <th>32</th>\n","      <td>load_dataset</td>\n","      <td>Load a dataset from the Hugging Face Hub, or a...</td>\n","      <td>```json\\n{\\n  \"name\": \"load_dataset\",\\n  \"desc...</td>\n","      <td>Below is a structured JSON representation of t...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e4c549e3-5ef7-452d-bec6-f003c7665fe7')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-e4c549e3-5ef7-452d-bec6-f003c7665fe7 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-e4c549e3-5ef7-452d-bec6-f003c7665fe7');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    <div id=\"df-d89552ba-64dc-43ff-b09c-e46581d8c020\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d89552ba-64dc-43ff-b09c-e46581d8c020')\"\n","                title=\"Suggest charts\"\n","                style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","      <script>\n","        async function quickchart(key) {\n","          const quickchartButtonEl =\n","            document.querySelector('#' + key + ' button');\n","          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","          quickchartButtonEl.classList.add('colab-df-spinner');\n","          try {\n","            const charts = await google.colab.kernel.invokeFunction(\n","                'suggestCharts', [key], {});\n","          } catch (error) {\n","            console.error('Error during call to suggestCharts:', error);\n","          }\n","          quickchartButtonEl.classList.remove('colab-df-spinner');\n","          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","        }\n","        (() => {\n","          let quickchartButtonEl =\n","            document.querySelector('#df-d89552ba-64dc-43ff-b09c-e46581d8c020 button');\n","          quickchartButtonEl.style.display =\n","            google.colab.kernel.accessAllowed ? 'block' : 'none';\n","        })();\n","      </script>\n","    </div>\n","\n","  <div id=\"id_65aeea91-e929-4057-ba49-49f44d664878\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_api')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_65aeea91-e929-4057-ba49-49f44d664878 button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('df_api');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"],"text/plain":["                         API Component  \\\n","0                           tensorflow   \n","1                     tensorflow.keras   \n","2                           Sequential   \n","3                                Dense   \n","4                                Input   \n","5                          Concatenate   \n","6             fetch_california_housing   \n","7                  make_classification   \n","8                     train_test_split   \n","9                       StandardScaler   \n","10                  LogisticRegression   \n","11                       make_pipeline   \n","12                                 PCA   \n","13                               torch   \n","14                            torch.nn   \n","15                         torch.optim   \n","16                         torchvision   \n","17              torchvision.transforms   \n","18                 torch.nn.functional   \n","19                             Dataset   \n","20                          DataLoader   \n","21                torchvision.datasets   \n","22                            ToTensor   \n","23                               numpy   \n","24                              pandas   \n","25                   matplotlib.pyplot   \n","26                       AutoTokenizer   \n","27             DataCollatorWithPadding   \n","28  AutoModelForSequenceClassification   \n","29                   TrainingArguments   \n","30                             Trainer   \n","31                      notebook_login   \n","32                        load_dataset   \n","\n","                                        Documentation  \\\n","0   Top-level module of TensorFlow. By convention,...   \n","1   DO NOT EDIT.  This file was autogenerated. Do ...   \n","2   `Sequential` groups a linear stack of layers i...   \n","3   Just your regular densely-connected NN layer. ...   \n","4   Used to instantiate a Keras tensor.  A Keras t...   \n","5   Concatenates a list of inputs.  It takes as in...   \n","6   Load the California housing dataset (regressio...   \n","7   Generate a random n-class classification probl...   \n","8   Split arrays or matrices into random train and...   \n","9   Standardize features by removing the mean and ...   \n","10  Logistic Regression (aka logit, MaxEnt) classi...   \n","11  Construct a :class:`Pipeline` from the given e...   \n","12  Principal component analysis (PCA).  Linear di...   \n","13  The torch package contains data structures for...   \n","14                            No documentation found.   \n","15  :mod:`torch.optim` is a package implementing v...   \n","16                            No documentation found.   \n","17                            No documentation found.   \n","18                              Functional interface.   \n","19  An abstract class representing a :class:`Datas...   \n","20  Data loader combines a dataset and a sampler, ...   \n","21                            No documentation found.   \n","22  Convert a PIL Image or ndarray to tensor and s...   \n","23  NumPy =====  Provides   1. An array object of ...   \n","24  pandas - a powerful data analysis and manipula...   \n","25  `matplotlib.pyplot` is a state-based interface...   \n","26  This is a generic tokenizer class that will be...   \n","27  Data collator that will dynamically pad the in...   \n","28  This is a generic model class that will be ins...   \n","29  TrainingArguments is the subset of the argumen...   \n","30  Trainer is a simple but feature-complete train...   \n","31  Displays a widget to log in to the HF website ...   \n","32  Load a dataset from the Hugging Face Hub, or a...   \n","\n","                                   api_extract_gemini  \\\n","0   ```json\\n{\\n  \"module\": {\\n    \"name\": \"tf\",\\n...   \n","1   ```json\\n{\\n  \"types\": {\\n    \"SomeType\": {\\n ...   \n","2   ```json\\n{\\n  \"description\": \"API representati...   \n","3   ```json\\n{\\n  \"name\": \"Dense\",\\n  \"type\": \"Lay...   \n","4   ```json\\n{\\n  \"types\": [\\n    {\\n      \"name\":...   \n","5   ```json\\n{\\n  \"name\": \"Concatenate\",\\n  \"type\"...   \n","6   ```json\\n{\\n  \"module\": \"sklearn.datasets\",\\n ...   \n","7   ```json\\n{\\n  \"function\": {\\n    \"name\": \"make...   \n","8   ```json\\n{\\n  \"function\": {\\n    \"name\": \"trai...   \n","9   ```json\\n{\\n  \"classes\": [\\n    {\\n      \"name...   \n","10  ```json\\n{\\n  \"name\": \"LogisticRegression\",\\n ...   \n","11  ```json\\n{\\n  \"api_element\": {\\n    \"type\": \"f...   \n","12  ```json\\n{\\n  \"class\": \"PCA\",\\n  \"description\"...   \n","13  ```json\\n{\\n  \"package\": {\\n    \"name\": \"torch...   \n","14  Based on the input provided (\"No documentation...   \n","15  ```json\\n{\\n  \"package\": {\\n    \"name\": \"torch...   \n","16  ```json\\n{\\n  \"status\": \"error\",\\n  \"message\":...   \n","17  Based on the input \"No documentation found,\" I...   \n","18  ```json\\n{\\n  \"api_description\": \"Structured r...   \n","19  ```json\\n{\\n  \"types\": [\\n    {\\n      \"name\":...   \n","20  ```json\\n{\\n  \"api_description\": \"Data loader ...   \n","21  ```json\\n{\\n  \"types\": [],\\n  \"functions\": [],...   \n","22  ```json\\n{\\n  \"api_component\": \"Transform\",\\n ...   \n","23  ```json\\n{\\n  \"name\": \"NumPy\",\\n  \"description...   \n","24  ```json\\n{\\n  \"library\": \"pandas\",\\n  \"descrip...   \n","25  ```json\\n{\\n  \"module\": {\\n    \"name\": \"matplo...   \n","26  ```json\\n{\\n  \"classes\": [\\n    {\\n      \"name...   \n","27  ```json\\n[\\n  {\\n    \"name\": \"DataCollatorWith...   \n","28  ```json\\n{\\n  \"types\": [\\n    {\\n      \"name\":...   \n","29  ```json\\n{\\n  \"class_name\": \"TrainingArguments...   \n","30  ```json\\n{\\n  \"api\": {\\n    \"name\": \"Hugging F...   \n","31  ```json\\n{\\n  \"api\": {\\n    \"name\": \"Hugging F...   \n","32  ```json\\n{\\n  \"name\": \"load_dataset\",\\n  \"desc...   \n","\n","                                      api_extract_gpt  \n","0   ```json\\n{\\n  \"module\": \"tensorflow\",\\n  \"alia...  \n","1   Of course! Please provide the API documentatio...  \n","2   Certainly! Here's a structured representation ...  \n","3   Here is a structured representation of the API...  \n","4   To extract the structured representation of th...  \n","5   ```json\\n{\\n  \"API\": \"Concatenate\",\\n  \"Type\":...  \n","6   Here's a structured representation of the API ...  \n","7   Here's a structured representation of the API ...  \n","8   Below is the structured representation of the ...  \n","9   ```json\\n{\\n  \"class\": \"StandardScaler\",\\n  \"d...  \n","10  ```json\\n{\\n  \"class\": \"LogisticRegression\",\\n...  \n","11  Here is the structured representation of the A...  \n","12  Here is a structured representation of the giv...  \n","13  Based on the provided description of the `torc...  \n","14  Unfortunately, without any documentation or de...  \n","15  To extract a structured representation for the...  \n","16  To create a structured representation of an AP...  \n","17  To create a structured representation of an AP...  \n","18  To create a structured representation of the A...  \n","19  ```json\\n{\\n  \"class\": \"Dataset\",\\n  \"descript...  \n","20  Here's a structured representation of the API ...  \n","21  Since there is no documentation provided, it's...  \n","22  ```json\\n{\\n  \"API\": {\\n    \"name\": \"ConvertTo...  \n","23  Here's a structured representation of the NumP...  \n","24  Certainly! Below is a structured representatio...  \n","25  ```json\\n{\\n  \"module\": \"matplotlib.pyplot\",\\n...  \n","26  Certainly! Based on the provided API documenta...  \n","27  ```json\\n{\\n  \"DataCollator\": {\\n    \"descript...  \n","28  Based on the provided API documentation, the s...  \n","29  Below is a structured JSON representation of t...  \n","30  Here is a structured representation of the API...  \n","31  ```json\\n{\\n  \"API\": {\\n    \"function\": \"noteb...  \n","32  Below is a structured JSON representation of t...  "]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["df_api = pd.read_csv(\"/content/drive/MyDrive/Stanford NLP/api_documentation_new.csv\")\n","df_api"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1749074189768,"user":{"displayName":"Neeharika Gupta","userId":"13434636909198909500"},"user_tz":420},"id":"Ljyzv4_2HhsW","outputId":"6767bab7-8571-42fc-9a22-cbc9a525c030"},"outputs":[{"data":{"text/plain":["dict_keys(['tensorflow', 'tensorflow.keras', 'Sequential', 'Dense', 'Input', 'Concatenate', 'fetch_california_housing', 'make_classification', 'train_test_split', 'StandardScaler', 'LogisticRegression', 'make_pipeline', 'PCA', 'torch', 'torch.nn', 'torch.optim', 'torchvision', 'torchvision.transforms', 'torch.nn.functional', 'Dataset', 'DataLoader', 'torchvision.datasets', 'ToTensor', 'numpy', 'pandas', 'matplotlib.pyplot', 'AutoTokenizer', 'DataCollatorWithPadding', 'AutoModelForSequenceClassification', 'TrainingArguments', 'Trainer', 'notebook_login', 'load_dataset'])"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["api_dict = dict(zip(df_api['API Component'], df_api['api_extract_gemini']))\n","api_dict.keys()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"05yEDD-4CRxx"},"outputs":[],"source":["api_dict"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":804},"executionInfo":{"elapsed":397,"status":"ok","timestamp":1749072964242,"user":{"displayName":"Neeharika Gupta","userId":"13434636909198909500"},"user_tz":420},"id":"qbDdESQypkxr","outputId":"967e4feb-0289-4e3b-c1fa-03fcdc9a5190"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"summary":"{\n  \"name\": \"df\",\n  \"rows\": 8,\n  \"fields\": [\n    {\n      \"column\": \"Task\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"Write a Python script using PyTorch to train a convolutional neural network (CNN) on the Cifar 10 dataset. Include data loading, model definition, training loop, and evaluation\",\n          \"Write a PyTorch class that subclasses torch.utils.data.Dataset to load image files and labels from a folder. Use this class with a DataLoader for batching. Use FashionMNIST dataset.\",\n          \"Write a Python function using TensorFlow to create and compile a simple linear regression model with one dense layer. Train it on the classic California housing dataset.\\n\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Code\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"import torch\\nimport torch.nn as nn\\nimport torch.optim as optim\\nimport torchvision\\nimport torchvision.transforms as transforms\\nimport torch.nn.functional as F\\n\\ntransform = transforms.Compose(\\n    [transforms.ToTensor(),\\n     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\\n\\ntrainset = torchvision.datasets.CIFAR10(root='./data', train=True,\\n                                        download=True, transform=transform)\\n                                        \\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\\n                                          shuffle=True, num_workers=2)\\n\\ntestset = torchvision.datasets.CIFAR10(root='./data', train=False,\\n                                       download=True, transform=transform)\\n                                       \\ntestloader = torch.utils.data.DataLoader(testset, batch_size=4,\\n                                         shuffle=False, num_workers=2)\\n\\nclasses = ('plane', 'car', 'bird', 'cat',\\n           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\\n\\nclass Net(nn.Module):\\n    def __init__(self):\\n        super(Net, self).__init__()\\n        self.conv1 = nn.Conv2d(3, 6, 5)\\n        self.pool = nn.MaxPool2d(2, 2)\\n        self.conv2 = nn.Conv2d(6, 16, 5)\\n        self.fc1 = nn.Linear(16 * 5 * 5, 120)\\n        self.fc2 = nn.Linear(120, 84)\\n        self.fc3 = nn.Linear(84, 10)\\n\\n    def forward(self, x):\\n        x = self.pool(F.relu(self.conv1(x)))\\n        x = self.pool(F.relu(self.conv2(x)))\\n        x = x.view(-1, 16 * 5 * 5)\\n        x = F.relu(self.fc1(x))\\n        x = F.relu(self.fc2(x))\\n        x = self.fc3(x)\\n        return x\\n\\nnet = Net()\\n\\ncriterion = nn.CrossEntropyLoss()\\noptimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\\n\\nfor epoch in range(2):  \\n\\n    running_loss = 0.0\\n    for i, data in enumerate(trainloader, 0):\\n        inputs, labels = data\\n\\n        optimizer.zero_grad()\\n\\n        outputs = net(inputs)\\n        loss = criterion(outputs, labels)\\n        loss.backward()\\n        optimizer.step()\\n\\n        running_loss += loss.item()\\n        if i % 2000 == 1999: \\n            print('[%d, %5d] loss: %.3f' %\\n                  (epoch + 1, i + 1, running_loss / 2000))\\n            running_loss = 0.0\\n\\ncorrect = 0\\ntotal = 0\\nwith torch.no_grad():\\n    for data in testloader:\\n        images, labels = data\\n        outputs = net(images)\\n        _, predicted = torch.max(outputs.data, 1)\\n        total += labels.size(0)\\n        correct += (predicted == labels).sum().item()\\n\\nprint('Accuracy of the network on the 10000 test images: %d %%' % (\\n    100 * correct / total))\",\n          \"from torch.utils.data import Dataset\\nfrom torchvision import datasets\\nfrom torchvision.transforms import ToTensor\\nfrom torch.utils.data import DataLoader\\n\\n\\ntraining_data = datasets.FashionMNIST(\\n    root=\\\"data\\\",\\n    train=True,\\n    download=True,\\n    transform=ToTensor()\\n)\\n\\ntest_data = datasets.FashionMNIST(\\n    root=\\\"data\\\",\\n    train=False,\\n    download=True,\\n    transform=ToTensor()\\n)\\n\\ntrain_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\\ntest_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)\",\n          \"import tensorflow as tf\\nfrom tensorflow import keras\\nfrom tensorflow.keras.models import Sequential\\nfrom tensorflow.keras.layers import Dense, Input, Concatenate\\n\\nfrom sklearn.datasets import fetch_california_housing\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.preprocessing import StandardScaler\\nimport matplotlib.pyplot as plt\\n\\nimport numpy as np\\nimport pandas as pd\\nhousing = fetch_california_housing()\\nXtrain, Xtest, ytrain, ytest = train_test_split(housing.data, housing.target, test_size = .2)\\nXtrain, Xval, ytrain, yval = train_test_split(Xtrain, ytrain, test_size=.2)\\n\\nmodel = Sequential([\\n    Dense(30, activation = 'relu', input_shape= Xtrain.shape[1:]),\\n    Dense(1)\\n])\\n\\nmodel.compile(loss='mean_squared_error', optimizer='adam')\\n\\n\\nhistory = model.fit(Xtrain, ytrain, validation_data=(Xval, yval), epochs=20)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"API doc\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          \"import tensorflow as tf\\nfrom tensorflow import keras\\nfrom tensorflow.keras.models import Sequential\\nfrom tensorflow.keras.layers import Dense, Input, Concatenate\\n\\nfrom sklearn.datasets import fetch_california_housing\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.preprocessing import StandardScaler\\nimport matplotlib.pyplot as plt\\n\\nimport numpy as np\\nimport pandas as pd\\n\",\n          \"import torch.nn as nn\\nimport torch.optim as optim\\nimport torchvision\\nimport torchvision.transforms as transforms\\nimport torch.nn.functional as F\\ntransforms.Compose\\ntorchvision.datasets.CIFAR10\\ntorch.utils.data.DataLoader\\nnn.Conv2d\\nnn.MaxPool2d\\nnn.Linear\\nF.relu\\nnn.CrossEntropyLoss\\noptim.SGD\\n\",\n          \"from torch.utils.data import Dataset\\nfrom torchvision import datasets\\nfrom torchvision.transforms import ToTensor\\nfrom torch.utils.data import DataLoader\\ndatasets.FashionMNIST\\n\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Few shot t1\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"Write a Python script using PyTorch to train a convolutional neural network (CNN) on the MNIST dataset. Include data loading, model definition, training loop, and evaluation\",\n          \"Write a PyTorch class that creates custom dataset. It takes in input of img dir and annotations csv file.\",\n          \"Create a python script that uses tensorflow to model a linear regression problem using randomly genenrated 50 datat points. You need to train the model and also get the predictions.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Few shot c1\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"import numpy as np\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\n\\n\\nimport torch\\nfrom torch import optim\\nfrom torch import nn\\nfrom torch.utils.data import DataLoader\\nfrom tqdm import tqdm\\n\\n# !pip install torchvision\\nimport torchvision\\n\\nimport torch.nn.functional as F\\nimport torchvision.datasets as datasets\\nimport torchvision.transforms as transforms\\n\\n# !pip install torchmetrics\\nimport torchmetrics\\n\\nbatch_size = 60\\n\\ntrain_dataset = datasets.MNIST(root=\\\"dataset/\\\", download=True, train=True, transform=transforms.ToTensor())\\n\\ntrain_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\\n\\ntest_dataset = datasets.MNIST(root=\\\"dataset/\\\", download=True, train=False, transform=transforms.ToTensor())\\n\\ntest_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)\\n\\nclass CNN(nn.Module):\\n   def __init__(self, in_channels, num_classes):\\n\\n       \\\"\\\"\\\"\\n       Building blocks of convolutional neural network.\\n\\n       Parameters:\\n           * in_channels: Number of channels in the input image (for grayscale images, 1)\\n           * num_classes: Number of classes to predict. In our problem, 10 (i.e digits from  0 to 9).\\n       \\\"\\\"\\\"\\n       super(CNN, self).__init__()\\n\\n       # 1st convolutional layer\\n       self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=8, kernel_size=3, padding=1)\\n       # Max pooling layer\\n       self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\\n       # 2nd convolutional layer\\n       self.conv2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, padding=1)\\n       # Fully connected layer\\n       self.fc1 = nn.Linear(16 * 7 * 7, num_classes)\\n\\n   def forward(self, x):\\n       \\\"\\\"\\\"\\n       Define the forward pass of the neural network.\\n\\n       Parameters:\\n           x: Input tensor.\\n\\n       Returns:\\n           torch.Tensor\\n               The output tensor after passing through the network.\\n       \\\"\\\"\\\"\\n       x = F.relu(self.conv1(x))  # Apply first convolution and ReLU activation\\n       x = self.pool(x)           # Apply max pooling\\n       x = F.relu(self.conv2(x))  # Apply second convolution and ReLU activation\\n       x = self.pool(x)           # Apply max pooling\\n       x = x.reshape(x.shape[0], -1)  # Flatten the tensor\\n       x = self.fc1(x)            # Apply fully connected layer\\n       return x\\n       x = x.reshape(x.shape[0], -1)  # Flatten the tensor\\n       x = self.fc1(x)            # Apply fully connected layer\\n       return x\\n\\n# Define the loss function\\ncriterion = nn.CrossEntropyLoss()\\n\\n# Define the optimizer\\noptimizer = optim.Adam(model.parameters(), lr=0.001)\\n\\nnum_epochs=10\\nfor epoch in range(num_epochs):\\n # Iterate over training batches\\n   print(f\\\"Epoch [{epoch + 1}/{num_epochs}]\\\")\\n\\n   for batch_index, (data, targets) in enumerate(tqdm(dataloader_train)):\\n       data = data.to(device)\\n       targets = targets.to(device)\\n       scores = model(data)\\n       loss = criterion(scores, targets)\\n       optimizer.zero_grad()\\n       loss.backward()\\n       optimizer.step()\\n# Set up of multiclass accuracy metric\\nacc = Accuracy(task=\\\"multiclass\\\",num_classes=10)\\n\\n# Iterate over the dataset batches\\nmodel.eval()\\nwith torch.no_grad():\\n   for images, labels in dataloader_test:\\n       # Get predicted probabilities for test data batch\\n       outputs = model(images)\\n       _, preds = torch.max(outputs, 1)\\n       acc(preds, labels)\\n       precision(preds, labels)\\n       recall(preds, labels)\\n\\n#Compute total test accuracy\\ntest_accuracy = acc.compute()\\nprint(f\\\"Test accuracy: {test_accuracy}\\\")\\n\",\n          \"\\nimport os\\nimport pandas as pd\\nfrom torchvision.io import read_image\\n\\nclass CustomImageDataset(Dataset):\\n    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\\n        self.img_labels = pd.read_csv(annotations_file)\\n        self.img_dir = img_dir\\n        self.transform = transform\\n        self.target_transform = target_transform\\n\\n    def __len__(self):\\n        return len(self.img_labels)\\n\\n    def __getitem__(self, idx):\\n        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\\n        image = read_image(img_path)\\n        label = self.img_labels.iloc[idx, 1]\\n        if self.transform:\\n            image = self.transform(image)\\n        if self.target_transform:\\n            label = self.target_transform(label)\\n        return image, label\\n\",\n          \"\\nimport numpy as np\\nimport tensorflow as tf\\nimport matplotlib.pyplot as plt\\n\\nnp.random.seed(101)\\n\\n# Generating random linear data\\n# There will be 50 data points ranging from 0 to 50\\nx = np.linspace(0, 50, 50)\\ny = np.linspace(0, 50, 50)\\n\\n# Adding noise to the random linear data\\nx += np.random.uniform(-4, 4, 50)\\ny += np.random.uniform(-4, 4, 50)\\n\\nn = len(x) # Number of data points\\n\\n# Plot of Training Data\\nplt.scatter(x, y)\\nplt.xlabel('x')\\nplt.ylabel('y')\\nplt.title(\\\"Training Data\\\")\\nplt.show()\\n\\nX = tf.placeholder(\\\"float\\\")\\nY = tf.placeholder(\\\"float\\\")\\n\\nW = tf.Variable(np.random.randn(), name = \\\"W\\\")\\nb = tf.Variable(np.random.randn(), name = \\\"b\\\")\\n\\nlearning_rate = 0.01\\ntraining_epochs = 1000\\n# Hypothesis\\ny_pred = tf.add(tf.multiply(X, W), b)\\n\\n# Mean Squared Error Cost Function\\ncost = tf.reduce_sum(tf.pow(y_pred-Y, 2)) / (2 * n)\\n\\n# Gradient Descent Optimizer\\noptimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\\n\\n# Global Variables Initializer\\ninit = tf.global_variables_initializer()\\n\\n# Starting the Tensorflow Session\\nwith tf.Session() as sess:\\n    \\n    # Initializing the Variables\\n    sess.run(init)\\n    \\n    # Iterating through all the epochs\\n    for epoch in range(training_epochs):\\n        \\n        # Feeding each data point into the optimizer using Feed Dictionary\\n        for (_x, _y) in zip(x, y):\\n            sess.run(optimizer, feed_dict = {X : _x, Y : _y})\\n        \\n        # Displaying the result after every 50 epochs\\n        if (epoch + 1) % 50 == 0:\\n            # Calculating the cost a every epoch\\n            c = sess.run(cost, feed_dict = {X : x, Y : y})\\n            print(\\\"Epoch\\\", (epoch + 1), \\\": cost =\\\", c, \\\"W =\\\", sess.run(W), \\\"b =\\\", sess.run(b))\\n    \\n    # Storing necessary values to be used outside the Session\\n    training_cost = sess.run(cost, feed_dict ={X: x, Y: y})\\n    weight = sess.run(W)\\n    bias = sess.run(b)\\n\\n# Calculating the predictions\\npredictions = weight * x + bias\\nprint(\\\"Training cost =\\\", training_cost, \\\"Weight =\\\", weight, \\\"bias =\\\", bias, '\\\\n')\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Few shot t2\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"Write a Python script using PyTorch to train a convolutional neural network (CNN) on the KMNIST dataset. Include data loading, model definition, training loop, and evaluation\",\n          \"Write a PyTorch class that subclasses torch.utils.data.Dataset to load 100 image files and labels from a folder. The data would be randomly created and the class should also be defined such that its a custom class.Use this class with a DataLoader for batching. \",\n          \"Write a Python function using TensorFlow to create and compile a simple linear regression model which has a normalizer layer and a dense layer. Train it on the fuel efficiency dataset.\\n\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Few shot c2\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"from torch.nn import Module\\nfrom torch.nn import Conv2d\\nfrom torch.nn import Linear\\nfrom torch.nn import MaxPool2d\\nfrom torch.nn import ReLU\\nfrom torch.nn import LogSoftmax\\nfrom torch import flatten\\nimport matplotlib\\nmatplotlib.use(\\\"Agg\\\")\\nfrom pyimagesearch.lenet import LeNet\\nfrom sklearn.metrics import classification_report\\nfrom torch.utils.data import random_split\\nfrom torch.utils.data import DataLoader\\nfrom torchvision.transforms import ToTensor\\nfrom torchvision.datasets import KMNIST\\nfrom torch.optim import Adam\\nfrom torch import nn\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\nimport argparse\\nimport torch\\nimport time\\n\\nclass LeNet(Module):\\n        def __init__(self, numChannels, classes):\\n                # call the parent constructor\\n                super(LeNet, self).__init__()\\n                # initialize first set of CONV => RELU => POOL layers\\n                self.conv1 = Conv2d(in_channels=numChannels, out_channels=20,\\n                        kernel_size=(5, 5))\\n                self.relu1 = ReLU()\\n                self.maxpool1 = MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\\n                # initialize second set of CONV => RELU => POOL layers\\n                self.conv2 = Conv2d(in_channels=20, out_channels=50,\\n                        kernel_size=(5, 5))\\n                self.relu2 = ReLU()\\n                self.maxpool2 = MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\\n                # initialize first (and only) set of FC => RELU layers\\n                self.fc1 = Linear(in_features=800, out_features=500)\\n                self.relu3 = ReLU()\\n                # initialize our softmax classifier\\n                self.fc2 = Linear(in_features=500, out_features=classes)\\n                self.logSoftmax = LogSoftmax(dim=1)\\n\\ndef forward(self, x):\\n                # pass the input through our first set of CONV => RELU =>\\n                # POOL layers\\n                x = self.conv1(x)\\n                x = self.relu1(x)\\n                x = self.maxpool1(x)\\n                # pass the output from the previous layer through the second\\n                # set of CONV => RELU => POOL layers\\n                x = self.conv2(x)\\n                x = self.relu2(x)\\n                x = self.maxpool2(x)\\n                # flatten the output from the previous layer and pass it\\n                # through our only set of FC => RELU layers\\n                x = flatten(x, 1)\\n                x = self.fc1(x)\\n                x = self.relu3(x)\\n                # pass the output to our softmax classifier to get our output\\n                # predictions\\n                x = self.fc2(x)\\n                output = self.logSoftmax(x)\\n                # return the output predictions\\n                return output\\n# construct the argument parser and parse the arguments\\nap = argparse.ArgumentParser()\\nap.add_argument(\\\"-m\\\", \\\"--model\\\", type=str, required=True,\\n        help=\\\"path to output trained model\\\")\\nap.add_argument(\\\"-p\\\", \\\"--plot\\\", type=str, required=True,\\n        help=\\\"path to output loss/accuracy plot\\\")\\nargs = vars(ap.parse_args())\\n\\nPyTorch: Training your first Convolutional Neural Network (CNN)\\n# define training hyperparameters\\nINIT_LR = 1e-3\\nBATCH_SIZE = 64\\nEPOCHS = 10\\n# define the train and val splits\\nTRAIN_SPLIT = 0.75\\nVAL_SPLIT = 1 - TRAIN_SPLIT\\n# set the device we will be using to train the model\\ndevice = torch.device(\\\"cuda\\\" if torch.cuda.is_available() else \\\"cpu\\\")\\n\\n# load the KMNIST dataset\\nprint(\\\"[INFO] loading the KMNIST dataset...\\\")\\ntrainData = KMNIST(root=\\\"data\\\", train=True, download=True,\\n        transform=ToTensor())\\ntestData = KMNIST(root=\\\"data\\\", train=False, download=True,\\n        transform=ToTensor())\\n# calculate the train/validation split\\nprint(\\\"[INFO] generating the train/validation split...\\\")\\nnumTrainSamples = int(len(trainData) * TRAIN_SPLIT)\\nnumValSamples = int(len(trainData) * VAL_SPLIT)\\n(trainData, valData) = random_split(trainData,\\n        [numTrainSamples, numValSamples],\\n        generator=torch.Generator().manual_seed(42))\\n\\n# initialize the train, validation, and test data loaders\\ntrainDataLoader = DataLoader(trainData, shuffle=True,\\n        batch_size=BATCH_SIZE)\\nvalDataLoader = DataLoader(valData, batch_size=BATCH_SIZE)\\ntestDataLoader = DataLoader(testData, batch_size=BATCH_SIZE)\\n# calculate steps per epoch for training and validation set\\ntrainSteps = len(trainDataLoader.dataset) // BATCH_SIZE\\nvalSteps = len(valDataLoader.dataset) // BATCH_SIZE\\n\\n# initialize the LeNet model\\nprint(\\\"[INFO] initializing the LeNet model...\\\")\\nmodel = LeNet(\\n        numChannels=1,\\n        classes=len(trainData.dataset.classes)).to(device)\\n# initialize our optimizer and loss function\\nopt = Adam(model.parameters(), lr=INIT_LR)\\nlossFn = nn.NLLLoss()\\n# initialize a dictionary to store training history\\nH = {\\n        \\\"train_loss\\\": [],\\n        \\\"train_acc\\\": [],\\n        \\\"val_loss\\\": [],\\n        \\\"val_acc\\\": []\\n}\\n# measure how long training is going to take\\nprint(\\\"[INFO] training the network...\\\")\\n\\n# loop over our epochs\\nfor e in range(0, EPOCHS):\\n        # set the model in training mode\\n        model.train()\\n        # initialize the total training and validation loss\\n        totalTrainLoss = 0\\n        totalValLoss = 0\\n        # initialize the number of correct predictions in the training\\n        # and validation step\\n        trainCorrect = 0\\n        valCorrect = 0\\n        # loop over the training set\\n        for (x, y) in trainDataLoader:\\n                # send the input to the device\\n                (x, y) = (x.to(device), y.to(device))\\n                # perform a forward pass and calculate the training loss\\n                pred = model(x)\\n                loss = lossFn(pred, y)\\n                # zero out the gradients, perform the backpropagation step,\\n                # and update the weights\\n                opt.zero_grad()\\n                loss.backward()\\n                opt.step()\\n                # add the loss to the total training loss so far and\\n                # calculate the number of correct predictions\\n                totalTrainLoss += loss\\n                trainCorrect += (pred.argmax(1) == y).type(\\n                        torch.float).sum().item()\\n\\n        # switch off autograd for evaluation\\n        with torch.no_grad():\\n                # set the model in evaluation mode\\n                model.eval()\\n                # loop over the validation set\\n                for (x, y) in valDataLoader:\\n                        # send the input to the device\\n                        (x, y) = (x.to(device), y.to(device))\\n                        # make the predictions and calculate the validation loss\\n                        pred = model(x)\\n                        totalValLoss += lossFn(pred, y)\\n                        # calculate the number of correct predictions\\n                        valCorrect += (pred.argmax(1) == y).type(\\n                                torch.float).sum().item()\\n\\n        # calculate the average training and validation loss\\n        avgTrainLoss = totalTrainLoss / trainSteps\\n        avgValLoss = totalValLoss / valSteps\\n        # calculate the training and validation accuracy\\n        trainCorrect = trainCorrect / len(trainDataLoader.dataset)\\n        valCorrect = valCorrect / len(valDataLoader.dataset)\\n        # update our training history\\n        H[\\\"train_loss\\\"].append(avgTrainLoss.cpu().detach().numpy())\\n        H[\\\"train_acc\\\"].append(trainCorrect)\\n        H[\\\"val_loss\\\"].append(avgValLoss.cpu().detach().numpy())\\n        H[\\\"val_acc\\\"].append(valCorrect)\\n        # print the model training and validation information\\n        print(\\\"[INFO] EPOCH: {}/{}\\\".format(e + 1, EPOCHS))\\n        print(\\\"Train loss: {:.6f}, Train accuracy: {:.4f}\\\".format(\\n                avgTrainLoss, trainCorrect))\\n        print(\\\"Val loss: {:.6f}, Val accuracy: {:.4f}\\\\n\\\".format(\\n                avgValLoss, valCorrect))\",\n          \"import torch\\nfrom torch.utils.data import Dataset, DataLoader\\nimport torch.nn as nn\\nimport torch.optim as optim\\nimport numpy as np\\n\\n# Custom Dataset class\\nclass CustomDataset(Dataset):\\n    def __init__(self, data, labels):\\n        self.data = data\\n        self.labels = labels\\n\\n    def __len__(self):\\n        return len(self.data)\\n\\n    def __getitem__(self, idx):\\n        sample = self.data[idx]\\n        label = self.labels[idx]\\n        return sample, label\\n\\n# Prepare data\\ndata = np.random.randn(100, 3, 32, 32)  # 100 samples of 3x32x32 images\\nlabels = np.random.randint(0, 10, size=(100,))  # 100 labels in the range 0-9\\n\\n# Create Dataset\\ndataset = CustomDataset(data, labels)\\n\\n# Create DataLoader\\ndataloader = DataLoader(dataset, batch_size=4, shuffle=True, num_workers=2)\\n\",\n          \"pip install -q seaborn\\n\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\nimport pandas as pd\\nimport seaborn as sns\\n\\n# Make NumPy printouts easier to read.\\nnp.set_printoptions(precision=3, suppress=True)\\nimport tensorflow as tf\\n\\nfrom tensorflow import keras\\nfrom tensorflow.keras import layers\\n\\nprint(tf.__version__)\\n\\nurl = 'http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data'\\ncolumn_names = ['MPG', 'Cylinders', 'Displacement', 'Horsepower', 'Weight',\\n                'Acceleration', 'Model Year', 'Origin']\\n\\nraw_dataset = pd.read_csv(url, names=column_names,\\n                          na_values='?', comment='\\\\t',\\n                          sep=' ', skipinitialspace=True)\\n\\ndataset = raw_dataset.copy()\\ndataset.tail()\\n\\ndataset.isna().sum()\\n\\ndataset = dataset.dropna()\\n\\ndataset['Origin'] = dataset['Origin'].map({1: 'USA', 2: 'Europe', 3: 'Japan'})\\n\\ndataset = pd.get_dummies(dataset, columns=['Origin'], prefix='', prefix_sep='')\\ndataset.tail()\\n\\ntrain_dataset = dataset.sample(frac=0.8, random_state=0)\\ntest_dataset = dataset.drop(train_dataset.index)\\n\\ntrain_features = train_dataset.copy()\\ntest_features = test_dataset.copy()\\n\\ntrain_labels = train_features.pop('MPG')\\ntest_labels = test_features.pop('MPG')\\n\\nhorsepower = np.array(train_features['Horsepower'])\\n\\nhorsepower_normalizer = layers.Normalization(input_shape=[1,], axis=None)\\nhorsepower_normalizer.adapt(horsepower)\\n\\nhorsepower_model = tf.keras.Sequential([\\n    horsepower_normalizer,\\n    layers.Dense(units=1)\\n])\\n\\nhorsepower_model.summary()\\n\\nhorsepower_model.compile(\\n    optimizer=tf.keras.optimizers.Adam(learning_rate=0.1),\\n    loss='mean_absolute_error')\\n\\n\\nhistory = horsepower_model.fit(\\n    train_features['Horsepower'],\\n    train_labels,\\n    epochs=100,\\n    # Suppress logging.\\n    verbose=0,\\n    # Calculate validation results on 20% of the training data.\\n    validation_split = 0.2)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Unnamed: 7\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"\\n\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}","type":"dataframe","variable_name":"df"},"text/html":["\n","  <div id=\"df-b1e442b2-4b6a-4930-8d78-756fe4e1ebd8\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Task</th>\n","      <th>Code</th>\n","      <th>API doc</th>\n","      <th>Few shot t1</th>\n","      <th>Few shot c1</th>\n","      <th>Few shot t2</th>\n","      <th>Few shot c2</th>\n","      <th>Unnamed: 7</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Write a Python function using TensorFlow to cr...</td>\n","      <td>import tensorflow as tf\\nfrom tensorflow impor...</td>\n","      <td>import tensorflow as tf\\nfrom tensorflow impor...</td>\n","      <td>Create a python script that uses tensorflow to...</td>\n","      <td>\\nimport numpy as np\\nimport tensorflow as tf\\...</td>\n","      <td>Write a Python function using TensorFlow to cr...</td>\n","      <td>pip install -q seaborn\\n\\nimport matplotlib.py...</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Write a Python script using PyTorch to train a...</td>\n","      <td>import torch\\nimport torch.nn as nn\\nimport to...</td>\n","      <td>import torch.nn as nn\\nimport torch.optim as o...</td>\n","      <td>Write a Python script using PyTorch to train a...</td>\n","      <td>import numpy as np\\nimport pandas as pd\\nimpor...</td>\n","      <td>Write a Python script using PyTorch to train a...</td>\n","      <td>from torch.nn import Module\\nfrom torch.nn imp...</td>\n","      <td>\\n</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Create a scikit-learn pipeline that generates ...</td>\n","      <td>from sklearn.datasets import make_classificati...</td>\n","      <td>from sklearn.datasets import make_classificati...</td>\n","      <td>Create a scikit learn pipeline that builds sta...</td>\n","      <td>from sklearn.pipeline import Pipeline\\nfrom sk...</td>\n","      <td>Create a scikit learn pipeline that builds sta...</td>\n","      <td>from sklearn.pipeline import Pipeline\\nfrom sk...</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Write a Python function that uses Hugging Face...</td>\n","      <td>from transformers import AutoTokenizer\\nfrom t...</td>\n","      <td>from transformers import AutoTokenizer\\nfrom t...</td>\n","      <td>In this example we learn to identify past vs p...</td>\n","      <td>\\nfrom typing import Callable, Dict, Sequence,...</td>\n","      <td>Write a python function that uses hugging face...</td>\n","      <td>\\nimport gradio as gr\\nfrom transformers impor...</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Build a TensorFlow model for image classificat...</td>\n","      <td>import tensorflow as tf\\nimport numpy as np\\n\\...</td>\n","      <td>import tensorflow as tf\\nimport numpy as np\\nt...</td>\n","      <td>Build an image recognition tensorflow model to...</td>\n","      <td>import matplotlib.pyplot as plt \\nimport numpy...</td>\n","      <td>Create an image classification model using ten...</td>\n","      <td>import tensorflow as tf   \\n\\n# Display the ve...</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>Write a PyTorch class that subclasses torch.ut...</td>\n","      <td>from torch.utils.data import Dataset\\nfrom tor...</td>\n","      <td>from torch.utils.data import Dataset\\nfrom tor...</td>\n","      <td>Write a PyTorch class that creates custom data...</td>\n","      <td>\\nimport os\\nimport pandas as pd\\nfrom torchvi...</td>\n","      <td>Write a PyTorch class that subclasses torch.ut...</td>\n","      <td>import torch\\nfrom torch.utils.data import Dat...</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>Write a Python script that loads the Iris data...</td>\n","      <td>\\nfrom sklearn import datasets\\nfrom sklearn.d...</td>\n","      <td>\\nfrom sklearn import datasets\\nfrom sklearn.d...</td>\n","      <td>Write a Python script that loads the UCI handw...</td>\n","      <td>import matplotlib.pyplot as plt\\nimport numpy ...</td>\n","      <td>Write a Python script that uses the dataset gi...</td>\n","      <td>import numpy as np\\nimport pandas as pd\\nfrom ...</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>Write a Java program using the Hadoop MapReduc...</td>\n","      <td>Mapper Code\\nimport java.io.IOException;      ...</td>\n","      <td>NaN</td>\n","      <td>Write a Java program using the Hadoop MapReduc...</td>\n","      <td>import java.io.IOException;\\nimport java.util....</td>\n","      <td>Write a Java program using the Hadoop MapReduc...</td>\n","      <td>package hadoop; \\n\\nimport java.util.*; \\n\\nim...</td>\n","      <td>\\n</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b1e442b2-4b6a-4930-8d78-756fe4e1ebd8')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-b1e442b2-4b6a-4930-8d78-756fe4e1ebd8 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-b1e442b2-4b6a-4930-8d78-756fe4e1ebd8');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    <div id=\"df-52b33392-d242-4d5c-8255-14fef6226f4a\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-52b33392-d242-4d5c-8255-14fef6226f4a')\"\n","                title=\"Suggest charts\"\n","                style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","      <script>\n","        async function quickchart(key) {\n","          const quickchartButtonEl =\n","            document.querySelector('#' + key + ' button');\n","          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","          quickchartButtonEl.classList.add('colab-df-spinner');\n","          try {\n","            const charts = await google.colab.kernel.invokeFunction(\n","                'suggestCharts', [key], {});\n","          } catch (error) {\n","            console.error('Error during call to suggestCharts:', error);\n","          }\n","          quickchartButtonEl.classList.remove('colab-df-spinner');\n","          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","        }\n","        (() => {\n","          let quickchartButtonEl =\n","            document.querySelector('#df-52b33392-d242-4d5c-8255-14fef6226f4a button');\n","          quickchartButtonEl.style.display =\n","            google.colab.kernel.accessAllowed ? 'block' : 'none';\n","        })();\n","      </script>\n","    </div>\n","\n","  <div id=\"id_96b6246b-f78d-426c-ba37-e681733ddb22\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_96b6246b-f78d-426c-ba37-e681733ddb22 button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('df');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"],"text/plain":["                                                Task  \\\n","0  Write a Python function using TensorFlow to cr...   \n","1  Write a Python script using PyTorch to train a...   \n","2  Create a scikit-learn pipeline that generates ...   \n","3  Write a Python function that uses Hugging Face...   \n","4  Build a TensorFlow model for image classificat...   \n","5  Write a PyTorch class that subclasses torch.ut...   \n","6  Write a Python script that loads the Iris data...   \n","7  Write a Java program using the Hadoop MapReduc...   \n","\n","                                                Code  \\\n","0  import tensorflow as tf\\nfrom tensorflow impor...   \n","1  import torch\\nimport torch.nn as nn\\nimport to...   \n","2  from sklearn.datasets import make_classificati...   \n","3  from transformers import AutoTokenizer\\nfrom t...   \n","4  import tensorflow as tf\\nimport numpy as np\\n\\...   \n","5  from torch.utils.data import Dataset\\nfrom tor...   \n","6  \\nfrom sklearn import datasets\\nfrom sklearn.d...   \n","7  Mapper Code\\nimport java.io.IOException;      ...   \n","\n","                                             API doc  \\\n","0  import tensorflow as tf\\nfrom tensorflow impor...   \n","1  import torch.nn as nn\\nimport torch.optim as o...   \n","2  from sklearn.datasets import make_classificati...   \n","3  from transformers import AutoTokenizer\\nfrom t...   \n","4  import tensorflow as tf\\nimport numpy as np\\nt...   \n","5  from torch.utils.data import Dataset\\nfrom tor...   \n","6  \\nfrom sklearn import datasets\\nfrom sklearn.d...   \n","7                                                NaN   \n","\n","                                         Few shot t1  \\\n","0  Create a python script that uses tensorflow to...   \n","1  Write a Python script using PyTorch to train a...   \n","2  Create a scikit learn pipeline that builds sta...   \n","3  In this example we learn to identify past vs p...   \n","4  Build an image recognition tensorflow model to...   \n","5  Write a PyTorch class that creates custom data...   \n","6  Write a Python script that loads the UCI handw...   \n","7  Write a Java program using the Hadoop MapReduc...   \n","\n","                                         Few shot c1  \\\n","0  \\nimport numpy as np\\nimport tensorflow as tf\\...   \n","1  import numpy as np\\nimport pandas as pd\\nimpor...   \n","2  from sklearn.pipeline import Pipeline\\nfrom sk...   \n","3  \\nfrom typing import Callable, Dict, Sequence,...   \n","4  import matplotlib.pyplot as plt \\nimport numpy...   \n","5  \\nimport os\\nimport pandas as pd\\nfrom torchvi...   \n","6  import matplotlib.pyplot as plt\\nimport numpy ...   \n","7  import java.io.IOException;\\nimport java.util....   \n","\n","                                         Few shot t2  \\\n","0  Write a Python function using TensorFlow to cr...   \n","1  Write a Python script using PyTorch to train a...   \n","2  Create a scikit learn pipeline that builds sta...   \n","3  Write a python function that uses hugging face...   \n","4  Create an image classification model using ten...   \n","5  Write a PyTorch class that subclasses torch.ut...   \n","6  Write a Python script that uses the dataset gi...   \n","7  Write a Java program using the Hadoop MapReduc...   \n","\n","                                         Few shot c2 Unnamed: 7  \n","0  pip install -q seaborn\\n\\nimport matplotlib.py...        NaN  \n","1  from torch.nn import Module\\nfrom torch.nn imp...         \\n  \n","2  from sklearn.pipeline import Pipeline\\nfrom sk...        NaN  \n","3  \\nimport gradio as gr\\nfrom transformers impor...        NaN  \n","4  import tensorflow as tf   \\n\\n# Display the ve...        NaN  \n","5  import torch\\nfrom torch.utils.data import Dat...        NaN  \n","6  import numpy as np\\nimport pandas as pd\\nfrom ...        NaN  \n","7  package hadoop; \\n\\nimport java.util.*; \\n\\nim...         \\n  "]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["df = pd.read_csv(\"/content/drive/MyDrive/Stanford NLP/dataset.csv\")\n","df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qt1AwCJKpggY"},"outputs":[],"source":["output=[]\n","\n","for index, row in df.iterrows():\n","    print(f\"Processing row {index+1}/{len(df)}...\")\n","\n","    try:\n","        prompt = (\n","            f\"Suggest an API usage pattern for the following task: :\\n\"\n","            f\"{row['Task'].strip()}\\n\\n\"\n","            f\"Based on the API structure provided below:\\n\"\n","            f\"{api_dict}\\n\\n\"\n","            f\"You can choose anything from given data or select a new API, if the provided one's are not able to fulfill the task.\"\n","        )\n","\n","        response = make_request(prompt,model)\n","    except Exception as e:\n","        print(f\"Error generating response: {e}\")\n","        response = f\"Error: {e}\"\n","\n","    output.append(response)\n","\n","    time.sleep(DELAY_BETWEEN_REQUESTS)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yOOqeB0FymFD"},"outputs":[],"source":["df_results = pd.DataFrame({'neural_gemini': output})\n","\n","output_path = \"/content/drive/MyDrive/Stanford NLP/neural_results.csv\"\n","df_results.to_csv(output_path, index=False)"]},{"cell_type":"markdown","metadata":{"id":"XhOvJ-Ly3eWj"},"source":["Generalization check"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G49s7rEZ3Go3"},"outputs":[],"source":["generlaization_data = [\"Create a python script that uses imbalanced-learn to do a binary classification model on Breast Cancer Wisconsin (Diagnostic) Dataset (WDBC), where your major class to minor class ratio is 99:1. You should use techniques like SMOTE to balance the dataset. Use a general model like XGboost. Do an analysis of metrics (like accuracy, precision, recall, f1 score, auc score and confusion matrix) before and after using imbalanced-learn techniques. \",\"Write a script in python that uses tensorflow and keras library. Your task is to work on Semantic Segmentation using Oxford Pets Dataset and a U-Net Model. The semantic segmentation task would be to differentiate between background and the pet. Also calculate pixel-wise classification accuracy, model loss and IoU.\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"srbi_bEU3bKY"},"outputs":[],"source":["generlaization_output=[]\n","for i in range(len(generlaization_data)):\n","    print(f\"Processing row {i+1}/{len(generlaization_data)}...\")\n","    try:\n","        prompt = (\n","            f\"Suggest an API usage pattern for the following task: :\\n\"\n","            f\"{generlaization_data[i].sttrip()}\\n\\n\"\n","            f\"Based on the API structure provided below:\\n\"\n","            f\"{api_dict}\\n\\n\"\n","            f\"You can choose anything from given data or select a new API, if the provided one's are not able to fulfill the task.\"\n","        )\n","\n","        response = make_request(prompt,model)\n","    except Exception as e:\n","        print(f\"Error generating response: {e}\")\n","        response = f\"Error: {e}\"\n","\n","    generlaization_output.append(response)"]},{"cell_type":"markdown","metadata":{"id":"DMP2q6Zu2mys"},"source":["###symbolic checking"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"collapsed":true,"executionInfo":{"elapsed":11495,"status":"ok","timestamp":1748891213129,"user":{"displayName":"Neeharika Gupta","userId":"13434636909198909500"},"user_tz":420},"id":"UmrsgY9FzMOR","outputId":"105c70a4-111f-45b9-c6f8-0fdfd27250d5"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"summary":"{\n  \"name\": \"df_api\",\n  \"rows\": 33,\n  \"fields\": [\n    {\n      \"column\": \"API Component\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 33,\n        \"samples\": [\n          \"notebook_login\",\n          \"torch.optim\",\n          \"AutoTokenizer\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Documentation\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 30,\n        \"samples\": [\n          \"Trainer is a simple but feature-complete training and eval loop for PyTorch, optimized for \\ud83e\\udd17 Transformers.  Args:     model ([`PreTrainedModel`] or `torch.nn.Module`, *optional*):         The model to train, evaluate or use for predictions. If not provided, a `model_init` must be passed.          <Tip>          [`Trainer`] is optimized to work with the [`PreTrainedModel`] provided by the library. You can still use         your own models defined as `torch.nn.Module` as long as they work the same way as the \\ud83e\\udd17 Transformers         models.          </Tip>      args ([`TrainingArguments`], *optional*):         The arguments to tweak for training. Will default to a basic instance of [`TrainingArguments`] with the         `output_dir` set to a directory named *tmp_trainer* in the current directory if not provided.     data_collator (`DataCollator`, *optional*):         The function to use to form a batch from a list of elements of `train_dataset` or `eval_dataset`. Will         default to [`default_data_collator`] if no `processing_class` is provided, an instance of         [`DataCollatorWithPadding`] otherwise if the processing_class is a feature extractor or tokenizer.     train_dataset (Union[`torch.utils.data.Dataset`, `torch.utils.data.IterableDataset`, `datasets.Dataset`], *optional*):         The dataset to use for training. If it is a [`~datasets.Dataset`], columns not accepted by the         `model.forward()` method are automatically removed.          Note that if it's a `torch.utils.data.IterableDataset` with some randomization and you are training in a         distributed fashion, your iterable dataset should either use a internal attribute `generator` that is a         `torch.Generator` for the randomization that must be identical on all processes (and the Trainer will         manually set the seed of this `generator` at each epoch) or have a `set_epoch()` method that internally         sets the seed of the RNGs used.     eval_dataset (Union[`torch.utils.data.Dataset`, Dict[str, `torch.utils.data.Dataset`, `datasets.Dataset`]), *optional*):          The dataset to use for evaluation. If it is a [`~datasets.Dataset`], columns not accepted by the          `model.forward()` method are automatically removed. If it is a dictionary, it will evaluate on each          dataset prepending the dictionary key to the metric name.     processing_class (`PreTrainedTokenizerBase` or `BaseImageProcessor` or `FeatureExtractionMixin` or `ProcessorMixin`, *optional*):         Processing class used to process the data. If provided, will be used to automatically process the inputs         for the model, and it will be saved along the model to make it easier to rerun an interrupted training or         reuse the fine-tuned model.         This supersedes the `tokenizer` argument, which is now deprecated.     model_init (`Callable[[], PreTrainedModel]`, *optional*):         A function that instantiates the model to be used. If provided, each call to [`~Trainer.train`] will start         from a new instance of the model as given by this function.          The function may have zero argument, or a single one containing the optuna/Ray Tune/SigOpt trial object, to         be able to choose different architectures according to hyper parameters (such as layer count, sizes of         inner layers, dropout probabilities etc).     compute_loss_func (`Callable`, *optional*):         A function that accepts the raw model outputs, labels, and the number of items in the entire accumulated         batch (batch_size * gradient_accumulation_steps) and returns the loss. For example, see the default [loss function](https://github.com/huggingface/transformers/blob/052e652d6d53c2b26ffde87e039b723949a53493/src/transformers/trainer.py#L3618) used by [`Trainer`].     compute_metrics (`Callable[[EvalPrediction], Dict]`, *optional*):         The function that will be used to compute metrics at evaluation. Must take a [`EvalPrediction`] and return         a dictionary string to metric values. *Note* When passing TrainingArgs with `batch_eval_metrics` set to         `True`, your compute_metrics function must take a boolean `compute_result` argument. This will be triggered         after the last eval batch to signal that the function needs to calculate and return the global summary         statistics rather than accumulating the batch-level statistics     callbacks (List of [`TrainerCallback`], *optional*):         A list of callbacks to customize the training loop. Will add those to the list of default callbacks         detailed in [here](callback).          If you want to remove one of the default callbacks used, use the [`Trainer.remove_callback`] method.     optimizers (`Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR]`, *optional*, defaults to `(None, None)`):         A tuple containing the optimizer and the scheduler to use. Will default to an instance of [`AdamW`] on your         model and a scheduler given by [`get_linear_schedule_with_warmup`] controlled by `args`.     optimizer_cls_and_kwargs (`Tuple[Type[torch.optim.Optimizer], Dict[str, Any]]`, *optional*):         A tuple containing the optimizer class and keyword arguments to use.         Overrides `optim` and `optim_args` in `args`. Incompatible with the `optimizers` argument.          Unlike `optimizers`, this argument avoids the need to place model parameters on the correct devices before initializing the Trainer.     preprocess_logits_for_metrics (`Callable[[torch.Tensor, torch.Tensor], torch.Tensor]`, *optional*):         A function that preprocess the logits right before caching them at each evaluation step. Must take two         tensors, the logits and the labels, and return the logits once processed as desired. The modifications made         by this function will be reflected in the predictions received by `compute_metrics`.          Note that the labels (second parameter) will be `None` if the dataset does not have them.  Important attributes:      - **model** -- Always points to the core model. If using a transformers model, it will be a [`PreTrainedModel`]       subclass.     - **model_wrapped** -- Always points to the most external model in case one or more other modules wrap the       original model. This is the model that should be used for the forward pass. For example, under `DeepSpeed`,       the inner model is wrapped in `DeepSpeed` and then again in `torch.nn.DistributedDataParallel`. If the inner       model hasn't been wrapped, then `self.model_wrapped` is the same as `self.model`.     - **is_model_parallel** -- Whether or not a model has been switched to a model parallel mode (different from       data parallelism, this means some of the model layers are split on different GPUs).     - **place_model_on_device** -- Whether or not to automatically place the model on the device - it will be set       to `False` if model parallel or deepspeed is used, or if the default       `TrainingArguments.place_model_on_device` is overridden to return `False` .     - **is_in_train** -- Whether or not a model is currently running `train` (e.g. when `evaluate` is called while       in `train`)\",\n          \":mod:`torch.optim` is a package implementing various optimization algorithms.  Most commonly used methods are already supported, and the interface is general enough, so that more sophisticated ones can also be easily integrated in the future.\",\n          \"This is a generic tokenizer class that will be instantiated as one of the tokenizer classes of the library when created with the [`AutoTokenizer.from_pretrained`] class method.  This class cannot be instantiated directly using `__init__()` (throws an error).\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"api_extract_gemini\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 33,\n        \"samples\": [\n          \"```json\\n{\\n  \\\"api\\\": {\\n    \\\"name\\\": \\\"Hugging Face Login API (Partial)\\\",\\n    \\\"description\\\": \\\"API functions related to user authentication and token management.\\\",\\n    \\\"functions\\\": [\\n      {\\n        \\\"name\\\": \\\"notebook_login\\\",\\n        \\\"description\\\": \\\"Displays a widget to log in to the HF website and store the token. Equivalent to `login` without passing a token when run in a notebook. Useful if you want to force the use of the notebook widget instead of a prompt in the terminal. For more details, see `login`.\\\",\\n        \\\"parameters\\\": [\\n          {\\n            \\\"name\\\": \\\"new_session\\\",\\n            \\\"type\\\": \\\"bool\\\",\\n            \\\"description\\\": \\\"If `True`, will request a token even if one is already saved on the machine.\\\",\\n            \\\"default_value\\\": true,\\n            \\\"required\\\": false\\n          },\\n          {\\n            \\\"name\\\": \\\"write_permission\\\",\\n            \\\"type\\\": \\\"bool\\\",\\n            \\\"description\\\": \\\"Ignored and deprecated argument.\\\",\\n            \\\"required\\\": false,\\n            \\\"notes\\\": [\\\"ignored\\\", \\\"deprecated\\\"]\\n          }\\n        ],\\n        \\\"returns\\\": null,\\n        \\\"relationships\\\": [\\n          {\\n            \\\"type\\\": \\\"related_to\\\",\\n            \\\"target\\\": \\\"login\\\",\\n            \\\"description\\\": \\\"Equivalent to `login` without a token in a notebook context, and provides a widget UI. See `login` for more details.\\\"\\n          }\\n        ],\\n        \\\"notes\\\": \\\"Specifically designed for notebook environments to provide a GUI login experience.\\\"\\n      }\\n    ],\\n    \\\"types\\\": [\\n      {\\n        \\\"name\\\": \\\"bool\\\",\\n        \\\"description\\\": \\\"A boolean value, either true or false.\\\"\\n      }\\n      // The type of the \\\"token\\\" is not explicitly defined as an input/output type\\n      // for this specific function's signature, so it's not listed here.\\n    ]\\n  }\\n}\\n```\",\n          \"```json\\n{\\n  \\\"package\\\": {\\n    \\\"name\\\": \\\"torch.optim\\\",\\n    \\\"description\\\": \\\"A package implementing various optimization algorithms. Most commonly used methods are already supported, and the interface is general enough, so that more sophisticated ones can also be easily integrated in the future.\\\",\\n    \\\"contents\\\": {\\n      \\\"types\\\": [\\n        {\\n          \\\"category\\\": \\\"Optimization Algorithm\\\",\\n          \\\"description\\\": \\\"Classes that implement specific optimization algorithms (e.g., SGD, Adam). The exact names and specific details of these types are not provided in the source documentation snippet.\\\",\\n          \\\"nature\\\": \\\"Class (inferred, as optimizers typically maintain state and have methods like step())\\\",\\n          \\\"count\\\": \\\"Multiple (implied by 'various', 'most commonly used')\\\"\\n        }\\n      ],\\n      \\\"functions\\\": [\\n        {\\n          \\\"category\\\": \\\"Interface Functionality\\\",\\n          \\\"description\\\": \\\"While specific functions within the package namespace are not named, the description implies a general interface, which would involve methods on the optimizer objects (e.g., step(), zero_grad()). The package itself might expose utility functions, but none are specified.\\\",\\n          \\\"details_available\\\": false\\n        }\\n      ]\\n    },\\n    \\\"relationships\\\": [\\n      {\\n        \\\"type\\\": \\\"Adherence to Interface\\\",\\n        \\\"description\\\": \\\"The optimization algorithm types/classes within this package adhere to a general interface.\\\",\\n        \\\"scope\\\": \\\"Internal package components (optimizer types)\\\",\\n        \\\"purpose\\\": \\\"Ensures compatibility, ease of use, and allows for integration of new optimizers.\\\"\\n      },\\n      {\\n        \\\"type\\\": \\\"Implementation Provider\\\",\\n        \\\"description\\\": \\\"The package provides concrete implementations of optimization algorithms.\\\",\\n        \\\"from\\\": \\\"torch.optim (package)\\\",\\n        \\\"to\\\": \\\"Optimization algorithms (conceptual/types)\\\"\\n      }\\n    ]\\n  }\\n}\\n```\",\n          \"```json\\n{\\n  \\\"classes\\\": [\\n    {\\n      \\\"name\\\": \\\"AutoTokenizer\\\",\\n      \\\"description\\\": \\\"A generic tokenizer class that will be instantiated as one of the specific tokenizer classes from the library.\\\",\\n      \\\"instantiation_notes\\\": \\\"Cannot be instantiated directly using `__init__()` (throws an error). Must be created using the `from_pretrained` class method.\\\",\\n      \\\"methods\\\": [\\n        {\\n          \\\"name\\\": \\\"from_pretrained\\\",\\n          \\\"type\\\": \\\"class method\\\",\\n          \\\"description\\\": \\\"Method used to instantiate the generic AutoTokenizer as a specific tokenizer class.\\\"\\n        }\\n      ]\\n    }\\n  ],\\n  \\\"types\\\": [],\\n  \\\"functions\\\": []\\n}\\n```\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"api_extract_gpt\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 33,\n        \"samples\": [\n          \"```json\\n{\\n  \\\"API\\\": {\\n    \\\"function\\\": \\\"notebook_login\\\",\\n    \\\"description\\\": \\\"Displays a widget to log in to the HF website and store the token. Equivalent to the login function without passing a token when run in a notebook. Useful to force the use of the notebook widget instead of a prompt in the terminal.\\\",\\n    \\\"arguments\\\": {\\n      \\\"new_session\\\": {\\n        \\\"type\\\": \\\"bool\\\",\\n        \\\"default\\\": true,\\n        \\\"description\\\": \\\"If True, will request a token even if one is already saved on the machine.\\\"\\n      },\\n      \\\"write_permission\\\": {\\n        \\\"type\\\": \\\"bool\\\",\\n        \\\"description\\\": \\\"Ignored and deprecated argument.\\\"\\n      }\\n    },\\n    \\\"related_functions\\\": [\\\"login\\\"]\\n  }\\n}\\n```\",\n          \"To extract a structured representation for the `torch.optim` API described, we will focus on the main components and relationships specified in the documentation. Since only a brief description is provided, the structured representation will be based on typical content expected in such an API, with placeholder values indicative of the expected content structure. If more concrete content was provided, it would enable a more accurate extraction. For now, the representation is general.\\n\\n```json\\n{\\n  \\\"module\\\": \\\"torch.optim\\\",\\n  \\\"description\\\": \\\"A package implementing various optimization algorithms with a general interface.\\\",\\n  \\\"types\\\": [],\\n  \\\"functions\\\": [],\\n  \\\"classes\\\": [\\n    {\\n      \\\"name\\\": \\\"<Optimizer Name>\\\",\\n      \\\"description\\\": \\\"A class implementing a specific optimization algorithm.\\\",\\n      \\\"methods\\\": [],\\n      \\\"properties\\\": [],\\n      \\\"relationships\\\": []\\n    }\\n  ]\\n}\\n```\\n\\nThis structured representation outlines the basic structure based on the given brief description. Typically, this would be expanded to include actual optimizer names and descriptions along with methods, properties, and relationships, depending on the full content of the module.\",\n          \"Certainly! Based on the provided API documentation, here's a structured JSON representation of the API, capturing the relevant types, functions, and their relationships:\\n\\n```json\\n{\\n  \\\"classes\\\": [\\n    {\\n      \\\"name\\\": \\\"Tokenizer\\\",\\n      \\\"description\\\": \\\"A generic tokenizer class that cannot be instantiated directly.\\\",\\n      \\\"constructor\\\": {\\n        \\\"method\\\": \\\"__init__\\\",\\n        \\\"instantiable\\\": false,\\n        \\\"error_message\\\": \\\"This class cannot be instantiated directly using __init__()\\\"\\n      },\\n      \\\"methods\\\": [],\\n      \\\"instantiation\\\": {\\n        \\\"method\\\": \\\"AutoTokenizer.from_pretrained\\\",\\n        \\\"description\\\": \\\"Instantiates the tokenizer as one of the library's tokenizer classes.\\\"\\n      }\\n    }\\n  ],\\n  \\\"methods\\\": [\\n    {\\n      \\\"class\\\": \\\"AutoTokenizer\\\",\\n      \\\"name\\\": \\\"from_pretrained\\\",\\n      \\\"description\\\": \\\"Class method to create a tokenizer object\\\",\\n      \\\"type\\\": \\\"class_method\\\"\\n    }\\n  ]\\n}\\n```\\n\\nThis JSON structure captures the key concepts from the API specification: the `Tokenizer` class, the fact that it cannot be instantiated directly, and how it can be instantiated through a class method of `AutoTokenizer`.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}","type":"dataframe","variable_name":"df_api"},"text/html":["\n","  <div id=\"df-125cac46-d9ad-4ca9-8610-b52641552d3d\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>API Component</th>\n","      <th>Documentation</th>\n","      <th>api_extract_gemini</th>\n","      <th>api_extract_gpt</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>tensorflow</td>\n","      <td>Top-level module of TensorFlow. By convention,...</td>\n","      <td>```json\\n{\\n  \"module\": {\\n    \"name\": \"tf\",\\n...</td>\n","      <td>```json\\n{\\n  \"module\": \"tensorflow\",\\n  \"alia...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>tensorflow.keras</td>\n","      <td>DO NOT EDIT.  This file was autogenerated. Do ...</td>\n","      <td>```json\\n{\\n  \"types\": {\\n    \"SomeType\": {\\n ...</td>\n","      <td>Of course! Please provide the API documentatio...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Sequential</td>\n","      <td>`Sequential` groups a linear stack of layers i...</td>\n","      <td>```json\\n{\\n  \"description\": \"API representati...</td>\n","      <td>Certainly! Here's a structured representation ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Dense</td>\n","      <td>Just your regular densely-connected NN layer. ...</td>\n","      <td>```json\\n{\\n  \"name\": \"Dense\",\\n  \"type\": \"Lay...</td>\n","      <td>Here is a structured representation of the API...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Input</td>\n","      <td>Used to instantiate a Keras tensor.  A Keras t...</td>\n","      <td>```json\\n{\\n  \"types\": [\\n    {\\n      \"name\":...</td>\n","      <td>To extract the structured representation of th...</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>Concatenate</td>\n","      <td>Concatenates a list of inputs.  It takes as in...</td>\n","      <td>```json\\n{\\n  \"name\": \"Concatenate\",\\n  \"type\"...</td>\n","      <td>```json\\n{\\n  \"API\": \"Concatenate\",\\n  \"Type\":...</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>fetch_california_housing</td>\n","      <td>Load the California housing dataset (regressio...</td>\n","      <td>```json\\n{\\n  \"module\": \"sklearn.datasets\",\\n ...</td>\n","      <td>Here's a structured representation of the API ...</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>make_classification</td>\n","      <td>Generate a random n-class classification probl...</td>\n","      <td>```json\\n{\\n  \"function\": {\\n    \"name\": \"make...</td>\n","      <td>Here's a structured representation of the API ...</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>train_test_split</td>\n","      <td>Split arrays or matrices into random train and...</td>\n","      <td>```json\\n{\\n  \"function\": {\\n    \"name\": \"trai...</td>\n","      <td>Below is the structured representation of the ...</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>StandardScaler</td>\n","      <td>Standardize features by removing the mean and ...</td>\n","      <td>```json\\n{\\n  \"classes\": [\\n    {\\n      \"name...</td>\n","      <td>```json\\n{\\n  \"class\": \"StandardScaler\",\\n  \"d...</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>LogisticRegression</td>\n","      <td>Logistic Regression (aka logit, MaxEnt) classi...</td>\n","      <td>```json\\n{\\n  \"name\": \"LogisticRegression\",\\n ...</td>\n","      <td>```json\\n{\\n  \"class\": \"LogisticRegression\",\\n...</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>make_pipeline</td>\n","      <td>Construct a :class:`Pipeline` from the given e...</td>\n","      <td>```json\\n{\\n  \"api_element\": {\\n    \"type\": \"f...</td>\n","      <td>Here is the structured representation of the A...</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>PCA</td>\n","      <td>Principal component analysis (PCA).  Linear di...</td>\n","      <td>```json\\n{\\n  \"class\": \"PCA\",\\n  \"description\"...</td>\n","      <td>Here is a structured representation of the giv...</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>torch</td>\n","      <td>The torch package contains data structures for...</td>\n","      <td>```json\\n{\\n  \"package\": {\\n    \"name\": \"torch...</td>\n","      <td>Based on the provided description of the `torc...</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>torch.nn</td>\n","      <td>No documentation found.</td>\n","      <td>Based on the input provided (\"No documentation...</td>\n","      <td>Unfortunately, without any documentation or de...</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>torch.optim</td>\n","      <td>:mod:`torch.optim` is a package implementing v...</td>\n","      <td>```json\\n{\\n  \"package\": {\\n    \"name\": \"torch...</td>\n","      <td>To extract a structured representation for the...</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>torchvision</td>\n","      <td>No documentation found.</td>\n","      <td>```json\\n{\\n  \"status\": \"error\",\\n  \"message\":...</td>\n","      <td>To create a structured representation of an AP...</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>torchvision.transforms</td>\n","      <td>No documentation found.</td>\n","      <td>Based on the input \"No documentation found,\" I...</td>\n","      <td>To create a structured representation of an AP...</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>torch.nn.functional</td>\n","      <td>Functional interface.</td>\n","      <td>```json\\n{\\n  \"api_description\": \"Structured r...</td>\n","      <td>To create a structured representation of the A...</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>Dataset</td>\n","      <td>An abstract class representing a :class:`Datas...</td>\n","      <td>```json\\n{\\n  \"types\": [\\n    {\\n      \"name\":...</td>\n","      <td>```json\\n{\\n  \"class\": \"Dataset\",\\n  \"descript...</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>DataLoader</td>\n","      <td>Data loader combines a dataset and a sampler, ...</td>\n","      <td>```json\\n{\\n  \"api_description\": \"Data loader ...</td>\n","      <td>Here's a structured representation of the API ...</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>torchvision.datasets</td>\n","      <td>No documentation found.</td>\n","      <td>```json\\n{\\n  \"types\": [],\\n  \"functions\": [],...</td>\n","      <td>Since there is no documentation provided, it's...</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>ToTensor</td>\n","      <td>Convert a PIL Image or ndarray to tensor and s...</td>\n","      <td>```json\\n{\\n  \"api_component\": \"Transform\",\\n ...</td>\n","      <td>```json\\n{\\n  \"API\": {\\n    \"name\": \"ConvertTo...</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>numpy</td>\n","      <td>NumPy =====  Provides   1. An array object of ...</td>\n","      <td>```json\\n{\\n  \"name\": \"NumPy\",\\n  \"description...</td>\n","      <td>Here's a structured representation of the NumP...</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>pandas</td>\n","      <td>pandas - a powerful data analysis and manipula...</td>\n","      <td>```json\\n{\\n  \"library\": \"pandas\",\\n  \"descrip...</td>\n","      <td>Certainly! Below is a structured representatio...</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>matplotlib.pyplot</td>\n","      <td>`matplotlib.pyplot` is a state-based interface...</td>\n","      <td>```json\\n{\\n  \"module\": {\\n    \"name\": \"matplo...</td>\n","      <td>```json\\n{\\n  \"module\": \"matplotlib.pyplot\",\\n...</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>AutoTokenizer</td>\n","      <td>This is a generic tokenizer class that will be...</td>\n","      <td>```json\\n{\\n  \"classes\": [\\n    {\\n      \"name...</td>\n","      <td>Certainly! Based on the provided API documenta...</td>\n","    </tr>\n","    <tr>\n","      <th>27</th>\n","      <td>DataCollatorWithPadding</td>\n","      <td>Data collator that will dynamically pad the in...</td>\n","      <td>```json\\n[\\n  {\\n    \"name\": \"DataCollatorWith...</td>\n","      <td>```json\\n{\\n  \"DataCollator\": {\\n    \"descript...</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>AutoModelForSequenceClassification</td>\n","      <td>This is a generic model class that will be ins...</td>\n","      <td>```json\\n{\\n  \"types\": [\\n    {\\n      \"name\":...</td>\n","      <td>Based on the provided API documentation, the s...</td>\n","    </tr>\n","    <tr>\n","      <th>29</th>\n","      <td>TrainingArguments</td>\n","      <td>TrainingArguments is the subset of the argumen...</td>\n","      <td>```json\\n{\\n  \"class_name\": \"TrainingArguments...</td>\n","      <td>Below is a structured JSON representation of t...</td>\n","    </tr>\n","    <tr>\n","      <th>30</th>\n","      <td>Trainer</td>\n","      <td>Trainer is a simple but feature-complete train...</td>\n","      <td>```json\\n{\\n  \"api\": {\\n    \"name\": \"Hugging F...</td>\n","      <td>Here is a structured representation of the API...</td>\n","    </tr>\n","    <tr>\n","      <th>31</th>\n","      <td>notebook_login</td>\n","      <td>Displays a widget to log in to the HF website ...</td>\n","      <td>```json\\n{\\n  \"api\": {\\n    \"name\": \"Hugging F...</td>\n","      <td>```json\\n{\\n  \"API\": {\\n    \"function\": \"noteb...</td>\n","    </tr>\n","    <tr>\n","      <th>32</th>\n","      <td>load_dataset</td>\n","      <td>Load a dataset from the Hugging Face Hub, or a...</td>\n","      <td>```json\\n{\\n  \"name\": \"load_dataset\",\\n  \"desc...</td>\n","      <td>Below is a structured JSON representation of t...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-125cac46-d9ad-4ca9-8610-b52641552d3d')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-125cac46-d9ad-4ca9-8610-b52641552d3d button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-125cac46-d9ad-4ca9-8610-b52641552d3d');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    <div id=\"df-8b88259d-b7ca-4140-ac1a-0ca67422283b\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-8b88259d-b7ca-4140-ac1a-0ca67422283b')\"\n","                title=\"Suggest charts\"\n","                style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","      <script>\n","        async function quickchart(key) {\n","          const quickchartButtonEl =\n","            document.querySelector('#' + key + ' button');\n","          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","          quickchartButtonEl.classList.add('colab-df-spinner');\n","          try {\n","            const charts = await google.colab.kernel.invokeFunction(\n","                'suggestCharts', [key], {});\n","          } catch (error) {\n","            console.error('Error during call to suggestCharts:', error);\n","          }\n","          quickchartButtonEl.classList.remove('colab-df-spinner');\n","          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","        }\n","        (() => {\n","          let quickchartButtonEl =\n","            document.querySelector('#df-8b88259d-b7ca-4140-ac1a-0ca67422283b button');\n","          quickchartButtonEl.style.display =\n","            google.colab.kernel.accessAllowed ? 'block' : 'none';\n","        })();\n","      </script>\n","    </div>\n","\n","  <div id=\"id_d350bc3e-0027-4a36-b20f-ee973e68ed60\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_api')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_d350bc3e-0027-4a36-b20f-ee973e68ed60 button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('df_api');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"],"text/plain":["                         API Component  \\\n","0                           tensorflow   \n","1                     tensorflow.keras   \n","2                           Sequential   \n","3                                Dense   \n","4                                Input   \n","5                          Concatenate   \n","6             fetch_california_housing   \n","7                  make_classification   \n","8                     train_test_split   \n","9                       StandardScaler   \n","10                  LogisticRegression   \n","11                       make_pipeline   \n","12                                 PCA   \n","13                               torch   \n","14                            torch.nn   \n","15                         torch.optim   \n","16                         torchvision   \n","17              torchvision.transforms   \n","18                 torch.nn.functional   \n","19                             Dataset   \n","20                          DataLoader   \n","21                torchvision.datasets   \n","22                            ToTensor   \n","23                               numpy   \n","24                              pandas   \n","25                   matplotlib.pyplot   \n","26                       AutoTokenizer   \n","27             DataCollatorWithPadding   \n","28  AutoModelForSequenceClassification   \n","29                   TrainingArguments   \n","30                             Trainer   \n","31                      notebook_login   \n","32                        load_dataset   \n","\n","                                        Documentation  \\\n","0   Top-level module of TensorFlow. By convention,...   \n","1   DO NOT EDIT.  This file was autogenerated. Do ...   \n","2   `Sequential` groups a linear stack of layers i...   \n","3   Just your regular densely-connected NN layer. ...   \n","4   Used to instantiate a Keras tensor.  A Keras t...   \n","5   Concatenates a list of inputs.  It takes as in...   \n","6   Load the California housing dataset (regressio...   \n","7   Generate a random n-class classification probl...   \n","8   Split arrays or matrices into random train and...   \n","9   Standardize features by removing the mean and ...   \n","10  Logistic Regression (aka logit, MaxEnt) classi...   \n","11  Construct a :class:`Pipeline` from the given e...   \n","12  Principal component analysis (PCA).  Linear di...   \n","13  The torch package contains data structures for...   \n","14                            No documentation found.   \n","15  :mod:`torch.optim` is a package implementing v...   \n","16                            No documentation found.   \n","17                            No documentation found.   \n","18                              Functional interface.   \n","19  An abstract class representing a :class:`Datas...   \n","20  Data loader combines a dataset and a sampler, ...   \n","21                            No documentation found.   \n","22  Convert a PIL Image or ndarray to tensor and s...   \n","23  NumPy =====  Provides   1. An array object of ...   \n","24  pandas - a powerful data analysis and manipula...   \n","25  `matplotlib.pyplot` is a state-based interface...   \n","26  This is a generic tokenizer class that will be...   \n","27  Data collator that will dynamically pad the in...   \n","28  This is a generic model class that will be ins...   \n","29  TrainingArguments is the subset of the argumen...   \n","30  Trainer is a simple but feature-complete train...   \n","31  Displays a widget to log in to the HF website ...   \n","32  Load a dataset from the Hugging Face Hub, or a...   \n","\n","                                   api_extract_gemini  \\\n","0   ```json\\n{\\n  \"module\": {\\n    \"name\": \"tf\",\\n...   \n","1   ```json\\n{\\n  \"types\": {\\n    \"SomeType\": {\\n ...   \n","2   ```json\\n{\\n  \"description\": \"API representati...   \n","3   ```json\\n{\\n  \"name\": \"Dense\",\\n  \"type\": \"Lay...   \n","4   ```json\\n{\\n  \"types\": [\\n    {\\n      \"name\":...   \n","5   ```json\\n{\\n  \"name\": \"Concatenate\",\\n  \"type\"...   \n","6   ```json\\n{\\n  \"module\": \"sklearn.datasets\",\\n ...   \n","7   ```json\\n{\\n  \"function\": {\\n    \"name\": \"make...   \n","8   ```json\\n{\\n  \"function\": {\\n    \"name\": \"trai...   \n","9   ```json\\n{\\n  \"classes\": [\\n    {\\n      \"name...   \n","10  ```json\\n{\\n  \"name\": \"LogisticRegression\",\\n ...   \n","11  ```json\\n{\\n  \"api_element\": {\\n    \"type\": \"f...   \n","12  ```json\\n{\\n  \"class\": \"PCA\",\\n  \"description\"...   \n","13  ```json\\n{\\n  \"package\": {\\n    \"name\": \"torch...   \n","14  Based on the input provided (\"No documentation...   \n","15  ```json\\n{\\n  \"package\": {\\n    \"name\": \"torch...   \n","16  ```json\\n{\\n  \"status\": \"error\",\\n  \"message\":...   \n","17  Based on the input \"No documentation found,\" I...   \n","18  ```json\\n{\\n  \"api_description\": \"Structured r...   \n","19  ```json\\n{\\n  \"types\": [\\n    {\\n      \"name\":...   \n","20  ```json\\n{\\n  \"api_description\": \"Data loader ...   \n","21  ```json\\n{\\n  \"types\": [],\\n  \"functions\": [],...   \n","22  ```json\\n{\\n  \"api_component\": \"Transform\",\\n ...   \n","23  ```json\\n{\\n  \"name\": \"NumPy\",\\n  \"description...   \n","24  ```json\\n{\\n  \"library\": \"pandas\",\\n  \"descrip...   \n","25  ```json\\n{\\n  \"module\": {\\n    \"name\": \"matplo...   \n","26  ```json\\n{\\n  \"classes\": [\\n    {\\n      \"name...   \n","27  ```json\\n[\\n  {\\n    \"name\": \"DataCollatorWith...   \n","28  ```json\\n{\\n  \"types\": [\\n    {\\n      \"name\":...   \n","29  ```json\\n{\\n  \"class_name\": \"TrainingArguments...   \n","30  ```json\\n{\\n  \"api\": {\\n    \"name\": \"Hugging F...   \n","31  ```json\\n{\\n  \"api\": {\\n    \"name\": \"Hugging F...   \n","32  ```json\\n{\\n  \"name\": \"load_dataset\",\\n  \"desc...   \n","\n","                                      api_extract_gpt  \n","0   ```json\\n{\\n  \"module\": \"tensorflow\",\\n  \"alia...  \n","1   Of course! Please provide the API documentatio...  \n","2   Certainly! Here's a structured representation ...  \n","3   Here is a structured representation of the API...  \n","4   To extract the structured representation of th...  \n","5   ```json\\n{\\n  \"API\": \"Concatenate\",\\n  \"Type\":...  \n","6   Here's a structured representation of the API ...  \n","7   Here's a structured representation of the API ...  \n","8   Below is the structured representation of the ...  \n","9   ```json\\n{\\n  \"class\": \"StandardScaler\",\\n  \"d...  \n","10  ```json\\n{\\n  \"class\": \"LogisticRegression\",\\n...  \n","11  Here is the structured representation of the A...  \n","12  Here is a structured representation of the giv...  \n","13  Based on the provided description of the `torc...  \n","14  Unfortunately, without any documentation or de...  \n","15  To extract a structured representation for the...  \n","16  To create a structured representation of an AP...  \n","17  To create a structured representation of an AP...  \n","18  To create a structured representation of the A...  \n","19  ```json\\n{\\n  \"class\": \"Dataset\",\\n  \"descript...  \n","20  Here's a structured representation of the API ...  \n","21  Since there is no documentation provided, it's...  \n","22  ```json\\n{\\n  \"API\": {\\n    \"name\": \"ConvertTo...  \n","23  Here's a structured representation of the NumP...  \n","24  Certainly! Below is a structured representatio...  \n","25  ```json\\n{\\n  \"module\": \"matplotlib.pyplot\",\\n...  \n","26  Certainly! Based on the provided API documenta...  \n","27  ```json\\n{\\n  \"DataCollator\": {\\n    \"descript...  \n","28  Based on the provided API documentation, the s...  \n","29  Below is a structured JSON representation of t...  \n","30  Here is a structured representation of the API...  \n","31  ```json\\n{\\n  \"API\": {\\n    \"function\": \"noteb...  \n","32  Below is a structured JSON representation of t...  "]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["df_api = pd.read_csv(\"/content/drive/MyDrive/Stanford NLP/api_documentation_new.csv\")\n","df_api"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":606},"executionInfo":{"elapsed":529481,"status":"ok","timestamp":1748905142358,"user":{"displayName":"Neeharika Gupta","userId":"13434636909198909500"},"user_tz":420},"id":"9ZHW1Nu28320","outputId":"c57be689-a295-4166-f033-87e5a0f6078c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Processing row 1/33...\n","Processing row 2/33...\n","Processing row 3/33...\n","Processing row 4/33...\n","Processing row 5/33...\n","Processing row 6/33...\n","Processing row 7/33...\n","Processing row 8/33...\n","Processing row 9/33...\n","Processing row 10/33...\n","Processing row 11/33...\n","Processing row 12/33...\n","Processing row 13/33...\n","Processing row 14/33...\n","Processing row 15/33...\n","Processing row 16/33...\n","Processing row 17/33...\n","Processing row 18/33...\n","Processing row 19/33...\n","Processing row 20/33...\n","Processing row 21/33...\n","Processing row 22/33...\n","Processing row 23/33...\n","Processing row 24/33...\n","Processing row 25/33...\n","Processing row 26/33...\n","Processing row 27/33...\n","Processing row 28/33...\n","Processing row 29/33...\n","Processing row 30/33...\n","Processing row 31/33...\n","Processing row 32/33...\n","Processing row 33/33...\n"]}],"source":["output = []\n","\n","for index, row in df_api.iterrows():\n","    print(f\"Processing row {index+1}/{len(df_api)}...\")\n","\n","    try:\n","        prompt = (\n","            f\"Given the API Component:\\n\"\n","            f\"{row['API Component'].strip()}\\n\"\n","            f\"Use the given extracted API structure and generate rule based checking constraints for each of the components. Just write the important constraints to check in a bulleted format.\\n\"\n","            # f\"Use the given extracted API structure and generate rule based checking constraints for each of the components by writing a python code to check those constraints.\\n\"\n","            f\"{row['api_extract_gemini']}\\n\\n\"\n","        )\n","\n","        response = make_request(prompt, model)\n","    except Exception as e:\n","        print(f\"Error generating response: {e}\")\n","        response = f\"Error: {e}\"\n","\n","    output.append(response)\n","\n","    time.sleep(DELAY_BETWEEN_REQUESTS)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MhAiUMDL4ec7"},"outputs":[],"source":["df_results = pd.DataFrame({'gemini_symbolic': output})\n","\n","output_path = \"/content/drive/MyDrive/Stanford NLP/symbolic_gemini.csv\"\n","df_results.to_csv(output_path, index=False)"]},{"cell_type":"markdown","metadata":{"id":"AXn0BZNaqUrq"},"source":["###Final code generation"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":804},"executionInfo":{"elapsed":460,"status":"ok","timestamp":1748919984155,"user":{"displayName":"Neeharika Gupta","userId":"13434636909198909500"},"user_tz":420},"id":"njSsAU8Dw3YL","outputId":"18d90209-d197-42ce-ba30-020b0d91df4c"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"summary":"{\n  \"name\": \"df\",\n  \"rows\": 8,\n  \"fields\": [\n    {\n      \"column\": \"Task\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"Write a Python script using PyTorch to train a convolutional neural network (CNN) on the Cifar 10 dataset. Include data loading, model definition, training loop, and evaluation\",\n          \"Write a PyTorch class that subclasses torch.utils.data.Dataset to load image files and labels from a folder. Use this class with a DataLoader for batching. Use FashionMNIST dataset.\",\n          \"Write a Python function using TensorFlow to create and compile a simple linear regression model with one dense layer. Train it on the classic California housing dataset.\\n\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Code\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"import torch\\nimport torch.nn as nn\\nimport torch.optim as optim\\nimport torchvision\\nimport torchvision.transforms as transforms\\nimport torch.nn.functional as F\\n\\ntransform = transforms.Compose(\\n    [transforms.ToTensor(),\\n     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\\n\\ntrainset = torchvision.datasets.CIFAR10(root='./data', train=True,\\n                                        download=True, transform=transform)\\n                                        \\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\\n                                          shuffle=True, num_workers=2)\\n\\ntestset = torchvision.datasets.CIFAR10(root='./data', train=False,\\n                                       download=True, transform=transform)\\n                                       \\ntestloader = torch.utils.data.DataLoader(testset, batch_size=4,\\n                                         shuffle=False, num_workers=2)\\n\\nclasses = ('plane', 'car', 'bird', 'cat',\\n           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\\n\\nclass Net(nn.Module):\\n    def __init__(self):\\n        super(Net, self).__init__()\\n        self.conv1 = nn.Conv2d(3, 6, 5)\\n        self.pool = nn.MaxPool2d(2, 2)\\n        self.conv2 = nn.Conv2d(6, 16, 5)\\n        self.fc1 = nn.Linear(16 * 5 * 5, 120)\\n        self.fc2 = nn.Linear(120, 84)\\n        self.fc3 = nn.Linear(84, 10)\\n\\n    def forward(self, x):\\n        x = self.pool(F.relu(self.conv1(x)))\\n        x = self.pool(F.relu(self.conv2(x)))\\n        x = x.view(-1, 16 * 5 * 5)\\n        x = F.relu(self.fc1(x))\\n        x = F.relu(self.fc2(x))\\n        x = self.fc3(x)\\n        return x\\n\\nnet = Net()\\n\\ncriterion = nn.CrossEntropyLoss()\\noptimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\\n\\nfor epoch in range(2):  \\n\\n    running_loss = 0.0\\n    for i, data in enumerate(trainloader, 0):\\n        inputs, labels = data\\n\\n        optimizer.zero_grad()\\n\\n        outputs = net(inputs)\\n        loss = criterion(outputs, labels)\\n        loss.backward()\\n        optimizer.step()\\n\\n        running_loss += loss.item()\\n        if i % 2000 == 1999: \\n            print('[%d, %5d] loss: %.3f' %\\n                  (epoch + 1, i + 1, running_loss / 2000))\\n            running_loss = 0.0\\n\\ncorrect = 0\\ntotal = 0\\nwith torch.no_grad():\\n    for data in testloader:\\n        images, labels = data\\n        outputs = net(images)\\n        _, predicted = torch.max(outputs.data, 1)\\n        total += labels.size(0)\\n        correct += (predicted == labels).sum().item()\\n\\nprint('Accuracy of the network on the 10000 test images: %d %%' % (\\n    100 * correct / total))\",\n          \"from torch.utils.data import Dataset\\nfrom torchvision import datasets\\nfrom torchvision.transforms import ToTensor\\nfrom torch.utils.data import DataLoader\\n\\n\\ntraining_data = datasets.FashionMNIST(\\n    root=\\\"data\\\",\\n    train=True,\\n    download=True,\\n    transform=ToTensor()\\n)\\n\\ntest_data = datasets.FashionMNIST(\\n    root=\\\"data\\\",\\n    train=False,\\n    download=True,\\n    transform=ToTensor()\\n)\\n\\ntrain_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\\ntest_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)\",\n          \"import tensorflow as tf\\nfrom tensorflow import keras\\nfrom tensorflow.keras.models import Sequential\\nfrom tensorflow.keras.layers import Dense, Input, Concatenate\\n\\nfrom sklearn.datasets import fetch_california_housing\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.preprocessing import StandardScaler\\nimport matplotlib.pyplot as plt\\n\\nimport numpy as np\\nimport pandas as pd\\nhousing = fetch_california_housing()\\nXtrain, Xtest, ytrain, ytest = train_test_split(housing.data, housing.target, test_size = .2)\\nXtrain, Xval, ytrain, yval = train_test_split(Xtrain, ytrain, test_size=.2)\\n\\nmodel = Sequential([\\n    Dense(30, activation = 'relu', input_shape= Xtrain.shape[1:]),\\n    Dense(1)\\n])\\n\\nmodel.compile(loss='mean_squared_error', optimizer='adam')\\n\\n\\nhistory = model.fit(Xtrain, ytrain, validation_data=(Xval, yval), epochs=20)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"API doc\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          \"import tensorflow as tf\\nfrom tensorflow import keras\\nfrom tensorflow.keras.models import Sequential\\nfrom tensorflow.keras.layers import Dense, Input, Concatenate\\n\\nfrom sklearn.datasets import fetch_california_housing\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.preprocessing import StandardScaler\\nimport matplotlib.pyplot as plt\\n\\nimport numpy as np\\nimport pandas as pd\\n\",\n          \"import torch.nn as nn\\nimport torch.optim as optim\\nimport torchvision\\nimport torchvision.transforms as transforms\\nimport torch.nn.functional as F\\ntransforms.Compose\\ntorchvision.datasets.CIFAR10\\ntorch.utils.data.DataLoader\\nnn.Conv2d\\nnn.MaxPool2d\\nnn.Linear\\nF.relu\\nnn.CrossEntropyLoss\\noptim.SGD\\n\",\n          \"from torch.utils.data import Dataset\\nfrom torchvision import datasets\\nfrom torchvision.transforms import ToTensor\\nfrom torch.utils.data import DataLoader\\ndatasets.FashionMNIST\\n\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Few shot t1\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"Write a Python script using PyTorch to train a convolutional neural network (CNN) on the MNIST dataset. Include data loading, model definition, training loop, and evaluation\",\n          \"Write a PyTorch class that creates custom dataset. It takes in input of img dir and annotations csv file.\",\n          \"Create a python script that uses tensorflow to model a linear regression problem using randomly genenrated 50 datat points. You need to train the model and also get the predictions.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Few shot c1\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"import numpy as np\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\n\\n\\nimport torch\\nfrom torch import optim\\nfrom torch import nn\\nfrom torch.utils.data import DataLoader\\nfrom tqdm import tqdm\\n\\n# !pip install torchvision\\nimport torchvision\\n\\nimport torch.nn.functional as F\\nimport torchvision.datasets as datasets\\nimport torchvision.transforms as transforms\\n\\n# !pip install torchmetrics\\nimport torchmetrics\\n\\nbatch_size = 60\\n\\ntrain_dataset = datasets.MNIST(root=\\\"dataset/\\\", download=True, train=True, transform=transforms.ToTensor())\\n\\ntrain_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\\n\\ntest_dataset = datasets.MNIST(root=\\\"dataset/\\\", download=True, train=False, transform=transforms.ToTensor())\\n\\ntest_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)\\n\\nclass CNN(nn.Module):\\n   def __init__(self, in_channels, num_classes):\\n\\n       \\\"\\\"\\\"\\n       Building blocks of convolutional neural network.\\n\\n       Parameters:\\n           * in_channels: Number of channels in the input image (for grayscale images, 1)\\n           * num_classes: Number of classes to predict. In our problem, 10 (i.e digits from  0 to 9).\\n       \\\"\\\"\\\"\\n       super(CNN, self).__init__()\\n\\n       # 1st convolutional layer\\n       self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=8, kernel_size=3, padding=1)\\n       # Max pooling layer\\n       self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\\n       # 2nd convolutional layer\\n       self.conv2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, padding=1)\\n       # Fully connected layer\\n       self.fc1 = nn.Linear(16 * 7 * 7, num_classes)\\n\\n   def forward(self, x):\\n       \\\"\\\"\\\"\\n       Define the forward pass of the neural network.\\n\\n       Parameters:\\n           x: Input tensor.\\n\\n       Returns:\\n           torch.Tensor\\n               The output tensor after passing through the network.\\n       \\\"\\\"\\\"\\n       x = F.relu(self.conv1(x))  # Apply first convolution and ReLU activation\\n       x = self.pool(x)           # Apply max pooling\\n       x = F.relu(self.conv2(x))  # Apply second convolution and ReLU activation\\n       x = self.pool(x)           # Apply max pooling\\n       x = x.reshape(x.shape[0], -1)  # Flatten the tensor\\n       x = self.fc1(x)            # Apply fully connected layer\\n       return x\\n       x = x.reshape(x.shape[0], -1)  # Flatten the tensor\\n       x = self.fc1(x)            # Apply fully connected layer\\n       return x\\n\\n# Define the loss function\\ncriterion = nn.CrossEntropyLoss()\\n\\n# Define the optimizer\\noptimizer = optim.Adam(model.parameters(), lr=0.001)\\n\\nnum_epochs=10\\nfor epoch in range(num_epochs):\\n # Iterate over training batches\\n   print(f\\\"Epoch [{epoch + 1}/{num_epochs}]\\\")\\n\\n   for batch_index, (data, targets) in enumerate(tqdm(dataloader_train)):\\n       data = data.to(device)\\n       targets = targets.to(device)\\n       scores = model(data)\\n       loss = criterion(scores, targets)\\n       optimizer.zero_grad()\\n       loss.backward()\\n       optimizer.step()\\n# Set up of multiclass accuracy metric\\nacc = Accuracy(task=\\\"multiclass\\\",num_classes=10)\\n\\n# Iterate over the dataset batches\\nmodel.eval()\\nwith torch.no_grad():\\n   for images, labels in dataloader_test:\\n       # Get predicted probabilities for test data batch\\n       outputs = model(images)\\n       _, preds = torch.max(outputs, 1)\\n       acc(preds, labels)\\n       precision(preds, labels)\\n       recall(preds, labels)\\n\\n#Compute total test accuracy\\ntest_accuracy = acc.compute()\\nprint(f\\\"Test accuracy: {test_accuracy}\\\")\\n\",\n          \"\\nimport os\\nimport pandas as pd\\nfrom torchvision.io import read_image\\n\\nclass CustomImageDataset(Dataset):\\n    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\\n        self.img_labels = pd.read_csv(annotations_file)\\n        self.img_dir = img_dir\\n        self.transform = transform\\n        self.target_transform = target_transform\\n\\n    def __len__(self):\\n        return len(self.img_labels)\\n\\n    def __getitem__(self, idx):\\n        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\\n        image = read_image(img_path)\\n        label = self.img_labels.iloc[idx, 1]\\n        if self.transform:\\n            image = self.transform(image)\\n        if self.target_transform:\\n            label = self.target_transform(label)\\n        return image, label\\n\",\n          \"\\nimport numpy as np\\nimport tensorflow as tf\\nimport matplotlib.pyplot as plt\\n\\nnp.random.seed(101)\\n\\n# Generating random linear data\\n# There will be 50 data points ranging from 0 to 50\\nx = np.linspace(0, 50, 50)\\ny = np.linspace(0, 50, 50)\\n\\n# Adding noise to the random linear data\\nx += np.random.uniform(-4, 4, 50)\\ny += np.random.uniform(-4, 4, 50)\\n\\nn = len(x) # Number of data points\\n\\n# Plot of Training Data\\nplt.scatter(x, y)\\nplt.xlabel('x')\\nplt.ylabel('y')\\nplt.title(\\\"Training Data\\\")\\nplt.show()\\n\\nX = tf.placeholder(\\\"float\\\")\\nY = tf.placeholder(\\\"float\\\")\\n\\nW = tf.Variable(np.random.randn(), name = \\\"W\\\")\\nb = tf.Variable(np.random.randn(), name = \\\"b\\\")\\n\\nlearning_rate = 0.01\\ntraining_epochs = 1000\\n# Hypothesis\\ny_pred = tf.add(tf.multiply(X, W), b)\\n\\n# Mean Squared Error Cost Function\\ncost = tf.reduce_sum(tf.pow(y_pred-Y, 2)) / (2 * n)\\n\\n# Gradient Descent Optimizer\\noptimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\\n\\n# Global Variables Initializer\\ninit = tf.global_variables_initializer()\\n\\n# Starting the Tensorflow Session\\nwith tf.Session() as sess:\\n    \\n    # Initializing the Variables\\n    sess.run(init)\\n    \\n    # Iterating through all the epochs\\n    for epoch in range(training_epochs):\\n        \\n        # Feeding each data point into the optimizer using Feed Dictionary\\n        for (_x, _y) in zip(x, y):\\n            sess.run(optimizer, feed_dict = {X : _x, Y : _y})\\n        \\n        # Displaying the result after every 50 epochs\\n        if (epoch + 1) % 50 == 0:\\n            # Calculating the cost a every epoch\\n            c = sess.run(cost, feed_dict = {X : x, Y : y})\\n            print(\\\"Epoch\\\", (epoch + 1), \\\": cost =\\\", c, \\\"W =\\\", sess.run(W), \\\"b =\\\", sess.run(b))\\n    \\n    # Storing necessary values to be used outside the Session\\n    training_cost = sess.run(cost, feed_dict ={X: x, Y: y})\\n    weight = sess.run(W)\\n    bias = sess.run(b)\\n\\n# Calculating the predictions\\npredictions = weight * x + bias\\nprint(\\\"Training cost =\\\", training_cost, \\\"Weight =\\\", weight, \\\"bias =\\\", bias, '\\\\n')\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Few shot t2\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"Write a Python script using PyTorch to train a convolutional neural network (CNN) on the KMNIST dataset. Include data loading, model definition, training loop, and evaluation\",\n          \"Write a PyTorch class that subclasses torch.utils.data.Dataset to load 100 image files and labels from a folder. The data would be randomly created and the class should also be defined such that its a custom class.Use this class with a DataLoader for batching. \",\n          \"Write a Python function using TensorFlow to create and compile a simple linear regression model which has a normalizer layer and a dense layer. Train it on the fuel efficiency dataset.\\n\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Few shot c2\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"from torch.nn import Module\\nfrom torch.nn import Conv2d\\nfrom torch.nn import Linear\\nfrom torch.nn import MaxPool2d\\nfrom torch.nn import ReLU\\nfrom torch.nn import LogSoftmax\\nfrom torch import flatten\\nimport matplotlib\\nmatplotlib.use(\\\"Agg\\\")\\nfrom pyimagesearch.lenet import LeNet\\nfrom sklearn.metrics import classification_report\\nfrom torch.utils.data import random_split\\nfrom torch.utils.data import DataLoader\\nfrom torchvision.transforms import ToTensor\\nfrom torchvision.datasets import KMNIST\\nfrom torch.optim import Adam\\nfrom torch import nn\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\nimport argparse\\nimport torch\\nimport time\\n\\nclass LeNet(Module):\\n        def __init__(self, numChannels, classes):\\n                # call the parent constructor\\n                super(LeNet, self).__init__()\\n                # initialize first set of CONV => RELU => POOL layers\\n                self.conv1 = Conv2d(in_channels=numChannels, out_channels=20,\\n                        kernel_size=(5, 5))\\n                self.relu1 = ReLU()\\n                self.maxpool1 = MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\\n                # initialize second set of CONV => RELU => POOL layers\\n                self.conv2 = Conv2d(in_channels=20, out_channels=50,\\n                        kernel_size=(5, 5))\\n                self.relu2 = ReLU()\\n                self.maxpool2 = MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\\n                # initialize first (and only) set of FC => RELU layers\\n                self.fc1 = Linear(in_features=800, out_features=500)\\n                self.relu3 = ReLU()\\n                # initialize our softmax classifier\\n                self.fc2 = Linear(in_features=500, out_features=classes)\\n                self.logSoftmax = LogSoftmax(dim=1)\\n\\ndef forward(self, x):\\n                # pass the input through our first set of CONV => RELU =>\\n                # POOL layers\\n                x = self.conv1(x)\\n                x = self.relu1(x)\\n                x = self.maxpool1(x)\\n                # pass the output from the previous layer through the second\\n                # set of CONV => RELU => POOL layers\\n                x = self.conv2(x)\\n                x = self.relu2(x)\\n                x = self.maxpool2(x)\\n                # flatten the output from the previous layer and pass it\\n                # through our only set of FC => RELU layers\\n                x = flatten(x, 1)\\n                x = self.fc1(x)\\n                x = self.relu3(x)\\n                # pass the output to our softmax classifier to get our output\\n                # predictions\\n                x = self.fc2(x)\\n                output = self.logSoftmax(x)\\n                # return the output predictions\\n                return output\\n# construct the argument parser and parse the arguments\\nap = argparse.ArgumentParser()\\nap.add_argument(\\\"-m\\\", \\\"--model\\\", type=str, required=True,\\n        help=\\\"path to output trained model\\\")\\nap.add_argument(\\\"-p\\\", \\\"--plot\\\", type=str, required=True,\\n        help=\\\"path to output loss/accuracy plot\\\")\\nargs = vars(ap.parse_args())\\n\\nPyTorch: Training your first Convolutional Neural Network (CNN)\\n# define training hyperparameters\\nINIT_LR = 1e-3\\nBATCH_SIZE = 64\\nEPOCHS = 10\\n# define the train and val splits\\nTRAIN_SPLIT = 0.75\\nVAL_SPLIT = 1 - TRAIN_SPLIT\\n# set the device we will be using to train the model\\ndevice = torch.device(\\\"cuda\\\" if torch.cuda.is_available() else \\\"cpu\\\")\\n\\n# load the KMNIST dataset\\nprint(\\\"[INFO] loading the KMNIST dataset...\\\")\\ntrainData = KMNIST(root=\\\"data\\\", train=True, download=True,\\n        transform=ToTensor())\\ntestData = KMNIST(root=\\\"data\\\", train=False, download=True,\\n        transform=ToTensor())\\n# calculate the train/validation split\\nprint(\\\"[INFO] generating the train/validation split...\\\")\\nnumTrainSamples = int(len(trainData) * TRAIN_SPLIT)\\nnumValSamples = int(len(trainData) * VAL_SPLIT)\\n(trainData, valData) = random_split(trainData,\\n        [numTrainSamples, numValSamples],\\n        generator=torch.Generator().manual_seed(42))\\n\\n# initialize the train, validation, and test data loaders\\ntrainDataLoader = DataLoader(trainData, shuffle=True,\\n        batch_size=BATCH_SIZE)\\nvalDataLoader = DataLoader(valData, batch_size=BATCH_SIZE)\\ntestDataLoader = DataLoader(testData, batch_size=BATCH_SIZE)\\n# calculate steps per epoch for training and validation set\\ntrainSteps = len(trainDataLoader.dataset) // BATCH_SIZE\\nvalSteps = len(valDataLoader.dataset) // BATCH_SIZE\\n\\n# initialize the LeNet model\\nprint(\\\"[INFO] initializing the LeNet model...\\\")\\nmodel = LeNet(\\n        numChannels=1,\\n        classes=len(trainData.dataset.classes)).to(device)\\n# initialize our optimizer and loss function\\nopt = Adam(model.parameters(), lr=INIT_LR)\\nlossFn = nn.NLLLoss()\\n# initialize a dictionary to store training history\\nH = {\\n        \\\"train_loss\\\": [],\\n        \\\"train_acc\\\": [],\\n        \\\"val_loss\\\": [],\\n        \\\"val_acc\\\": []\\n}\\n# measure how long training is going to take\\nprint(\\\"[INFO] training the network...\\\")\\n\\n# loop over our epochs\\nfor e in range(0, EPOCHS):\\n        # set the model in training mode\\n        model.train()\\n        # initialize the total training and validation loss\\n        totalTrainLoss = 0\\n        totalValLoss = 0\\n        # initialize the number of correct predictions in the training\\n        # and validation step\\n        trainCorrect = 0\\n        valCorrect = 0\\n        # loop over the training set\\n        for (x, y) in trainDataLoader:\\n                # send the input to the device\\n                (x, y) = (x.to(device), y.to(device))\\n                # perform a forward pass and calculate the training loss\\n                pred = model(x)\\n                loss = lossFn(pred, y)\\n                # zero out the gradients, perform the backpropagation step,\\n                # and update the weights\\n                opt.zero_grad()\\n                loss.backward()\\n                opt.step()\\n                # add the loss to the total training loss so far and\\n                # calculate the number of correct predictions\\n                totalTrainLoss += loss\\n                trainCorrect += (pred.argmax(1) == y).type(\\n                        torch.float).sum().item()\\n\\n        # switch off autograd for evaluation\\n        with torch.no_grad():\\n                # set the model in evaluation mode\\n                model.eval()\\n                # loop over the validation set\\n                for (x, y) in valDataLoader:\\n                        # send the input to the device\\n                        (x, y) = (x.to(device), y.to(device))\\n                        # make the predictions and calculate the validation loss\\n                        pred = model(x)\\n                        totalValLoss += lossFn(pred, y)\\n                        # calculate the number of correct predictions\\n                        valCorrect += (pred.argmax(1) == y).type(\\n                                torch.float).sum().item()\\n\\n        # calculate the average training and validation loss\\n        avgTrainLoss = totalTrainLoss / trainSteps\\n        avgValLoss = totalValLoss / valSteps\\n        # calculate the training and validation accuracy\\n        trainCorrect = trainCorrect / len(trainDataLoader.dataset)\\n        valCorrect = valCorrect / len(valDataLoader.dataset)\\n        # update our training history\\n        H[\\\"train_loss\\\"].append(avgTrainLoss.cpu().detach().numpy())\\n        H[\\\"train_acc\\\"].append(trainCorrect)\\n        H[\\\"val_loss\\\"].append(avgValLoss.cpu().detach().numpy())\\n        H[\\\"val_acc\\\"].append(valCorrect)\\n        # print the model training and validation information\\n        print(\\\"[INFO] EPOCH: {}/{}\\\".format(e + 1, EPOCHS))\\n        print(\\\"Train loss: {:.6f}, Train accuracy: {:.4f}\\\".format(\\n                avgTrainLoss, trainCorrect))\\n        print(\\\"Val loss: {:.6f}, Val accuracy: {:.4f}\\\\n\\\".format(\\n                avgValLoss, valCorrect))\",\n          \"import torch\\nfrom torch.utils.data import Dataset, DataLoader\\nimport torch.nn as nn\\nimport torch.optim as optim\\nimport numpy as np\\n\\n# Custom Dataset class\\nclass CustomDataset(Dataset):\\n    def __init__(self, data, labels):\\n        self.data = data\\n        self.labels = labels\\n\\n    def __len__(self):\\n        return len(self.data)\\n\\n    def __getitem__(self, idx):\\n        sample = self.data[idx]\\n        label = self.labels[idx]\\n        return sample, label\\n\\n# Prepare data\\ndata = np.random.randn(100, 3, 32, 32)  # 100 samples of 3x32x32 images\\nlabels = np.random.randint(0, 10, size=(100,))  # 100 labels in the range 0-9\\n\\n# Create Dataset\\ndataset = CustomDataset(data, labels)\\n\\n# Create DataLoader\\ndataloader = DataLoader(dataset, batch_size=4, shuffle=True, num_workers=2)\\n\",\n          \"pip install -q seaborn\\n\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\nimport pandas as pd\\nimport seaborn as sns\\n\\n# Make NumPy printouts easier to read.\\nnp.set_printoptions(precision=3, suppress=True)\\nimport tensorflow as tf\\n\\nfrom tensorflow import keras\\nfrom tensorflow.keras import layers\\n\\nprint(tf.__version__)\\n\\nurl = 'http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data'\\ncolumn_names = ['MPG', 'Cylinders', 'Displacement', 'Horsepower', 'Weight',\\n                'Acceleration', 'Model Year', 'Origin']\\n\\nraw_dataset = pd.read_csv(url, names=column_names,\\n                          na_values='?', comment='\\\\t',\\n                          sep=' ', skipinitialspace=True)\\n\\ndataset = raw_dataset.copy()\\ndataset.tail()\\n\\ndataset.isna().sum()\\n\\ndataset = dataset.dropna()\\n\\ndataset['Origin'] = dataset['Origin'].map({1: 'USA', 2: 'Europe', 3: 'Japan'})\\n\\ndataset = pd.get_dummies(dataset, columns=['Origin'], prefix='', prefix_sep='')\\ndataset.tail()\\n\\ntrain_dataset = dataset.sample(frac=0.8, random_state=0)\\ntest_dataset = dataset.drop(train_dataset.index)\\n\\ntrain_features = train_dataset.copy()\\ntest_features = test_dataset.copy()\\n\\ntrain_labels = train_features.pop('MPG')\\ntest_labels = test_features.pop('MPG')\\n\\nhorsepower = np.array(train_features['Horsepower'])\\n\\nhorsepower_normalizer = layers.Normalization(input_shape=[1,], axis=None)\\nhorsepower_normalizer.adapt(horsepower)\\n\\nhorsepower_model = tf.keras.Sequential([\\n    horsepower_normalizer,\\n    layers.Dense(units=1)\\n])\\n\\nhorsepower_model.summary()\\n\\nhorsepower_model.compile(\\n    optimizer=tf.keras.optimizers.Adam(learning_rate=0.1),\\n    loss='mean_absolute_error')\\n\\n\\nhistory = horsepower_model.fit(\\n    train_features['Horsepower'],\\n    train_labels,\\n    epochs=100,\\n    # Suppress logging.\\n    verbose=0,\\n    # Calculate validation results on 20% of the training data.\\n    validation_split = 0.2)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Unnamed: 7\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"\\n\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}","type":"dataframe","variable_name":"df"},"text/html":["\n","  <div id=\"df-ac08de91-ec0b-4918-ab6a-0c0bb1bc18cf\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Task</th>\n","      <th>Code</th>\n","      <th>API doc</th>\n","      <th>Few shot t1</th>\n","      <th>Few shot c1</th>\n","      <th>Few shot t2</th>\n","      <th>Few shot c2</th>\n","      <th>Unnamed: 7</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Write a Python function using TensorFlow to cr...</td>\n","      <td>import tensorflow as tf\\nfrom tensorflow impor...</td>\n","      <td>import tensorflow as tf\\nfrom tensorflow impor...</td>\n","      <td>Create a python script that uses tensorflow to...</td>\n","      <td>\\nimport numpy as np\\nimport tensorflow as tf\\...</td>\n","      <td>Write a Python function using TensorFlow to cr...</td>\n","      <td>pip install -q seaborn\\n\\nimport matplotlib.py...</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Write a Python script using PyTorch to train a...</td>\n","      <td>import torch\\nimport torch.nn as nn\\nimport to...</td>\n","      <td>import torch.nn as nn\\nimport torch.optim as o...</td>\n","      <td>Write a Python script using PyTorch to train a...</td>\n","      <td>import numpy as np\\nimport pandas as pd\\nimpor...</td>\n","      <td>Write a Python script using PyTorch to train a...</td>\n","      <td>from torch.nn import Module\\nfrom torch.nn imp...</td>\n","      <td>\\n</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Create a scikit-learn pipeline that generates ...</td>\n","      <td>from sklearn.datasets import make_classificati...</td>\n","      <td>from sklearn.datasets import make_classificati...</td>\n","      <td>Create a scikit learn pipeline that builds sta...</td>\n","      <td>from sklearn.pipeline import Pipeline\\nfrom sk...</td>\n","      <td>Create a scikit learn pipeline that builds sta...</td>\n","      <td>from sklearn.pipeline import Pipeline\\nfrom sk...</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Write a Python function that uses Hugging Face...</td>\n","      <td>from transformers import AutoTokenizer\\nfrom t...</td>\n","      <td>from transformers import AutoTokenizer\\nfrom t...</td>\n","      <td>In this example we learn to identify past vs p...</td>\n","      <td>\\nfrom typing import Callable, Dict, Sequence,...</td>\n","      <td>Write a python function that uses hugging face...</td>\n","      <td>\\nimport gradio as gr\\nfrom transformers impor...</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Build a TensorFlow model for image classificat...</td>\n","      <td>import tensorflow as tf\\nimport numpy as np\\n\\...</td>\n","      <td>import tensorflow as tf\\nimport numpy as np\\nt...</td>\n","      <td>Build an image recognition tensorflow model to...</td>\n","      <td>import matplotlib.pyplot as plt \\nimport numpy...</td>\n","      <td>Create an image classification model using ten...</td>\n","      <td>import tensorflow as tf   \\n\\n# Display the ve...</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>Write a PyTorch class that subclasses torch.ut...</td>\n","      <td>from torch.utils.data import Dataset\\nfrom tor...</td>\n","      <td>from torch.utils.data import Dataset\\nfrom tor...</td>\n","      <td>Write a PyTorch class that creates custom data...</td>\n","      <td>\\nimport os\\nimport pandas as pd\\nfrom torchvi...</td>\n","      <td>Write a PyTorch class that subclasses torch.ut...</td>\n","      <td>import torch\\nfrom torch.utils.data import Dat...</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>Write a Python script that loads the Iris data...</td>\n","      <td>\\nfrom sklearn import datasets\\nfrom sklearn.d...</td>\n","      <td>\\nfrom sklearn import datasets\\nfrom sklearn.d...</td>\n","      <td>Write a Python script that loads the UCI handw...</td>\n","      <td>import matplotlib.pyplot as plt\\nimport numpy ...</td>\n","      <td>Write a Python script that uses the dataset gi...</td>\n","      <td>import numpy as np\\nimport pandas as pd\\nfrom ...</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>Write a Java program using the Hadoop MapReduc...</td>\n","      <td>Mapper Code\\nimport java.io.IOException;      ...</td>\n","      <td>NaN</td>\n","      <td>Write a Java program using the Hadoop MapReduc...</td>\n","      <td>import java.io.IOException;\\nimport java.util....</td>\n","      <td>Write a Java program using the Hadoop MapReduc...</td>\n","      <td>package hadoop; \\n\\nimport java.util.*; \\n\\nim...</td>\n","      <td>\\n</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ac08de91-ec0b-4918-ab6a-0c0bb1bc18cf')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-ac08de91-ec0b-4918-ab6a-0c0bb1bc18cf button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-ac08de91-ec0b-4918-ab6a-0c0bb1bc18cf');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    <div id=\"df-6c38b8b4-bf3b-463c-998f-e02bbb08e991\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-6c38b8b4-bf3b-463c-998f-e02bbb08e991')\"\n","                title=\"Suggest charts\"\n","                style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","      <script>\n","        async function quickchart(key) {\n","          const quickchartButtonEl =\n","            document.querySelector('#' + key + ' button');\n","          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","          quickchartButtonEl.classList.add('colab-df-spinner');\n","          try {\n","            const charts = await google.colab.kernel.invokeFunction(\n","                'suggestCharts', [key], {});\n","          } catch (error) {\n","            console.error('Error during call to suggestCharts:', error);\n","          }\n","          quickchartButtonEl.classList.remove('colab-df-spinner');\n","          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","        }\n","        (() => {\n","          let quickchartButtonEl =\n","            document.querySelector('#df-6c38b8b4-bf3b-463c-998f-e02bbb08e991 button');\n","          quickchartButtonEl.style.display =\n","            google.colab.kernel.accessAllowed ? 'block' : 'none';\n","        })();\n","      </script>\n","    </div>\n","\n","  <div id=\"id_0a88b58e-d75e-499d-8f04-359cce7e88bf\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_0a88b58e-d75e-499d-8f04-359cce7e88bf button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('df');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"],"text/plain":["                                                Task  \\\n","0  Write a Python function using TensorFlow to cr...   \n","1  Write a Python script using PyTorch to train a...   \n","2  Create a scikit-learn pipeline that generates ...   \n","3  Write a Python function that uses Hugging Face...   \n","4  Build a TensorFlow model for image classificat...   \n","5  Write a PyTorch class that subclasses torch.ut...   \n","6  Write a Python script that loads the Iris data...   \n","7  Write a Java program using the Hadoop MapReduc...   \n","\n","                                                Code  \\\n","0  import tensorflow as tf\\nfrom tensorflow impor...   \n","1  import torch\\nimport torch.nn as nn\\nimport to...   \n","2  from sklearn.datasets import make_classificati...   \n","3  from transformers import AutoTokenizer\\nfrom t...   \n","4  import tensorflow as tf\\nimport numpy as np\\n\\...   \n","5  from torch.utils.data import Dataset\\nfrom tor...   \n","6  \\nfrom sklearn import datasets\\nfrom sklearn.d...   \n","7  Mapper Code\\nimport java.io.IOException;      ...   \n","\n","                                             API doc  \\\n","0  import tensorflow as tf\\nfrom tensorflow impor...   \n","1  import torch.nn as nn\\nimport torch.optim as o...   \n","2  from sklearn.datasets import make_classificati...   \n","3  from transformers import AutoTokenizer\\nfrom t...   \n","4  import tensorflow as tf\\nimport numpy as np\\nt...   \n","5  from torch.utils.data import Dataset\\nfrom tor...   \n","6  \\nfrom sklearn import datasets\\nfrom sklearn.d...   \n","7                                                NaN   \n","\n","                                         Few shot t1  \\\n","0  Create a python script that uses tensorflow to...   \n","1  Write a Python script using PyTorch to train a...   \n","2  Create a scikit learn pipeline that builds sta...   \n","3  In this example we learn to identify past vs p...   \n","4  Build an image recognition tensorflow model to...   \n","5  Write a PyTorch class that creates custom data...   \n","6  Write a Python script that loads the UCI handw...   \n","7  Write a Java program using the Hadoop MapReduc...   \n","\n","                                         Few shot c1  \\\n","0  \\nimport numpy as np\\nimport tensorflow as tf\\...   \n","1  import numpy as np\\nimport pandas as pd\\nimpor...   \n","2  from sklearn.pipeline import Pipeline\\nfrom sk...   \n","3  \\nfrom typing import Callable, Dict, Sequence,...   \n","4  import matplotlib.pyplot as plt \\nimport numpy...   \n","5  \\nimport os\\nimport pandas as pd\\nfrom torchvi...   \n","6  import matplotlib.pyplot as plt\\nimport numpy ...   \n","7  import java.io.IOException;\\nimport java.util....   \n","\n","                                         Few shot t2  \\\n","0  Write a Python function using TensorFlow to cr...   \n","1  Write a Python script using PyTorch to train a...   \n","2  Create a scikit learn pipeline that builds sta...   \n","3  Write a python function that uses hugging face...   \n","4  Create an image classification model using ten...   \n","5  Write a PyTorch class that subclasses torch.ut...   \n","6  Write a Python script that uses the dataset gi...   \n","7  Write a Java program using the Hadoop MapReduc...   \n","\n","                                         Few shot c2 Unnamed: 7  \n","0  pip install -q seaborn\\n\\nimport matplotlib.py...        NaN  \n","1  from torch.nn import Module\\nfrom torch.nn imp...         \\n  \n","2  from sklearn.pipeline import Pipeline\\nfrom sk...        NaN  \n","3  \\nimport gradio as gr\\nfrom transformers impor...        NaN  \n","4  import tensorflow as tf   \\n\\n# Display the ve...        NaN  \n","5  import torch\\nfrom torch.utils.data import Dat...        NaN  \n","6  import numpy as np\\nimport pandas as pd\\nfrom ...        NaN  \n","7  package hadoop; \\n\\nimport java.util.*; \\n\\nim...         \\n  "]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["df= pd.read_csv(\"/content/drive/MyDrive/Stanford NLP/dataset.csv\")\n","df"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1748919986333,"user":{"displayName":"Neeharika Gupta","userId":"13434636909198909500"},"user_tz":420},"id":"OATUQ9YAx-FA","outputId":"2e82c454-f240-4b64-cacb-340b237a0fec"},"outputs":[{"data":{"text/plain":["['Write a Python function using TensorFlow to create and compile a simple linear regression model with one dense layer. Train it on the classic California housing dataset.\\n',\n"," 'Write a Python script using PyTorch to train a convolutional neural network (CNN) on the Cifar 10 dataset. Include data loading, model definition, training loop, and evaluation',\n"," 'Create a scikit-learn pipeline that generates a random n-class classification problem and creates a pipeline to compute the mean and standard deviation on a training set so as to be able to later re-apply the same transformation on the testing set. Train the model Fit the dataset using the same pipeline and compute score on the test set. ',\n"," 'Write a Python function that uses Hugging Face to train a model(distilbert/distilbert-base-uncaseddistilbert/distilbert-base-uncased) using the IMDb Reviews dataset. Its  binary classification task. Involve any necessary preprocessing steps if required.',\n"," 'Build a TensorFlow model for image classification using the Fashion MNIST dataset and integrate early stopping and model checkpoint callbacks during training.',\n"," 'Write a PyTorch class that subclasses torch.utils.data.Dataset to load image files and labels from a folder. Use this class with a DataLoader for batching. Use FashionMNIST dataset.',\n"," 'Write a Python script that loads the Iris dataset, reduces its dimensionality using PCA, and fits a k-nearest neighbors classifier using scikit-learn.',\n"," 'Write a Java program using the Hadoop MapReduce API to count the frequency of each word in a large text file, such as a sample from Project Gutenberg.']"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["tasks=df['Task'].to_list()\n","tasks"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":300},"executionInfo":{"elapsed":7544,"status":"ok","timestamp":1748919957218,"user":{"displayName":"Neeharika Gupta","userId":"13434636909198909500"},"user_tz":420},"id":"oj4CkM_8vkqv","outputId":"0f828e15-9d6f-4f84-8cec-9200a6e487f8"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"summary":"{\n  \"name\": \"df_neural\",\n  \"rows\": 8,\n  \"fields\": [\n    {\n      \"column\": \"neural_gemini\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"```python\\nimport torch\\nimport torch.nn as nn\\nimport torch.optim as optim\\nimport torchvision\\nimport torchvision.transforms as transforms\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\n# --- Configuration ---\\n# Using basic Python types (int, float, bool)\\nBATCH_SIZE = 128\\nLEARNING_RATE = 0.001\\nNUM_EPOCHS = 10\\nDEVICE = torch.device(\\\"cuda\\\" if torch.cuda.is_available() else \\\"cpu\\\")\\nprint(f\\\"Using device: {DEVICE}\\\")\\n\\n# --- Data Loading ---\\n# Based on DataLoader, Dataset, ToTensor API documentation concepts\\n# We use torchvision.datasets.CIFAR10 which is a subclass of torch.utils.data.Dataset\\n# ToTensor is used for conversion, as described in its documentation.\\n# DataLoader is used for batching, shuffling, and multi-processing.\\n\\n# Define transformations\\n# Using torchvision.transforms\\ntransform = transforms.Compose(\\n    [transforms.ToTensor(), # Uses the ToTensor transform (converts PIL/ndarray to Tensor, scales [0, 255] to [0, 1] for uint8)\\n     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]) # Standard normalization for CIFAR10\\n\\n# Load CIFAR10 dataset\\n# Using torchvision.datasets.CIFAR10 (implied Dataset usage)\\n# DataLoader constructor usage with dataset, batch_size, shuffle, num_workers\\ntrainset = torchvision.datasets.CIFAR10(root='./data', train=True,\\n                                        download=True, transform=transform)\\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE,\\n                                          shuffle=True, num_workers=2)\\n\\ntestset = torchvision.datasets.CIFAR10(root='./data', train=False,\\n                                       download=True, transform=transform)\\ntestloader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE,\\n                                         shuffle=False, num_workers=2)\\n\\nclasses = ('plane', 'car', 'bird', 'cat',\\n           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\\n\\n# --- Model Definition ---\\n# Based on the concept of torch.nn.Module and standard CNN layers (though specific layers not documented)\\n# We define a simple CNN by subclassing torch.nn.Module\\n# __init__ method defines the layers, forward method defines the data flow.\\n# Uses Conv2d, MaxPool2d, ReLU, Flatten, Linear layers from torch.nn (standard PyTorch pattern)\\n\\nclass SimpleCNN(nn.Module):\\n    def __init__(self):\\n        # Calls the parent constructor\\n        super(SimpleCNN, self).__init__()\\n        # Define convolutional layers\\n        # Using torch.nn.Conv2d\\n        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1) # Input channels=3 (RGB), output channels=32\\n        # Using torch.nn.ReLU (activation function)\\n        self.relu = nn.ReLU()\\n        # Using torch.nn.MaxPool2d\\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2) # Reduces spatial dimensions by half\\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1) # Input channels=32, output channels=64\\n        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1) # Input channels=64, output channels=128\\n\\n        # Define fully connected layers (Dense in Keras, Linear in PyTorch)\\n        # Calculate input size for the first linear layer based on image size (32x32) and pooling\\n        # (32 / 2 / 2 / 2)^2 * 128 channels = (4^2) * 128 = 16 * 128 = 2048\\n        # Using torch.nn.Linear\\n        self.fc1 = nn.Linear(128 * 4 * 4, 512) # Input features based on Conv/Pool output, output features=512\\n        self.fc2 = nn.Linear(512, 10) # Input features=512, output features=10 (for 10 classes)\\n\\n    def forward(self, x):\\n        # Implement the forward pass\\n        # Using torch.nn.functional for functional API like pooling and activation\\n        x = self.pool(self.relu(self.conv1(x)))\\n        x = self.pool(self.relu(self.conv2(x)))\\n        x = self.pool(self.relu(self.conv3(x)))\\n        # Flatten the tensor before passing to fully connected layers\\n        # Using torch.flatten (or x.view/reshape)\\n        x = torch.flatten(x, 1) # flatten all dimensions except batch\\n        x = self.relu(self.fc1(x))\\n        x = self.fc2(x) # Output layer, no activation here (CrossEntropyLoss includes Softmax)\\n        return x\\n\\n# Instantiate the model and move it to the chosen device\\nmodel = SimpleCNN()\\n# Using Tensor.to(device) method\\nmodel.to(DEVICE)\\n\\n# --- Loss Function and Optimizer ---\\n# Using standard PyTorch loss function and optimizer (specific classes not documented)\\n# Using torch.nn.CrossEntropyLoss (loss function)\\ncriterion = nn.CrossEntropyLoss()\\n# Using torch.optim package concept (Optimizer instantiation and methods)\\n# Using Adam optimizer from torch.optim and its constructor with model parameters and learning rate\\noptimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\\n\\n# --- Training Loop ---\\n# Iterates over DataLoader, performs forward/backward passes, updates weights.\\n\\nprint(\\\"Starting training...\\\")\\nfor epoch in range(NUM_EPOCHS):  # Loop over the dataset multiple times\\n    # Set model to training mode\\n    # Using torch.nn.Module.train() method\\n    model.train()\\n    running_loss = 0.0\\n    # Iterate over the DataLoader (using its iterable behavior)\\n    for i, data in enumerate(trainloader, 0):\\n        # Get the inputs; data is a list of [inputs, labels]\\n        inputs, labels = data\\n        # Move data to the appropriate device\\n        # Using Tensor.to(device) method\\n        inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\\n\\n        # Zero the parameter gradients\\n        # Using optimizer.zero_grad() method (part of torch.optim interface)\\n        optimizer.zero_grad()\\n\\n        # Forward pass\\n        # Calling the model instance (which calls the forward method)\\n        outputs = model(inputs)\\n        # Calculate loss\\n        # Using the criterion (nn.CrossEntropyLoss instance)\\n        loss = criterion(outputs, labels)\\n\\n        # Backward pass\\n        # Using Tensor.backward() method\\n        loss.backward()\\n        # Optimize\\n        # Using optimizer.step() method (part of torch.optim interface)\\n        optimizer.step()\\n\\n        # Print statistics\\n        # Using Tensor.item() method to get scalar value\\n        running_loss += loss.item()\\n        if i % 100 == 99:    # print every 100 mini-batches\\n            print(f'Epoch {epoch + 1}, Batch {i + 1}: loss = {running_loss / 100:.3f}')\\n            running_loss = 0.0\\n\\nprint('Finished Training')\\n\\n# --- Evaluation ---\\n# Iterates over test DataLoader, calculates accuracy.\\n\\nprint(\\\"Starting evaluation...\\\")\\n# Set model to evaluation mode\\n# Using torch.nn.Module.eval() method\\nmodel.eval()\\ncorrect = 0\\ntotal = 0\\n# Disable gradient calculation during evaluation\\n# Using torch.no_grad() context manager\\nwith torch.no_grad():\\n    # Iterate over the test DataLoader\\n    for data in testloader:\\n        images, labels = data\\n        # Move data to device\\n        # Using Tensor.to(device) method\\n        images, labels = images.to(DEVICE), labels.to(DEVICE)\\n        # Forward pass (inference)\\n        outputs = model(images)\\n        # Get predictions\\n        # Using Tensor.argmax() method to find the index of the max value (predicted class)\\n        # Using Tensor.to(device).cpu() if necessary to move to CPU for numpy conversion or other operations\\n        _, predicted = torch.max(outputs.data, 1)\\n        # Calculate total samples\\n        # Using Tensor.size() method\\n        total += labels.size(0)\\n        # Calculate correct predictions\\n        # Using Tensor comparison (==) and sum() method\\n        correct += (predicted == labels).sum().item() # Using item() to get scalar\\n\\n# Calculate and print accuracy\\naccuracy = 100 * correct / total\\nprint(f'Accuracy of the network on the 10000 test images: {accuracy:.2f} %')\\n\\n# --- Optional: Visualize some predictions ---\\n# Using matplotlib.pyplot (plt alias) for plotting\\ndef imshow(img):\\n    # Unnormalize image for display\\n    img = img / 2 + 0.5     # unnormalize\\n    # Using numpy.transpose and numpy.asarray\\n    npimg = img.numpy()\\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\\n    plt.show()\\n\\n# Get some random test images\\ndataiter = iter(testloader)\\nimages, labels = next(dataiter)\\n\\n# Print images\\n# Using matplotlib.pyplot (plt) and torchvision.utils.make_grid\\n# imshow(torchvision.utils.make_grid(images))\\n# print('GroundTruth: ', ' '.join(f'{classes[labels[j]]:5s}' for j in range(BATCH_SIZE))) # Assuming BATCH_SIZE <= len(images)\\n\\n# Make predictions\\nimages = images.to(DEVICE) # Move to device for inference\\noutputs = model(images)\\n# Using Tensor.max()\\n_, predicted = torch.max(outputs, 1)\\n\\n# Print predictions\\n# print('Predicted:   ', ' '.join(f'{classes[predicted[j]]:5s}' for j in range(BATCH_SIZE)))\\n\\n```\",\n          \"```python\\nimport torch\\nimport torch.utils.data as data\\nimport torchvision.datasets as datasets\\nimport torchvision.transforms as transforms\\nimport os\\nfrom PIL import Image\\nfrom pathlib import Path\\nimport numpy as np\\n\\n# --- Step 1: Simulate the folder structure using the actual FashionMNIST data ---\\n# This step is necessary because torchvision.datasets.FashionMNIST loads data\\n# from internal structures (like .gz files) rather than a user-provided folder\\n# structure where labels are determined by directory names.\\n# We will download the data, then save it into the desired folder format.\\n\\ndef save_fashion_mnist_to_folder(root_dir=\\\"fashion_mnist_folder\\\"):\\n    \\\"\\\"\\\"\\n    Downloads FashionMNIST and saves images into a folder structure:\\n    root_dir/label/image_idx.png\\n    \\\"\\\"\\\"\\n    # Define a temporary directory to store the original downloaded data\\n    temp_download_dir = \\\"./fashion_mnist_temp\\\"\\n\\n    # Download the standard FashionMNIST dataset\\n    print(f\\\"Downloading FashionMNIST to {temp_download_dir}...\\\")\\n    train_dataset = datasets.FashionMNIST(root=temp_download_dir, train=True, download=True)\\n    test_dataset = datasets.FashionMNIST(root=temp_download_dir, train=False, download=True)\\n    print(\\\"Download complete.\\\")\\n\\n    # Combine train and test data for saving\\n    all_data = train_dataset.data.numpy()\\n    all_targets = train_dataset.targets.numpy()\\n    all_data = np.concatenate((all_data, test_dataset.data.numpy()), axis=0)\\n    all_targets = np.concatenate((all_targets, test_dataset.targets.numpy()), axis=0)\\n\\n    print(f\\\"Saving FashionMNIST images to folder structure in {root_dir}...\\\")\\n    # Create the root directory if it doesn't exist\\n    Path(root_dir).mkdir(parents=True, exist_ok=True)\\n\\n    # Create subdirectories for each class label\\n    for i in range(10):\\n        class_dir = Path(root_dir) / str(i)\\n        class_dir.mkdir(parents=True, exist_ok=True)\\n\\n    # Save each image\\n    for i in range(len(all_data)):\\n        image_np = all_data[i]\\n        label = all_targets[i]\\n        class_dir = Path(root_dir) / str(label)\\n        image_path = class_dir / f\\\"image_{i:05d}.png\\\" # Use 5 digits for indexing\\n\\n        # Convert numpy array to PIL Image (FashionMNIST is grayscale)\\n        img = Image.fromarray(image_np, mode='L')\\n        img.save(image_path)\\n\\n    print(f\\\"Finished saving {len(all_data)} images.\\\")\\n    # Note: In a real scenario, you might want to remove the temporary download dir\\n    # import shutil\\n    # shutil.rmtree(temp_download_dir)\\n\\n\\n# --- Step 2: Create the custom Dataset class subclasing torch.utils.data.Dataset ---\\n\\n# Based on the Dataset API:\\n# - Subclasses torch.utils.data.Dataset\\n# - Must implement __getitem__(self, index) -> sample\\n# - Should implement __len__(self) -> int (required for default DataLoader options)\\n\\nclass FashionMNISTFolderDataset(data.Dataset):\\n    \\\"\\\"\\\"\\n    Custom Dataset to load FashionMNIST-like images and labels from a folder structure.\\n    Assumes structure: root_dir/label/image_name.png\\n    \\\"\\\"\\\"\\n    def __init__(self, root_dir, transform=None):\\n        \\\"\\\"\\\"\\n        Initializes the dataset by scanning the folder structure.\\n\\n        Args:\\n            root_dir (string): Directory with all the images structured in subfolders.\\n            transform (callable, optional): Optional transform to be applied on a sample.\\n        \\\"\\\"\\\"\\n        self.root_dir = Path(root_dir)\\n        self.transform = transform\\n        self.image_paths = []\\n        self.labels = []\\n        self.classes = [] # Store class names/labels found\\n\\n        # Scan the directory structure\\n        print(f\\\"Scanning images in {self.root_dir}...\\\")\\n        for class_dir in sorted(self.root_dir.iterdir()): # Sort for consistent order\\n            if class_dir.is_dir():\\n                class_name = class_dir.name # Folder name is the class label (as string)\\n                # Attempt to convert class_name to integer label\\n                try:\\n                    label = int(class_name)\\n                    self.classes.append(label)\\n                except ValueError:\\n                    print(f\\\"Warning: Skipping directory '{class_dir}' as it's not an integer label.\\\")\\n                    continue\\n\\n                # Sort image files for consistent order\\n                for img_path in sorted(class_dir.glob(\\\"*.png\\\")): # Assuming .png from simulation\\n                    self.image_paths.append(img_path)\\n                    self.labels.append(label) # Store integer label\\n\\n        if not self.image_paths:\\n            raise RuntimeError(f\\\"Found 0 images in subfolders of {self.root_dir}. \\\"\\n                               \\\"Please ensure the directory structure is correct and contains .png files.\\\")\\n\\n        print(f\\\"Found {len(self.image_paths)} images belonging to {len(self.classes)} classes.\\\")\\n\\n    def __len__(self):\\n        \\\"\\\"\\\"\\n        Returns the total number of samples (images).\\n        Matches the Dataset API requirement for __len__.\\n        \\\"\\\"\\\"\\n        return len(self.image_paths)\\n\\n    def __getitem__(self, idx):\\n        \\\"\\\"\\\"\\n        Loads and returns one sample (image and label) from the dataset at the given index.\\n        Matches the Dataset API requirement for __getitem__.\\n\\n        Args:\\n            idx (int): Index of the sample to load.\\n\\n        Returns:\\n            tuple: (image, label) where image is a Tensor and label is an integer.\\n        \\\"\\\"\\\"\\n        if torch.is_tensor(idx):\\n            idx = idx.tolist()\\n\\n        img_path = self.image_paths[idx]\\n        label = self.labels[idx]\\n\\n        # Load image using Pillow (PIL)\\n        image = Image.open(img_path).convert('L') # FashionMNIST is grayscale ('L')\\n\\n        # Apply transformations\\n        if self.transform:\\n            image = self.transform(image)\\n\\n        return image, label\\n\\n# --- Step 3: Define Transforms ---\\n# We need at least ToTensor. FashionMNIST images are 28x28 grayscale.\\n# ToTensor will convert PIL Image or ndarray (H x W x C) to Tensor (C x H x W)\\n# and scale uint8 [0, 255] to float [0.0, 1.0].\\n# Based on ToTensor API: converts PIL/ndarray to torch.Tensor, scales [0, 255] to [0.0, 1.0].\\n\\n# For grayscale, PIL 'L' mode (8-bit pixels, grayscale) will be scaled.\\n# ToTensor description: \\\"Scales values accordingly for specific input types.\\\"\\n# \\\"PIL Image.Image with modes ['L', ..., '1']\\\" -> scales to [0.0, 1.0]\\n\\nimage_transforms = transforms.Compose([\\n    transforms.ToTensor(), # Converts PIL Image (H, W) to Tensor (1, H, W) and scales [0, 255] to [0.0, 1.0]\\n    # Add other transforms here if needed, e.g.,\\n    # transforms.Normalize((0.5,), (0.5,)) # Normalize grayscale images\\n])\\n\\n# --- Step 4: Use the custom Dataset with DataLoader for batching ---\\n\\n# Based on DataLoader API:\\n# - Combines a dataset and a sampler, provides an iterable over the dataset.\\n# - Constructor takes 'dataset', 'batch_size', 'shuffle', 'num_workers', etc.\\n\\nif __name__ == \\\"__main__\\\":\\n    # Define the root directory for our simulated dataset\\n    simulated_root = \\\"./custom_fashion_mnist_data\\\"\\n\\n    # Simulate the data if it doesn't exist\\n    # Check if any class directory exists, assuming data is saved if any subdir exists\\n    if not any(Path(simulated_root).iterdir()):\\n         save_fashion_mnist_to_folder(simulated_root)\\n    else:\\n         print(f\\\"Data already found in {simulated_root}. Skipping simulation.\\\")\\n\\n\\n    # Instantiate the custom dataset\\n    print(\\\"\\\\nCreating custom dataset...\\\")\\n    try:\\n        custom_dataset = FashionMNISTFolderDataset(root_dir=simulated_root, transform=image_transforms)\\n        print(f\\\"Dataset created with {len(custom_dataset)} samples.\\\")\\n    except RuntimeError as e:\\n        print(f\\\"Error creating dataset: {e}\\\")\\n        print(\\\"Please ensure the simulation step ran correctly or the directory structure is valid.\\\")\\n        exit()\\n\\n\\n    # Define DataLoader parameters\\n    batch_size = 64\\n    shuffle_data = True # Set to True to shuffle data at each epoch\\n    num_workers = 2     # Number of subprocesses for data loading (0 means main process)\\n                        # Based on DataLoader API: num_workers\\n\\n    # Instantiate the DataLoader\\n    print(f\\\"\\\\nCreating DataLoader with batch_size={batch_size}, shuffle={shuffle_data}, num_workers={num_workers}...\\\")\\n    dataloader = data.DataLoader(\\n        dataset=custom_dataset,\\n        batch_size=batch_size,\\n        shuffle=shuffle_data,\\n        num_workers=num_workers,\\n        # pin_memory=True # Often useful for GPU training, based on DataLoader API\\n    )\\n    print(\\\"DataLoader created.\\\")\\n\\n    # --- Step 5: Iterate through the DataLoader to get batches ---\\n\\n    print(\\\"\\\\nIterating through DataLoader for one epoch:\\\")\\n    # The DataLoader is an iterable, as described in the DataLoader API\\n    for i, (images, labels) in enumerate(dataloader):\\n        # 'images' will be a Tensor batch of shape (batch_size, C, H, W)\\n        # 'labels' will be a Tensor batch of shape (batch_size,)\\n        # The batch structure is handled by DataLoader's default collate_fn\\n\\n        print(f\\\"Batch {i+1}:\\\")\\n        print(f\\\"  Images batch shape: {images.shape}\\\")\\n        print(f\\\"  Labels batch shape: {labels.shape}\\\")\\n        print(f\\\"  Image dtype: {images.dtype}\\\")\\n        print(f\\\"  Label dtype: {labels.dtype}\\\")\\n\\n        # Example: Print first few labels in the batch\\n        print(f\\\"  First 5 labels in batch: {labels[:5].tolist()}\\\")\\n\\n        # Stop after a few batches for demonstration\\n        if i >= 2: # Process first 3 batches\\n            break\\n\\n    print(\\\"\\\\nFinished iterating through a few batches.\\\")\\n    print(f\\\"Total batches per epoch: {len(dataloader)}\\\")\\n\\n    # The custom dataset and dataloader can now be used in a standard PyTorch training loop\\n    # e.g., iterate dataloader, pass images to model, compute loss, backprop, etc.\\n\\n```\\n\\n**Explanation:**\\n\\n1.  **Simulate Data (`save_fashion_mnist_to_folder`)**: This is an auxiliary function created specifically to fulfill the \\\"load from a folder\\\" requirement using the FashionMNIST data source. It downloads the standard `torchvision.datasets.FashionMNIST`, then manually saves each image into a directory structure (`root_dir/label_int/image_idx.png`) that our custom dataset can understand. This bridging step is necessary because the standard `FashionMNIST` dataset class doesn't operate by scanning a user-defined folder of loose image files.\\n2.  **`FashionMNISTFolderDataset` Class**:\\n    *   It inherits from `torch.utils.data.Dataset`, as required by the task and indicated by the `Dataset` API documentation.\\n    *   **`__init__`**: This method scans the provided `root_dir`. It iterates through subdirectories (which represent labels), lists all image files within them, and stores their paths and corresponding integer labels in lists (`self.image_paths` and `self.labels`). This pre-processing step is crucial for map-style datasets to know the size of the dataset and quickly retrieve any item by index.\\n    *   **`__len__`**: This method returns the total number of image paths collected. The `Dataset` API specifies that `__len__` is optional but expected by `DataLoader` for default sampler behavior and `len(dataset)` calls.\\n    *   **`__getitem__`**: This is the core method required by the `Dataset` API. Given an `index`, it retrieves the stored image path and label, loads the image using Pillow (`PIL.Image.open`), applies the specified `transform` (like `ToTensor`), and returns the image tensor and the integer label.\\n3.  **Transforms (`image_transforms`)**:\\n    *   We define a `transforms.Compose` which is a common way to chain multiple transformations.\\n    *   `transforms.ToTensor()` is included, as described in the `ToTensor` API. It correctly handles the conversion from a grayscale PIL Image to a PyTorch Tensor and scales the pixel values from the original [0, 255] range to [0.0, 1.0].\\n4.  **DataLoader Usage**:\\n    *   An instance of our custom `FashionMNISTFolderDataset` is created, pointing to the root directory of the simulated data and providing the desired transformations.\\n    *   An instance of `torch.utils.data.DataLoader` is created, as per the `DataLoader` API. We pass our `custom_dataset`, set the `batch_size`, enable `shuffle` to randomize the data order each epoch, and set `num_workers` for potentially faster data loading (though 0 is fine for basic testing).\\n    *   The `DataLoader` object is an iterable. We demonstrate this by looping through it. In each iteration, the `DataLoader` automatically calls our dataset's `__getitem__` for a batch of indices (handled by an internal Sampler), combines the individual samples into tensors (handled by an internal `collate_fn`), and yields the batch (`images`, `labels`).\\n\\nThis pattern effectively demonstrates how to use `torch.utils.data.Dataset` and `torch.utils.data.DataLoader` to load data from a custom folder structure, using the FashionMNIST dataset as the source of the data to be loaded.\",\n          \"```python\\nimport tensorflow as tf\\nfrom sklearn.datasets import fetch_california_housing\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.preprocessing import StandardScaler\\nimport numpy as np\\n\\ndef train_linear_regression_model():\\n    \\\"\\\"\\\"\\n    Creates, compiles, and trains a simple linear regression model\\n    on the California housing dataset using TensorFlow/Keras.\\n    \\\"\\\"\\\"\\n    print(\\\"Loading and preparing data...\\\")\\n\\n    # 1. Load the California housing dataset\\n    # Using sklearn.datasets.fetch_california_housing\\n    # API Reference: {'fetch_california_housing': '...'}\\n    X, y = fetch_california_housing(return_X_y=True)\\n    # X is features, y is target (median house value)\\n\\n    # Data has 8 features (dimensionality: 8)\\n    n_features = X.shape[1]\\n    # Target is a single value (real 0.15 - 5, in units of $100,000)\\n    n_targets = 1\\n\\n    print(f\\\"Dataset loaded. Features shape: {X.shape}, Target shape: {y.shape}\\\")\\n\\n    # 2. Split data into training and testing sets\\n    # Using sklearn.model_selection.train_test_split\\n    # API Reference: {'train_test_split': '...'}\\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\\n\\n    print(f\\\"Data split into training ({X_train.shape[0]} samples) and testing ({X_test.shape[0]} samples).\\\")\\n\\n    # 3. Scale features and target\\n    # Using sklearn.preprocessing.StandardScaler\\n    # API Reference: {'StandardScaler': '...'}\\n    # It's good practice to scale features for many models, including neural networks.\\n    # Scaling the target can also improve training stability in regression.\\n\\n    scaler_X = StandardScaler()\\n    X_train_scaled = scaler_X.fit_transform(X_train) # Fit on training data, then transform\\n    X_test_scaled = scaler_X.transform(X_test)     # Use scaler fitted on training data to transform test data\\n\\n    # Reshape y to be 2D for the scaler (StandardScaler expects shape (n_samples, n_features))\\n    scaler_y = StandardScaler()\\n    y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1))\\n    y_test_scaled = scaler_y.transform(y_test.reshape(-1, 1))\\n\\n    print(\\\"Features and target scaled.\\\")\\n\\n    # 4. Define the simple linear regression model using Keras Sequential API\\n    # A linear regression model is essentially a single Dense layer with no activation.\\n    # Using tensorflow.keras.Sequential and tensorflow.keras.layers.Dense\\n    # API References: {'tensorflow': '...', 'tensorflow.keras': '...', 'Sequential': '...', 'Dense': '...'}\\n\\n    model = tf.keras.Sequential([\\n        # The Dense layer will take care of the linear transformation (W*x + b)\\n        # units=1 because we are predicting a single value (house price)\\n        # activation='linear' is the default and is suitable for linear regression output\\n        # input_shape is required for the first layer if not using the functional API or model.build\\n        tf.keras.layers.Dense(units=n_targets, input_shape=(n_features,))\\n    ])\\n\\n    print(\\\"Model defined: Sequential with one Dense layer.\\\")\\n\\n    # 5. Compile the model\\n    # Using keras.Model.compile\\n    # API Reference: {'Sequential': '...'} (as Sequential inherits from Model)\\n    # optimizer: 'adam' is a good general-purpose optimizer.\\n    # loss: 'mse' (Mean Squared Error) is the standard loss function for regression.\\n    model.compile(optimizer='adam', loss='mse')\\n\\n    print(\\\"Model compiled with Adam optimizer and Mean Squared Error loss.\\\")\\n\\n    # Display model summary (optional)\\n    model.summary()\\n\\n    # 6. Train the model\\n    # Using keras.Model.fit\\n    # API Reference: {'Sequential': '...'} (as Sequential inherits from Model)\\n    # x, y: Scaled training data\\n    # epochs: Number of passes over the training data\\n    # validation_data: Optional data to evaluate the model on after each epoch (monitors overfitting)\\n    print(\\\"Training the model...\\\")\\n    history = model.fit(X_train_scaled, y_train_scaled, epochs=50, batch_size=32, validation_data=(X_test_scaled, y_test_scaled))\\n\\n    print(\\\"Training finished.\\\")\\n\\n    # Optional: Evaluate the model on the test set\\n    # Using keras.Model.evaluate\\n    # API Reference: Mentioned in Sequential as a common method but not detailed\\n    loss = model.evaluate(X_test_scaled, y_test_scaled, verbose=0)\\n    print(f\\\"Test Loss (MSE): {loss}\\\")\\n\\n    # The trained model can now be used for predictions\\n    # To predict, use model.predict(X_new_scaled) and inverse_transform with scaler_y\\n    # Example prediction on the first test sample:\\n    # sample_prediction_scaled = model.predict(X_test_scaled[0].reshape(1, -1))\\n    # sample_prediction_original_scale = scaler_y.inverse_transform(sample_prediction_scaled)\\n    # print(f\\\"Example prediction (scaled): {sample_prediction_scaled[0][0]:.4f}\\\")\\n    # print(f\\\"Example prediction (original scale): ${sample_prediction_original_scale[0][0]*100000:.2f}\\\")\\n    # print(f\\\"Actual value (original scale): ${y_test[0]*100000:.2f}\\\")\\n\\n\\n    return model, scaler_X, scaler_y # Return the trained model and scalers\\n\\n# Example usage:\\nif __name__ == \\\"__main__\\\":\\n    trained_model, feature_scaler, target_scaler = train_linear_regression_model()\\n\\n    # Demonstrate a prediction\\n    print(\\\"\\\\nDemonstrating prediction...\\\")\\n    # Take the first sample from the *original* test data\\n    sample_X_original = X_test[0]\\n    sample_y_original = y_test[0]\\n\\n    # Scale the sample using the fitted feature scaler\\n    sample_X_scaled = feature_scaler.transform(sample_X_original.reshape(1, -1))\\n\\n    # Make a prediction using the trained model\\n    sample_prediction_scaled = trained_model.predict(sample_X_scaled)\\n\\n    # Inverse transform the prediction using the fitted target scaler to get original scale\\n    sample_prediction_original_scale = target_scaler.inverse_transform(sample_prediction_scaled)\\n\\n    print(f\\\"Original Sample Features: {sample_X_original}\\\")\\n    print(f\\\"Actual Median House Value: ${sample_y_original * 100000:.2f}\\\")\\n    print(f\\\"Predicted Median House Value: ${sample_prediction_original_scale[0][0] * 100000:.2f}\\\")\\n\\n```\\n\\n**API Usage Pattern Explanation:**\\n\\n1.  **Data Loading (`sklearn.datasets.fetch_california_housing`):**\\n    *   The first step in most ML tasks is data acquisition.\\n    *   `fetch_california_housing` is used directly to load the standard dataset.\\n    *   The `return_X_y=True` parameter is a common pattern in scikit-learn datasets to get features (`X`) and target (`y`) as separate NumPy arrays, which is convenient for subsequent processing and feeding into models like Keras.\\n    *   This function is called without arguments or with simple boolean flags based on its API description.\\n\\n2.  **Data Splitting (`sklearn.model_selection.train_test_split`):**\\n    *   It's crucial to split data to evaluate the model's performance on unseen examples.\\n    *   `train_test_split` takes the feature and target arrays (`X`, `y`) and splits them into corresponding training and testing subsets (`X_train`, `X_test`, `y_train`, `y_test`).\\n    *   Key parameters used are `test_size` (proportion of data for testing) and `random_state` (for reproducibility).\\n\\n3.  **Data Preprocessing/Scaling (`sklearn.preprocessing.StandardScaler`):**\\n    *   Neural networks, especially simple ones, often benefit from input features being on a similar scale. The target variable scale can also affect training stability.\\n    *   `StandardScaler` is instantiated for both features and the target.\\n    *   The `fit` method is called *only* on the training data (`X_train`, `y_train.reshape(-1, 1)`) to learn the mean and standard deviation from the training distribution.\\n    *   The `transform` method is then called on *both* the training and testing data (`X_train`, `X_test`, `y_train`, `y_test`) using the *same* fitted scaler. This ensures test data is scaled consistently with the training data, preventing data leakage from the test set into the training process. The target `y` is reshaped because `StandardScaler` expects a 2D array.\\n\\n4.  **Model Definition (`tf.keras.Sequential`, `tf.keras.layers.Dense`):**\\n    *   `tf.keras.Sequential` is used for building models layer-by-layer in a linear stack, which is perfect for this simple model.\\n    *   An instance of `tf.keras.Sequential` is created.\\n    *   The single layer, `tf.keras.layers.Dense`, is added to the sequential model using the `.add()` method.\\n    *   `Dense` is configured with `units=1` because linear regression predicts a single output value (the house price).\\n    *   `input_shape=(n_features,)` is specified in the first layer. This tells Keras the shape of a single input sample, allowing the model to be built immediately (rather than using the Delayed-Build Pattern which builds the model on the first `fit` call). `n_features` is derived from the data shape. The default `activation='linear'` is suitable for linear regression.\\n\\n5.  **Model Compilation (`model.compile`):**\\n    *   Before training, the model must be configured. This involves specifying the optimizer and the loss function.\\n    *   The `.compile()` method of the `Sequential` model (inherited from `keras.Model`) is called.\\n    *   `optimizer='adam'` is chosen as a robust general-purpose optimizer. Keras accepts string names for common optimizers.\\n    *   `loss='mse'` (Mean Squared Error) is the standard loss function for regression tasks. Keras accepts string names for common losses.\\n\\n6.  **Model Training (`model.fit`):**\\n    *   The model is trained using the `.fit()` method.\\n    *   It takes the scaled training features (`X_train_scaled`) and scaled training targets (`y_train_scaled`).\\n    *   `epochs` determines how many times the training process iterates over the entire dataset.\\n    *   `batch_size` specifies the number of samples processed before updating the model weights.\\n    *   `validation_data=(X_test_scaled, y_test_scaled)` is provided to monitor the model's performance on the test set after each epoch. This is helpful for detecting overfitting.\\n\\nThis pattern follows the standard machine learning workflow: data loading -> preprocessing -> model definition -> model compilation -> model training. It specifically leverages the Keras Sequential API for simplicity and the scikit-learn utilities for dataset handling and standard preprocessing steps, integrating APIs from both libraries as needed.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}","type":"dataframe","variable_name":"df_neural"},"text/html":["\n","  <div id=\"df-28b35d8b-a8db-4de3-b05f-fbb813d73ce2\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>neural_gemini</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>```python\\nimport tensorflow as tf\\nfrom sklea...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>```python\\nimport torch\\nimport torch.nn as nn...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Okay, here is an API usage pattern using sciki...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>```python\\nimport torch\\nfrom datasets import ...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Okay, let's outline the API usage pattern for ...</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>```python\\nimport torch\\nimport torch.utils.da...</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>```python\\nimport matplotlib.pyplot as plt  # ...</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>Okay, the provided API structures (`tensorflow...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-28b35d8b-a8db-4de3-b05f-fbb813d73ce2')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-28b35d8b-a8db-4de3-b05f-fbb813d73ce2 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-28b35d8b-a8db-4de3-b05f-fbb813d73ce2');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    <div id=\"df-12ba7800-5bf7-4193-a411-7f6fe36117e0\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-12ba7800-5bf7-4193-a411-7f6fe36117e0')\"\n","                title=\"Suggest charts\"\n","                style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","      <script>\n","        async function quickchart(key) {\n","          const quickchartButtonEl =\n","            document.querySelector('#' + key + ' button');\n","          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","          quickchartButtonEl.classList.add('colab-df-spinner');\n","          try {\n","            const charts = await google.colab.kernel.invokeFunction(\n","                'suggestCharts', [key], {});\n","          } catch (error) {\n","            console.error('Error during call to suggestCharts:', error);\n","          }\n","          quickchartButtonEl.classList.remove('colab-df-spinner');\n","          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","        }\n","        (() => {\n","          let quickchartButtonEl =\n","            document.querySelector('#df-12ba7800-5bf7-4193-a411-7f6fe36117e0 button');\n","          quickchartButtonEl.style.display =\n","            google.colab.kernel.accessAllowed ? 'block' : 'none';\n","        })();\n","      </script>\n","    </div>\n","\n","  <div id=\"id_5c0504b5-904b-4666-ad6c-105788fee2f2\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_neural')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_5c0504b5-904b-4666-ad6c-105788fee2f2 button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('df_neural');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"],"text/plain":["                                       neural_gemini\n","0  ```python\\nimport tensorflow as tf\\nfrom sklea...\n","1  ```python\\nimport torch\\nimport torch.nn as nn...\n","2  Okay, here is an API usage pattern using sciki...\n","3  ```python\\nimport torch\\nfrom datasets import ...\n","4  Okay, let's outline the API usage pattern for ...\n","5  ```python\\nimport torch\\nimport torch.utils.da...\n","6  ```python\\nimport matplotlib.pyplot as plt  # ...\n","7  Okay, the provided API structures (`tensorflow..."]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["df_neural = pd.read_csv(\"/content/drive/MyDrive/Stanford NLP/neural_results_gemini.csv\")\n","df_neural"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":160},"executionInfo":{"elapsed":164090,"status":"ok","timestamp":1748920387955,"user":{"displayName":"Neeharika Gupta","userId":"13434636909198909500"},"user_tz":420},"id":"Hv6B3gS2wegp","outputId":"418beccb-7f1b-4d4d-cfc0-cf48ec5bd5c2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Processing row 1/8...\n","Processing row 2/8...\n","Processing row 3/8...\n","Processing row 4/8...\n","Processing row 5/8...\n","Processing row 6/8...\n","Processing row 7/8...\n","Processing row 8/8...\n"]}],"source":["output = []\n","\n","for index, row in df_neural.iterrows():\n","    print(f\"Processing row {index+1}/{len(df)}...\")\n","\n","    try:\n","        prompt = (\n","            f\"Generate complete code that solves the following task using the synthesized API usage pattern:\\n\"\n","            f\"{tasks[index]}\\n\"\n","            f\"{df_neural['neural_gemini']}\\n\\n\"\n","        )\n","\n","        response = make_request(prompt,model)\n","    except Exception as e:\n","        print(f\"Error generating response: {e}\")\n","        response = f\"Error: {e}\"\n","\n","    output.append(response)\n","\n","    time.sleep(DELAY_BETWEEN_REQUESTS)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s5LLqkaGCSdq"},"outputs":[],"source":["df_results = pd.DataFrame({'final_gemini': output})\n","\n","output_path = \"/content/drive/MyDrive/Stanford NLP/final_generated_code_gemini.csv\"\n","df_results.to_csv(output_path, index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DYpxgrH8v_sI"},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOsZp6pOcDT/NUBhACGu01/","mount_file_id":"1BgODjBXNAIReX4GviYMXfoO6uf-i1H4k","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
