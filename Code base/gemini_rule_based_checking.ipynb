{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMorTT+b8BUgBjZteFgIrYK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DJWlP8rrbIuz","executionInfo":{"status":"ok","timestamp":1748917231204,"user_tz":420,"elapsed":3088,"user":{"displayName":"Neeharika Gupta","userId":"13434636909198909500"}},"outputId":"478a9d7e-ff4d-4243-ca1b-0cad3aef2a85"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: google-generativeai in /usr/local/lib/python3.11/dist-packages (0.8.5)\n","Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (0.6.15)\n","Requirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.24.2)\n","Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.169.0)\n","Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.38.0)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (5.29.4)\n","Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.11.4)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.67.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.13.2)\n","Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n","Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (1.70.0)\n","Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (2.32.3)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n","Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n","Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n","Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (4.1.1)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.4.1)\n","Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.0)\n","Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.0)\n","Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.3)\n","Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2025.4.26)\n"]}],"source":["!pip install google-generativeai"]},{"cell_type":"code","source":["import os\n","import google.generativeai as genai\n","import pandas as pd\n","import time\n","from google.api_core.exceptions import TooManyRequests"],"metadata":{"id":"BoTHSQodcv01","executionInfo":{"status":"ok","timestamp":1748917235368,"user_tz":420,"elapsed":4162,"user":{"displayName":"Neeharika Gupta","userId":"13434636909198909500"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"US5EpU-1pwOQ","executionInfo":{"status":"ok","timestamp":1748917141268,"user_tz":420,"elapsed":17519,"user":{"displayName":"Neeharika Gupta","userId":"13434636909198909500"}},"outputId":"6496a920-1836-4dfd-baa0-108b95d3a397"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["df= pd.read_csv(\"/content/drive/MyDrive/Stanford NLP/dataset.csv\")\n","df"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":804},"id":"njSsAU8Dw3YL","executionInfo":{"status":"ok","timestamp":1748917149083,"user_tz":420,"elapsed":7823,"user":{"displayName":"Neeharika Gupta","userId":"13434636909198909500"}},"outputId":"d1d33d2a-2c90-4a20-c4db-67f73747efd9"},"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                                Task  \\\n","0  Write a Python function using TensorFlow to cr...   \n","1  Write a Python script using PyTorch to train a...   \n","2  Create a scikit-learn pipeline that generates ...   \n","3  Write a Python function that uses Hugging Face...   \n","4  Build a TensorFlow model for image classificat...   \n","5  Write a PyTorch class that subclasses torch.ut...   \n","6  Write a Python script that loads the Iris data...   \n","7  Write a Java program using the Hadoop MapReduc...   \n","\n","                                                Code  \\\n","0  import tensorflow as tf\\nfrom tensorflow impor...   \n","1  import torch\\nimport torch.nn as nn\\nimport to...   \n","2  from sklearn.datasets import make_classificati...   \n","3  from transformers import AutoTokenizer\\nfrom t...   \n","4  import tensorflow as tf\\nimport numpy as np\\n\\...   \n","5  from torch.utils.data import Dataset\\nfrom tor...   \n","6  \\nfrom sklearn import datasets\\nfrom sklearn.d...   \n","7  Mapper Code\\nimport java.io.IOException;      ...   \n","\n","                                             API doc  \\\n","0  import tensorflow as tf\\nfrom tensorflow impor...   \n","1  import torch.nn as nn\\nimport torch.optim as o...   \n","2  from sklearn.datasets import make_classificati...   \n","3  from transformers import AutoTokenizer\\nfrom t...   \n","4  import tensorflow as tf\\nimport numpy as np\\nt...   \n","5  from torch.utils.data import Dataset\\nfrom tor...   \n","6  \\nfrom sklearn import datasets\\nfrom sklearn.d...   \n","7                                                NaN   \n","\n","                                         Few shot t1  \\\n","0  Create a python script that uses tensorflow to...   \n","1  Write a Python script using PyTorch to train a...   \n","2  Create a scikit learn pipeline that builds sta...   \n","3  In this example we learn to identify past vs p...   \n","4  Build an image recognition tensorflow model to...   \n","5  Write a PyTorch class that creates custom data...   \n","6  Write a Python script that loads the UCI handw...   \n","7  Write a Java program using the Hadoop MapReduc...   \n","\n","                                         Few shot c1  \\\n","0  \\nimport numpy as np\\nimport tensorflow as tf\\...   \n","1  import numpy as np\\nimport pandas as pd\\nimpor...   \n","2  from sklearn.pipeline import Pipeline\\nfrom sk...   \n","3  \\nfrom typing import Callable, Dict, Sequence,...   \n","4  import matplotlib.pyplot as plt \\nimport numpy...   \n","5  \\nimport os\\nimport pandas as pd\\nfrom torchvi...   \n","6  import matplotlib.pyplot as plt\\nimport numpy ...   \n","7  import java.io.IOException;\\nimport java.util....   \n","\n","                                         Few shot t2  \\\n","0  Write a Python function using TensorFlow to cr...   \n","1  Write a Python script using PyTorch to train a...   \n","2  Create a scikit learn pipeline that builds sta...   \n","3  Write a python function that uses hugging face...   \n","4  Create an image classification model using ten...   \n","5  Write a PyTorch class that subclasses torch.ut...   \n","6  Write a Python script that uses the dataset gi...   \n","7  Write a Java program using the Hadoop MapReduc...   \n","\n","                                         Few shot c2 Unnamed: 7  \n","0  pip install -q seaborn\\n\\nimport matplotlib.py...        NaN  \n","1  from torch.nn import Module\\nfrom torch.nn imp...         \\n  \n","2  from sklearn.pipeline import Pipeline\\nfrom sk...        NaN  \n","3  \\nimport gradio as gr\\nfrom transformers impor...        NaN  \n","4  import tensorflow as tf   \\n\\n# Display the ve...        NaN  \n","5  import torch\\nfrom torch.utils.data import Dat...        NaN  \n","6  import numpy as np\\nimport pandas as pd\\nfrom ...        NaN  \n","7  package hadoop; \\n\\nimport java.util.*; \\n\\nim...         \\n  "],"text/html":["\n","  <div id=\"df-a89dfe7e-f0cb-4f58-a20e-5999102a83b7\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Task</th>\n","      <th>Code</th>\n","      <th>API doc</th>\n","      <th>Few shot t1</th>\n","      <th>Few shot c1</th>\n","      <th>Few shot t2</th>\n","      <th>Few shot c2</th>\n","      <th>Unnamed: 7</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Write a Python function using TensorFlow to cr...</td>\n","      <td>import tensorflow as tf\\nfrom tensorflow impor...</td>\n","      <td>import tensorflow as tf\\nfrom tensorflow impor...</td>\n","      <td>Create a python script that uses tensorflow to...</td>\n","      <td>\\nimport numpy as np\\nimport tensorflow as tf\\...</td>\n","      <td>Write a Python function using TensorFlow to cr...</td>\n","      <td>pip install -q seaborn\\n\\nimport matplotlib.py...</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Write a Python script using PyTorch to train a...</td>\n","      <td>import torch\\nimport torch.nn as nn\\nimport to...</td>\n","      <td>import torch.nn as nn\\nimport torch.optim as o...</td>\n","      <td>Write a Python script using PyTorch to train a...</td>\n","      <td>import numpy as np\\nimport pandas as pd\\nimpor...</td>\n","      <td>Write a Python script using PyTorch to train a...</td>\n","      <td>from torch.nn import Module\\nfrom torch.nn imp...</td>\n","      <td>\\n</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Create a scikit-learn pipeline that generates ...</td>\n","      <td>from sklearn.datasets import make_classificati...</td>\n","      <td>from sklearn.datasets import make_classificati...</td>\n","      <td>Create a scikit learn pipeline that builds sta...</td>\n","      <td>from sklearn.pipeline import Pipeline\\nfrom sk...</td>\n","      <td>Create a scikit learn pipeline that builds sta...</td>\n","      <td>from sklearn.pipeline import Pipeline\\nfrom sk...</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Write a Python function that uses Hugging Face...</td>\n","      <td>from transformers import AutoTokenizer\\nfrom t...</td>\n","      <td>from transformers import AutoTokenizer\\nfrom t...</td>\n","      <td>In this example we learn to identify past vs p...</td>\n","      <td>\\nfrom typing import Callable, Dict, Sequence,...</td>\n","      <td>Write a python function that uses hugging face...</td>\n","      <td>\\nimport gradio as gr\\nfrom transformers impor...</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Build a TensorFlow model for image classificat...</td>\n","      <td>import tensorflow as tf\\nimport numpy as np\\n\\...</td>\n","      <td>import tensorflow as tf\\nimport numpy as np\\nt...</td>\n","      <td>Build an image recognition tensorflow model to...</td>\n","      <td>import matplotlib.pyplot as plt \\nimport numpy...</td>\n","      <td>Create an image classification model using ten...</td>\n","      <td>import tensorflow as tf   \\n\\n# Display the ve...</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>Write a PyTorch class that subclasses torch.ut...</td>\n","      <td>from torch.utils.data import Dataset\\nfrom tor...</td>\n","      <td>from torch.utils.data import Dataset\\nfrom tor...</td>\n","      <td>Write a PyTorch class that creates custom data...</td>\n","      <td>\\nimport os\\nimport pandas as pd\\nfrom torchvi...</td>\n","      <td>Write a PyTorch class that subclasses torch.ut...</td>\n","      <td>import torch\\nfrom torch.utils.data import Dat...</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>Write a Python script that loads the Iris data...</td>\n","      <td>\\nfrom sklearn import datasets\\nfrom sklearn.d...</td>\n","      <td>\\nfrom sklearn import datasets\\nfrom sklearn.d...</td>\n","      <td>Write a Python script that loads the UCI handw...</td>\n","      <td>import matplotlib.pyplot as plt\\nimport numpy ...</td>\n","      <td>Write a Python script that uses the dataset gi...</td>\n","      <td>import numpy as np\\nimport pandas as pd\\nfrom ...</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>Write a Java program using the Hadoop MapReduc...</td>\n","      <td>Mapper Code\\nimport java.io.IOException;      ...</td>\n","      <td>NaN</td>\n","      <td>Write a Java program using the Hadoop MapReduc...</td>\n","      <td>import java.io.IOException;\\nimport java.util....</td>\n","      <td>Write a Java program using the Hadoop MapReduc...</td>\n","      <td>package hadoop; \\n\\nimport java.util.*; \\n\\nim...</td>\n","      <td>\\n</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a89dfe7e-f0cb-4f58-a20e-5999102a83b7')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-a89dfe7e-f0cb-4f58-a20e-5999102a83b7 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-a89dfe7e-f0cb-4f58-a20e-5999102a83b7');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    <div id=\"df-f9d8fe57-57a9-4919-8f4e-380ab88c6125\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f9d8fe57-57a9-4919-8f4e-380ab88c6125')\"\n","                title=\"Suggest charts\"\n","                style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","      <script>\n","        async function quickchart(key) {\n","          const quickchartButtonEl =\n","            document.querySelector('#' + key + ' button');\n","          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","          quickchartButtonEl.classList.add('colab-df-spinner');\n","          try {\n","            const charts = await google.colab.kernel.invokeFunction(\n","                'suggestCharts', [key], {});\n","          } catch (error) {\n","            console.error('Error during call to suggestCharts:', error);\n","          }\n","          quickchartButtonEl.classList.remove('colab-df-spinner');\n","          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","        }\n","        (() => {\n","          let quickchartButtonEl =\n","            document.querySelector('#df-f9d8fe57-57a9-4919-8f4e-380ab88c6125 button');\n","          quickchartButtonEl.style.display =\n","            google.colab.kernel.accessAllowed ? 'block' : 'none';\n","        })();\n","      </script>\n","    </div>\n","\n","  <div id=\"id_300c602d-539f-4134-bcf0-1d3dae90b2f3\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_300c602d-539f-4134-bcf0-1d3dae90b2f3 button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('df');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"df","summary":"{\n  \"name\": \"df\",\n  \"rows\": 8,\n  \"fields\": [\n    {\n      \"column\": \"Task\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"Write a Python script using PyTorch to train a convolutional neural network (CNN) on the Cifar 10 dataset. Include data loading, model definition, training loop, and evaluation\",\n          \"Write a PyTorch class that subclasses torch.utils.data.Dataset to load image files and labels from a folder. Use this class with a DataLoader for batching. Use FashionMNIST dataset.\",\n          \"Write a Python function using TensorFlow to create and compile a simple linear regression model with one dense layer. Train it on the classic California housing dataset.\\n\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Code\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"import torch\\nimport torch.nn as nn\\nimport torch.optim as optim\\nimport torchvision\\nimport torchvision.transforms as transforms\\nimport torch.nn.functional as F\\n\\ntransform = transforms.Compose(\\n    [transforms.ToTensor(),\\n     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\\n\\ntrainset = torchvision.datasets.CIFAR10(root='./data', train=True,\\n                                        download=True, transform=transform)\\n                                        \\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\\n                                          shuffle=True, num_workers=2)\\n\\ntestset = torchvision.datasets.CIFAR10(root='./data', train=False,\\n                                       download=True, transform=transform)\\n                                       \\ntestloader = torch.utils.data.DataLoader(testset, batch_size=4,\\n                                         shuffle=False, num_workers=2)\\n\\nclasses = ('plane', 'car', 'bird', 'cat',\\n           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\\n\\nclass Net(nn.Module):\\n    def __init__(self):\\n        super(Net, self).__init__()\\n        self.conv1 = nn.Conv2d(3, 6, 5)\\n        self.pool = nn.MaxPool2d(2, 2)\\n        self.conv2 = nn.Conv2d(6, 16, 5)\\n        self.fc1 = nn.Linear(16 * 5 * 5, 120)\\n        self.fc2 = nn.Linear(120, 84)\\n        self.fc3 = nn.Linear(84, 10)\\n\\n    def forward(self, x):\\n        x = self.pool(F.relu(self.conv1(x)))\\n        x = self.pool(F.relu(self.conv2(x)))\\n        x = x.view(-1, 16 * 5 * 5)\\n        x = F.relu(self.fc1(x))\\n        x = F.relu(self.fc2(x))\\n        x = self.fc3(x)\\n        return x\\n\\nnet = Net()\\n\\ncriterion = nn.CrossEntropyLoss()\\noptimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\\n\\nfor epoch in range(2):  \\n\\n    running_loss = 0.0\\n    for i, data in enumerate(trainloader, 0):\\n        inputs, labels = data\\n\\n        optimizer.zero_grad()\\n\\n        outputs = net(inputs)\\n        loss = criterion(outputs, labels)\\n        loss.backward()\\n        optimizer.step()\\n\\n        running_loss += loss.item()\\n        if i % 2000 == 1999: \\n            print('[%d, %5d] loss: %.3f' %\\n                  (epoch + 1, i + 1, running_loss / 2000))\\n            running_loss = 0.0\\n\\ncorrect = 0\\ntotal = 0\\nwith torch.no_grad():\\n    for data in testloader:\\n        images, labels = data\\n        outputs = net(images)\\n        _, predicted = torch.max(outputs.data, 1)\\n        total += labels.size(0)\\n        correct += (predicted == labels).sum().item()\\n\\nprint('Accuracy of the network on the 10000 test images: %d %%' % (\\n    100 * correct / total))\",\n          \"from torch.utils.data import Dataset\\nfrom torchvision import datasets\\nfrom torchvision.transforms import ToTensor\\nfrom torch.utils.data import DataLoader\\n\\n\\ntraining_data = datasets.FashionMNIST(\\n    root=\\\"data\\\",\\n    train=True,\\n    download=True,\\n    transform=ToTensor()\\n)\\n\\ntest_data = datasets.FashionMNIST(\\n    root=\\\"data\\\",\\n    train=False,\\n    download=True,\\n    transform=ToTensor()\\n)\\n\\ntrain_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\\ntest_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)\",\n          \"import tensorflow as tf\\nfrom tensorflow import keras\\nfrom tensorflow.keras.models import Sequential\\nfrom tensorflow.keras.layers import Dense, Input, Concatenate\\n\\nfrom sklearn.datasets import fetch_california_housing\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.preprocessing import StandardScaler\\nimport matplotlib.pyplot as plt\\n\\nimport numpy as np\\nimport pandas as pd\\nhousing = fetch_california_housing()\\nXtrain, Xtest, ytrain, ytest = train_test_split(housing.data, housing.target, test_size = .2)\\nXtrain, Xval, ytrain, yval = train_test_split(Xtrain, ytrain, test_size=.2)\\n\\nmodel = Sequential([\\n    Dense(30, activation = 'relu', input_shape= Xtrain.shape[1:]),\\n    Dense(1)\\n])\\n\\nmodel.compile(loss='mean_squared_error', optimizer='adam')\\n\\n\\nhistory = model.fit(Xtrain, ytrain, validation_data=(Xval, yval), epochs=20)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"API doc\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          \"import tensorflow as tf\\nfrom tensorflow import keras\\nfrom tensorflow.keras.models import Sequential\\nfrom tensorflow.keras.layers import Dense, Input, Concatenate\\n\\nfrom sklearn.datasets import fetch_california_housing\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.preprocessing import StandardScaler\\nimport matplotlib.pyplot as plt\\n\\nimport numpy as np\\nimport pandas as pd\\n\",\n          \"import torch.nn as nn\\nimport torch.optim as optim\\nimport torchvision\\nimport torchvision.transforms as transforms\\nimport torch.nn.functional as F\\ntransforms.Compose\\ntorchvision.datasets.CIFAR10\\ntorch.utils.data.DataLoader\\nnn.Conv2d\\nnn.MaxPool2d\\nnn.Linear\\nF.relu\\nnn.CrossEntropyLoss\\noptim.SGD\\n\",\n          \"from torch.utils.data import Dataset\\nfrom torchvision import datasets\\nfrom torchvision.transforms import ToTensor\\nfrom torch.utils.data import DataLoader\\ndatasets.FashionMNIST\\n\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Few shot t1\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"Write a Python script using PyTorch to train a convolutional neural network (CNN) on the MNIST dataset. Include data loading, model definition, training loop, and evaluation\",\n          \"Write a PyTorch class that creates custom dataset. It takes in input of img dir and annotations csv file.\",\n          \"Create a python script that uses tensorflow to model a linear regression problem using randomly genenrated 50 datat points. You need to train the model and also get the predictions.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Few shot c1\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"import numpy as np\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\n\\n\\nimport torch\\nfrom torch import optim\\nfrom torch import nn\\nfrom torch.utils.data import DataLoader\\nfrom tqdm import tqdm\\n\\n# !pip install torchvision\\nimport torchvision\\n\\nimport torch.nn.functional as F\\nimport torchvision.datasets as datasets\\nimport torchvision.transforms as transforms\\n\\n# !pip install torchmetrics\\nimport torchmetrics\\n\\nbatch_size = 60\\n\\ntrain_dataset = datasets.MNIST(root=\\\"dataset/\\\", download=True, train=True, transform=transforms.ToTensor())\\n\\ntrain_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\\n\\ntest_dataset = datasets.MNIST(root=\\\"dataset/\\\", download=True, train=False, transform=transforms.ToTensor())\\n\\ntest_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)\\n\\nclass CNN(nn.Module):\\n   def __init__(self, in_channels, num_classes):\\n\\n       \\\"\\\"\\\"\\n       Building blocks of convolutional neural network.\\n\\n       Parameters:\\n           * in_channels: Number of channels in the input image (for grayscale images, 1)\\n           * num_classes: Number of classes to predict. In our problem, 10 (i.e digits from  0 to 9).\\n       \\\"\\\"\\\"\\n       super(CNN, self).__init__()\\n\\n       # 1st convolutional layer\\n       self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=8, kernel_size=3, padding=1)\\n       # Max pooling layer\\n       self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\\n       # 2nd convolutional layer\\n       self.conv2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, padding=1)\\n       # Fully connected layer\\n       self.fc1 = nn.Linear(16 * 7 * 7, num_classes)\\n\\n   def forward(self, x):\\n       \\\"\\\"\\\"\\n       Define the forward pass of the neural network.\\n\\n       Parameters:\\n           x: Input tensor.\\n\\n       Returns:\\n           torch.Tensor\\n               The output tensor after passing through the network.\\n       \\\"\\\"\\\"\\n       x = F.relu(self.conv1(x))  # Apply first convolution and ReLU activation\\n       x = self.pool(x)           # Apply max pooling\\n       x = F.relu(self.conv2(x))  # Apply second convolution and ReLU activation\\n       x = self.pool(x)           # Apply max pooling\\n       x = x.reshape(x.shape[0], -1)  # Flatten the tensor\\n       x = self.fc1(x)            # Apply fully connected layer\\n       return x\\n       x = x.reshape(x.shape[0], -1)  # Flatten the tensor\\n       x = self.fc1(x)            # Apply fully connected layer\\n       return x\\n\\n# Define the loss function\\ncriterion = nn.CrossEntropyLoss()\\n\\n# Define the optimizer\\noptimizer = optim.Adam(model.parameters(), lr=0.001)\\n\\nnum_epochs=10\\nfor epoch in range(num_epochs):\\n # Iterate over training batches\\n   print(f\\\"Epoch [{epoch + 1}/{num_epochs}]\\\")\\n\\n   for batch_index, (data, targets) in enumerate(tqdm(dataloader_train)):\\n       data = data.to(device)\\n       targets = targets.to(device)\\n       scores = model(data)\\n       loss = criterion(scores, targets)\\n       optimizer.zero_grad()\\n       loss.backward()\\n       optimizer.step()\\n# Set up of multiclass accuracy metric\\nacc = Accuracy(task=\\\"multiclass\\\",num_classes=10)\\n\\n# Iterate over the dataset batches\\nmodel.eval()\\nwith torch.no_grad():\\n   for images, labels in dataloader_test:\\n       # Get predicted probabilities for test data batch\\n       outputs = model(images)\\n       _, preds = torch.max(outputs, 1)\\n       acc(preds, labels)\\n       precision(preds, labels)\\n       recall(preds, labels)\\n\\n#Compute total test accuracy\\ntest_accuracy = acc.compute()\\nprint(f\\\"Test accuracy: {test_accuracy}\\\")\\n\",\n          \"\\nimport os\\nimport pandas as pd\\nfrom torchvision.io import read_image\\n\\nclass CustomImageDataset(Dataset):\\n    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\\n        self.img_labels = pd.read_csv(annotations_file)\\n        self.img_dir = img_dir\\n        self.transform = transform\\n        self.target_transform = target_transform\\n\\n    def __len__(self):\\n        return len(self.img_labels)\\n\\n    def __getitem__(self, idx):\\n        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\\n        image = read_image(img_path)\\n        label = self.img_labels.iloc[idx, 1]\\n        if self.transform:\\n            image = self.transform(image)\\n        if self.target_transform:\\n            label = self.target_transform(label)\\n        return image, label\\n\",\n          \"\\nimport numpy as np\\nimport tensorflow as tf\\nimport matplotlib.pyplot as plt\\n\\nnp.random.seed(101)\\n\\n# Generating random linear data\\n# There will be 50 data points ranging from 0 to 50\\nx = np.linspace(0, 50, 50)\\ny = np.linspace(0, 50, 50)\\n\\n# Adding noise to the random linear data\\nx += np.random.uniform(-4, 4, 50)\\ny += np.random.uniform(-4, 4, 50)\\n\\nn = len(x) # Number of data points\\n\\n# Plot of Training Data\\nplt.scatter(x, y)\\nplt.xlabel('x')\\nplt.ylabel('y')\\nplt.title(\\\"Training Data\\\")\\nplt.show()\\n\\nX = tf.placeholder(\\\"float\\\")\\nY = tf.placeholder(\\\"float\\\")\\n\\nW = tf.Variable(np.random.randn(), name = \\\"W\\\")\\nb = tf.Variable(np.random.randn(), name = \\\"b\\\")\\n\\nlearning_rate = 0.01\\ntraining_epochs = 1000\\n# Hypothesis\\ny_pred = tf.add(tf.multiply(X, W), b)\\n\\n# Mean Squared Error Cost Function\\ncost = tf.reduce_sum(tf.pow(y_pred-Y, 2)) / (2 * n)\\n\\n# Gradient Descent Optimizer\\noptimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\\n\\n# Global Variables Initializer\\ninit = tf.global_variables_initializer()\\n\\n# Starting the Tensorflow Session\\nwith tf.Session() as sess:\\n    \\n    # Initializing the Variables\\n    sess.run(init)\\n    \\n    # Iterating through all the epochs\\n    for epoch in range(training_epochs):\\n        \\n        # Feeding each data point into the optimizer using Feed Dictionary\\n        for (_x, _y) in zip(x, y):\\n            sess.run(optimizer, feed_dict = {X : _x, Y : _y})\\n        \\n        # Displaying the result after every 50 epochs\\n        if (epoch + 1) % 50 == 0:\\n            # Calculating the cost a every epoch\\n            c = sess.run(cost, feed_dict = {X : x, Y : y})\\n            print(\\\"Epoch\\\", (epoch + 1), \\\": cost =\\\", c, \\\"W =\\\", sess.run(W), \\\"b =\\\", sess.run(b))\\n    \\n    # Storing necessary values to be used outside the Session\\n    training_cost = sess.run(cost, feed_dict ={X: x, Y: y})\\n    weight = sess.run(W)\\n    bias = sess.run(b)\\n\\n# Calculating the predictions\\npredictions = weight * x + bias\\nprint(\\\"Training cost =\\\", training_cost, \\\"Weight =\\\", weight, \\\"bias =\\\", bias, '\\\\n')\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Few shot t2\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"Write a Python script using PyTorch to train a convolutional neural network (CNN) on the KMNIST dataset. Include data loading, model definition, training loop, and evaluation\",\n          \"Write a PyTorch class that subclasses torch.utils.data.Dataset to load 100 image files and labels from a folder. The data would be randomly created and the class should also be defined such that its a custom class.Use this class with a DataLoader for batching. \",\n          \"Write a Python function using TensorFlow to create and compile a simple linear regression model which has a normalizer layer and a dense layer. Train it on the fuel efficiency dataset.\\n\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Few shot c2\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"from torch.nn import Module\\nfrom torch.nn import Conv2d\\nfrom torch.nn import Linear\\nfrom torch.nn import MaxPool2d\\nfrom torch.nn import ReLU\\nfrom torch.nn import LogSoftmax\\nfrom torch import flatten\\nimport matplotlib\\nmatplotlib.use(\\\"Agg\\\")\\nfrom pyimagesearch.lenet import LeNet\\nfrom sklearn.metrics import classification_report\\nfrom torch.utils.data import random_split\\nfrom torch.utils.data import DataLoader\\nfrom torchvision.transforms import ToTensor\\nfrom torchvision.datasets import KMNIST\\nfrom torch.optim import Adam\\nfrom torch import nn\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\nimport argparse\\nimport torch\\nimport time\\n\\nclass LeNet(Module):\\n        def __init__(self, numChannels, classes):\\n                # call the parent constructor\\n                super(LeNet, self).__init__()\\n                # initialize first set of CONV => RELU => POOL layers\\n                self.conv1 = Conv2d(in_channels=numChannels, out_channels=20,\\n                        kernel_size=(5, 5))\\n                self.relu1 = ReLU()\\n                self.maxpool1 = MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\\n                # initialize second set of CONV => RELU => POOL layers\\n                self.conv2 = Conv2d(in_channels=20, out_channels=50,\\n                        kernel_size=(5, 5))\\n                self.relu2 = ReLU()\\n                self.maxpool2 = MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\\n                # initialize first (and only) set of FC => RELU layers\\n                self.fc1 = Linear(in_features=800, out_features=500)\\n                self.relu3 = ReLU()\\n                # initialize our softmax classifier\\n                self.fc2 = Linear(in_features=500, out_features=classes)\\n                self.logSoftmax = LogSoftmax(dim=1)\\n\\ndef forward(self, x):\\n                # pass the input through our first set of CONV => RELU =>\\n                # POOL layers\\n                x = self.conv1(x)\\n                x = self.relu1(x)\\n                x = self.maxpool1(x)\\n                # pass the output from the previous layer through the second\\n                # set of CONV => RELU => POOL layers\\n                x = self.conv2(x)\\n                x = self.relu2(x)\\n                x = self.maxpool2(x)\\n                # flatten the output from the previous layer and pass it\\n                # through our only set of FC => RELU layers\\n                x = flatten(x, 1)\\n                x = self.fc1(x)\\n                x = self.relu3(x)\\n                # pass the output to our softmax classifier to get our output\\n                # predictions\\n                x = self.fc2(x)\\n                output = self.logSoftmax(x)\\n                # return the output predictions\\n                return output\\n# construct the argument parser and parse the arguments\\nap = argparse.ArgumentParser()\\nap.add_argument(\\\"-m\\\", \\\"--model\\\", type=str, required=True,\\n        help=\\\"path to output trained model\\\")\\nap.add_argument(\\\"-p\\\", \\\"--plot\\\", type=str, required=True,\\n        help=\\\"path to output loss/accuracy plot\\\")\\nargs = vars(ap.parse_args())\\n\\nPyTorch: Training your first Convolutional Neural Network (CNN)\\n# define training hyperparameters\\nINIT_LR = 1e-3\\nBATCH_SIZE = 64\\nEPOCHS = 10\\n# define the train and val splits\\nTRAIN_SPLIT = 0.75\\nVAL_SPLIT = 1 - TRAIN_SPLIT\\n# set the device we will be using to train the model\\ndevice = torch.device(\\\"cuda\\\" if torch.cuda.is_available() else \\\"cpu\\\")\\n\\n# load the KMNIST dataset\\nprint(\\\"[INFO] loading the KMNIST dataset...\\\")\\ntrainData = KMNIST(root=\\\"data\\\", train=True, download=True,\\n        transform=ToTensor())\\ntestData = KMNIST(root=\\\"data\\\", train=False, download=True,\\n        transform=ToTensor())\\n# calculate the train/validation split\\nprint(\\\"[INFO] generating the train/validation split...\\\")\\nnumTrainSamples = int(len(trainData) * TRAIN_SPLIT)\\nnumValSamples = int(len(trainData) * VAL_SPLIT)\\n(trainData, valData) = random_split(trainData,\\n        [numTrainSamples, numValSamples],\\n        generator=torch.Generator().manual_seed(42))\\n\\n# initialize the train, validation, and test data loaders\\ntrainDataLoader = DataLoader(trainData, shuffle=True,\\n        batch_size=BATCH_SIZE)\\nvalDataLoader = DataLoader(valData, batch_size=BATCH_SIZE)\\ntestDataLoader = DataLoader(testData, batch_size=BATCH_SIZE)\\n# calculate steps per epoch for training and validation set\\ntrainSteps = len(trainDataLoader.dataset) // BATCH_SIZE\\nvalSteps = len(valDataLoader.dataset) // BATCH_SIZE\\n\\n# initialize the LeNet model\\nprint(\\\"[INFO] initializing the LeNet model...\\\")\\nmodel = LeNet(\\n        numChannels=1,\\n        classes=len(trainData.dataset.classes)).to(device)\\n# initialize our optimizer and loss function\\nopt = Adam(model.parameters(), lr=INIT_LR)\\nlossFn = nn.NLLLoss()\\n# initialize a dictionary to store training history\\nH = {\\n        \\\"train_loss\\\": [],\\n        \\\"train_acc\\\": [],\\n        \\\"val_loss\\\": [],\\n        \\\"val_acc\\\": []\\n}\\n# measure how long training is going to take\\nprint(\\\"[INFO] training the network...\\\")\\n\\n# loop over our epochs\\nfor e in range(0, EPOCHS):\\n        # set the model in training mode\\n        model.train()\\n        # initialize the total training and validation loss\\n        totalTrainLoss = 0\\n        totalValLoss = 0\\n        # initialize the number of correct predictions in the training\\n        # and validation step\\n        trainCorrect = 0\\n        valCorrect = 0\\n        # loop over the training set\\n        for (x, y) in trainDataLoader:\\n                # send the input to the device\\n                (x, y) = (x.to(device), y.to(device))\\n                # perform a forward pass and calculate the training loss\\n                pred = model(x)\\n                loss = lossFn(pred, y)\\n                # zero out the gradients, perform the backpropagation step,\\n                # and update the weights\\n                opt.zero_grad()\\n                loss.backward()\\n                opt.step()\\n                # add the loss to the total training loss so far and\\n                # calculate the number of correct predictions\\n                totalTrainLoss += loss\\n                trainCorrect += (pred.argmax(1) == y).type(\\n                        torch.float).sum().item()\\n\\n        # switch off autograd for evaluation\\n        with torch.no_grad():\\n                # set the model in evaluation mode\\n                model.eval()\\n                # loop over the validation set\\n                for (x, y) in valDataLoader:\\n                        # send the input to the device\\n                        (x, y) = (x.to(device), y.to(device))\\n                        # make the predictions and calculate the validation loss\\n                        pred = model(x)\\n                        totalValLoss += lossFn(pred, y)\\n                        # calculate the number of correct predictions\\n                        valCorrect += (pred.argmax(1) == y).type(\\n                                torch.float).sum().item()\\n\\n        # calculate the average training and validation loss\\n        avgTrainLoss = totalTrainLoss / trainSteps\\n        avgValLoss = totalValLoss / valSteps\\n        # calculate the training and validation accuracy\\n        trainCorrect = trainCorrect / len(trainDataLoader.dataset)\\n        valCorrect = valCorrect / len(valDataLoader.dataset)\\n        # update our training history\\n        H[\\\"train_loss\\\"].append(avgTrainLoss.cpu().detach().numpy())\\n        H[\\\"train_acc\\\"].append(trainCorrect)\\n        H[\\\"val_loss\\\"].append(avgValLoss.cpu().detach().numpy())\\n        H[\\\"val_acc\\\"].append(valCorrect)\\n        # print the model training and validation information\\n        print(\\\"[INFO] EPOCH: {}/{}\\\".format(e + 1, EPOCHS))\\n        print(\\\"Train loss: {:.6f}, Train accuracy: {:.4f}\\\".format(\\n                avgTrainLoss, trainCorrect))\\n        print(\\\"Val loss: {:.6f}, Val accuracy: {:.4f}\\\\n\\\".format(\\n                avgValLoss, valCorrect))\",\n          \"import torch\\nfrom torch.utils.data import Dataset, DataLoader\\nimport torch.nn as nn\\nimport torch.optim as optim\\nimport numpy as np\\n\\n# Custom Dataset class\\nclass CustomDataset(Dataset):\\n    def __init__(self, data, labels):\\n        self.data = data\\n        self.labels = labels\\n\\n    def __len__(self):\\n        return len(self.data)\\n\\n    def __getitem__(self, idx):\\n        sample = self.data[idx]\\n        label = self.labels[idx]\\n        return sample, label\\n\\n# Prepare data\\ndata = np.random.randn(100, 3, 32, 32)  # 100 samples of 3x32x32 images\\nlabels = np.random.randint(0, 10, size=(100,))  # 100 labels in the range 0-9\\n\\n# Create Dataset\\ndataset = CustomDataset(data, labels)\\n\\n# Create DataLoader\\ndataloader = DataLoader(dataset, batch_size=4, shuffle=True, num_workers=2)\\n\",\n          \"pip install -q seaborn\\n\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\nimport pandas as pd\\nimport seaborn as sns\\n\\n# Make NumPy printouts easier to read.\\nnp.set_printoptions(precision=3, suppress=True)\\nimport tensorflow as tf\\n\\nfrom tensorflow import keras\\nfrom tensorflow.keras import layers\\n\\nprint(tf.__version__)\\n\\nurl = 'http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data'\\ncolumn_names = ['MPG', 'Cylinders', 'Displacement', 'Horsepower', 'Weight',\\n                'Acceleration', 'Model Year', 'Origin']\\n\\nraw_dataset = pd.read_csv(url, names=column_names,\\n                          na_values='?', comment='\\\\t',\\n                          sep=' ', skipinitialspace=True)\\n\\ndataset = raw_dataset.copy()\\ndataset.tail()\\n\\ndataset.isna().sum()\\n\\ndataset = dataset.dropna()\\n\\ndataset['Origin'] = dataset['Origin'].map({1: 'USA', 2: 'Europe', 3: 'Japan'})\\n\\ndataset = pd.get_dummies(dataset, columns=['Origin'], prefix='', prefix_sep='')\\ndataset.tail()\\n\\ntrain_dataset = dataset.sample(frac=0.8, random_state=0)\\ntest_dataset = dataset.drop(train_dataset.index)\\n\\ntrain_features = train_dataset.copy()\\ntest_features = test_dataset.copy()\\n\\ntrain_labels = train_features.pop('MPG')\\ntest_labels = test_features.pop('MPG')\\n\\nhorsepower = np.array(train_features['Horsepower'])\\n\\nhorsepower_normalizer = layers.Normalization(input_shape=[1,], axis=None)\\nhorsepower_normalizer.adapt(horsepower)\\n\\nhorsepower_model = tf.keras.Sequential([\\n    horsepower_normalizer,\\n    layers.Dense(units=1)\\n])\\n\\nhorsepower_model.summary()\\n\\nhorsepower_model.compile(\\n    optimizer=tf.keras.optimizers.Adam(learning_rate=0.1),\\n    loss='mean_absolute_error')\\n\\n\\nhistory = horsepower_model.fit(\\n    train_features['Horsepower'],\\n    train_labels,\\n    epochs=100,\\n    # Suppress logging.\\n    verbose=0,\\n    # Calculate validation results on 20% of the training data.\\n    validation_split = 0.2)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Unnamed: 7\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"\\n\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["tasks=df['Task'].to_list()\n","tasks"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OATUQ9YAx-FA","executionInfo":{"status":"ok","timestamp":1748917149087,"user_tz":420,"elapsed":5,"user":{"displayName":"Neeharika Gupta","userId":"13434636909198909500"}},"outputId":"fa1ced67-0757-421d-ed3f-664b6959174f"},"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Write a Python function using TensorFlow to create and compile a simple linear regression model with one dense layer. Train it on the classic California housing dataset.\\n',\n"," 'Write a Python script using PyTorch to train a convolutional neural network (CNN) on the Cifar 10 dataset. Include data loading, model definition, training loop, and evaluation',\n"," 'Create a scikit-learn pipeline that generates a random n-class classification problem and creates a pipeline to compute the mean and standard deviation on a training set so as to be able to later re-apply the same transformation on the testing set. Train the model Fit the dataset using the same pipeline and compute score on the test set. ',\n"," 'Write a Python function that uses Hugging Face to train a model(distilbert/distilbert-base-uncaseddistilbert/distilbert-base-uncased) using the IMDb Reviews dataset. Its  binary classification task. Involve any necessary preprocessing steps if required.',\n"," 'Build a TensorFlow model for image classification using the Fashion MNIST dataset and integrate early stopping and model checkpoint callbacks during training.',\n"," 'Write a PyTorch class that subclasses torch.utils.data.Dataset to load image files and labels from a folder. Use this class with a DataLoader for batching. Use FashionMNIST dataset.',\n"," 'Write a Python script that loads the Iris dataset, reduces its dimensionality using PCA, and fits a k-nearest neighbors classifier using scikit-learn.',\n"," 'Write a Java program using the Hadoop MapReduce API to count the frequency of each word in a large text file, such as a sample from Project Gutenberg.']"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":300},"id":"U_TZS9vXixKg","executionInfo":{"status":"ok","timestamp":1748917149564,"user_tz":420,"elapsed":476,"user":{"displayName":"Neeharika Gupta","userId":"13434636909198909500"}},"outputId":"5eb494de-01cf-4b22-fe34-83c738b5a06c"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                          neural_gpt\n","0  To complete your task of creating and training...\n","1  To address your request to train a convolution...\n","2  To accomplish the task of generating a random ...\n","3  To accomplish the task of training a DistilBER...\n","4  To build a TensorFlow model for image classifi...\n","5  To load images from a folder using PyTorch, yo...\n","6  To accomplish the task of loading the Iris dat...\n","7  To accomplish the task of counting the frequen..."],"text/html":["\n","  <div id=\"df-1d02fcc7-3a77-4ccb-9aa1-bb6e7edee5c6\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>neural_gpt</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>To complete your task of creating and training...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>To address your request to train a convolution...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>To accomplish the task of generating a random ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>To accomplish the task of training a DistilBER...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>To build a TensorFlow model for image classifi...</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>To load images from a folder using PyTorch, yo...</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>To accomplish the task of loading the Iris dat...</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>To accomplish the task of counting the frequen...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1d02fcc7-3a77-4ccb-9aa1-bb6e7edee5c6')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-1d02fcc7-3a77-4ccb-9aa1-bb6e7edee5c6 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-1d02fcc7-3a77-4ccb-9aa1-bb6e7edee5c6');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    <div id=\"df-f00e452f-8cf1-4ae0-80f5-52997f035f2f\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f00e452f-8cf1-4ae0-80f5-52997f035f2f')\"\n","                title=\"Suggest charts\"\n","                style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","      <script>\n","        async function quickchart(key) {\n","          const quickchartButtonEl =\n","            document.querySelector('#' + key + ' button');\n","          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","          quickchartButtonEl.classList.add('colab-df-spinner');\n","          try {\n","            const charts = await google.colab.kernel.invokeFunction(\n","                'suggestCharts', [key], {});\n","          } catch (error) {\n","            console.error('Error during call to suggestCharts:', error);\n","          }\n","          quickchartButtonEl.classList.remove('colab-df-spinner');\n","          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","        }\n","        (() => {\n","          let quickchartButtonEl =\n","            document.querySelector('#df-f00e452f-8cf1-4ae0-80f5-52997f035f2f button');\n","          quickchartButtonEl.style.display =\n","            google.colab.kernel.accessAllowed ? 'block' : 'none';\n","        })();\n","      </script>\n","    </div>\n","\n","  <div id=\"id_b3428d56-1caf-4ce3-bd2a-580d99093da9\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_neural')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_b3428d56-1caf-4ce3-bd2a-580d99093da9 button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('df_neural');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"df_neural","summary":"{\n  \"name\": \"df_neural\",\n  \"rows\": 8,\n  \"fields\": [\n    {\n      \"column\": \"neural_gpt\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"To address your request to train a convolutional neural network (CNN) on the CIFAR-10 dataset using PyTorch, we'll define the key components of a script:\\n\\n1. **Data Loading**: Use `torchvision.datasets` for loading CIFAR-10 and `torch.utils.data.DataLoader` for batching.\\n2. **Model Definition**: Define a CNN using `torch.nn` components like `torch.nn.Conv2d`, `torch.nn.ReLU`, `torch.nn.MaxPool2d`, and fully connected layers (`torch.nn.Linear`).\\n3. **Training Loop**: Use `torch` to define the training process, including forward, backward passes, and optimizer steps.\\n4. **Evaluation**: Validate the model on a test dataset.\\n\\nHere's a complete example script:\\n\\n```python\\nimport torch\\nimport torch.nn as nn\\nimport torch.optim as optim\\nimport torchvision\\nimport torchvision.transforms as transforms\\nfrom torchvision import datasets\\n\\n# Load CIFAR-10 dataset\\ntransform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\\ntrain_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=4, shuffle=True)\\n\\ntest_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=4, shuffle=False)\\n\\n# Define a simple CNN model\\nclass SimpleCNN(nn.Module):\\n    def __init__(self):\\n        super(SimpleCNN, self).__init__()\\n        self.conv1 = nn.Conv2d(3, 6, 5)\\n        self.pool = nn.MaxPool2d(2, 2)\\n        self.conv2 = nn.Conv2d(6, 16, 5)\\n        self.fc1 = nn.Linear(16 * 5 * 5, 120)\\n        self.fc2 = nn.Linear(120, 84)\\n        self.fc3 = nn.Linear(84, 10)\\n\\n    def forward(self, x):\\n        x = self.pool(F.relu(self.conv1(x)))\\n        x = self.pool(F.relu(self.conv2(x)))\\n        x = x.view(-1, 16 * 5 * 5)\\n        x = F.relu(self.fc1(x))\\n        x = F.relu(self.fc2(x))\\n        x = self.fc3(x)\\n        return x\\n\\n# Initialize the model, criterion and optimizer\\nmodel = SimpleCNN()\\ncriterion = nn.CrossEntropyLoss()\\noptimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\\n\\n# Training loop\\nfor epoch in range(2):  # loop over the dataset multiple times\\n    running_loss = 0.0\\n    for i, data in enumerate(train_loader, 0):\\n        inputs, labels = data\\n        \\n        # zero the parameter gradients\\n        optimizer.zero_grad()\\n\\n        # forward + backward + optimize\\n        outputs = model(inputs)\\n        loss = criterion(outputs, labels)\\n        loss.backward()\\n        optimizer.step()\\n\\n        # print statistics\\n        running_loss += loss.item()\\n        if i % 2000 == 1999:  # print every 2000 mini-batches\\n            print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 2000))\\n            running_loss = 0.0\\n\\nprint('Finished Training')\\n\\n# Evaluate the model\\ncorrect = 0\\ntotal = 0\\n\\nwith torch.no_grad():\\n    for data in test_loader:\\n        images, labels = data\\n        outputs = model(images)\\n        _, predicted = torch.max(outputs.data, 1)\\n        total += labels.size(0)\\n        correct += (predicted == labels).sum().item()\\n\\nprint('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))\\n```\\n\\n### Explanation:\\n- **Data Loading**: Transforms include normalization; data loaders handle batching.\\n- **Model**: A simple CNN with two convolutional layers followed by pooling and three fully connected layers.\\n- **Training**: Stochastic Gradient Descent (SGD) optimizer is used with a cross-entropy loss function. The training loop iterates over the epochs training the model.\\n- **Evaluation**: After training, the accuracy of the model is evaluated on the test dataset.\\n\\nEnsure you have PyTorch and torchvision installed to run the script. Adjust the hyperparameters (like epochs, learning rate) as necessary for better performance.\",\n          \"To load images from a folder using PyTorch, you'll want to create a custom dataset by subclassing `torch.utils.data.Dataset`. Given your task is to work with the FashionMNIST dataset, which is built into torchvision, you won't typically need to manually load it from images in a folder. However, I can guide you on how you would create such a dataset class if you were to do it in a more general context.\\n\\nHere's how you can create a custom dataset that loads images from a folder and uses a corresponding label file. This example assumes you have images in a folder and a text or CSV file with labels.\\n\\nFirst, install the necessary libraries:\\n```shell\\npip install torch torchvision\\n```\\n\\nThen you can follow this code to create a custom dataset:\\n\\n```python\\nimport os\\nfrom torch.utils.data import Dataset, DataLoader\\nfrom torchvision import transforms\\nfrom PIL import Image\\n\\nclass CustomImageDataset(Dataset):\\n    def __init__(self, image_folder, labels_file, transform=None):\\n        self.image_folder = image_folder\\n        self.transform = transform\\n        self.image_labels = []\\n\\n        with open(labels_file, 'r') as f:\\n            for line in f:\\n                image_name, label = line.strip().split(',')\\n                self.image_labels.append((image_name, int(label)))\\n\\n    def __len__(self):\\n        return len(self.image_labels)\\n\\n    def __getitem__(self, idx):\\n        img_name, label = self.image_labels[idx]\\n        image_path = os.path.join(self.image_folder, img_name)\\n        image = Image.open(image_path)\\n        \\n        if self.transform:\\n            image = self.transform(image)\\n\\n        return image, label\\n\\n# Define a transform if needed\\ntransform = transforms.Compose([\\n    transforms.Resize((28, 28)),\\n    transforms.ToTensor(),\\n])\\n\\n# Usage\\nimage_folder = 'path/to/images/'\\nlabels_file = 'path/to/labels.txt'\\n\\ncustom_dataset = CustomImageDataset(image_folder, labels_file, transform=transform)\\ndata_loader = DataLoader(custom_dataset, batch_size=32, shuffle=True)\\n\\n# Iterate through the data loader\\nfor images, labels in data_loader:\\n    print(images.shape, labels.shape)\\n```\\n\\n### Important Notes:\\n- `labels_file`: It assumes that the labels are stored in a text file where each line contains an image name and its corresponding label separated by a comma, like `'image1.png,0'`.\\n- Transform: It includes resizing to 28x28, which is the size expected for FashionMNIST images and then converts images to tensors.\\n\\n### Using FashionMNIST:\\nIf you specifically want to use the FashionMNIST dataset with a DataLoader, you can directly use torchvision's dataset:\\n\\n```python\\nfrom torchvision.datasets import FashionMNIST\\nfrom torch.utils.data.dataloader import DataLoader\\n\\n# Define the transformation\\ntransform = transforms.Compose([\\n    transforms.ToTensor(),\\n])\\n\\nfashion_mnist_dataset = FashionMNIST(root='data', train=True, download=True, transform=transform)\\nfashion_mnist_loader = DataLoader(fashion_mnist_dataset, batch_size=32, shuffle=True)\\n\\n# Iterate through the FashionMNIST data loader\\nfor images, labels in fashion_mnist_loader:\\n    print(images.shape, labels.shape)\\n```\\n\\n`FashionMNIST` is available directly from `torchvision.datasets`, and using it as shown allows you to focus on your machine learning tasks quickly without having to manage the images yourself.\",\n          \"To complete your task of creating and training a simple linear regression model with TensorFlow on the California housing dataset, I suggest the following usage pattern to use with TensorFlow and related modules. Here's the full code for your task:\\n\\n```python\\nimport tensorflow as tf\\nfrom tensorflow.keras.models import Sequential\\nfrom tensorflow.keras.layers import Dense\\nfrom sklearn.datasets import fetch_california_housing\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.preprocessing import StandardScaler\\n\\ndef train_linear_regression_model():\\n    # Load the California housing dataset\\n    housing_data = fetch_california_housing()\\n    X, y = housing_data.data, housing_data.target\\n\\n    # Split the data into training and test sets\\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\\n\\n    # Standardize the dataset\\n    scaler = StandardScaler()\\n    X_train = scaler.fit_transform(X_train)\\n    X_test = scaler.transform(X_test)\\n\\n    # Define the model architecture\\n    model = Sequential()\\n    model.add(Dense(units=1, activation='linear', input_shape=(X_train.shape[1],)))\\n\\n    # Compile the model\\n    model.compile(optimizer='sgd', loss='mse')\\n\\n    # Train the model\\n    model.fit(X_train, y_train, epochs=100, batch_size=32, verbose=1)\\n\\n    # Evaluate the model\\n    loss = model.evaluate(X_test, y_test)\\n    print(f'Test Loss: {loss}')\\n\\ntrain_linear_regression_model()\\n```\\n\\n### Explanation:\\n1. **Data Loading**: The California housing dataset is loaded using `fetch_california_housing()`, which provides you with the features (`X`) and target (`y`).\\n\\n2. **Data Splitting**: Use `train_test_split` to divide the dataset into a training set and a test set.\\n\\n3. **Data Standardization**: To standardize the dataset, I've used `StandardScaler` from `sklearn.preprocessing`.\\n\\n4. **Model Definition**: A simple linear model is defined using `Sequential` and a single `Dense` layer with one unit, suited for linear regression tasks.\\n\\n5. **Model Compilation**: The model is compiled using the stochastic gradient descent ('sgd') optimizer and mean squared error ('mse') as the loss function.\\n\\n6. **Training**: The model is trained with the `fit` method for 100 epochs with a batch size of 32. Adjust the number of epochs and batch size depending on your testing needs.\\n\\n7. **Evaluation**: The trained model is evaluated using the test data, and the test loss is printed.\\n\\n### Notes:\\n- TensorFlow provides integration with Keras, so it's easy to build and train deep learning models using the high-level Keras API.\\n- Ensure that you have installed TensorFlow and SciKit-Learn, as they are required for this code to run. You can install them using `pip install tensorflow scikit-learn`.\\n\\nThis code sets up the essential framework, but you can further improve and modify it according to your project's requirements.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":4}],"source":["df_neural = pd.read_csv(\"/content/drive/MyDrive/Stanford NLP/neural_results_gpt.csv\")\n","df_neural"]},{"cell_type":"code","source":["df_api = pd.read_csv(\"/content/drive/MyDrive/Stanford NLP/api_documentation_new.csv\")\n","df_api"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"collapsed":true,"id":"3FMxoWaKujoY","executionInfo":{"status":"ok","timestamp":1748917155364,"user_tz":420,"elapsed":1314,"user":{"displayName":"Neeharika Gupta","userId":"13434636909198909500"}},"outputId":"d84ab200-6ea5-4098-978f-51aa342547f7"},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                         API Component  \\\n","0                           tensorflow   \n","1                     tensorflow.keras   \n","2                           Sequential   \n","3                                Dense   \n","4                                Input   \n","5                          Concatenate   \n","6             fetch_california_housing   \n","7                  make_classification   \n","8                     train_test_split   \n","9                       StandardScaler   \n","10                  LogisticRegression   \n","11                       make_pipeline   \n","12                                 PCA   \n","13                               torch   \n","14                            torch.nn   \n","15                         torch.optim   \n","16                         torchvision   \n","17              torchvision.transforms   \n","18                 torch.nn.functional   \n","19                             Dataset   \n","20                          DataLoader   \n","21                torchvision.datasets   \n","22                            ToTensor   \n","23                               numpy   \n","24                              pandas   \n","25                   matplotlib.pyplot   \n","26                       AutoTokenizer   \n","27             DataCollatorWithPadding   \n","28  AutoModelForSequenceClassification   \n","29                   TrainingArguments   \n","30                             Trainer   \n","31                      notebook_login   \n","32                        load_dataset   \n","\n","                                        Documentation  \\\n","0   Top-level module of TensorFlow. By convention,...   \n","1   DO NOT EDIT.  This file was autogenerated. Do ...   \n","2   `Sequential` groups a linear stack of layers i...   \n","3   Just your regular densely-connected NN layer. ...   \n","4   Used to instantiate a Keras tensor.  A Keras t...   \n","5   Concatenates a list of inputs.  It takes as in...   \n","6   Load the California housing dataset (regressio...   \n","7   Generate a random n-class classification probl...   \n","8   Split arrays or matrices into random train and...   \n","9   Standardize features by removing the mean and ...   \n","10  Logistic Regression (aka logit, MaxEnt) classi...   \n","11  Construct a :class:`Pipeline` from the given e...   \n","12  Principal component analysis (PCA).  Linear di...   \n","13  The torch package contains data structures for...   \n","14                            No documentation found.   \n","15  :mod:`torch.optim` is a package implementing v...   \n","16                            No documentation found.   \n","17                            No documentation found.   \n","18                              Functional interface.   \n","19  An abstract class representing a :class:`Datas...   \n","20  Data loader combines a dataset and a sampler, ...   \n","21                            No documentation found.   \n","22  Convert a PIL Image or ndarray to tensor and s...   \n","23  NumPy =====  Provides   1. An array object of ...   \n","24  pandas - a powerful data analysis and manipula...   \n","25  `matplotlib.pyplot` is a state-based interface...   \n","26  This is a generic tokenizer class that will be...   \n","27  Data collator that will dynamically pad the in...   \n","28  This is a generic model class that will be ins...   \n","29  TrainingArguments is the subset of the argumen...   \n","30  Trainer is a simple but feature-complete train...   \n","31  Displays a widget to log in to the HF website ...   \n","32  Load a dataset from the Hugging Face Hub, or a...   \n","\n","                                   api_extract_gemini  \\\n","0   ```json\\n{\\n  \"module\": {\\n    \"name\": \"tf\",\\n...   \n","1   ```json\\n{\\n  \"types\": {\\n    \"SomeType\": {\\n ...   \n","2   ```json\\n{\\n  \"description\": \"API representati...   \n","3   ```json\\n{\\n  \"name\": \"Dense\",\\n  \"type\": \"Lay...   \n","4   ```json\\n{\\n  \"types\": [\\n    {\\n      \"name\":...   \n","5   ```json\\n{\\n  \"name\": \"Concatenate\",\\n  \"type\"...   \n","6   ```json\\n{\\n  \"module\": \"sklearn.datasets\",\\n ...   \n","7   ```json\\n{\\n  \"function\": {\\n    \"name\": \"make...   \n","8   ```json\\n{\\n  \"function\": {\\n    \"name\": \"trai...   \n","9   ```json\\n{\\n  \"classes\": [\\n    {\\n      \"name...   \n","10  ```json\\n{\\n  \"name\": \"LogisticRegression\",\\n ...   \n","11  ```json\\n{\\n  \"api_element\": {\\n    \"type\": \"f...   \n","12  ```json\\n{\\n  \"class\": \"PCA\",\\n  \"description\"...   \n","13  ```json\\n{\\n  \"package\": {\\n    \"name\": \"torch...   \n","14  Based on the input provided (\"No documentation...   \n","15  ```json\\n{\\n  \"package\": {\\n    \"name\": \"torch...   \n","16  ```json\\n{\\n  \"status\": \"error\",\\n  \"message\":...   \n","17  Based on the input \"No documentation found,\" I...   \n","18  ```json\\n{\\n  \"api_description\": \"Structured r...   \n","19  ```json\\n{\\n  \"types\": [\\n    {\\n      \"name\":...   \n","20  ```json\\n{\\n  \"api_description\": \"Data loader ...   \n","21  ```json\\n{\\n  \"types\": [],\\n  \"functions\": [],...   \n","22  ```json\\n{\\n  \"api_component\": \"Transform\",\\n ...   \n","23  ```json\\n{\\n  \"name\": \"NumPy\",\\n  \"description...   \n","24  ```json\\n{\\n  \"library\": \"pandas\",\\n  \"descrip...   \n","25  ```json\\n{\\n  \"module\": {\\n    \"name\": \"matplo...   \n","26  ```json\\n{\\n  \"classes\": [\\n    {\\n      \"name...   \n","27  ```json\\n[\\n  {\\n    \"name\": \"DataCollatorWith...   \n","28  ```json\\n{\\n  \"types\": [\\n    {\\n      \"name\":...   \n","29  ```json\\n{\\n  \"class_name\": \"TrainingArguments...   \n","30  ```json\\n{\\n  \"api\": {\\n    \"name\": \"Hugging F...   \n","31  ```json\\n{\\n  \"api\": {\\n    \"name\": \"Hugging F...   \n","32  ```json\\n{\\n  \"name\": \"load_dataset\",\\n  \"desc...   \n","\n","                                      api_extract_gpt  \n","0   ```json\\n{\\n  \"module\": \"tensorflow\",\\n  \"alia...  \n","1   Of course! Please provide the API documentatio...  \n","2   Certainly! Here's a structured representation ...  \n","3   Here is a structured representation of the API...  \n","4   To extract the structured representation of th...  \n","5   ```json\\n{\\n  \"API\": \"Concatenate\",\\n  \"Type\":...  \n","6   Here's a structured representation of the API ...  \n","7   Here's a structured representation of the API ...  \n","8   Below is the structured representation of the ...  \n","9   ```json\\n{\\n  \"class\": \"StandardScaler\",\\n  \"d...  \n","10  ```json\\n{\\n  \"class\": \"LogisticRegression\",\\n...  \n","11  Here is the structured representation of the A...  \n","12  Here is a structured representation of the giv...  \n","13  Based on the provided description of the `torc...  \n","14  Unfortunately, without any documentation or de...  \n","15  To extract a structured representation for the...  \n","16  To create a structured representation of an AP...  \n","17  To create a structured representation of an AP...  \n","18  To create a structured representation of the A...  \n","19  ```json\\n{\\n  \"class\": \"Dataset\",\\n  \"descript...  \n","20  Here's a structured representation of the API ...  \n","21  Since there is no documentation provided, it's...  \n","22  ```json\\n{\\n  \"API\": {\\n    \"name\": \"ConvertTo...  \n","23  Here's a structured representation of the NumP...  \n","24  Certainly! Below is a structured representatio...  \n","25  ```json\\n{\\n  \"module\": \"matplotlib.pyplot\",\\n...  \n","26  Certainly! Based on the provided API documenta...  \n","27  ```json\\n{\\n  \"DataCollator\": {\\n    \"descript...  \n","28  Based on the provided API documentation, the s...  \n","29  Below is a structured JSON representation of t...  \n","30  Here is a structured representation of the API...  \n","31  ```json\\n{\\n  \"API\": {\\n    \"function\": \"noteb...  \n","32  Below is a structured JSON representation of t...  "],"text/html":["\n","  <div id=\"df-43d2b119-0c38-4a90-8cc2-17b6d0a6ec70\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>API Component</th>\n","      <th>Documentation</th>\n","      <th>api_extract_gemini</th>\n","      <th>api_extract_gpt</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>tensorflow</td>\n","      <td>Top-level module of TensorFlow. By convention,...</td>\n","      <td>```json\\n{\\n  \"module\": {\\n    \"name\": \"tf\",\\n...</td>\n","      <td>```json\\n{\\n  \"module\": \"tensorflow\",\\n  \"alia...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>tensorflow.keras</td>\n","      <td>DO NOT EDIT.  This file was autogenerated. Do ...</td>\n","      <td>```json\\n{\\n  \"types\": {\\n    \"SomeType\": {\\n ...</td>\n","      <td>Of course! Please provide the API documentatio...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Sequential</td>\n","      <td>`Sequential` groups a linear stack of layers i...</td>\n","      <td>```json\\n{\\n  \"description\": \"API representati...</td>\n","      <td>Certainly! Here's a structured representation ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Dense</td>\n","      <td>Just your regular densely-connected NN layer. ...</td>\n","      <td>```json\\n{\\n  \"name\": \"Dense\",\\n  \"type\": \"Lay...</td>\n","      <td>Here is a structured representation of the API...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Input</td>\n","      <td>Used to instantiate a Keras tensor.  A Keras t...</td>\n","      <td>```json\\n{\\n  \"types\": [\\n    {\\n      \"name\":...</td>\n","      <td>To extract the structured representation of th...</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>Concatenate</td>\n","      <td>Concatenates a list of inputs.  It takes as in...</td>\n","      <td>```json\\n{\\n  \"name\": \"Concatenate\",\\n  \"type\"...</td>\n","      <td>```json\\n{\\n  \"API\": \"Concatenate\",\\n  \"Type\":...</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>fetch_california_housing</td>\n","      <td>Load the California housing dataset (regressio...</td>\n","      <td>```json\\n{\\n  \"module\": \"sklearn.datasets\",\\n ...</td>\n","      <td>Here's a structured representation of the API ...</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>make_classification</td>\n","      <td>Generate a random n-class classification probl...</td>\n","      <td>```json\\n{\\n  \"function\": {\\n    \"name\": \"make...</td>\n","      <td>Here's a structured representation of the API ...</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>train_test_split</td>\n","      <td>Split arrays or matrices into random train and...</td>\n","      <td>```json\\n{\\n  \"function\": {\\n    \"name\": \"trai...</td>\n","      <td>Below is the structured representation of the ...</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>StandardScaler</td>\n","      <td>Standardize features by removing the mean and ...</td>\n","      <td>```json\\n{\\n  \"classes\": [\\n    {\\n      \"name...</td>\n","      <td>```json\\n{\\n  \"class\": \"StandardScaler\",\\n  \"d...</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>LogisticRegression</td>\n","      <td>Logistic Regression (aka logit, MaxEnt) classi...</td>\n","      <td>```json\\n{\\n  \"name\": \"LogisticRegression\",\\n ...</td>\n","      <td>```json\\n{\\n  \"class\": \"LogisticRegression\",\\n...</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>make_pipeline</td>\n","      <td>Construct a :class:`Pipeline` from the given e...</td>\n","      <td>```json\\n{\\n  \"api_element\": {\\n    \"type\": \"f...</td>\n","      <td>Here is the structured representation of the A...</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>PCA</td>\n","      <td>Principal component analysis (PCA).  Linear di...</td>\n","      <td>```json\\n{\\n  \"class\": \"PCA\",\\n  \"description\"...</td>\n","      <td>Here is a structured representation of the giv...</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>torch</td>\n","      <td>The torch package contains data structures for...</td>\n","      <td>```json\\n{\\n  \"package\": {\\n    \"name\": \"torch...</td>\n","      <td>Based on the provided description of the `torc...</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>torch.nn</td>\n","      <td>No documentation found.</td>\n","      <td>Based on the input provided (\"No documentation...</td>\n","      <td>Unfortunately, without any documentation or de...</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>torch.optim</td>\n","      <td>:mod:`torch.optim` is a package implementing v...</td>\n","      <td>```json\\n{\\n  \"package\": {\\n    \"name\": \"torch...</td>\n","      <td>To extract a structured representation for the...</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>torchvision</td>\n","      <td>No documentation found.</td>\n","      <td>```json\\n{\\n  \"status\": \"error\",\\n  \"message\":...</td>\n","      <td>To create a structured representation of an AP...</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>torchvision.transforms</td>\n","      <td>No documentation found.</td>\n","      <td>Based on the input \"No documentation found,\" I...</td>\n","      <td>To create a structured representation of an AP...</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>torch.nn.functional</td>\n","      <td>Functional interface.</td>\n","      <td>```json\\n{\\n  \"api_description\": \"Structured r...</td>\n","      <td>To create a structured representation of the A...</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>Dataset</td>\n","      <td>An abstract class representing a :class:`Datas...</td>\n","      <td>```json\\n{\\n  \"types\": [\\n    {\\n      \"name\":...</td>\n","      <td>```json\\n{\\n  \"class\": \"Dataset\",\\n  \"descript...</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>DataLoader</td>\n","      <td>Data loader combines a dataset and a sampler, ...</td>\n","      <td>```json\\n{\\n  \"api_description\": \"Data loader ...</td>\n","      <td>Here's a structured representation of the API ...</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>torchvision.datasets</td>\n","      <td>No documentation found.</td>\n","      <td>```json\\n{\\n  \"types\": [],\\n  \"functions\": [],...</td>\n","      <td>Since there is no documentation provided, it's...</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>ToTensor</td>\n","      <td>Convert a PIL Image or ndarray to tensor and s...</td>\n","      <td>```json\\n{\\n  \"api_component\": \"Transform\",\\n ...</td>\n","      <td>```json\\n{\\n  \"API\": {\\n    \"name\": \"ConvertTo...</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>numpy</td>\n","      <td>NumPy =====  Provides   1. An array object of ...</td>\n","      <td>```json\\n{\\n  \"name\": \"NumPy\",\\n  \"description...</td>\n","      <td>Here's a structured representation of the NumP...</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>pandas</td>\n","      <td>pandas - a powerful data analysis and manipula...</td>\n","      <td>```json\\n{\\n  \"library\": \"pandas\",\\n  \"descrip...</td>\n","      <td>Certainly! Below is a structured representatio...</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>matplotlib.pyplot</td>\n","      <td>`matplotlib.pyplot` is a state-based interface...</td>\n","      <td>```json\\n{\\n  \"module\": {\\n    \"name\": \"matplo...</td>\n","      <td>```json\\n{\\n  \"module\": \"matplotlib.pyplot\",\\n...</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>AutoTokenizer</td>\n","      <td>This is a generic tokenizer class that will be...</td>\n","      <td>```json\\n{\\n  \"classes\": [\\n    {\\n      \"name...</td>\n","      <td>Certainly! Based on the provided API documenta...</td>\n","    </tr>\n","    <tr>\n","      <th>27</th>\n","      <td>DataCollatorWithPadding</td>\n","      <td>Data collator that will dynamically pad the in...</td>\n","      <td>```json\\n[\\n  {\\n    \"name\": \"DataCollatorWith...</td>\n","      <td>```json\\n{\\n  \"DataCollator\": {\\n    \"descript...</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>AutoModelForSequenceClassification</td>\n","      <td>This is a generic model class that will be ins...</td>\n","      <td>```json\\n{\\n  \"types\": [\\n    {\\n      \"name\":...</td>\n","      <td>Based on the provided API documentation, the s...</td>\n","    </tr>\n","    <tr>\n","      <th>29</th>\n","      <td>TrainingArguments</td>\n","      <td>TrainingArguments is the subset of the argumen...</td>\n","      <td>```json\\n{\\n  \"class_name\": \"TrainingArguments...</td>\n","      <td>Below is a structured JSON representation of t...</td>\n","    </tr>\n","    <tr>\n","      <th>30</th>\n","      <td>Trainer</td>\n","      <td>Trainer is a simple but feature-complete train...</td>\n","      <td>```json\\n{\\n  \"api\": {\\n    \"name\": \"Hugging F...</td>\n","      <td>Here is a structured representation of the API...</td>\n","    </tr>\n","    <tr>\n","      <th>31</th>\n","      <td>notebook_login</td>\n","      <td>Displays a widget to log in to the HF website ...</td>\n","      <td>```json\\n{\\n  \"api\": {\\n    \"name\": \"Hugging F...</td>\n","      <td>```json\\n{\\n  \"API\": {\\n    \"function\": \"noteb...</td>\n","    </tr>\n","    <tr>\n","      <th>32</th>\n","      <td>load_dataset</td>\n","      <td>Load a dataset from the Hugging Face Hub, or a...</td>\n","      <td>```json\\n{\\n  \"name\": \"load_dataset\",\\n  \"desc...</td>\n","      <td>Below is a structured JSON representation of t...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-43d2b119-0c38-4a90-8cc2-17b6d0a6ec70')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-43d2b119-0c38-4a90-8cc2-17b6d0a6ec70 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-43d2b119-0c38-4a90-8cc2-17b6d0a6ec70');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    <div id=\"df-affb188e-d883-4822-b8d8-f8c60f26b3a6\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-affb188e-d883-4822-b8d8-f8c60f26b3a6')\"\n","                title=\"Suggest charts\"\n","                style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","      <script>\n","        async function quickchart(key) {\n","          const quickchartButtonEl =\n","            document.querySelector('#' + key + ' button');\n","          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","          quickchartButtonEl.classList.add('colab-df-spinner');\n","          try {\n","            const charts = await google.colab.kernel.invokeFunction(\n","                'suggestCharts', [key], {});\n","          } catch (error) {\n","            console.error('Error during call to suggestCharts:', error);\n","          }\n","          quickchartButtonEl.classList.remove('colab-df-spinner');\n","          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","        }\n","        (() => {\n","          let quickchartButtonEl =\n","            document.querySelector('#df-affb188e-d883-4822-b8d8-f8c60f26b3a6 button');\n","          quickchartButtonEl.style.display =\n","            google.colab.kernel.accessAllowed ? 'block' : 'none';\n","        })();\n","      </script>\n","    </div>\n","\n","  <div id=\"id_9770aa7a-ed7c-4192-a563-eb435738c480\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_api')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_9770aa7a-ed7c-4192-a563-eb435738c480 button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('df_api');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"df_api","summary":"{\n  \"name\": \"df_api\",\n  \"rows\": 33,\n  \"fields\": [\n    {\n      \"column\": \"API Component\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 33,\n        \"samples\": [\n          \"notebook_login\",\n          \"torch.optim\",\n          \"AutoTokenizer\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Documentation\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 30,\n        \"samples\": [\n          \"Trainer is a simple but feature-complete training and eval loop for PyTorch, optimized for \\ud83e\\udd17 Transformers.  Args:     model ([`PreTrainedModel`] or `torch.nn.Module`, *optional*):         The model to train, evaluate or use for predictions. If not provided, a `model_init` must be passed.          <Tip>          [`Trainer`] is optimized to work with the [`PreTrainedModel`] provided by the library. You can still use         your own models defined as `torch.nn.Module` as long as they work the same way as the \\ud83e\\udd17 Transformers         models.          </Tip>      args ([`TrainingArguments`], *optional*):         The arguments to tweak for training. Will default to a basic instance of [`TrainingArguments`] with the         `output_dir` set to a directory named *tmp_trainer* in the current directory if not provided.     data_collator (`DataCollator`, *optional*):         The function to use to form a batch from a list of elements of `train_dataset` or `eval_dataset`. Will         default to [`default_data_collator`] if no `processing_class` is provided, an instance of         [`DataCollatorWithPadding`] otherwise if the processing_class is a feature extractor or tokenizer.     train_dataset (Union[`torch.utils.data.Dataset`, `torch.utils.data.IterableDataset`, `datasets.Dataset`], *optional*):         The dataset to use for training. If it is a [`~datasets.Dataset`], columns not accepted by the         `model.forward()` method are automatically removed.          Note that if it's a `torch.utils.data.IterableDataset` with some randomization and you are training in a         distributed fashion, your iterable dataset should either use a internal attribute `generator` that is a         `torch.Generator` for the randomization that must be identical on all processes (and the Trainer will         manually set the seed of this `generator` at each epoch) or have a `set_epoch()` method that internally         sets the seed of the RNGs used.     eval_dataset (Union[`torch.utils.data.Dataset`, Dict[str, `torch.utils.data.Dataset`, `datasets.Dataset`]), *optional*):          The dataset to use for evaluation. If it is a [`~datasets.Dataset`], columns not accepted by the          `model.forward()` method are automatically removed. If it is a dictionary, it will evaluate on each          dataset prepending the dictionary key to the metric name.     processing_class (`PreTrainedTokenizerBase` or `BaseImageProcessor` or `FeatureExtractionMixin` or `ProcessorMixin`, *optional*):         Processing class used to process the data. If provided, will be used to automatically process the inputs         for the model, and it will be saved along the model to make it easier to rerun an interrupted training or         reuse the fine-tuned model.         This supersedes the `tokenizer` argument, which is now deprecated.     model_init (`Callable[[], PreTrainedModel]`, *optional*):         A function that instantiates the model to be used. If provided, each call to [`~Trainer.train`] will start         from a new instance of the model as given by this function.          The function may have zero argument, or a single one containing the optuna/Ray Tune/SigOpt trial object, to         be able to choose different architectures according to hyper parameters (such as layer count, sizes of         inner layers, dropout probabilities etc).     compute_loss_func (`Callable`, *optional*):         A function that accepts the raw model outputs, labels, and the number of items in the entire accumulated         batch (batch_size * gradient_accumulation_steps) and returns the loss. For example, see the default [loss function](https://github.com/huggingface/transformers/blob/052e652d6d53c2b26ffde87e039b723949a53493/src/transformers/trainer.py#L3618) used by [`Trainer`].     compute_metrics (`Callable[[EvalPrediction], Dict]`, *optional*):         The function that will be used to compute metrics at evaluation. Must take a [`EvalPrediction`] and return         a dictionary string to metric values. *Note* When passing TrainingArgs with `batch_eval_metrics` set to         `True`, your compute_metrics function must take a boolean `compute_result` argument. This will be triggered         after the last eval batch to signal that the function needs to calculate and return the global summary         statistics rather than accumulating the batch-level statistics     callbacks (List of [`TrainerCallback`], *optional*):         A list of callbacks to customize the training loop. Will add those to the list of default callbacks         detailed in [here](callback).          If you want to remove one of the default callbacks used, use the [`Trainer.remove_callback`] method.     optimizers (`Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR]`, *optional*, defaults to `(None, None)`):         A tuple containing the optimizer and the scheduler to use. Will default to an instance of [`AdamW`] on your         model and a scheduler given by [`get_linear_schedule_with_warmup`] controlled by `args`.     optimizer_cls_and_kwargs (`Tuple[Type[torch.optim.Optimizer], Dict[str, Any]]`, *optional*):         A tuple containing the optimizer class and keyword arguments to use.         Overrides `optim` and `optim_args` in `args`. Incompatible with the `optimizers` argument.          Unlike `optimizers`, this argument avoids the need to place model parameters on the correct devices before initializing the Trainer.     preprocess_logits_for_metrics (`Callable[[torch.Tensor, torch.Tensor], torch.Tensor]`, *optional*):         A function that preprocess the logits right before caching them at each evaluation step. Must take two         tensors, the logits and the labels, and return the logits once processed as desired. The modifications made         by this function will be reflected in the predictions received by `compute_metrics`.          Note that the labels (second parameter) will be `None` if the dataset does not have them.  Important attributes:      - **model** -- Always points to the core model. If using a transformers model, it will be a [`PreTrainedModel`]       subclass.     - **model_wrapped** -- Always points to the most external model in case one or more other modules wrap the       original model. This is the model that should be used for the forward pass. For example, under `DeepSpeed`,       the inner model is wrapped in `DeepSpeed` and then again in `torch.nn.DistributedDataParallel`. If the inner       model hasn't been wrapped, then `self.model_wrapped` is the same as `self.model`.     - **is_model_parallel** -- Whether or not a model has been switched to a model parallel mode (different from       data parallelism, this means some of the model layers are split on different GPUs).     - **place_model_on_device** -- Whether or not to automatically place the model on the device - it will be set       to `False` if model parallel or deepspeed is used, or if the default       `TrainingArguments.place_model_on_device` is overridden to return `False` .     - **is_in_train** -- Whether or not a model is currently running `train` (e.g. when `evaluate` is called while       in `train`)\",\n          \":mod:`torch.optim` is a package implementing various optimization algorithms.  Most commonly used methods are already supported, and the interface is general enough, so that more sophisticated ones can also be easily integrated in the future.\",\n          \"This is a generic tokenizer class that will be instantiated as one of the tokenizer classes of the library when created with the [`AutoTokenizer.from_pretrained`] class method.  This class cannot be instantiated directly using `__init__()` (throws an error).\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"api_extract_gemini\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 33,\n        \"samples\": [\n          \"```json\\n{\\n  \\\"api\\\": {\\n    \\\"name\\\": \\\"Hugging Face Login API (Partial)\\\",\\n    \\\"description\\\": \\\"API functions related to user authentication and token management.\\\",\\n    \\\"functions\\\": [\\n      {\\n        \\\"name\\\": \\\"notebook_login\\\",\\n        \\\"description\\\": \\\"Displays a widget to log in to the HF website and store the token. Equivalent to `login` without passing a token when run in a notebook. Useful if you want to force the use of the notebook widget instead of a prompt in the terminal. For more details, see `login`.\\\",\\n        \\\"parameters\\\": [\\n          {\\n            \\\"name\\\": \\\"new_session\\\",\\n            \\\"type\\\": \\\"bool\\\",\\n            \\\"description\\\": \\\"If `True`, will request a token even if one is already saved on the machine.\\\",\\n            \\\"default_value\\\": true,\\n            \\\"required\\\": false\\n          },\\n          {\\n            \\\"name\\\": \\\"write_permission\\\",\\n            \\\"type\\\": \\\"bool\\\",\\n            \\\"description\\\": \\\"Ignored and deprecated argument.\\\",\\n            \\\"required\\\": false,\\n            \\\"notes\\\": [\\\"ignored\\\", \\\"deprecated\\\"]\\n          }\\n        ],\\n        \\\"returns\\\": null,\\n        \\\"relationships\\\": [\\n          {\\n            \\\"type\\\": \\\"related_to\\\",\\n            \\\"target\\\": \\\"login\\\",\\n            \\\"description\\\": \\\"Equivalent to `login` without a token in a notebook context, and provides a widget UI. See `login` for more details.\\\"\\n          }\\n        ],\\n        \\\"notes\\\": \\\"Specifically designed for notebook environments to provide a GUI login experience.\\\"\\n      }\\n    ],\\n    \\\"types\\\": [\\n      {\\n        \\\"name\\\": \\\"bool\\\",\\n        \\\"description\\\": \\\"A boolean value, either true or false.\\\"\\n      }\\n      // The type of the \\\"token\\\" is not explicitly defined as an input/output type\\n      // for this specific function's signature, so it's not listed here.\\n    ]\\n  }\\n}\\n```\",\n          \"```json\\n{\\n  \\\"package\\\": {\\n    \\\"name\\\": \\\"torch.optim\\\",\\n    \\\"description\\\": \\\"A package implementing various optimization algorithms. Most commonly used methods are already supported, and the interface is general enough, so that more sophisticated ones can also be easily integrated in the future.\\\",\\n    \\\"contents\\\": {\\n      \\\"types\\\": [\\n        {\\n          \\\"category\\\": \\\"Optimization Algorithm\\\",\\n          \\\"description\\\": \\\"Classes that implement specific optimization algorithms (e.g., SGD, Adam). The exact names and specific details of these types are not provided in the source documentation snippet.\\\",\\n          \\\"nature\\\": \\\"Class (inferred, as optimizers typically maintain state and have methods like step())\\\",\\n          \\\"count\\\": \\\"Multiple (implied by 'various', 'most commonly used')\\\"\\n        }\\n      ],\\n      \\\"functions\\\": [\\n        {\\n          \\\"category\\\": \\\"Interface Functionality\\\",\\n          \\\"description\\\": \\\"While specific functions within the package namespace are not named, the description implies a general interface, which would involve methods on the optimizer objects (e.g., step(), zero_grad()). The package itself might expose utility functions, but none are specified.\\\",\\n          \\\"details_available\\\": false\\n        }\\n      ]\\n    },\\n    \\\"relationships\\\": [\\n      {\\n        \\\"type\\\": \\\"Adherence to Interface\\\",\\n        \\\"description\\\": \\\"The optimization algorithm types/classes within this package adhere to a general interface.\\\",\\n        \\\"scope\\\": \\\"Internal package components (optimizer types)\\\",\\n        \\\"purpose\\\": \\\"Ensures compatibility, ease of use, and allows for integration of new optimizers.\\\"\\n      },\\n      {\\n        \\\"type\\\": \\\"Implementation Provider\\\",\\n        \\\"description\\\": \\\"The package provides concrete implementations of optimization algorithms.\\\",\\n        \\\"from\\\": \\\"torch.optim (package)\\\",\\n        \\\"to\\\": \\\"Optimization algorithms (conceptual/types)\\\"\\n      }\\n    ]\\n  }\\n}\\n```\",\n          \"```json\\n{\\n  \\\"classes\\\": [\\n    {\\n      \\\"name\\\": \\\"AutoTokenizer\\\",\\n      \\\"description\\\": \\\"A generic tokenizer class that will be instantiated as one of the specific tokenizer classes from the library.\\\",\\n      \\\"instantiation_notes\\\": \\\"Cannot be instantiated directly using `__init__()` (throws an error). Must be created using the `from_pretrained` class method.\\\",\\n      \\\"methods\\\": [\\n        {\\n          \\\"name\\\": \\\"from_pretrained\\\",\\n          \\\"type\\\": \\\"class method\\\",\\n          \\\"description\\\": \\\"Method used to instantiate the generic AutoTokenizer as a specific tokenizer class.\\\"\\n        }\\n      ]\\n    }\\n  ],\\n  \\\"types\\\": [],\\n  \\\"functions\\\": []\\n}\\n```\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"api_extract_gpt\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 33,\n        \"samples\": [\n          \"```json\\n{\\n  \\\"API\\\": {\\n    \\\"function\\\": \\\"notebook_login\\\",\\n    \\\"description\\\": \\\"Displays a widget to log in to the HF website and store the token. Equivalent to the login function without passing a token when run in a notebook. Useful to force the use of the notebook widget instead of a prompt in the terminal.\\\",\\n    \\\"arguments\\\": {\\n      \\\"new_session\\\": {\\n        \\\"type\\\": \\\"bool\\\",\\n        \\\"default\\\": true,\\n        \\\"description\\\": \\\"If True, will request a token even if one is already saved on the machine.\\\"\\n      },\\n      \\\"write_permission\\\": {\\n        \\\"type\\\": \\\"bool\\\",\\n        \\\"description\\\": \\\"Ignored and deprecated argument.\\\"\\n      }\\n    },\\n    \\\"related_functions\\\": [\\\"login\\\"]\\n  }\\n}\\n```\",\n          \"To extract a structured representation for the `torch.optim` API described, we will focus on the main components and relationships specified in the documentation. Since only a brief description is provided, the structured representation will be based on typical content expected in such an API, with placeholder values indicative of the expected content structure. If more concrete content was provided, it would enable a more accurate extraction. For now, the representation is general.\\n\\n```json\\n{\\n  \\\"module\\\": \\\"torch.optim\\\",\\n  \\\"description\\\": \\\"A package implementing various optimization algorithms with a general interface.\\\",\\n  \\\"types\\\": [],\\n  \\\"functions\\\": [],\\n  \\\"classes\\\": [\\n    {\\n      \\\"name\\\": \\\"<Optimizer Name>\\\",\\n      \\\"description\\\": \\\"A class implementing a specific optimization algorithm.\\\",\\n      \\\"methods\\\": [],\\n      \\\"properties\\\": [],\\n      \\\"relationships\\\": []\\n    }\\n  ]\\n}\\n```\\n\\nThis structured representation outlines the basic structure based on the given brief description. Typically, this would be expanded to include actual optimizer names and descriptions along with methods, properties, and relationships, depending on the full content of the module.\",\n          \"Certainly! Based on the provided API documentation, here's a structured JSON representation of the API, capturing the relevant types, functions, and their relationships:\\n\\n```json\\n{\\n  \\\"classes\\\": [\\n    {\\n      \\\"name\\\": \\\"Tokenizer\\\",\\n      \\\"description\\\": \\\"A generic tokenizer class that cannot be instantiated directly.\\\",\\n      \\\"constructor\\\": {\\n        \\\"method\\\": \\\"__init__\\\",\\n        \\\"instantiable\\\": false,\\n        \\\"error_message\\\": \\\"This class cannot be instantiated directly using __init__()\\\"\\n      },\\n      \\\"methods\\\": [],\\n      \\\"instantiation\\\": {\\n        \\\"method\\\": \\\"AutoTokenizer.from_pretrained\\\",\\n        \\\"description\\\": \\\"Instantiates the tokenizer as one of the library's tokenizer classes.\\\"\\n      }\\n    }\\n  ],\\n  \\\"methods\\\": [\\n    {\\n      \\\"class\\\": \\\"AutoTokenizer\\\",\\n      \\\"name\\\": \\\"from_pretrained\\\",\\n      \\\"description\\\": \\\"Class method to create a tokenizer object\\\",\\n      \\\"type\\\": \\\"class_method\\\"\\n    }\\n  ]\\n}\\n```\\n\\nThis JSON structure captures the key concepts from the API specification: the `Tokenizer` class, the fact that it cannot be instantiated directly, and how it can be instantiated through a class method of `AutoTokenizer`.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["df_gpt_sym = pd.read_csv(\"/content/drive/MyDrive/Stanford NLP/symbolic_gpt.csv\")\n","df_gpt_sym"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"Tn8of7UwqQU3","executionInfo":{"status":"ok","timestamp":1748917170738,"user_tz":420,"elapsed":494,"user":{"displayName":"Neeharika Gupta","userId":"13434636909198909500"}},"outputId":"c24c8d0d-d5da-49f6-a9f2-78b9dc919647"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                         gpt_symbolic\n","0   Here are important rule-based checking constra...\n","1   To generate rule-based checking constraints fo...\n","2   Based on the given API documentation for the `...\n","3   Here are rule-based checking constraints for e...\n","4   Here are the rule-based checking constraints f...\n","5   To ensure the correct usage of the \"Concatenat...\n","6   Here are the rule-based checking constraints f...\n","7   Based on the detailed API structure provided f...\n","8   Here are the important rule-based checking con...\n","9   Here are the rule-based checking constraints f...\n","10  Here are the rule-based checking constraints f...\n","11  Based on the provided JSON representation of t...\n","12  Based on the provided JSON structure of the PC...\n","13  Based on the structured representation of the ...\n","14  Since you're specifically asking about `torch....\n","15  Based on the provided structured representatio...\n","16  Based on the given API structure for an imagin...\n","17  Let's consider developing rule-based constrain...\n","18  To create rule-based checking constraints for ...\n","19  Here are the rule-based checking constraints f...\n","20  Here are rule-based checking constraints for t...\n","21  To create rule-based checking constraints for ...\n","22  To ensure that the usage of the `ConvertToTens...\n","23  Based on the provided JSON structure of the Nu...\n","24  Based on the provided structure of the pandas ...\n","25  Here are the rule-based checking constraints f...\n","26  Here are the rule-based checking constraints f...\n","27  Here are the rule-based checking constraints f...\n","28  Based on the provided JSON representation of t...\n","29  To ensure that `TrainingArguments` is used cor...\n","30  Based on the provided API documentation for th...\n","31  Here are rule-based checking constraints for t...\n","32  When creating rule-based checking constraints ..."],"text/html":["\n","  <div id=\"df-afaf42ab-25f2-4ae6-a8fa-83e9d53d6267\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>gpt_symbolic</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Here are important rule-based checking constra...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>To generate rule-based checking constraints fo...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Based on the given API documentation for the `...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Here are rule-based checking constraints for e...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Here are the rule-based checking constraints f...</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>To ensure the correct usage of the \"Concatenat...</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>Here are the rule-based checking constraints f...</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>Based on the detailed API structure provided f...</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>Here are the important rule-based checking con...</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>Here are the rule-based checking constraints f...</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>Here are the rule-based checking constraints f...</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>Based on the provided JSON representation of t...</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>Based on the provided JSON structure of the PC...</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>Based on the structured representation of the ...</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>Since you're specifically asking about `torch....</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>Based on the provided structured representatio...</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>Based on the given API structure for an imagin...</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>Let's consider developing rule-based constrain...</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>To create rule-based checking constraints for ...</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>Here are the rule-based checking constraints f...</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>Here are rule-based checking constraints for t...</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>To create rule-based checking constraints for ...</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>To ensure that the usage of the `ConvertToTens...</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>Based on the provided JSON structure of the Nu...</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>Based on the provided structure of the pandas ...</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>Here are the rule-based checking constraints f...</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>Here are the rule-based checking constraints f...</td>\n","    </tr>\n","    <tr>\n","      <th>27</th>\n","      <td>Here are the rule-based checking constraints f...</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>Based on the provided JSON representation of t...</td>\n","    </tr>\n","    <tr>\n","      <th>29</th>\n","      <td>To ensure that `TrainingArguments` is used cor...</td>\n","    </tr>\n","    <tr>\n","      <th>30</th>\n","      <td>Based on the provided API documentation for th...</td>\n","    </tr>\n","    <tr>\n","      <th>31</th>\n","      <td>Here are rule-based checking constraints for t...</td>\n","    </tr>\n","    <tr>\n","      <th>32</th>\n","      <td>When creating rule-based checking constraints ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-afaf42ab-25f2-4ae6-a8fa-83e9d53d6267')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-afaf42ab-25f2-4ae6-a8fa-83e9d53d6267 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-afaf42ab-25f2-4ae6-a8fa-83e9d53d6267');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    <div id=\"df-e34f73eb-3fbc-4126-a66a-c9903dd93991\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e34f73eb-3fbc-4126-a66a-c9903dd93991')\"\n","                title=\"Suggest charts\"\n","                style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","      <script>\n","        async function quickchart(key) {\n","          const quickchartButtonEl =\n","            document.querySelector('#' + key + ' button');\n","          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","          quickchartButtonEl.classList.add('colab-df-spinner');\n","          try {\n","            const charts = await google.colab.kernel.invokeFunction(\n","                'suggestCharts', [key], {});\n","          } catch (error) {\n","            console.error('Error during call to suggestCharts:', error);\n","          }\n","          quickchartButtonEl.classList.remove('colab-df-spinner');\n","          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","        }\n","        (() => {\n","          let quickchartButtonEl =\n","            document.querySelector('#df-e34f73eb-3fbc-4126-a66a-c9903dd93991 button');\n","          quickchartButtonEl.style.display =\n","            google.colab.kernel.accessAllowed ? 'block' : 'none';\n","        })();\n","      </script>\n","    </div>\n","\n","  <div id=\"id_e614cfe6-a497-49e9-bfaa-fe54e4eff236\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_gpt_sym')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_e614cfe6-a497-49e9-bfaa-fe54e4eff236 button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('df_gpt_sym');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"df_gpt_sym","summary":"{\n  \"name\": \"df_gpt_sym\",\n  \"rows\": 33,\n  \"fields\": [\n    {\n      \"column\": \"gpt_symbolic\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 33,\n        \"samples\": [\n          \"Here are rule-based checking constraints for the `notebook_login` API component based on the provided structure:\\n\\n- **Function Name:**\\n  - Ensure the function name is `notebook_login`.\\n\\n- **Description:**\\n  - Verify that the function provides a mechanism to log in by displaying a widget.\\n  - Confirm that the purpose is equivalent to the `login` function but without requiring a token in a notebook context.\\n\\n- **Arguments:**\\n  - **`new_session`:**\\n    - Must be a boolean (`bool` type).\\n    - Default value should be `true`.\\n    - When `new_session` is `true`, it should request a token even if one is already stored.\\n  \\n  - **`write_permission`:**\\n    - Must be a boolean (`bool` type).\\n    - Should be flagged as ignored and deprecated. \\n\\n- **Related Functions:**\\n  - Confirm that the function is related to `login` and its behavior should align with the expectations set in the related `login` function, specifically in a notebook environment.\\n\\n- **General Constraints:**\\n  - Validate that no additional unexpected arguments are provided to the API.\\n  - Ensure the intended use of the `notebook_login` is within a notebook, intending to prefer the widget over a terminal prompt.\",\n          \"Based on the provided structured representation for the `torch.optim` API, where we have mainly a placeholder for a class representing an optimizer, here are some key constraints one might check for the components of this module:\\n\\n### General Constraints for `torch.optim` Module\\n- Ensure that `torch.optim` is a valid module in the PyTorch library.\\n- Verify that `torch.optim` contains classes, specifically optimizer implementations.\\n\\n### Constraints for Optimizer Classes\\n- **Naming**\\n  - Each optimizer class should have a descriptive and unique name.\\n  - Class names should follow the CamelCase convention.\\n  \\n- **Description**\\n  - Each optimizer class should have a clear and concise description explaining the optimization algorithm it implements.\\n\\n- **Constructor**\\n  - Each optimizer class should have a constructor method (`__init__`).\\n  - The constructor should require `params` (learnable parameters) as a mandatory argument.\\n  - Common hyperparameters such as `lr` (learning rate) should be available and correctly initialized.\\n\\n- **Methods**\\n  - Each optimizer class must implement the `step()` method to update parameters.\\n  - A `zero_grad()` method should be available to clear old gradients, if applicable.\\n\\n- **Properties**\\n  - Common properties like `defaults` should be checked in each optimizer class.\\n  - Ensure presence of any other algorithm-specific properties required for configuration.\\n\\n- **Inheritance and Relationships**\\n  - All optimizer classes should inherit from a common base optimizer class, typically `torch.optim.Optimizer`.\\n  - Check any module-specific relationships relevant to the optimizer's design.\\n\\n### Module Specific Constraints\\n- **Integration**\\n  - Optimizer classes should properly integrate with the broader PyTorch ecosystem, i.e., compatible with `torch.nn.Module` for model parameters.\\n  \\n- **Documentation**\\n  - All components, including classes, methods, and properties, must be documented with appropriate docstrings explaining usage, parameters, and return types.\\n\\n- **Compatibility**\\n  - Ensure backward compatibility for existing APIs if newer versions introduce changes to any optimizer class.\\n\\nThese constraints are essential for maintaining consistency, functionality, and usability within the `torch.optim` module, and they help validate both the implementation and user understanding of the components within the API.\",\n          \"Here are the rule-based checking constraints for the `AutoTokenizer` API component, based on the given JSON structure:\\n\\n- **Tokenizer Class Constraints:**\\n  - The `Tokenizer` class is abstract and cannot be instantiated directly. Attempting to do so should check for and raise an error with the message \\\"This class cannot be instantiated directly using __init__()\\\".\\n\\n- **AutoTokenizer Instantiation Constraints:**\\n  - The `from_pretrained` method of the `AutoTokenizer` class is the correct way to create an instance of a tokenizer.\\n  - Ensure that `from_pretrained` is used as a class method, not an instance method.\\n  - Confirm that `from_pretrained` returns a valid tokenizer object, which is an instance of one of the library's tokenizer classes but not the abstract `Tokenizer` class itself.\\n\\n- **General Method Constraints:**\\n  - When invoking `from_pretrained`, verify the necessary parameters are provided if required (e.g., model identifier or path).\\n  - Check for any exceptions or errors that `from_pretrained` might need to handle, such as missing or incorrect model identifiers.\\n  \\nThese constraints ensure that the `AutoTokenizer` API is used correctly and as intended by the design.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["api_dict = dict(zip(df_api['API Component'], df_gpt_sym['gpt_symbolic']))\n","api_dict.keys()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BN17ZOpFqIKL","executionInfo":{"status":"ok","timestamp":1748917183363,"user_tz":420,"elapsed":6,"user":{"displayName":"Neeharika Gupta","userId":"13434636909198909500"}},"outputId":"77e52644-db47-4cd0-d8da-88a8710590fd"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["dict_keys(['tensorflow', 'tensorflow.keras', 'Sequential', 'Dense', 'Input', 'Concatenate', 'fetch_california_housing', 'make_classification', 'train_test_split', 'StandardScaler', 'LogisticRegression', 'make_pipeline', 'PCA', 'torch', 'torch.nn', 'torch.optim', 'torchvision', 'torchvision.transforms', 'torch.nn.functional', 'Dataset', 'DataLoader', 'torchvision.datasets', 'ToTensor', 'numpy', 'pandas', 'matplotlib.pyplot', 'AutoTokenizer', 'DataCollatorWithPadding', 'AutoModelForSequenceClassification', 'TrainingArguments', 'Trainer', 'notebook_login', 'load_dataset'])"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["api_key = \"AIzaSyCqwq4vI9eSbtrjaqPl9A8SbTp6mIQgkM4\"\n","genai.configure(api_key=api_key)\n","model = genai.GenerativeModel('models/gemini-2.5-flash-preview-04-17')"],"metadata":{"id":"HwiFdq_kc5Yc","executionInfo":{"status":"ok","timestamp":1748917235373,"user_tz":420,"elapsed":1,"user":{"displayName":"Neeharika Gupta","userId":"13434636909198909500"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["REQUESTS_PER_MINUTE = 10\n","DELAY_BETWEEN_REQUESTS = 60 / REQUESTS_PER_MINUTE\n","\n","def make_request(prompt, model, model_name=model):\n","    response = model.generate_content(prompt)\n","    return response.text"],"metadata":{"id":"qygU4u92IEpW","executionInfo":{"status":"ok","timestamp":1748917235533,"user_tz":420,"elapsed":5,"user":{"displayName":"Neeharika Gupta","userId":"13434636909198909500"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["output = []\n","\n","for index, row in df_neural.iterrows():\n","    print(f\"Processing row {index+1}/{len(df)}...\")\n","\n","    try:\n","        prompt = (\n","            f\"Given the task and its code:\\n\"\n","            f\"{tasks[index]}\\n\"\n","            f\"{df_neural['neural_gpt']}\\n\\n\"\n","            f\" identify the APIs being used based on the task. Then, refer to the extracted documentation data provided below to verify their usage. If you encounter any inconsistencies or constraint violations, report them in a single line. If no issues are found, simply reply with 'perfect'.\\n\"\n","            f\"{api_dict}\\n\\n\"\n","        )\n","\n","        response = make_request(prompt,model)\n","    except Exception as e:\n","        print(f\"Error generating response: {e}\")\n","        response = f\"Error: {e}\"\n","\n","    output.append(response)\n","\n","    time.sleep(DELAY_BETWEEN_REQUESTS)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":160},"id":"RCAXwC_oly8N","executionInfo":{"status":"ok","timestamp":1748917418857,"user_tz":420,"elapsed":127133,"user":{"displayName":"Neeharika Gupta","userId":"13434636909198909500"}},"outputId":"0ef83cca-7aa2-46b4-dd8d-9757f30044fa"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Processing row 1/8...\n","Processing row 2/8...\n","Processing row 3/8...\n","Processing row 4/8...\n","Processing row 5/8...\n","Processing row 6/8...\n","Processing row 7/8...\n","Processing row 8/8...\n"]}]},{"cell_type":"code","source":["output"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TooeJhYmmfNp","executionInfo":{"status":"ok","timestamp":1748917423032,"user_tz":420,"elapsed":5,"user":{"displayName":"Neeharika Gupta","userId":"13434636909198909500"}},"outputId":"d38093d3-9f32-4694-8337-c5cd5fa6d26a"},"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[\"Based on the task description and standard practices for implementing it using the provided APIs, the following components would likely be used:\\n\\n1.  `tensorflow`\\n2.  `tensorflow.keras`\\n3.  `Sequential`\\n4.  `Dense`\\n5.  `fetch_california_housing`\\n6.  `train_test_split`\\n7.  `StandardScaler`\\n8.  `numpy` (used implicitly for array handling)\\n\\nReferring to the extracted documentation data:\\n\\n*   `tensorflow`: The documentation covers general import and alias usage (`import tensorflow as tf`), which is standard.\\n*   `tensorflow.keras`: The documentation covers models (`Sequential`), layers (`Dense`), optimizers, loss functions, and training (`fit`), all necessary for the task. It mentions compatibility between layers and appropriate choices for optimizer/loss for the task. A simple linear regression fits these constraints (e.g., `Dense(1)`, `loss='mse'`, `optimizer='adam'`).\\n*   `Sequential`: Documentation covers initialization, adding layers (`add`), compilation (`compile`), and training (`fit`). Adding a `Dense` layer is compliant. `compile` expects string identifiers for optimizer/loss. `fit` expects array-like data after compilation.\\n*   `Dense`: Documentation covers attributes like `units` (positive integer) and the operation. A `Dense(units=1)` for a single output in regression is compliant. `input_shape` should match the feature count.\\n*   `fetch_california_housing`: Documentation covers loading the dataset, specifying parameters like `return_X_y` to get data and target arrays. This is the standard way to load the dataset for use with scikit-learn/Keras.\\n*   `train_test_split`: Documentation covers splitting arrays (`X`, `y`) of the same length, which matches the output of `fetch_california_housing` when `return_X_y=True`.\\n*   `StandardScaler`: Documentation covers initializing the scaler and using `fit_transform` on the training data and `transform` on the test data, ensuring `fit` is called first and feature counts match. This is standard preprocessing for scaled regression.\\n*   `numpy`: Documentation covers general array handling which is underlying for the data used by scikit-learn and Keras.\\n\\nThe task requires creating a simple linear regression model with one dense layer (e.g., `Dense(1)`), compiling it with an appropriate optimizer and loss for regression (e.g., 'adam', 'mse'), and training it on the California housing dataset which would involve loading the data, splitting it, and scaling it. All steps using the identified APIs, as described in the task, are consistent with the provided documentation constraints for these components.\\n\\nperfect\",\n"," 'perfect',\n"," 'perfect',\n"," 'perfect',\n"," 'perfect',\n"," 'perfect',\n"," 'perfect',\n"," 'The documentation does not contain information on the Hadoop MapReduce API required by the task.']"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["df_results = pd.DataFrame({'gpt_check': output})\n","\n","output_path = \"/content/drive/MyDrive/Stanford NLP/check_gpt.csv\"\n","df_results.to_csv(output_path, index=False)"],"metadata":{"id":"0QAAKcg_mApa","executionInfo":{"status":"ok","timestamp":1748917449460,"user_tz":420,"elapsed":8,"user":{"displayName":"Neeharika Gupta","userId":"13434636909198909500"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"bOAmboTdmnKF"},"execution_count":null,"outputs":[]}]}