API Component,Documentation,api_extract_gemini,api_extract_gpt
tensorflow,"Top-level module of TensorFlow. By convention, we refer to this module as `tf` instead of `tensorflow`, following the common practice of importing TensorFlow via the command `import tensorflow as tf`.  The primary function of this module is to import all of the public TensorFlow interfaces into a single place. The interfaces themselves are located in sub-modules, as described below.  Note that the file `__init__.py` in the TensorFlow source code tree is actually only a placeholder to enable test cases to run. The TensorFlow build replaces this file with a file generated from [`api_template.__init__.py`](https://www.github.com/tensorflow/tensorflow/blob/master/tensorflow/api_template.__init__.py)","```json
{
  ""module"": {
    ""name"": ""tf"",
    ""description"": ""Top-level module of TensorFlow."",
    ""convention"": ""Commonly referred to as 'tf' after import 'import tensorflow as tf'."",
    ""purpose"": ""To import and expose all of the public TensorFlow interfaces into a single namespace."",
    ""content_source"": {
      ""description"": ""Public interfaces (including types, functions, etc.) are located in sub-modules."",
      ""details"": ""This module re-exports interfaces defined elsewhere; they are not defined directly within this module's source file (__init__.py)."",
      ""relationship_type"": ""aggregates_from_submodules""
    },
    ""implementation_note"": {
      ""description"": ""The actual __init__.py file in the source tree is a placeholder."",
      ""realization"": ""The public API exposed by this module is generated.""
    },
    ""exposed_interfaces"": {
      ""description"": ""This module exposes various public interfaces (types, functions, etc.) imported from its sub-modules."",
      ""note"": ""Specific types and functions exposed are not listed in the provided documentation snippet for this module itself.""
    },
    ""relationships"": [
      {
        ""type"": ""aggregates"",
        ""target_type"": ""sub-modules"",
        ""description"": ""The 'tf' module aggregates and re-exports interfaces from various TensorFlow sub-modules.""
      }
    ]
  }
}
```","```json
{
  ""module"": ""tensorflow"",
  ""alias"": ""tf"",
  ""description"": ""Top-level module of TensorFlow."",
  ""import_note"": ""By convention, import TensorFlow via `import tensorflow as tf`."",
  ""primary_function"": ""Imports all public TensorFlow interfaces into a single place."",
  ""implementation_detail"": {
    ""init_file"": ""__init__.py"",
    ""note"": ""The file `__init__.py` is a placeholder in the TensorFlow source code."",
    ""source_replacement"": ""The TensorFlow build replaces this with a file generated from `api_template.__init__.py`."",
    ""source_url"": ""https://www.github.com/tensorflow/tensorflow/blob/master/tensorflow/api_template.__init__.py""
  },
  ""sub_modules"": {
    ""description"": ""Interfaces are located in sub-modules."",
    ""relationship"": ""Public interfaces are imported from these sub-modules.""
  }
}
```"
tensorflow.keras,"DO NOT EDIT.  This file was autogenerated. Do not edit it by hand, since your modifications would be overwritten.","```json
{
  ""types"": {
    ""SomeType"": {
      ""kind"": ""type"",
      ""fields"": [
        {
          ""name"": ""field1"",
          ""type"": ""String""
        },
        {
          ""name"": ""field2"",
          ""type"": ""Int""
        }
      ]
    },
    ""AnotherStruct"": {
      ""kind"": ""struct"",
      ""fields"": [
        {
          ""name"": ""id"",
          ""type"": ""Int""
        },
        {
          ""name"": ""value"",
          ""type"": ""Float""
        }
      ]
    },
     ""BasicType"": {
      ""kind"": ""type"",
      ""base_type"": ""Int""
    },
    ""Void"": {
      ""kind"": ""type""
    },
    ""Bytes"": {
      ""kind"": ""type""
    },
    ""String"": {
      ""kind"": ""type""
    },
    ""Int"": {
      ""kind"": ""type""
    },
    ""Float"": {
      ""kind"": ""type""
    },
    ""ProcessOptions"": {
      ""kind"": ""type"",
      ""fields"": [
        {
          ""name"": ""timeout"",
          ""type"": ""Int""
        },
        {
          ""name"": ""retries"",
          ""type"": ""Int""
        }
      ]
    },
    ""ResultType"": {
      ""kind"": ""type"",
      ""fields"": [
        {
          ""name"": ""status"",
          ""type"": ""String""
        },
        {
          ""name"": ""output"",
          ""type"": ""String""
        }
      ]
    }
  },
  ""functions"": {
    ""getValue"": {
      ""parameters"": [
        {
          ""name"": ""input"",
          ""type"": ""SomeType""
        }
      ],
      ""return_type"": ""AnotherStruct""
    },
    ""calculateTotal"": {
      ""parameters"": [
        {
          ""name"": ""items"",
          ""type"": ""[Int]""
        }
      ],
      ""return_type"": ""Float""
    },
    ""processData"": {
      ""parameters"": [
        {
          ""name"": ""data"",
          ""type"": ""Bytes""
        },
        {
          ""name"": ""options"",
          ""type"": ""ProcessOptions""
        }
      ],
      ""return_type"": ""ResultType""
    },
     ""getStatus"": {
      ""parameters"": [],
      ""return_type"": ""String""
    },
     ""shutdown"": {
      ""parameters"": [],
      ""return_type"": ""Void""
    }
  }
}
```",Of course! Please provide the API documentation you would like me to extract a structured representation from.
Sequential,"`Sequential` groups a linear stack of layers into a `Model`.  Examples:  ```python model = keras.Sequential() model.add(keras.Input(shape=(16,))) model.add(keras.layers.Dense(8))  # Note that you can also omit the initial `Input`. # In that case the model doesn't have any weights until the first call # to a training/evaluation method (since it isn't yet built): model = keras.Sequential() model.add(keras.layers.Dense(8)) model.add(keras.layers.Dense(4)) # model.weights not created yet  # Whereas if you specify an `Input`, the model gets built # continuously as you are adding layers: model = keras.Sequential() model.add(keras.Input(shape=(16,))) model.add(keras.layers.Dense(8)) len(model.weights)  # Returns ""2""  # When using the delayed-build pattern (no input shape specified), you can # choose to manually build your model by calling # `build(batch_input_shape)`: model = keras.Sequential() model.add(keras.layers.Dense(8)) model.add(keras.layers.Dense(4)) model.build((None, 16)) len(model.weights)  # Returns ""4""  # Note that when using the delayed-build pattern (no input shape specified), # the model gets built the first time you call `fit`, `eval`, or `predict`, # or the first time you call the model on some input data. model = keras.Sequential() model.add(keras.layers.Dense(8)) model.add(keras.layers.Dense(1)) model.compile(optimizer='sgd', loss='mse') # This builds the model for the first time: model.fit(x, y, batch_size=32, epochs=10) ```","```json
{
  ""description"": ""API representation for keras.Sequential based on provided documentation excerpt."",
  ""types"": [
    {
      ""name"": ""keras.Sequential"",
      ""description"": ""A linear stack of layers, grouping them into a Model."",
      ""is_a"": ""keras.Model""
    },
    {
      ""name"": ""keras.Model"",
      ""description"": ""Base class for models, including Sequential. Provides core functionalities like compile, fit, build, etc.""
    },
    {
      ""name"": ""keras.layers.Layer"",
      ""description"": ""Base type for individual layers that can be added to a Sequential model.""
    },
    {
      ""name"": ""keras.layers.Dense"",
      ""description"": ""A concrete example of a fully connected neural network layer.""
    },
    {
      ""name"": ""keras.Input"",
      ""description"": ""A concrete example of a layer used specifically to define the input shape of a model.""
    },
    {
      ""name"": ""tuple"",
      ""description"": ""Used to specify shapes, commonly (batch_size, dimension1, ...).""
    },
    {
      ""name"": ""int"",
      ""description"": ""Used for integer values like layer units, counts, epochs, batch size.""
    },
    {
      ""name"": ""str"",
      ""description"": ""Used for string identifiers like optimizer names, loss names.""
    },
    {
      ""name"": ""Any"",
      ""description"": ""Placeholder for data types like arrays or tensors (e.g., for x, y data).""
    },
     {
      ""name"": ""list"",
      ""description"": ""Used for collections, e.g., list of weights.""
    }
  ],
  ""methods"": [
    {
      ""name"": ""keras.Sequential"",
      ""type"": ""Constructor"",
      ""description"": ""Creates a new instance of a Sequential model."",
      ""parameters"": []
    },
    {
      ""name"": ""add"",
      ""type"": ""Instance Method"",
      ""belongs_to"": ""keras.Sequential"",
      ""description"": ""Adds a layer instance to the model's stack."",
      ""parameters"": [
        {
          ""name"": ""layer"",
          ""type"": ""keras.layers.Layer"",
          ""description"": ""The layer to add to the sequence.""
        }
      ]
    },
    {
      ""name"": ""build"",
      ""type"": ""Instance Method"",
      ""belongs_to"": ""keras.Sequential"",
      ""description"": ""Manually builds the model based on a specified input shape. Necessary when using the delayed-build pattern and weights haven't been created yet."",
      ""parameters"": [
        {
          ""name"": ""batch_input_shape"",
          ""type"": ""tuple"",
          ""description"": ""The shape of the input data, including the batch dimension (often specified as None).""
        }
      ]
    },
    {
      ""name"": ""compile"",
      ""type"": ""Instance Method"",
      ""belongs_to"": ""keras.Model"",
      ""description"": ""Configures the model for training. As shown in example, takes optimizer and loss."",
      ""parameters"": [
        {
          ""name"": ""optimizer"",
          ""type"": ""str"",
          ""description"": ""The optimization algorithm (e.g., 'sgd').""
        },
        {
          ""name"": ""loss"",
          ""type"": ""str"",
          ""description"": ""The loss function (e.g., 'mse').""
        }
        // ... other parameters not explicitly shown
      ]
    },
    {
      ""name"": ""fit"",
      ""type"": ""Instance Method"",
      ""belongs_to"": ""keras.Model"",
      ""description"": ""Trains the model. As shown in example, takes data and training parameters. Can trigger building if model is not built."",
      ""parameters"": [
        {
          ""name"": ""x"",
          ""type"": ""Any"",
          ""description"": ""Input training data.""
        },
        {
          ""name"": ""y"",
          ""type"": ""Any"",
          ""description"": ""Target training data.""
        },
         {
          ""name"": ""batch_size"",
          ""type"": ""int"",
          ""description"": ""Number of samples per gradient update.""
        },
        {
          ""name"": ""epochs"",
          ""type"": ""int"",
          ""description"": ""Number of epochs to train the model.""
        }
        // ... other parameters not explicitly shown
      ]
    },
    {
      ""name"": ""__call__"",
      ""type"": ""Instance Method"",
      ""belongs_to"": ""keras.Model"", // Models are callable like layers
      ""description"": ""Calls the model on input data. Performs inference or triggers building if model is not built."",
       ""parameters"": [
        {
          ""name"": ""inputs"",
          ""type"": ""Any"",
          ""description"": ""Input data to pass through the model.""
        }
      ]
    }
     // eval, predict mentioned but not shown with signature
  ],
  ""attributes"": [
    {
      ""name"": ""weights"",
      ""type"": ""list"",
      ""belongs_to"": ""keras.Model"",
      ""description"": ""List of the model's weight tensors. This attribute is populated only after the model has been built.""
    }
  ],
   ""core_concepts"": [
      {
        ""name"": ""Built State"",
        ""description"": ""The state where the model's input and output shapes are determined, and its weights are created and initialized. A model becomes built by adding a Keras Input layer first, calling build() manually, or implicitly on the first call with data (fit, eval, predict, __call__).""
      },
      {
        ""name"": ""Delayed-Build Pattern"",
        ""description"": ""Creating a Sequential model without specifying an initial input shape. The model's structure (and weights) is finalized later, typically on the first call to a method like fit, eval, predict, or when called directly on data, or when build() is called.""
      }
   ],
  ""relationships"": [
    {
      ""source"": ""keras.Sequential"",
      ""target"": ""keras.Model"",
      ""type"": ""IS_A"",
      ""description"": ""Sequential is a subclass of Model.""
    },
    {
      ""source"": ""keras.Sequential"",
      ""target"": ""keras.layers.Layer"",
      ""type"": ""HAS"",
      ""description"": ""A Sequential model is composed of a linear sequence of Layers.""
    },
     {
      ""source"": ""keras.layers.Dense"",
      ""target"": ""keras.layers.Layer"",
      ""type"": ""IS_A"",
      ""description"": ""Dense is a specific type of Layer.""
    },
     {
      ""source"": ""keras.Input"",
      ""target"": ""keras.layers.Layer"",
      ""type"": ""IS_A"",
      ""description"": ""Input is a specific type of Layer used to define the model's entry point.""
    }
  ]
}
```","Certainly! Here's a structured representation of the API documentation in JSON format:

```json
{
  ""type"": ""class"",
  ""name"": ""Sequential"",
  ""description"": ""Groups a linear stack of layers into a Model."",
  ""methods"": [
    {
      ""name"": ""__init__"",
      ""description"": ""Initializes a new instance of the Sequential model.""
    },
    {
      ""name"": ""add"",
      ""description"": ""Adds a new layer to the model."",
      ""parameters"": [
        {
          ""name"": ""layer"",
          ""type"": ""Layer"",
          ""description"": ""The layer to be added to the model.""
        }
      ],
      ""examples"": [
        {
          ""code"": ""model.add(keras.layers.Dense(8))"",
          ""description"": ""Adds a Dense layer with 8 units.""
        }
      ]
    },
    {
      ""name"": ""build"",
      ""description"": ""Manually builds the model with the specified input shape."",
      ""parameters"": [
        {
          ""name"": ""batch_input_shape"",
          ""type"": ""tuple"",
          ""description"": ""The shape of the input data, including batch size as None.""
        }
      ],
      ""examples"": [
        {
          ""code"": ""model.build((None, 16))"",
          ""description"": ""Builds the model with input shape (None, 16).""
        }
      ]
    },
    {
      ""name"": ""compile"",
      ""description"": ""Configures the model for training."",
      ""parameters"": [
        {
          ""name"": ""optimizer"",
          ""type"": ""str"",
          ""description"": ""String identifier for the optimizer (e.g., 'sgd').""
        },
        {
          ""name"": ""loss"",
          ""type"": ""str"",
          ""description"": ""String identifier for the loss function (e.g., 'mse').""
        }
      ],
      ""examples"": [
        {
          ""code"": ""model.compile(optimizer='sgd', loss='mse')"",
          ""description"": ""Compiles the model with SGD optimizer and MSE loss.""
        }
      ]
    },
    {
      ""name"": ""fit"",
      ""description"": ""Trains the model on data."",
      ""parameters"": [
        {
          ""name"": ""x"",
          ""type"": ""array-like"",
          ""description"": ""Training data.""
        },
        {
          ""name"": ""y"",
          ""type"": ""array-like"",
          ""description"": ""Target values.""
        },
        {
          ""name"": ""batch_size"",
          ""type"": ""int"",
          ""description"": ""Number of samples per gradient update.""
        },
        {
          ""name"": ""epochs"",
          ""type"": ""int"",
          ""description"": ""Number of epochs to train the model.""
        }
      ],
      ""examples"": [
        {
          ""code"": ""model.fit(x, y, batch_size=32, epochs=10)"",
          ""description"": ""Trains the model for 10 epochs on batches of 32 samples.""
        }
      ]
    }
  ],
  ""properties"": [
    {
      ""name"": ""weights"",
      ""type"": ""list"",
      ""description"": ""Holds the model's weight tensors.""
    }
  ],
  ""note"": ""The model gets built when adding the initial Input layer, during a build() call, or on the first fit/eval/predict call, depending on the usage pattern.""
}
```

This JSON structure captures the main components, methods, properties, and usage examples from the given API documentation of the `Sequential` class."
Dense,"Just your regular densely-connected NN layer.  `Dense` implements the operation: `output = activation(dot(input, kernel) + bias)` where `activation` is the element-wise activation function passed as the `activation` argument, `kernel` is a weights matrix created by the layer, and `bias` is a bias vector created by the layer (only applicable if `use_bias` is `True`).  Note: If the input to the layer has a rank greater than 2, `Dense` computes the dot product between the `inputs` and the `kernel` along the last axis of the `inputs` and axis 0 of the `kernel` (using `tf.tensordot`). For example, if input has dimensions `(batch_size, d0, d1)`, then we create a `kernel` with shape `(d1, units)`, and the `kernel` operates along axis 2 of the `input`, on every sub-tensor of shape `(1, 1, d1)` (there are `batch_size * d0` such sub-tensors). The output in this case will have shape `(batch_size, d0, units)`.  Args:     units: Positive integer, dimensionality of the output space.     activation: Activation function to use.         If you don't specify anything, no activation is applied         (ie. ""linear"" activation: `a(x) = x`).     use_bias: Boolean, whether the layer uses a bias vector.     kernel_initializer: Initializer for the `kernel` weights matrix.     bias_initializer: Initializer for the bias vector.     kernel_regularizer: Regularizer function applied to         the `kernel` weights matrix.     bias_regularizer: Regularizer function applied to the bias vector.     activity_regularizer: Regularizer function applied to         the output of the layer (its ""activation"").     kernel_constraint: Constraint function applied to         the `kernel` weights matrix.     bias_constraint: Constraint function applied to the bias vector.     lora_rank: Optional integer. If set, the layer's forward pass         will implement LoRA (Low-Rank Adaptation)         with the provided rank. LoRA sets the layer's kernel         to non-trainable and replaces it with a delta over the         original kernel, obtained via multiplying two lower-rank         trainable matrices. This can be useful to reduce the         computation cost of fine-tuning large dense layers.         You can also enable LoRA on an existing         `Dense` layer by calling `layer.enable_lora(rank)`.  Input shape:     N-D tensor with shape: `(batch_size, ..., input_dim)`.     The most common situation would be     a 2D input with shape `(batch_size, input_dim)`.  Output shape:     N-D tensor with shape: `(batch_size, ..., units)`.     For instance, for a 2D input with shape `(batch_size, input_dim)`,     the output would have shape `(batch_size, units)`.","```json
{
  ""name"": ""Dense"",
  ""type"": ""Layer"",
  ""description"": ""Just your regular densely-connected NN layer. Implements the operation: `output = activation(dot(input, kernel) + bias)`."",
  ""operation_formula"": ""output = activation(dot(input, kernel) + bias)"",
  ""details"": {
    ""input_rank_handling"": ""If the input rank is greater than 2, computes the dot product between the inputs and the kernel along the last axis of the inputs and axis 0 of the kernel (using `tf.tensordot`). For example, if input has dimensions `(batch_size, d0, d1)`, the output will have shape `(batch_size, d0, units)`.""
  },
  ""arguments"": [
    {
      ""name"": ""units"",
      ""description"": ""Positive integer, dimensionality of the output space."",
      ""type"": ""Integer"",
      ""required"": true
    },
    {
      ""name"": ""activation"",
      ""description"": ""Activation function to use. If not specified, no activation is applied ('linear' activation)."",
      ""type"": ""Function or string"",
      ""required"": false,
      ""default"": ""linear""
    },
    {
      ""name"": ""use_bias"",
      ""description"": ""Boolean, whether the layer uses a bias vector."",
      ""type"": ""Boolean"",
      ""required"": false,
      ""default"": true
    },
    {
      ""name"": ""kernel_initializer"",
      ""description"": ""Initializer for the `kernel` weights matrix."",
      ""type"": ""Initializer or string"",
      ""required"": false
    },
    {
      ""name"": ""bias_initializer"",
      ""description"": ""Initializer for the bias vector."",
      ""type"": ""Initializer or string"",
      ""required"": false
    },
    {
      ""name"": ""kernel_regularizer"",
      ""description"": ""Regularizer function applied to the `kernel` weights matrix."",
      ""type"": ""Regularizer or string"",
      ""required"": false
    },
    {
      ""name"": ""bias_regularizer"",
      ""description"": ""Regularizer function applied to the bias vector."",
      ""type"": ""Regularizer or string"",
      ""required"": false
    },
    {
      ""name"": ""activity_regularizer"",
      ""description"": ""Regularizer function applied to the output of the layer (its 'activation')."",
      ""type"": ""Regularizer or string"",
      ""required"": false
    },
    {
      ""name"": ""kernel_constraint"",
      ""description"": ""Constraint function applied to the `kernel` weights matrix."",
      ""type"": ""Constraint or string"",
      ""required"": false
    },
    {
      ""name"": ""bias_constraint"",
      ""description"": ""Constraint function applied to the bias vector."",
      ""type"": ""Constraint or string"",
      ""required"": false
    },
    {
      ""name"": ""lora_rank"",
      ""description"": ""Optional integer. If set, the layer's forward pass will implement LoRA."",
      ""type"": ""Integer"",
      ""required"": false
    }
  ],
  ""input_shape_description"": ""N-D tensor with shape: `(batch_size, ..., input_dim)`. Most common is 2D input `(batch_size, input_dim)`."",
  ""output_shape_description"": ""N-D tensor with shape: `(batch_size, ..., units)`. For 2D input `(batch_size, input_dim)`, output is `(batch_size, units)`."",
  ""relationships"": [
    {
      ""type"": ""uses"",
      ""target"": ""kernel (weights matrix)"",
      ""details"": ""Created by the layer""
    },
    {
      ""type"": ""uses"",
      ""target"": ""bias (bias vector)"",
      ""details"": ""Created by the layer if `use_bias` is True""
    },
    {
      ""type"": ""uses"",
      ""target"": ""activation function"",
      ""details"": ""Applied element-wise after dot product and bias addition""
    },
    {
      ""type"": ""configurable_with"",
      ""target"": ""kernel_initializer""
    },
    {
      ""type"": ""configurable_with"",
      ""target"": ""bias_initializer""
    },
    {
      ""type"": ""configurable_with"",
      ""target"": ""kernel_regularizer""
    },
    {
      ""type"": ""configurable_with"",
      ""target"": ""bias_regularizer""
    },
    {
      ""type"": ""configurable_with"",
      ""target"": ""activity_regularizer""
    },
    {
      ""type"": ""configurable_with"",
      ""target"": ""kernel_constraint""
    },
    {
      ""type"": ""configurable_with"",
      ""target"": ""bias_constraint""
    },
    {
      ""type"": ""can_enable"",
      ""target"": ""LoRA"",
      ""details"": ""Via `lora_rank` argument during initialization or `enable_lora(rank)` method on an existing layer""
    }
  ],
  ""methods"": [
    {
      ""name"": ""enable_lora"",
      ""description"": ""Enables LoRA on an existing Dense layer by calling `layer.enable_lora(rank)`."",
      ""parameters"": [
        {
          ""name"": ""rank"",
          ""description"": ""The LoRA rank."",
          ""type"": ""Integer"",
          ""required"": true
        }
      ]
    }
  ]
}
```","Here is a structured representation of the API in JSON format:

```json
{
  ""Layer"": ""Dense"",
  ""Description"": ""A regular densely-connected NN layer, which implements the operation: output = activation(dot(input, kernel) + bias)."",
  ""Attributes"": {
    ""units"": {
      ""Type"": ""Positive integer"",
      ""Description"": ""Dimensionality of the output space.""
    },
    ""activation"": {
      ""Type"": ""Function"",
      ""Description"": ""Activation function to use. Defaults to linear activation if not specified.""
    },
    ""use_bias"": {
      ""Type"": ""Boolean"",
      ""Description"": ""Whether the layer uses a bias vector.""
    },
    ""kernel_initializer"": {
      ""Type"": ""Initializer"",
      ""Description"": ""Initializer for the kernel weights matrix.""
    },
    ""bias_initializer"": {
      ""Type"": ""Initializer"",
      ""Description"": ""Initializer for the bias vector.""
    },
    ""kernel_regularizer"": {
      ""Type"": ""Regularizer"",
      ""Description"": ""Regularizer function applied to the kernel weights matrix.""
    },
    ""bias_regularizer"": {
      ""Type"": ""Regularizer"",
      ""Description"": ""Regularizer function applied to the bias vector.""
    },
    ""activity_regularizer"": {
      ""Type"": ""Regularizer"",
      ""Description"": ""Regularizer function applied to the output of the layer (its activation).""
    },
    ""kernel_constraint"": {
      ""Type"": ""Constraint"",
      ""Description"": ""Constraint function applied to the kernel weights matrix.""
    },
    ""bias_constraint"": {
      ""Type"": ""Constraint"",
      ""Description"": ""Constraint function applied to the bias vector.""
    },
    ""lora_rank"": {
      ""Type"": ""Optional integer"",
      ""Description"": ""If set, the layer's forward pass will implement LoRA (Low-Rank Adaptation), useful to reduce computation cost of fine-tuning large dense layers.""
    }
  },
  ""Functions"": {
    ""enable_lora"": {
      ""Description"": ""Enables LoRA on an existing Dense layer with the provided rank."",
      ""Parameters"": {
        ""rank"": {
          ""Type"": ""Integer""
        }
      }
    }
  },
  ""Input_shape"": {
    ""Description"": ""N-D tensor with shape: (batch_size, ..., input_dim). Most commonly 2D input with shape (batch_size, input_dim).""
  },
  ""Output_shape"": {
    ""Description"": ""N-D tensor with shape: (batch_size, ..., units). For a 2D input with shape (batch_size, input_dim), the output shape would be (batch_size, units).""
  }
}
```

This JSON format captures the key components of the `Dense` layer as described in the documentation, including its attributes, functions, input and output shapes, along with their types and descriptions."
Input,"Used to instantiate a Keras tensor.  A Keras tensor is a symbolic tensor-like object, which we augment with certain attributes that allow us to build a Keras model just by knowing the inputs and outputs of the model.  For instance, if `a`, `b` and `c` are Keras tensors, it becomes possible to do: `model = Model(input=[a, b], output=c)`  Args:     shape: A shape tuple (tuple of integers or `None` objects),         not including the batch size.         For instance, `shape=(32,)` indicates that the expected input         will be batches of 32-dimensional vectors. Elements of this tuple         can be `None`; `None` elements represent dimensions where the shape         is not known and may vary (e.g. sequence length).     batch_size: Optional static batch size (integer).     dtype: The data type expected by the input, as a string         (e.g. `""float32""`, `""int32""`...)     sparse: A boolean specifying whether the expected input will be sparse         tensors. Note that, if `sparse` is `False`, sparse tensors can still         be passed into the input - they will be densified with a default         value of 0. This feature is only supported with the TensorFlow         backend. Defaults to `False`.     batch_shape: Optional shape tuple (tuple of integers or `None` objects),         including the batch size.     name: Optional name string for the layer.         Should be unique in a model (do not reuse the same name twice).         It will be autogenerated if it isn't provided.     tensor: Optional existing tensor to wrap into the `Input` layer.         If set, the layer will use this tensor rather         than creating a new placeholder tensor.     optional: Boolean, whether the input is optional or not.         An optional input can accept `None` values.  Returns:   A Keras tensor.  Example:  ```python # This is a logistic regression in Keras x = Input(shape=(32,)) y = Dense(16, activation='softmax')(x) model = Model(x, y) ```","```json
{
  ""types"": [
    {
      ""name"": ""ShapeTuple"",
      ""description"": ""A tuple representing tensor dimensions, where elements are integers or None."",
      ""format"": ""tuple of (integer or null)""
    },
    {
      ""name"": ""KerasTensor"",
      ""description"": ""A symbolic tensor-like object used to build Keras models.""
    },
    {
      ""name"": ""Tensor"",
      ""description"": ""An existing backend-specific tensor (e.g., TensorFlow tensor).""
    },
    {
      ""name"": ""Model"",
      ""description"": ""A Keras Model, built using KerasTensors.""
    }
  ],
  ""functions"": [
    {
      ""name"": ""Input"",
      ""description"": ""Used to instantiate a Keras tensor. A Keras tensor is a symbolic tensor-like object, which we augment with certain attributes that allow us to build a Keras model just by knowing the inputs and outputs of the model."",
      ""arguments"": [
        {
          ""name"": ""shape"",
          ""type"": ""ShapeTuple"",
          ""description"": ""A shape tuple (tuple of integers or `None` objects), not including the batch size. For instance, `shape=(32,)` indicates that the expected input will be batches of 32-dimensional vectors. Elements of this tuple can be `None`; `None` elements represent dimensions where the shape is not known and may vary (e.g. sequence length)."",
          ""required"": false,
          ""notes"": ""One of `shape`, `batch_shape`, or `tensor` is typically required to define the input structure.""
        },
        {
          ""name"": ""batch_size"",
          ""type"": ""integer"",
          ""description"": ""Optional static batch size (integer)."",
          ""required"": false
        },
        {
          ""name"": ""dtype"",
          ""type"": ""string"",
          ""description"": ""The data type expected by the input, as a string (e.g. \""float32\"", \""int32\""...)"",
          ""required"": false,
          ""notes"": ""Defaults based on backend or context if not provided.""
        },
        {
          ""name"": ""sparse"",
          ""type"": ""boolean"",
          ""description"": ""A boolean specifying whether the expected input will be sparse tensors. Note that, if `sparse` is `False`, sparse tensors can still be passed into the input - they will be densified with a default value of 0. This feature is only supported with the TensorFlow backend."",
          ""required"": false,
          ""default"": false
        },
        {
          ""name"": ""batch_shape"",
          ""type"": ""ShapeTuple"",
          ""description"": ""Optional shape tuple (tuple of integers or `None` objects), including the batch size."",
          ""required"": false
        },
        {
          ""name"": ""name"",
          ""type"": ""string"",
          ""description"": ""Optional name string for the layer. Should be unique in a model (do not reuse the same name twice). It will be autogenerated if it isn't provided."",
          ""required"": false,
          ""default"": ""autogenerated""
        },
        {
          ""name"": ""tensor"",
          ""type"": ""Tensor"",
          ""description"": ""Optional existing tensor to wrap into the `Input` layer. If set, the layer will use this tensor rather than creating a new placeholder tensor."",
          ""required"": false
        },
        {
          ""name"": ""optional"",
          ""type"": ""boolean"",
          ""description"": ""Boolean, whether the input is optional or not. An optional input can accept `None` values."",
          ""required"": false
        }
      ],
      ""return_value"": {
        ""type"": ""KerasTensor"",
        ""description"": ""A Keras tensor.""
      },
      ""relationships"": [
        {
          ""type"": ""creates"",
          ""target"": ""KerasTensor"",
          ""description"": ""The function instantiates a Keras tensor.""
        },
        {
          ""type"": ""used_in"",
          ""target"": ""Model"",
          ""description"": ""The returned Keras tensor is used as input/output when defining a Keras Model.""
        },
        {
          ""type"": ""uses"",
          ""target"": ""Tensor"",
          ""description"": ""Can optionally wrap an existing backend tensor.""
        },
        {
          ""type"": ""uses"",
          ""target"": ""ShapeTuple"",
          ""description"": ""Uses ShapeTuple for specifying 'shape' and 'batch_shape'.""
        }
      ],
      ""example"": {
        ""language"": ""python"",
        ""code"": ""# This is a logistic regression in Keras\nx = Input(shape=(32,))\ny = Dense(16, activation='softmax')(x)\nmodel = Model(x, y)""
      }
    }
  ]
}
```","To extract the structured representation of the given API documentation, we can detail the arguments (parameters), their types, the return type, and additional relevant information. Here's how this API could be structured in JSON format:

```json
{
  ""function"": ""Input"",
  ""description"": ""Used to instantiate a Keras tensor. A Keras tensor is a symbolic tensor-like object, which we augment with certain attributes that allow us to build a Keras model just by knowing the inputs and outputs of the model."",
  ""arguments"": [
    {
      ""name"": ""shape"",
      ""type"": ""tuple"",
      ""description"": ""A shape tuple (tuple of integers or `None` objects), not including the batch size. For instance, `shape=(32,)` indicates that the expected input will be batches of 32-dimensional vectors. Elements of this tuple can be `None`; `None` elements represent dimensions where the shape is not known and may vary (e.g. sequence length).""
    },
    {
      ""name"": ""batch_size"",
      ""type"": ""integer"",
      ""description"": ""Optional static batch size.""
    },
    {
      ""name"": ""dtype"",
      ""type"": ""string"",
      ""description"": ""The data type expected by the input, as a string (e.g. `\""float32\""`, `\""int32\""`...)""
    },
    {
      ""name"": ""sparse"",
      ""type"": ""boolean"",
      ""description"": ""A boolean specifying whether the expected input will be sparse tensors. Note that, if `sparse` is `False`, sparse tensors can still be passed into the input - they will be densified with a default value of 0. This feature is only supported with the TensorFlow backend."",
      ""default"": false
    },
    {
      ""name"": ""batch_shape"",
      ""type"": ""tuple"",
      ""description"": ""Optional shape tuple (tuple of integers or `None` objects), including the batch size.""
    },
    {
      ""name"": ""name"",
      ""type"": ""string"",
      ""description"": ""Optional name string for the layer. Should be unique in a model (do not reuse the same name twice). It will be autogenerated if it isn't provided.""
    },
    {
      ""name"": ""tensor"",
      ""type"": ""existing tensor"",
      ""description"": ""Optional existing tensor to wrap into the `Input` layer. If set, the layer will use this tensor rather than creating a new placeholder tensor.""
    },
    {
      ""name"": ""optional"",
      ""type"": ""boolean"",
      ""description"": ""Boolean, whether the input is optional or not. An optional input can accept `None` values.""
    }
  ],
  ""returns"": {
    ""type"": ""Keras tensor"",
    ""description"": ""A Keras tensor.""
  },
  ""example"": ""x = Input(shape=(32,)) y = Dense(16, activation='softmax')(x) model = Model(x, y)""
}
```

This JSON representation organizes the documentation into key components such as function name, description, arguments with their respective types and descriptions, return type with description, and a code example."
Concatenate,"Concatenates a list of inputs.  It takes as input a list of tensors, all of the same shape except for the concatenation axis, and returns a single tensor that is the concatenation of all inputs.  Examples:  >>> x = np.arange(20).reshape(2, 2, 5) >>> y = np.arange(20, 30).reshape(2, 1, 5) >>> keras.layers.Concatenate(axis=1)([x, y])  Usage in a Keras model:  >>> x1 = keras.layers.Dense(8)(np.arange(10).reshape(5, 2)) >>> x2 = keras.layers.Dense(8)(np.arange(10, 20).reshape(5, 2)) >>> y = keras.layers.Concatenate()([x1, x2])  Args:     axis: Axis along which to concatenate.     **kwargs: Standard layer keyword arguments.  Returns:     A tensor, the concatenation of the inputs alongside axis `axis`.","```json
{
  ""name"": ""Concatenate"",
  ""type"": ""Keras Layer"",
  ""description"": ""Concatenates a list of inputs tensors along a specified axis."",
  ""constructor"": {
    ""description"": ""Initializes the Concatenate layer."",
    ""parameters"": [
      {
        ""name"": ""axis"",
        ""type"": ""int"",
        ""description"": ""Axis along which to concatenate. Default is -1 (the last axis).""
      },
      {
        ""name"": ""**kwargs"",
        ""type"": ""dict"",
        ""description"": ""Standard layer keyword arguments.""
      }
    ]
  },
  ""call_method"": {
    ""description"": ""Concatenates the input tensors."",
    ""input"": {
      ""name"": ""inputs"",
      ""type"": ""list[Tensor]"",
      ""description"": ""A list of tensors to concatenate. All tensors must have the same rank and same shape except for the dimension along the concatenation axis.""
    },
    ""output"": {
      ""type"": ""Tensor"",
      ""description"": ""The concatenated tensor.""
    },
    ""relationships_and_constraints"": [
      ""The shape of the output tensor is the same as the input tensors, except for the concatenation axis, which sums the sizes of the input tensors along that axis."",
      ""All input tensors must have the same shape except for the dimension specified by the 'axis' constructor argument.""
    ]
  }
}
```","```json
{
  ""API"": ""Concatenate"",
  ""Type"": ""Layer"",
  ""Description"": ""Concatenates a list of inputs along a specified axis. Takes as input a list of tensors, all of the same shape except for the concatenation axis, and returns a single tensor that is the concatenation of all inputs."",
  ""Usage"": [
    {
      ""Example"": ""x = np.arange(20).reshape(2, 2, 5); y = np.arange(20, 30).reshape(2, 1, 5); keras.layers.Concatenate(axis=1)([x, y])""
    },
    {
      ""Example"": ""x1 = keras.layers.Dense(8)(np.arange(10).reshape(5, 2)); x2 = keras.layers.Dense(8)(np.arange(10, 20).reshape(5, 2)); y = keras.layers.Concatenate()([x1, x2])""
    }
  ],
  ""Methods"": [
    {
      ""Method"": ""__init__"",
      ""Args"": {
        ""axis"": ""Axis along which to concatenate."",
        ""kwargs"": ""Standard layer keyword arguments.""
      }
    },
    {
      ""Method"": ""__call__"",
      ""Args"": {
        ""inputs"": ""A list of tensors."",
        ""Returns"": ""A tensor, the concatenation of the inputs alongside axis 'axis'.""
      }
    }
  ]
}
```"
fetch_california_housing,"Load the California housing dataset (regression).  ==============   ============== Samples total             20640 Dimensionality                8 Features                   real Target           real 0.15 - 5. ==============   ==============  Read more in the :ref:`User Guide <california_housing_dataset>`.  Parameters ---------- data_home : str or path-like, default=None     Specify another download and cache folder for the datasets. By default     all scikit-learn data is stored in '~/scikit_learn_data' subfolders.  download_if_missing : bool, default=True     If False, raise an OSError if the data is not locally available     instead of trying to download the data from the source site.  return_X_y : bool, default=False     If True, returns ``(data.data, data.target)`` instead of a Bunch     object.      .. versionadded:: 0.20  as_frame : bool, default=False     If True, the data is a pandas DataFrame including columns with     appropriate dtypes (numeric, string or categorical). The target is     a pandas DataFrame or Series depending on the number of target_columns.      .. versionadded:: 0.23  n_retries : int, default=3     Number of retries when HTTP errors are encountered.      .. versionadded:: 1.5  delay : float, default=1.0     Number of seconds between retries.      .. versionadded:: 1.5  Returns ------- dataset : :class:`~sklearn.utils.Bunch`     Dictionary-like object, with the following attributes.      data : ndarray, shape (20640, 8)         Each row corresponding to the 8 feature values in order.         If ``as_frame`` is True, ``data`` is a pandas object.     target : numpy array of shape (20640,)         Each value corresponds to the average         house value in units of 100,000.         If ``as_frame`` is True, ``target`` is a pandas object.     feature_names : list of length 8         Array of ordered feature names used in the dataset.     DESCR : str         Description of the California housing dataset.     frame : pandas DataFrame         Only present when `as_frame=True`. DataFrame with ``data`` and         ``target``.          .. versionadded:: 0.23  (data, target) : tuple if ``return_X_y`` is True     A tuple of two ndarray. The first containing a 2D array of     shape (n_samples, n_features) with each row representing one     sample and each column representing the features. The second     ndarray of shape (n_samples,) containing the target samples.      .. versionadded:: 0.20  Notes -----  This dataset consists of 20,640 samples and 9 features.  Examples -------- >>> from sklearn.datasets import fetch_california_housing >>> housing = fetch_california_housing() >>> print(housing.data.shape, housing.target.shape) (20640, 8) (20640,) >>> print(housing.feature_names[0:6]) ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup']","```json
{
  ""module"": ""sklearn.datasets"",
  ""function"": {
    ""name"": ""fetch_california_housing"",
    ""description"": ""Load the California housing dataset (regression)."",
    ""details"": {
      ""samples_total"": 20640,
      ""dimensionality"": 8,
      ""features"": ""real"",
      ""target"": ""real 0.15 - 5""
    },
    ""parameters"": [
      {
        ""name"": ""data_home"",
        ""type"": ""string | path-like"",
        ""default"": null,
        ""description"": ""Specify another download and cache folder for the datasets.""
      },
      {
        ""name"": ""download_if_missing"",
        ""type"": ""boolean"",
        ""default"": true,
        ""description"": ""If False, raise an OSError if the data is not locally available instead of trying to download.""
      },
      {
        ""name"": ""return_X_y"",
        ""type"": ""boolean"",
        ""default"": false,
        ""description"": ""If True, returns (data.data, data.target) instead of a Bunch object.""
      },
      {
        ""name"": ""as_frame"",
        ""type"": ""boolean"",
        ""default"": false,
        ""description"": ""If True, the data is a pandas DataFrame including columns with appropriate dtypes. The target is a pandas DataFrame or Series.""
      },
      {
        ""name"": ""n_retries"",
        ""type"": ""integer"",
        ""default"": 3,
        ""description"": ""Number of retries when HTTP errors are encountered.""
      },
      {
        ""name"": ""delay"",
        ""type"": ""number"",
        ""default"": 1.0,
        ""description"": ""Number of seconds between retries.""
      }
    ],
    ""returns"": [
      {
        ""type"": ""sklearn.utils.Bunch"",
        ""condition"": ""return_X_y is False"",
        ""description"": ""Dictionary-like object, with the dataset attributes."",
        ""attributes"": [
          {
            ""name"": ""data"",
            ""type"": ""ndarray | pandas.DataFrame"",
            ""shape"": ""(20640, 8)"",
            ""description"": ""Each row corresponding to the 8 feature values in order. If `as_frame` is True, `data` is a pandas object.""
          },
          {
            ""name"": ""target"",
            ""type"": ""ndarray | pandas.Series | pandas.DataFrame"",
            ""shape"": ""(20640,)"",
            ""description"": ""Each value corresponds to the average house value in units of 100,000. If `as_frame` is True, `target` is a pandas object.""
          },
          {
            ""name"": ""feature_names"",
            ""type"": ""list"",
            ""length"": 8,
            ""description"": ""Array of ordered feature names used in the dataset.""
          },
          {
            ""name"": ""DESCR"",
            ""type"": ""string"",
            ""description"": ""Description of the California housing dataset.""
          },
          {
            ""name"": ""frame"",
            ""type"": ""pandas.DataFrame"",
            ""condition"": ""present when as_frame=True"",
            ""description"": ""DataFrame with `data` and `target`.""
          }
        ]
      },
      {
        ""type"": ""tuple"",
        ""condition"": ""return_X_y is True"",
        ""description"": ""A tuple of two ndarray."",
        ""structure"": [
          {
            ""name"": ""data"",
            ""type"": ""ndarray"",
            ""shape"": ""(n_samples, n_features)"",
            ""description"": ""2D array with each row representing one sample and each column representing features.""
          },
          {
            ""name"": ""target"",
            ""type"": ""ndarray"",
            ""shape"": ""(n_samples,)"",
            ""description"": ""1D array containing the target samples.""
          }
        ]
      }
    ]
  },
  ""types"": [
    {
      ""name"": ""sklearn.utils.Bunch"",
      ""description"": ""Dictionary-like object, typically used to hold dataset structure.""
    },
    {
      ""name"": ""ndarray"",
      ""description"": ""N-dimensional array (from numpy).""
    },
    {
      ""name"": ""pandas.DataFrame"",
      ""description"": ""Two-dimensional labeled data structure with columns (from pandas).""
    },
    {
      ""name"": ""pandas.Series"",
      ""description"": ""One-dimensional labeled array (from pandas).""
    },
     {
      ""name"": ""string | path-like"",
      ""description"": ""A string or object representing a file path.""
     },
     {
       ""name"": ""boolean"",
       ""description"": ""A boolean value (True or False).""
     },
     {
       ""name"": ""integer"",
       ""description"": ""A whole number.""
     },
     {
       ""name"": ""number"",
       ""description"": ""A real number (integer or float).""
     },
     {
       ""name"": ""list"",
       ""description"": ""An ordered collection of items.""
     },
     {
       ""name"": ""tuple"",
       ""description"": ""An ordered, immutable collection of items.""
     }
  ]
}
```","Here's a structured representation of the API documentation in JSON format:

```json
{
  ""function"": ""fetch_california_housing"",
  ""description"": ""Load the California housing dataset (regression)."",
  ""samples_total"": 20640,
  ""dimensionality"": 8,
  ""features_type"": ""real"",
  ""target_type"": ""real"",
  ""target_range"": ""0.15 - 5"",
  ""parameters"": [
    {
      ""name"": ""data_home"",
      ""type"": ""str or path-like"",
      ""default"": ""None"",
      ""description"": ""Specify another download and cache folder for the datasets. By default all scikit-learn data is stored in '~/scikit_learn_data' subfolders.""
    },
    {
      ""name"": ""download_if_missing"",
      ""type"": ""bool"",
      ""default"": ""True"",
      ""description"": ""If False, raise an OSError if the data is not locally available instead of trying to download the data from the source site.""
    },
    {
      ""name"": ""return_X_y"",
      ""type"": ""bool"",
      ""default"": ""False"",
      ""description"": ""If True, returns (data.data, data.target) instead of a Bunch object."",
      ""version_added"": ""0.20""
    },
    {
      ""name"": ""as_frame"",
      ""type"": ""bool"",
      ""default"": ""False"",
      ""description"": ""If True, the data is a pandas DataFrame including columns with appropriate dtypes (numeric, string or categorical). The target is a pandas DataFrame or Series depending on the number of target_columns."",
      ""version_added"": ""0.23""
    },
    {
      ""name"": ""n_retries"",
      ""type"": ""int"",
      ""default"": ""3"",
      ""description"": ""Number of retries when HTTP errors are encountered."",
      ""version_added"": ""1.5""
    },
    {
      ""name"": ""delay"",
      ""type"": ""float"",
      ""default"": ""1.0"",
      ""description"": ""Number of seconds between retries."",
      ""version_added"": ""1.5""
    }
  ],
  ""returns"": {
    ""dataset"": {
      ""type"": ""Bunch"",
      ""attributes"": [
        {
          ""name"": ""data"",
          ""type"": ""ndarray"",
          ""shape"": ""(20640, 8)"",
          ""description"": ""Each row corresponding to the 8 feature values in order. If as_frame is True, data is a pandas object.""
        },
        {
          ""name"": ""target"",
          ""type"": ""numpy array"",
          ""shape"": ""(20640,)"",
          ""description"": ""Each value corresponds to the average house value in units of 100,000. If as_frame is True, target is a pandas object.""
        },
        {
          ""name"": ""feature_names"",
          ""type"": ""list"",
          ""length"": 8,
          ""description"": ""Array of ordered feature names used in the dataset.""
        },
        {
          ""name"": ""DESCR"",
          ""type"": ""str"",
          ""description"": ""Description of the California housing dataset.""
        },
        {
          ""name"": ""frame"",
          ""type"": ""pandas DataFrame"",
          ""description"": ""Only present when as_frame=True. DataFrame with data and target.""
        }
      ]
    },
    ""data_target_tuple"": {
      ""type"": ""tuple"",
      ""description"": ""A tuple of two ndarray. The first containing a 2D array of shape (n_samples, n_features) with each row representing one sample and each column representing the features. The second ndarray of shape (n_samples,) containing the target samples."",
      ""version_added"": ""0.20""
    }
  },
  ""examples"": [
    ""from sklearn.datasets import fetch_california_housing"",
    ""housing = fetch_california_housing()"",
    ""print(housing.data.shape, housing.target.shape)"",
    ""print(housing.feature_names[0:6])""
  ]
}
```"
make_classification,"Generate a random n-class classification problem.  This initially creates clusters of points normally distributed (std=1) about vertices of an ``n_informative``-dimensional hypercube with sides of length ``2*class_sep`` and assigns an equal number of clusters to each class. It introduces interdependence between these features and adds various types of further noise to the data.  Without shuffling, ``X`` horizontally stacks features in the following order: the primary ``n_informative`` features, followed by ``n_redundant`` linear combinations of the informative features, followed by ``n_repeated`` duplicates, drawn randomly with replacement from the informative and redundant features. The remaining features are filled with random noise. Thus, without shuffling, all useful features are contained in the columns ``X[:, :n_informative + n_redundant + n_repeated]``.  Read more in the :ref:`User Guide <sample_generators>`.  Parameters ---------- n_samples : int, default=100     The number of samples.  n_features : int, default=20     The total number of features. These comprise ``n_informative``     informative features, ``n_redundant`` redundant features,     ``n_repeated`` duplicated features and     ``n_features-n_informative-n_redundant-n_repeated`` useless features     drawn at random.  n_informative : int, default=2     The number of informative features. Each class is composed of a number     of gaussian clusters each located around the vertices of a hypercube     in a subspace of dimension ``n_informative``. For each cluster,     informative features are drawn independently from  N(0, 1) and then     randomly linearly combined within each cluster in order to add     covariance. The clusters are then placed on the vertices of the     hypercube.  n_redundant : int, default=2     The number of redundant features. These features are generated as     random linear combinations of the informative features.  n_repeated : int, default=0     The number of duplicated features, drawn randomly from the informative     and the redundant features.  n_classes : int, default=2     The number of classes (or labels) of the classification problem.  n_clusters_per_class : int, default=2     The number of clusters per class.  weights : array-like of shape (n_classes,) or (n_classes - 1,),              default=None     The proportions of samples assigned to each class. If None, then     classes are balanced. Note that if ``len(weights) == n_classes - 1``,     then the last class weight is automatically inferred.     More than ``n_samples`` samples may be returned if the sum of     ``weights`` exceeds 1. Note that the actual class proportions will     not exactly match ``weights`` when ``flip_y`` isn't 0.  flip_y : float, default=0.01     The fraction of samples whose class is assigned randomly. Larger     values introduce noise in the labels and make the classification     task harder. Note that the default setting flip_y > 0 might lead     to less than ``n_classes`` in y in some cases.  class_sep : float, default=1.0     The factor multiplying the hypercube size.  Larger values spread     out the clusters/classes and make the classification task easier.  hypercube : bool, default=True     If True, the clusters are put on the vertices of a hypercube. If     False, the clusters are put on the vertices of a random polytope.  shift : float, ndarray of shape (n_features,) or None, default=0.0     Shift features by the specified value. If None, then features     are shifted by a random value drawn in [-class_sep, class_sep].  scale : float, ndarray of shape (n_features,) or None, default=1.0     Multiply features by the specified value. If None, then features     are scaled by a random value drawn in [1, 100]. Note that scaling     happens after shifting.  shuffle : bool, default=True     Shuffle the samples and the features.  random_state : int, RandomState instance or None, default=None     Determines random number generation for dataset creation. Pass an int     for reproducible output across multiple function calls.     See :term:`Glossary <random_state>`.  Returns ------- X : ndarray of shape (n_samples, n_features)     The generated samples.  y : ndarray of shape (n_samples,)     The integer labels for class membership of each sample.  See Also -------- make_blobs : Simplified variant. make_multilabel_classification : Unrelated generator for multilabel tasks.  Notes ----- The algorithm is adapted from Guyon [1] and was designed to generate the ""Madelon"" dataset.  References ---------- .. [1] I. Guyon, ""Design of experiments for the NIPS 2003 variable        selection benchmark"", 2003.  Examples -------- >>> from sklearn.datasets import make_classification >>> X, y = make_classification(random_state=42) >>> X.shape (100, 20) >>> y.shape (100,) >>> list(y[:5]) [np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(0)]","```json
{
  ""function"": {
    ""name"": ""make_classification"",
    ""module"": ""sklearn.datasets"",
    ""description"": ""Generate a random n-class classification problem. This initially creates clusters of points normally distributed (std=1) about vertices of an ``n_informative``-dimensional hypercube with sides of length ``2*class_sep`` and assigns an equal number of clusters to each class. It introduces interdependence between these features and adds various types of further noise to the data. Without shuffling, ``X`` horizontally stacks features in the following order: the primary ``n_informative`` features, followed by ``n_redundant`` linear combinations of the informative features, followed by ``n_repeated`` duplicates, drawn randomly with replacement from the informative and redundant features. The remaining features are filled with random noise. Thus, without shuffling, all useful features are contained in the columns ``X[:, :n_informative + n_redundant + n_repeated]``."",
    ""parameters"": [
      {
        ""name"": ""n_samples"",
        ""type"": ""int"",
        ""default"": 100,
        ""description"": ""The number of samples.""
      },
      {
        ""name"": ""n_features"",
        ""type"": ""int"",
        ""default"": 20,
        ""description"": ""The total number of features. These comprise ``n_informative`` informative features, ``n_redundant`` redundant features, ``n_repeated`` duplicated features and ``n_features-n_informative-n_redundant-n_repeated`` useless features drawn at random.""
      },
      {
        ""name"": ""n_informative"",
        ""type"": ""int"",
        ""default"": 2,
        ""description"": ""The number of informative features. Each class is composed of a number of gaussian clusters each located around the vertices of a hypercube in a subspace of dimension ``n_informative``. For each cluster, informative features are drawn independently from N(0, 1) and then randomly linearly combined within each cluster in order to add covariance. The clusters are then placed on the vertices of the hypercube.""
      },
      {
        ""name"": ""n_redundant"",
        ""type"": ""int"",
        ""default"": 2,
        ""description"": ""The number of redundant features. These features are generated as random linear combinations of the informative features.""
      },
      {
        ""name"": ""n_repeated"",
        ""type"": ""int"",
        ""default"": 0,
        ""description"": ""The number of duplicated features, drawn randomly from the informative and the redundant features.""
      },
      {
        ""name"": ""n_classes"",
        ""type"": ""int"",
        ""default"": 2,
        ""description"": ""The number of classes (or labels) of the classification problem.""
      },
      {
        ""name"": ""n_clusters_per_class"",
        ""type"": ""int"",
        ""default"": 2,
        ""description"": ""The number of clusters per class.""
      },
      {
        ""name"": ""weights"",
        ""type"": ""array-like of shape (n_classes,) or (n_classes - 1,) or None"",
        ""default"": null,
        ""description"": ""The proportions of samples assigned to each class. If None, then classes are balanced. Note that if ``len(weights) == n_classes - 1``, then the last class weight is automatically inferred. More than ``n_samples`` samples may be returned if the sum of ``weights`` exceeds 1. Note that the actual class proportions will not exactly match ``weights`` when ``flip_y`` isn't 0.""
      },
      {
        ""name"": ""flip_y"",
        ""type"": ""float"",
        ""default"": 0.01,
        ""description"": ""The fraction of samples whose class is assigned randomly. Larger values introduce noise in the labels and make the classification task harder. Note that the default setting flip_y > 0 might lead to less than ``n_classes`` in y in some cases.""
      },
      {
        ""name"": ""class_sep"",
        ""type"": ""float"",
        ""default"": 1.0,
        ""description"": ""The factor multiplying the hypercube size. Larger values spread out the clusters/classes and make the classification task easier.""
      },
      {
        ""name"": ""hypercube"",
        ""type"": ""bool"",
        ""default"": true,
        ""description"": ""If True, the clusters are put on the vertices of a hypercube. If False, the clusters are put on the vertices of a random polytope.""
      },
      {
        ""name"": ""shift"",
        ""type"": ""float, ndarray of shape (n_features,) or None"",
        ""default"": 0.0,
        ""description"": ""Shift features by the specified value. If None, then features are shifted by a random value drawn in [-class_sep, class_sep].""
      },
      {
        ""name"": ""scale"",
        ""type"": ""float, ndarray of shape (n_features,) or None"",
        ""default"": 1.0,
        ""description"": ""Multiply features by the specified value. If None, then features are scaled by a random value drawn in [1, 100]. Note that scaling happens after shifting.""
      },
      {
        ""name"": ""shuffle"",
        ""type"": ""bool"",
        ""default"": true,
        ""description"": ""Shuffle the samples and the features.""
      },
      {
        ""name"": ""random_state"",
        ""type"": ""int, RandomState instance or None"",
        ""default"": null,
        ""description"": ""Determines random number generation for dataset creation. Pass an int for reproducible output across multiple function calls.""
      }
    ],
    ""returns"": [
      {
        ""name"": ""X"",
        ""type"": ""ndarray of shape (n_samples, n_features)"",
        ""description"": ""The generated samples.""
      },
      {
        ""name"": ""y"",
        ""type"": ""ndarray of shape (n_samples,)"",
        ""description"": ""The integer labels for class membership of each sample.""
      }
    ],
    ""see_also"": [
      {
        ""name"": ""make_blobs"",
        ""description"": ""Simplified variant.""
      },
      {
        ""name"": ""make_multilabel_classification"",
        ""description"": ""Unrelated generator for multilabel tasks.""
      }
    ],
    ""notes"": ""The algorithm is adapted from Guyon [1] and was designed to generate the \""Madelon\"" dataset."",
    ""references"": [
      ""I. Guyon, \""Design of experiments for the NIPS 2003 variable selection benchmark\"", 2003.""
    ]
  }
}
```","Here's a structured representation of the API in JSON format:

```json
{
  ""function"": ""make_classification"",
  ""description"": ""Generate a random n-class classification problem."",
  ""parameters"": [
    {
      ""name"": ""n_samples"",
      ""type"": ""int"",
      ""default"": 100,
      ""description"": ""The number of samples.""
    },
    {
      ""name"": ""n_features"",
      ""type"": ""int"",
      ""default"": 20,
      ""description"": ""The total number of features. These comprise n_informative informative features, n_redundant redundant features, n_repeated duplicated features and n_features-n_informative-n_redundant-n_repeated useless features drawn at random.""
    },
    {
      ""name"": ""n_informative"",
      ""type"": ""int"",
      ""default"": 2,
      ""description"": ""The number of informative features. Each class is composed of a number of gaussian clusters located around the vertices of a hypercube in a subspace of dimension n_informative.""
    },
    {
      ""name"": ""n_redundant"",
      ""type"": ""int"",
      ""default"": 2,
      ""description"": ""The number of redundant features. These features are generated as random linear combinations of the informative features.""
    },
    {
      ""name"": ""n_repeated"",
      ""type"": ""int"",
      ""default"": 0,
      ""description"": ""The number of duplicated features, drawn randomly from the informative and the redundant features.""
    },
    {
      ""name"": ""n_classes"",
      ""type"": ""int"",
      ""default"": 2,
      ""description"": ""The number of classes (or labels) of the classification problem.""
    },
    {
      ""name"": ""n_clusters_per_class"",
      ""type"": ""int"",
      ""default"": 2,
      ""description"": ""The number of clusters per class.""
    },
    {
      ""name"": ""weights"",
      ""type"": ""array-like"",
      ""shape"": ""(n_classes,) or (n_classes - 1,)"",
      ""default"": null,
      ""description"": ""The proportions of samples assigned to each class. If None, then classes are balanced.""
    },
    {
      ""name"": ""flip_y"",
      ""type"": ""float"",
      ""default"": 0.01,
      ""description"": ""The fraction of samples whose class is assigned randomly. Larger values introduce noise in the labels and make the classification task harder.""
    },
    {
      ""name"": ""class_sep"",
      ""type"": ""float"",
      ""default"": 1.0,
      ""description"": ""The factor multiplying the hypercube size. Larger values spread out the clusters/classes and make the classification task easier.""
    },
    {
      ""name"": ""hypercube"",
      ""type"": ""bool"",
      ""default"": true,
      ""description"": ""If True, the clusters are put on the vertices of a hypercube. If False, the clusters are put on the vertices of a random polytope.""
    },
    {
      ""name"": ""shift"",
      ""type"": [""float"", ""ndarray""],
      ""shape"": ""(n_features,)"",
      ""default"": 0.0,
      ""description"": ""Shift features by the specified value. If None, then features are shifted by a random value drawn in [-class_sep, class_sep].""
    },
    {
      ""name"": ""scale"",
      ""type"": [""float"", ""ndarray""],
      ""shape"": ""(n_features,)"",
      ""default"": 1.0,
      ""description"": ""Multiply features by the specified value. If None, then features are scaled by a random value drawn in [1, 100].""
    },
    {
      ""name"": ""shuffle"",
      ""type"": ""bool"",
      ""default"": true,
      ""description"": ""Shuffle the samples and the features.""
    },
    {
      ""name"": ""random_state"",
      ""type"": [""int"", ""RandomState instance"", ""None""],
      ""default"": null,
      ""description"": ""Determines random number generation for dataset creation. Pass an int for reproducible output across multiple function calls.""
    }
  ],
  ""returns"": [
    {
      ""name"": ""X"",
      ""type"": ""ndarray"",
      ""shape"": ""(n_samples, n_features)"",
      ""description"": ""The generated samples.""
    },
    {
      ""name"": ""y"",
      ""type"": ""ndarray"",
      ""shape"": ""(n_samples,)"",
      ""description"": ""The integer labels for class membership of each sample.""
    }
  ],
  ""see_also"": [
    {
      ""function"": ""make_blobs"",
      ""description"": ""Simplified variant.""
    },
    {
      ""function"": ""make_multilabel_classification"",
      ""description"": ""Unrelated generator for multilabel tasks.""
    }
  ],
  ""notes"": [
    ""The algorithm is adapted from Guyon and was designed to generate the Madelon dataset.""
  ],
  ""references"": [
    {
      ""author"": ""I. Guyon"",
      ""title"": ""Design of experiments for the NIPS 2003 variable selection benchmark"",
      ""year"": 2003
    }
  ],
  ""examples"": [
    ""from sklearn.datasets import make_classification"",
    ""X, y = make_classification(random_state=42)"",
    ""X.shape (100, 20)"",
    ""y.shape (100,)"",
    ""list(y[:5]) [np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(0)]""
  ]
}
```"
train_test_split,"Split arrays or matrices into random train and test subsets.  Quick utility that wraps input validation, ``next(ShuffleSplit().split(X, y))``, and application to input data into a single call for splitting (and optionally subsampling) data into a one-liner.  Read more in the :ref:`User Guide <cross_validation>`.  Parameters ---------- *arrays : sequence of indexables with same length / shape[0]     Allowed inputs are lists, numpy arrays, scipy-sparse     matrices or pandas dataframes.  test_size : float or int, default=None     If float, should be between 0.0 and 1.0 and represent the proportion     of the dataset to include in the test split. If int, represents the     absolute number of test samples. If None, the value is set to the     complement of the train size. If ``train_size`` is also None, it will     be set to 0.25.  train_size : float or int, default=None     If float, should be between 0.0 and 1.0 and represent the     proportion of the dataset to include in the train split. If     int, represents the absolute number of train samples. If None,     the value is automatically set to the complement of the test size.  random_state : int, RandomState instance or None, default=None     Controls the shuffling applied to the data before applying the split.     Pass an int for reproducible output across multiple function calls.     See :term:`Glossary <random_state>`.  shuffle : bool, default=True     Whether or not to shuffle the data before splitting. If shuffle=False     then stratify must be None.  stratify : array-like, default=None     If not None, data is split in a stratified fashion, using this as     the class labels.     Read more in the :ref:`User Guide <stratification>`.  Returns ------- splitting : list, length=2 * len(arrays)     List containing train-test split of inputs.      .. versionadded:: 0.16         If the input is sparse, the output will be a         ``scipy.sparse.csr_matrix``. Else, output type is the same as the         input type.  Examples -------- >>> import numpy as np >>> from sklearn.model_selection import train_test_split >>> X, y = np.arange(10).reshape((5, 2)), range(5) >>> X array([[0, 1],        [2, 3],        [4, 5],        [6, 7],        [8, 9]]) >>> list(y) [0, 1, 2, 3, 4]  >>> X_train, X_test, y_train, y_test = train_test_split( ...     X, y, test_size=0.33, random_state=42) ... >>> X_train array([[4, 5],        [0, 1],        [6, 7]]) >>> y_train [2, 0, 3] >>> X_test array([[2, 3],        [8, 9]]) >>> y_test [1, 4]  >>> train_test_split(y, shuffle=False) [[0, 1, 2], [3, 4]]","```json
{
  ""function"": {
    ""name"": ""train_test_split"",
    ""description"": ""Split arrays or matrices into random train and test subsets. Quick utility that wraps input validation, next(ShuffleSplit().split(X, y)), and application to input data into a single call for splitting (and optionally subsampling) data into a one-liner."",
    ""parameters"": [
      {
        ""name"": ""arrays"",
        ""type"": ""sequence of indexables"",
        ""description"": ""Allowed inputs are lists, numpy arrays, scipy-sparse matrices or pandas dataframes. Must have same length / shape[0]."",
        ""is_variadic"": true,
        ""required"": true
      },
      {
        ""name"": ""test_size"",
        ""type"": ""float or int or None"",
        ""description"": ""If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the test split. If int, represents the absolute number of test samples. If None, the value is set to the complement of the train size. If train_size is also None, it will be set to 0.25."",
        ""default"": ""None"",
        ""required"": false
      },
      {
        ""name"": ""train_size"",
        ""type"": ""float or int or None"",
        ""description"": ""If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the train split. If int, represents the absolute number of train samples. If None, the value is automatically set to the complement of the test size."",
        ""default"": ""None"",
        ""required"": false
      },
      {
        ""name"": ""random_state"",
        ""type"": ""int or RandomState instance or None"",
        ""description"": ""Controls the shuffling applied to the data before applying the split. Pass an int for reproducible output across multiple function calls. See Glossary."",
        ""default"": ""None"",
        ""required"": false
      },
      {
        ""name"": ""shuffle"",
        ""type"": ""bool"",
        ""description"": ""Whether or not to shuffle the data before splitting. If shuffle=False then stratify must be None."",
        ""default"": ""True"",
        ""required"": false
      },
      {
        ""name"": ""stratify"",
        ""type"": ""array-like or None"",
        ""description"": ""If not None, data is split in a stratified fashion, using this as the class labels."",
        ""default"": ""None"",
        ""required"": false
      }
    ],
    ""returns"": {
      ""type"": ""list"",
      ""description"": ""List containing train-test split of inputs. Length is 2 * len(arrays). If the input is sparse, the output will be a scipy.sparse.csr_matrix. Else, output type is the same as the input type.""
    },
    ""references"": [
        {
            ""type"": ""User Guide"",
            ""ref"": ""cross_validation""
        },
        {
            ""type"": ""User Guide"",
            ""ref"": ""stratification""
        },
        {
            ""type"": ""Glossary"",
            ""ref"": ""random_state""
        }
    ]
  }
}
```","Below is the structured representation of the API, represented in JSON format:

```json
{
  ""function"": ""train_test_split"",
  ""description"": ""Split arrays or matrices into random train and test subsets. A utility that wraps input validation, next(ShuffleSplit().split(X, y)), and application to input data into a single call for splitting (and optionally subsampling) data into a one-liner."",
  ""parameters"": [
    {
      ""name"": ""*arrays"",
      ""type"": ""sequence of indexables"",
      ""description"": ""Allowed inputs are lists, numpy arrays, scipy-sparse matrices or pandas dataframes."",
      ""requirements"": ""same length / shape[0]""
    },
    {
      ""name"": ""test_size"",
      ""type"": ""float or int"",
      ""default"": ""None"",
      ""description"": ""If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the test split. If int, represents the absolute number of test samples. If None, the value is set to the complement of the train size. If train_size is also None, it will be set to 0.25.""
    },
    {
      ""name"": ""train_size"",
      ""type"": ""float or int"",
      ""default"": ""None"",
      ""description"": ""If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the train split. If int, represents the absolute number of train samples. If None, the value is automatically set to the complement of the test size.""
    },
    {
      ""name"": ""random_state"",
      ""type"": ""int, RandomState instance or None"",
      ""default"": ""None"",
      ""description"": ""Controls the shuffling applied to the data before applying the split. Pass an int for reproducible output across multiple function calls. See Glossary <random_state>.""
    },
    {
      ""name"": ""shuffle"",
      ""type"": ""bool"",
      ""default"": ""True"",
      ""description"": ""Whether or not to shuffle the data before splitting. If shuffle=False then stratify must be None.""
    },
    {
      ""name"": ""stratify"",
      ""type"": ""array-like"",
      ""default"": ""None"",
      ""description"": ""If not None, data is split in a stratified fashion, using this as the class labels. Read more in the User Guide <stratification>.""
    }
  ],
  ""returns"": {
    ""name"": ""splitting"",
    ""type"": ""list"",
    ""length"": ""2 * len(arrays)"",
    ""description"": ""List containing train-test split of inputs. If the input is sparse, the output will be a scipy.sparse.csr_matrix. Else, output type is the same as the input type."",
    ""versionadded"": ""0.16""
  },
  ""examples"": [
    {
      ""code"": ""import numpy as np\nfrom sklearn.model_selection import train_test_split\nX, y = np.arange(10).reshape((5, 2)), range(5)\nX\n# Result: array([[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]])\nlist(y)\n# Result: [0, 1, 2, 3, 4]"",
      ""description"": ""Basic setup for data.""
    },
    {
      ""code"": ""X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\nX_train\n# Result: array([[4, 5], [0, 1], [6, 7]])\ny_train\n# Result: [2, 0, 3]\nX_test\n# Result: array([[2, 3], [8, 9]])\ny_test\n# Result: [1, 4]"",
      ""description"": ""Example of train-test split with test_size=0.33 and random_state=42.""
    },
    {
      ""code"": ""train_test_split(y, shuffle=False)\n# Result: [[0, 1, 2], [3, 4]]"",
      ""description"": ""Example of splitting without shuffling.""
    }
  ]
}
``` 

This representation in JSON captures the structure, parameters, return type, and examples provided in the original API documentation."
StandardScaler,"Standardize features by removing the mean and scaling to unit variance.  The standard score of a sample `x` is calculated as:  .. code-block:: text      z = (x - u) / s  where `u` is the mean of the training samples or zero if `with_mean=False`, and `s` is the standard deviation of the training samples or one if `with_std=False`.  Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Mean and standard deviation are then stored to be used on later data using :meth:`transform`.  Standardization of a dataset is a common requirement for many machine learning estimators: they might behave badly if the individual features do not more or less look like standard normally distributed data (e.g. Gaussian with 0 mean and unit variance).  For instance many elements used in the objective function of a learning algorithm (such as the RBF kernel of Support Vector Machines or the L1 and L2 regularizers of linear models) assume that all features are centered around 0 and have variance in the same order. If a feature has a variance that is orders of magnitude larger than others, it might dominate the objective function and make the estimator unable to learn from other features correctly as expected.  `StandardScaler` is sensitive to outliers, and the features may scale differently from each other in the presence of outliers. For an example visualization, refer to :ref:`Compare StandardScaler with other scalers <plot_all_scaling_standard_scaler_section>`.  This scaler can also be applied to sparse CSR or CSC matrices by passing `with_mean=False` to avoid breaking the sparsity structure of the data.  Read more in the :ref:`User Guide <preprocessing_scaler>`.  Parameters ---------- copy : bool, default=True     If False, try to avoid a copy and do inplace scaling instead.     This is not guaranteed to always work inplace; e.g. if the data is     not a NumPy array or scipy.sparse CSR matrix, a copy may still be     returned.  with_mean : bool, default=True     If True, center the data before scaling.     This does not work (and will raise an exception) when attempted on     sparse matrices, because centering them entails building a dense     matrix which in common use cases is likely to be too large to fit in     memory.  with_std : bool, default=True     If True, scale the data to unit variance (or equivalently,     unit standard deviation).  Attributes ---------- scale_ : ndarray of shape (n_features,) or None     Per feature relative scaling of the data to achieve zero mean and unit     variance. Generally this is calculated using `np.sqrt(var_)`. If a     variance is zero, we can't achieve unit variance, and the data is left     as-is, giving a scaling factor of 1. `scale_` is equal to `None`     when `with_std=False`.      .. versionadded:: 0.17        *scale_*  mean_ : ndarray of shape (n_features,) or None     The mean value for each feature in the training set.     Equal to ``None`` when ``with_mean=False`` and ``with_std=False``.  var_ : ndarray of shape (n_features,) or None     The variance for each feature in the training set. Used to compute     `scale_`. Equal to ``None`` when ``with_mean=False`` and     ``with_std=False``.  n_features_in_ : int     Number of features seen during :term:`fit`.      .. versionadded:: 0.24  feature_names_in_ : ndarray of shape (`n_features_in_`,)     Names of features seen during :term:`fit`. Defined only when `X`     has feature names that are all strings.      .. versionadded:: 1.0  n_samples_seen_ : int or ndarray of shape (n_features,)     The number of samples processed by the estimator for each feature.     If there are no missing samples, the ``n_samples_seen`` will be an     integer, otherwise it will be an array of dtype int. If     `sample_weights` are used it will be a float (if no missing data)     or an array of dtype float that sums the weights seen so far.     Will be reset on new calls to fit, but increments across     ``partial_fit`` calls.  See Also -------- scale : Equivalent function without the estimator API.  :class:`~sklearn.decomposition.PCA` : Further removes the linear     correlation across features with 'whiten=True'.  Notes ----- NaNs are treated as missing values: disregarded in fit, and maintained in transform.  We use a biased estimator for the standard deviation, equivalent to `numpy.std(x, ddof=0)`. Note that the choice of `ddof` is unlikely to affect model performance.  Examples -------- >>> from sklearn.preprocessing import StandardScaler >>> data = [[0, 0], [0, 0], [1, 1], [1, 1]] >>> scaler = StandardScaler() >>> print(scaler.fit(data)) StandardScaler() >>> print(scaler.mean_) [0.5 0.5] >>> print(scaler.transform(data)) [[-1. -1.]  [-1. -1.]  [ 1.  1.]  [ 1.  1.]] >>> print(scaler.transform([[2, 2]])) [[3. 3.]]","```json
{
  ""classes"": [
    {
      ""name"": ""StandardScaler"",
      ""description"": ""Standardize features by removing the mean and scaling to unit variance. The standard score of a sample `x` is calculated as `z = (x - u) / s` where `u` is the mean of the training samples or zero if `with_mean=False`, and `s` is the standard deviation of the training samples or one if `with_std=False`. Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Mean and standard deviation are then stored to be used on later data using the `transform` method."",
      ""parameters"": [
        {
          ""name"": ""copy"",
          ""type"": ""bool"",
          ""default"": ""True"",
          ""description"": ""If False, try to avoid a copy and do inplace scaling instead. This is not guaranteed to always work inplace; e.g. if the data is not a NumPy array or scipy.sparse CSR matrix, a copy may still be returned.""
        },
        {
          ""name"": ""with_mean"",
          ""type"": ""bool"",
          ""default"": ""True"",
          ""description"": ""If True, center the data before scaling. This does not work (and will raise an exception) when attempted on sparse matrices, because centering them entails building a dense matrix which in common use cases is likely to be too large to fit in memory.""
        },
        {
          ""name"": ""with_std"",
          ""type"": ""bool"",
          ""default"": ""True"",
          ""description"": ""If True, scale the data to unit variance (or equivalently, unit standard deviation).""
        }
      ],
      ""attributes"": [
        {
          ""name"": ""scale_"",
          ""type"": ""ndarray of shape (n_features,) or None"",
          ""description"": ""Per feature relative scaling of the data to achieve zero mean and unit variance. Generally this is calculated using `np.sqrt(var_)`. If a variance is zero, we can't achieve unit variance, and the data is left as-is, giving a scaling factor of 1. `scale_` is equal to `None` when `with_std=False`.""
        },
        {
          ""name"": ""mean_"",
          ""type"": ""ndarray of shape (n_features,) or None"",
          ""description"": ""The mean value for each feature in the training set. Equal to ``None`` when ``with_mean=False`` and ``with_std=False``.""
        },
        {
          ""name"": ""var_"",
          ""type"": ""ndarray of shape (n_features,) or None"",
          ""description"": ""The variance for each feature in the training set. Used to compute `scale_`. Equal to ``None`` when ``with_mean=False`` and ``with_std=False``.""
        },
        {
          ""name"": ""n_features_in_"",
          ""type"": ""int"",
          ""description"": ""Number of features seen during fit.""
        },
        {
          ""name"": ""feature_names_in_"",
          ""type"": ""ndarray of shape (n_features_in_,) or None"",
          ""description"": ""Names of features seen during fit. Defined only when `X` has feature names that are all strings.""
        },
        {
          ""name"": ""n_samples_seen_"",
          ""type"": ""int or ndarray of shape (n_features,) or float"",
          ""description"": ""The number of samples processed by the estimator for each feature. If there are no missing samples, the ``n_samples_seen`` will be an integer, otherwise it will be an array of dtype int. If `sample_weights` are used it will be a float (if no missing data) or an array of dtype float that sums the weights seen so far. Will be reset on new calls to fit, but increments across `partial_fit` calls.""
        }
      ],
       ""methods"": [
           {
              ""name"": ""fit"",
              ""description"": ""Compute the mean and standard deviation on the training data.""
              // Takes X, y (optional), sample_weight (optional)
          },
           {
              ""name"": ""transform"",
              ""description"": ""Perform standardization by centering and scaling using the mean and standard deviation computed during fit.""
              // Takes X
          },
          {
              ""name"": ""fit_transform"",
              ""description"": ""Fit to data, then transform it.""
              // Takes X, y (optional), sample_weight (optional)
          }
      ],
      ""see_also"": [
        {
          ""name"": ""scale"",
          ""type"": ""function"",
          ""description"": ""Equivalent function without the estimator API.""
        },
        {
          ""name"": ""PCA"",
          ""type"": ""class"",
          ""description"": ""Further removes the linear correlation across features with 'whiten=True'.""
        }
      ],
      ""notes"": [
        ""NaNs are treated as missing values: disregarded in fit, and maintained in transform."",
        ""We use a biased estimator for the standard deviation, equivalent to `numpy.std(x, ddof=0)`. Note that the choice of `ddof` is unlikely to affect model performance.""
      ],
      ""example"": {
          ""description"": ""Basic usage example"",
          ""code"": "">>> from sklearn.preprocessing import StandardScaler\n>>> data = [[0, 0], [0, 0], [1, 1], [1, 1]]\n>>> scaler = StandardScaler()\n>>> print(scaler.fit(data))\nStandardScaler()\n>>> print(scaler.mean_)\n[0.5 0.5]\n>>> print(scaler.transform(data))\n[[-1. -1.]\n [-1. -1.]\n [ 1.  1.]\n [ 1.  1.]]\n>>> print(scaler.transform([[2, 2]]))\n[[3. 3.]]""
      }
    }
  ]
}
```","```json
{
  ""class"": ""StandardScaler"",
  ""description"": ""Standardize features by removing the mean and scaling to unit variance."",
  ""methods"": [
    {
      ""name"": ""fit"",
      ""description"": ""Compute the mean and std to be used for later scaling."",
      ""example"": ""scaler.fit(data)""
    },
    {
      ""name"": ""transform"",
      ""description"": ""Perform standardization by centering and scaling."",
      ""example"": ""scaler.transform(data)"",
      ""relationships"": [
        ""requires fit to be performed first""
      ]
    },
    {
      ""name"": ""fit_transform"",
      ""description"": ""Fit to data, then transform it."",
      ""example"": ""scaler.fit_transform(data)"",
      ""relationships"": [
        ""combines fit and transform""
      ]
    }
  ],
  ""parameters"": [
    {
      ""name"": ""copy"",
      ""type"": ""bool"",
      ""default"": true,
      ""description"": ""If False, try to avoid a copy and do inplace scaling instead. Not guaranteed for all data types.""
    },
    {
      ""name"": ""with_mean"",
      ""type"": ""bool"",
      ""default"": true,
      ""description"": ""If True, center the data before scaling. Not applicable to sparse matrices.""
    },
    {
      ""name"": ""with_std"",
      ""type"": ""bool"",
      ""default"": true,
      ""description"": ""If True, scale the data to unit variance.""
    }
  ],
  ""attributes"": [
    {
      ""name"": ""scale_"",
      ""type"": ""ndarray of shape (n_features,) or None"",
      ""description"": ""Per feature relative scaling."",
      ""version_added"": ""0.17""
    },
    {
      ""name"": ""mean_"",
      ""type"": ""ndarray of shape (n_features,) or None"",
      ""description"": ""Mean value for each feature in the training set.""
    },
    {
      ""name"": ""var_"",
      ""type"": ""ndarray of shape (n_features,) or None"",
      ""description"": ""Variance for each feature in the training set.""
    },
    {
      ""name"": ""n_features_in_"",
      ""type"": ""int"",
      ""description"": ""Number of features seen during fit."",
      ""version_added"": ""0.24""
    },
    {
      ""name"": ""feature_names_in_"",
      ""type"": ""ndarray of shape (n_features_in_,)"",
      ""description"": ""Names of features seen during fit."",
      ""version_added"": ""1.0""
    },
    {
      ""name"": ""n_samples_seen_"",
      ""type"": ""int or ndarray of shape (n_features,)"",
      ""description"": ""Number of samples processed for each feature, varies with missing data.""
    }
  ],
  ""relationships"": {
    ""equivalent_function"": ""scale"",
    ""see_also"": [
      {
        ""reference"": ""PCA"",
        ""description"": ""Further removes the linear correlation across features with 'whiten=True'.""
      }
    ],
    ""special_notes"": {
      ""NaNs"": ""Treated as missing values: disregarded in fit, maintained in transform."",
      ""standard_deviation_estimator"": ""Biased estimator equivalent to numpy.std(x, ddof=0).""
    }
  },
  ""examples"": [
    {
      ""example"": [
        ""from sklearn.preprocessing import StandardScaler"",
        ""data = [[0, 0], [0, 0], [1, 1], [1, 1]]"",
        ""scaler = StandardScaler()"",
        ""scaler.fit(data)"",
        ""scaler.mean_ # Output: [0.5 0.5]"",
        ""scaler.transform(data) # Output: [[-1. -1.]  [-1. -1.]  [ 1.  1.]  [ 1.  1.]]"",
        ""scaler.transform([[2, 2]]) # Output: [[3. 3.]]""
      ],
      ""description"": ""Example usage of StandardScaler.""
    }
  ]
}
```"
LogisticRegression,"Logistic Regression (aka logit, MaxEnt) classifier.  This class implements regularized logistic regression using the 'liblinear' library, 'newton-cg', 'sag', 'saga' and 'lbfgs' solvers. **Note that regularization is applied by default**. It can handle both dense and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit floats for optimal performance; any other input format will be converted (and copied).  The 'newton-cg', 'sag', and 'lbfgs' solvers support only L2 regularization with primal formulation, or no regularization. The 'liblinear' solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty. The Elastic-Net regularization is only supported by the 'saga' solver.  For :term:`multiclass` problems, only 'newton-cg', 'sag', 'saga' and 'lbfgs' handle multinomial loss. 'liblinear' and 'newton-cholesky' only handle binary classification but can be extended to handle multiclass by using :class:`~sklearn.multiclass.OneVsRestClassifier`.  Read more in the :ref:`User Guide <logistic_regression>`.  Parameters ---------- penalty : {'l1', 'l2', 'elasticnet', None}, default='l2'     Specify the norm of the penalty:      - `None`: no penalty is added;     - `'l2'`: add a L2 penalty term and it is the default choice;     - `'l1'`: add a L1 penalty term;     - `'elasticnet'`: both L1 and L2 penalty terms are added.      .. warning::        Some penalties may not work with some solvers. See the parameter        `solver` below, to know the compatibility between the penalty and        solver.      .. versionadded:: 0.19        l1 penalty with SAGA solver (allowing 'multinomial' + L1)  dual : bool, default=False     Dual (constrained) or primal (regularized, see also     :ref:`this equation <regularized-logistic-loss>`) formulation. Dual formulation     is only implemented for l2 penalty with liblinear solver. Prefer dual=False when     n_samples > n_features.  tol : float, default=1e-4     Tolerance for stopping criteria.  C : float, default=1.0     Inverse of regularization strength; must be a positive float.     Like in support vector machines, smaller values specify stronger     regularization.  fit_intercept : bool, default=True     Specifies if a constant (a.k.a. bias or intercept) should be     added to the decision function.  intercept_scaling : float, default=1     Useful only when the solver 'liblinear' is used     and self.fit_intercept is set to True. In this case, x becomes     [x, self.intercept_scaling],     i.e. a ""synthetic"" feature with constant value equal to     intercept_scaling is appended to the instance vector.     The intercept becomes ``intercept_scaling * synthetic_feature_weight``.      Note! the synthetic feature weight is subject to l1/l2 regularization     as all other features.     To lessen the effect of regularization on synthetic feature weight     (and therefore on the intercept) intercept_scaling has to be increased.  class_weight : dict or 'balanced', default=None     Weights associated with classes in the form ``{class_label: weight}``.     If not given, all classes are supposed to have weight one.      The ""balanced"" mode uses the values of y to automatically adjust     weights inversely proportional to class frequencies in the input data     as ``n_samples / (n_classes * np.bincount(y))``.      Note that these weights will be multiplied with sample_weight (passed     through the fit method) if sample_weight is specified.      .. versionadded:: 0.17        *class_weight='balanced'*  random_state : int, RandomState instance, default=None     Used when ``solver`` == 'sag', 'saga' or 'liblinear' to shuffle the     data. See :term:`Glossary <random_state>` for details.  solver : {'lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga'},             default='lbfgs'      Algorithm to use in the optimization problem. Default is 'lbfgs'.     To choose a solver, you might want to consider the following aspects:      - For small datasets, 'liblinear' is a good choice, whereas 'sag'       and 'saga' are faster for large ones;     - For :term:`multiclass` problems, all solvers except 'liblinear' minimize the       full multinomial loss;     - 'liblinear' can only handle binary classification by default. To apply a       one-versus-rest scheme for the multiclass setting one can wrap it with the       :class:`~sklearn.multiclass.OneVsRestClassifier`.     - 'newton-cholesky' is a good choice for       `n_samples` >> `n_features * n_classes`, especially with one-hot encoded       categorical features with rare categories. Be aware that the memory usage       of this solver has a quadratic dependency on `n_features * n_classes`       because it explicitly computes the full Hessian matrix.      .. warning::        The choice of the algorithm depends on the penalty chosen and on        (multinomial) multiclass support:         ================= ============================== ======================        solver            penalty                        multinomial multiclass        ================= ============================== ======================        'lbfgs'           'l2', None                     yes        'liblinear'       'l1', 'l2'                     no        'newton-cg'       'l2', None                     yes        'newton-cholesky' 'l2', None                     no        'sag'             'l2', None                     yes        'saga'            'elasticnet', 'l1', 'l2', None yes        ================= ============================== ======================      .. note::        'sag' and 'saga' fast convergence is only guaranteed on features        with approximately the same scale. You can preprocess the data with        a scaler from :mod:`sklearn.preprocessing`.      .. seealso::        Refer to the :ref:`User Guide <Logistic_regression>` for more        information regarding :class:`LogisticRegression` and more specifically the        :ref:`Table <logistic_regression_solvers>`        summarizing solver/penalty supports.      .. versionadded:: 0.17        Stochastic Average Gradient descent solver.     .. versionadded:: 0.19        SAGA solver.     .. versionchanged:: 0.22         The default solver changed from 'liblinear' to 'lbfgs' in 0.22.     .. versionadded:: 1.2        newton-cholesky solver.  max_iter : int, default=100     Maximum number of iterations taken for the solvers to converge.  multi_class : {'auto', 'ovr', 'multinomial'}, default='auto'     If the option chosen is 'ovr', then a binary problem is fit for each     label. For 'multinomial' the loss minimised is the multinomial loss fit     across the entire probability distribution, *even when the data is     binary*. 'multinomial' is unavailable when solver='liblinear'.     'auto' selects 'ovr' if the data is binary, or if solver='liblinear',     and otherwise selects 'multinomial'.      .. versionadded:: 0.18        Stochastic Average Gradient descent solver for 'multinomial' case.     .. versionchanged:: 0.22         Default changed from 'ovr' to 'auto' in 0.22.     .. deprecated:: 1.5        ``multi_class`` was deprecated in version 1.5 and will be removed in 1.7.        From then on, the recommended 'multinomial' will always be used for        `n_classes >= 3`.        Solvers that do not support 'multinomial' will raise an error.        Use `sklearn.multiclass.OneVsRestClassifier(LogisticRegression())` if you        still want to use OvR.  verbose : int, default=0     For the liblinear and lbfgs solvers set verbose to any positive     number for verbosity.  warm_start : bool, default=False     When set to True, reuse the solution of the previous call to fit as     initialization, otherwise, just erase the previous solution.     Useless for liblinear solver. See :term:`the Glossary <warm_start>`.      .. versionadded:: 0.17        *warm_start* to support *lbfgs*, *newton-cg*, *sag*, *saga* solvers.  n_jobs : int, default=None     Number of CPU cores used when parallelizing over classes if     multi_class='ovr'"". This parameter is ignored when the ``solver`` is     set to 'liblinear' regardless of whether 'multi_class' is specified or     not. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`     context. ``-1`` means using all processors.     See :term:`Glossary <n_jobs>` for more details.  l1_ratio : float, default=None     The Elastic-Net mixing parameter, with ``0 <= l1_ratio <= 1``. Only     used if ``penalty='elasticnet'``. Setting ``l1_ratio=0`` is equivalent     to using ``penalty='l2'``, while setting ``l1_ratio=1`` is equivalent     to using ``penalty='l1'``. For ``0 < l1_ratio <1``, the penalty is a     combination of L1 and L2.  Attributes ----------  classes_ : ndarray of shape (n_classes, )     A list of class labels known to the classifier.  coef_ : ndarray of shape (1, n_features) or (n_classes, n_features)     Coefficient of the features in the decision function.      `coef_` is of shape (1, n_features) when the given problem is binary.     In particular, when `multi_class='multinomial'`, `coef_` corresponds     to outcome 1 (True) and `-coef_` corresponds to outcome 0 (False).  intercept_ : ndarray of shape (1,) or (n_classes,)     Intercept (a.k.a. bias) added to the decision function.      If `fit_intercept` is set to False, the intercept is set to zero.     `intercept_` is of shape (1,) when the given problem is binary.     In particular, when `multi_class='multinomial'`, `intercept_`     corresponds to outcome 1 (True) and `-intercept_` corresponds to     outcome 0 (False).  n_features_in_ : int     Number of features seen during :term:`fit`.      .. versionadded:: 0.24  feature_names_in_ : ndarray of shape (`n_features_in_`,)     Names of features seen during :term:`fit`. Defined only when `X`     has feature names that are all strings.      .. versionadded:: 1.0  n_iter_ : ndarray of shape (n_classes,) or (1, )     Actual number of iterations for all classes. If binary or multinomial,     it returns only 1 element. For liblinear solver, only the maximum     number of iteration across all classes is given.      .. versionchanged:: 0.20          In SciPy <= 1.0.0 the number of lbfgs iterations may exceed         ``max_iter``. ``n_iter_`` will now report at most ``max_iter``.  See Also -------- SGDClassifier : Incrementally trained logistic regression (when given     the parameter ``loss=""log_loss""``). LogisticRegressionCV : Logistic regression with built-in cross validation.  Notes ----- The underlying C implementation uses a random number generator to select features when fitting the model. It is thus not uncommon, to have slightly different results for the same input data. If that happens, try with a smaller tol parameter.  Predict output may not match that of standalone liblinear in certain cases. See :ref:`differences from liblinear <liblinear_differences>` in the narrative documentation.  References ----------  L-BFGS-B -- Software for Large-scale Bound-constrained Optimization     Ciyou Zhu, Richard Byrd, Jorge Nocedal and Jose Luis Morales.     http://users.iems.northwestern.edu/~nocedal/lbfgsb.html  LIBLINEAR -- A Library for Large Linear Classification     https://www.csie.ntu.edu.tw/~cjlin/liblinear/  SAG -- Mark Schmidt, Nicolas Le Roux, and Francis Bach     Minimizing Finite Sums with the Stochastic Average Gradient     https://hal.inria.fr/hal-00860051/document  SAGA -- Defazio, A., Bach F. & Lacoste-Julien S. (2014).         :arxiv:`""SAGA: A Fast Incremental Gradient Method With Support         for Non-Strongly Convex Composite Objectives"" <1407.0202>`  Hsiang-Fu Yu, Fang-Lan Huang, Chih-Jen Lin (2011). Dual coordinate descent     methods for logistic regression and maximum entropy models.     Machine Learning 85(1-2):41-75.     https://www.csie.ntu.edu.tw/~cjlin/papers/maxent_dual.pdf  Examples -------- >>> from sklearn.datasets import load_iris >>> from sklearn.linear_model import LogisticRegression >>> X, y = load_iris(return_X_y=True) >>> clf = LogisticRegression(random_state=0).fit(X, y) >>> clf.predict(X[:2, :]) array([0, 0]) >>> clf.predict_proba(X[:2, :]) array([[9.8...e-01, 1.8...e-02, 1.4...e-08],        [9.7...e-01, 2.8...e-02, ...e-08]]) >>> clf.score(X, y) 0.97...  For a comaprison of the LogisticRegression with other classifiers see: :ref:`sphx_glr_auto_examples_classification_plot_classification_probability.py`.","```json
{
  ""name"": ""LogisticRegression"",
  ""description"": ""Logistic Regression (aka logit, MaxEnt) classifier. This class implements regularized logistic regression using the 'liblinear' library, 'newton-cg', 'sag', 'saga' and 'lbfgs' solvers.\n**Note that regularization is applied by default**.\nIt can handle both dense and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit floats for optimal performance; any other input format will be converted (and copied).\nThe 'newton-cg', 'sag', and 'lbfgs' solvers support only L2 regularization with primal formulation, or no regularization. The 'liblinear' solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty. The Elastic-Net regularization is only supported by the 'saga' solver.\nFor multiclass problems, only 'newton-cg', 'sag', 'saga' and 'lbfgs' handle multinomial loss. 'liblinear' and 'newton-cholesky' only handle binary classification but can be extended to handle multiclass by using `sklearn.multiclass.OneVsRestClassifier`.\nRead more in the User Guide."",
  ""parameters"": [
    {
      ""name"": ""penalty"",
      ""type"": ""('l1', 'l2', 'elasticnet', None)"",
      ""default"": ""'l2'"",
      ""description"": ""Specify the norm of the penalty:\n- `None`: no penalty is added;\n- `'l2'`: add a L2 penalty term and it is the default choice;\n- `'l1'`: add a L1 penalty term;\n- `'elasticnet'`: both L1 and L2 penalty terms are added.\n\nWarning: Some penalties may not work with some solvers. See the parameter `solver` below, to know the compatibility between the penalty and solver.\n\nversionadded:: 0.19\nl1 penalty with SAGA solver (allowing 'multinomial' + L1)""
    },
    {
      ""name"": ""dual"",
      ""type"": ""bool"",
      ""default"": ""False"",
      ""description"": ""Dual (constrained) or primal (regularized, see also this equation) formulation. Dual formulation is only implemented for l2 penalty with liblinear solver. Prefer dual=False when n_samples > n_features.""
    },
    {
      ""name"": ""tol"",
      ""type"": ""float"",
      ""default"": ""1e-4"",
      ""description"": ""Tolerance for stopping criteria.""
    },
    {
      ""name"": ""C"",
      ""type"": ""float"",
      ""default"": ""1.0"",
      ""description"": ""Inverse of regularization strength; must be a positive float. Like in support vector machines, smaller values specify stronger regularization.""
    },
    {
      ""name"": ""fit_intercept"",
      ""type"": ""bool"",
      ""default"": ""True"",
      ""description"": ""Specifies if a constant (a.k.a. bias or intercept) should be added to the decision function.""
    },
    {
      ""name"": ""intercept_scaling"",
      ""type"": ""float"",
      ""default"": ""1"",
      ""description"": ""Useful only when the solver 'liblinear' is used and self.fit_intercept is set to True. In this case, x becomes [x, self.intercept_scaling], i.e. a \""synthetic\"" feature with constant value equal to intercept_scaling is appended to the instance vector. The intercept becomes ``intercept_scaling * synthetic_feature_weight``. Note! the synthetic feature weight is subject to l1/l2 regularization as all other features. To lessen the effect of regularization on synthetic feature weight (and therefore on the intercept) intercept_scaling has to be increased.""
    },
    {
      ""name"": ""class_weight"",
      ""type"": ""dict or 'balanced'"",
      ""default"": ""None"",
      ""description"": ""Weights associated with classes in the form ``{class_label: weight}``. If not given, all classes are supposed to have weight one. The \""balanced\"" mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as ``n_samples / (n_classes * np.bincount(y))``. Note that these weights will be multiplied with sample_weight (passed through the fit method) if sample_weight is specified.\n\nversionadded:: 0.17\n*class_weight='balanced'*""
    },
    {
      ""name"": ""random_state"",
      ""type"": ""int, RandomState instance or None"",
      ""default"": ""None"",
      ""description"": ""Used when ``solver`` == 'sag', 'saga' or 'liblinear' to shuffle the data. See Glossary for details.""
    },
    {
      ""name"": ""solver"",
      ""type"": ""('lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga')"",
      ""default"": ""'lbfgs'"",
      ""description"": ""Algorithm to use in the optimization problem. Default is 'lbfgs'. To choose a solver, you might want to consider the following aspects:\n- For small datasets, 'liblinear' is a good choice, whereas 'sag' and 'saga' are faster for large ones;\n- For multiclass problems, all solvers except 'liblinear' minimize the full multinomial loss;\n- 'liblinear' can only handle binary classification by default. To apply a one-versus-rest scheme for the multiclass setting one can wrap it with the `sklearn.multiclass.OneVsRestClassifier`.\n- 'newton-cholesky' is a good choice for `n_samples` >> `n_features * n_classes`, especially with one-hot encoded categorical features with rare categories. Be aware that the memory usage of this solver has a quadratic dependency on `n_features * n_classes` because it explicitly computes the full Hessian matrix.\n\nWarning: The choice of the algorithm depends on the penalty chosen and on (multinomial) multiclass support:\n\n================= ============================== ======================\nsolver            penalty                        multinomial multiclass\n================= ============================== ======================\n'lbfgs'           'l2', None                     yes\n'liblinear'       'l1', 'l2'                     no\n'newton-cg'       'l2', None                     yes\n'newton-cholesky' 'l2', None                     no\n'sag'             'l2', None                     yes\n'saga'            'elasticnet', 'l1', 'l2', None yes\n================= ============================== ======================\n\nNote: 'sag' and 'saga' fast convergence is only guaranteed on features with approximately the same scale. You can preprocess the data with a scaler from `sklearn.preprocessing`.\n\nSeealso: Refer to the User Guide for more information regarding `LogisticRegression` and more specifically the Table summarizing solver/penalty supports.\n\nversionadded:: 0.17\nStochastic Average Gradient descent solver.\nversionadded:: 0.19\nSAGA solver.\nversionchanged:: 0.22\nThe default solver changed from 'liblinear' to 'lbfgs' in 0.22.\nversionadded:: 1.2\nnewton-cholesky solver.""
    },
    {
      ""name"": ""max_iter"",
      ""type"": ""int"",
      ""default"": ""100"",
      ""description"": ""Maximum number of iterations taken for the solvers to converge.""
    },
    {
      ""name"": ""multi_class"",
      ""type"": ""('auto', 'ovr', 'multinomial')"",
      ""default"": ""'auto'"",
      ""description"": ""If the option chosen is 'ovr', then a binary problem is fit for each label. For 'multinomial' the loss minimised is the multinomial loss fit across the entire probability distribution, *even when the data is binary*. 'multinomial' is unavailable when solver='liblinear'. 'auto' selects 'ovr' if the data is binary, or if solver='liblinear', and otherwise selects 'multinomial'.\n\nversionadded:: 0.18\nStochastic Average Gradient descent solver for 'multinomial' case.\nversionchanged:: 0.22\nDefault changed from 'ovr' to 'auto' in 0.22.\n\ndeprecated:: 1.5\n``multi_class`` was deprecated in version 1.5 and will be removed in 1.7. From then on, the recommended 'multinomial' will always be used for `n_classes >= 3`. Solvers that do not support 'multinomial' will raise an error. Use `sklearn.multiclass.OneVsRestClassifier(LogisticRegression())` if you still want to use OvR.""
    },
    {
      ""name"": ""verbose"",
      ""type"": ""int"",
      ""default"": ""0"",
      ""description"": ""For the liblinear and lbfgs solvers set verbose to any positive number for verbosity.""
    },
    {
      ""name"": ""warm_start"",
      ""type"": ""bool"",
      ""default"": ""False"",
      ""description"": ""When set to True, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution. Useless for liblinear solver. See the Glossary for details.\n\nversionadded:: 0.17\n*warm_start* to support *lbfgs*, *newton-cg*, *sag*, *saga* solvers.""
    },
    {
      ""name"": ""n_jobs"",
      ""type"": ""int or None"",
      ""default"": ""None"",
      ""description"": ""Number of CPU cores used when parallelizing over classes if multi_class='ovr'\"". This parameter is ignored when the ``solver`` is set to 'liblinear' regardless of whether 'multi_class' is specified or not. ``None`` means 1 unless in a `joblib.parallel_backend` context. ``-1`` means using all processors. See Glossary for more details.""
    },
    {
      ""name"": ""l1_ratio"",
      ""type"": ""float or None"",
      ""default"": ""None"",
      ""description"": ""The Elastic-Net mixing parameter, with ``0 <= l1_ratio <= 1``. Only used if ``penalty='elasticnet'``. Setting ``l1_ratio=0`` is equivalent to using ``penalty='l2'``, while setting ``l1_ratio=1`` is equivalent to using ``penalty='l1'``. For ``0 < l1_ratio <1``, the penalty is a combination of L1 and L2.""
    }
  ],
  ""attributes"": [
    {
      ""name"": ""classes_"",
      ""type"": ""ndarray of shape (n_classes, )"",
      ""description"": ""A list of class labels known to the classifier.""
    },
    {
      ""name"": ""coef_"",
      ""type"": ""ndarray of shape (1, n_features) or (n_classes, n_features)"",
      ""description"": ""Coefficient of the features in the decision function. `coef_` is of shape (1, n_features) when the given problem is binary. In particular, when `multi_class='multinomial'`, `coef_` corresponds to outcome 1 (True) and `-coef_` corresponds to outcome 0 (False).""
    },
    {
      ""name"": ""intercept_"",
      ""type"": ""ndarray of shape (1,) or (n_classes,)"",
      ""description"": ""Intercept (a.k.a. bias) added to the decision function. If `fit_intercept` is set to False, the intercept is set to zero. `intercept_` is of shape (1,) when the given problem is binary. In particular, when `multi_class='multinomial'`, `intercept_` corresponds to outcome 1 (True) and `-intercept_` corresponds to outcome 0 (False).""
    },
    {
      ""name"": ""n_features_in_"",
      ""type"": ""int"",
      ""description"": ""Number of features seen during fit.\n\nversionadded:: 0.24""
    },
    {
      ""name"": ""feature_names_in_"",
      ""type"": ""ndarray of shape (n_features_in_,)"",
      ""description"": ""Names of features seen during fit. Defined only when `X` has feature names that are all strings.\n\nversionadded:: 1.0""
    },
    {
      ""name"": ""n_iter_"",
      ""type"": ""ndarray of shape (n_classes,) or (1, )"",
      ""description"": ""Actual number of iterations for all classes. If binary or multinomial, it returns only 1 element. For liblinear solver, only the maximum number of iteration across all classes is given.\n\nversionchanged:: 0.20\nIn SciPy <= 1.0.0 the number of lbfgs iterations may exceed ``max_iter``. ``n_iter_`` will now report at most ``max_iter``.""
    }
  ],
   ""methods"": [
    {
      ""name"": ""fit"",
      ""description"": ""Fit the model according to the given training data.""
    },
     {
      ""name"": ""predict"",
      ""description"": ""Predict class labels for samples in X.""
    },
    {
      ""name"": ""predict_proba"",
      ""description"": ""Probability estimates.""
    },
    {
      ""name"": ""score"",
      ""description"": ""Return the mean accuracy on the given test data and labels.""
    }
   ],
  ""see_also"": [
    {
      ""name"": ""SGDClassifier"",
      ""description"": ""Incrementally trained logistic regression (when given the parameter ``loss=\""log_loss\""``).""
    },
    {
      ""name"": ""LogisticRegressionCV"",
      ""description"": ""Logistic regression with built-in cross validation.""
    },
    {
      ""name"": ""sklearn.multiclass.OneVsRestClassifier"",
      ""description"": ""Used to extend binary classifiers like 'liblinear' and 'newton-cholesky' to handle multiclass problems.""
    }
  ],
  ""notes"": ""The underlying C implementation uses a random number generator to select features when fitting the model. It is thus not uncommon, to have slightly different results for the same input data. If that happens, try with a smaller tol parameter.\nPredict output may not match that of standalone liblinear in certain cases. See differences from liblinear in the narrative documentation."",
  ""references"": [
    ""L-BFGS-B -- Software for Large-scale Bound-constrained Optimization Ciyou Zhu, Richard Byrd, Jorge Nocedal and Jose Luis Morales. http://users.iems.northwestern.edu/~nocedal/lbfgsb.html"",
    ""LIBLINEAR -- A Library for Large Linear Classification https://www.csie.ntu.edu.tw/~cjlin/liblinear/"",
    ""SAG -- Mark Schmidt, Nicolas Le Roux, and Francis Bach Minimizing Finite Sums with the Stochastic Average Gradient https://hal.inria.fr/hal-00860051/document"",
    ""SAGA -- Defazio, A., Bach F. & Lacoste-Julien S. (2014). \""SAGA: A Fast Incremental Gradient Method With Support for Non-Strongly Convex Composite Objectives\"" <1407.0202> arXiv:1407.0202"",
    ""Hsiang-Fu Yu, Fang-Lan Huang, Chih-Jen Lin (2011). Dual coordinate descent methods for logistic regression and maximum entropy models. Machine Learning 85(1-2):41-75. https://www.csie.ntu.edu.tw/~cjlin/papers/maxent_dual.pdf""
  ],
  ""example"": "">>> from sklearn.datasets import load_iris\n>>> from sklearn.linear_model import LogisticRegression\n>>> X, y = load_iris(return_X_y=True)\n>>> clf = LogisticRegression(random_state=0).fit(X, y)\n>>> clf.predict(X[:2, :])\narray([0, 0])\n>>> clf.predict_proba(X[:2, :])\narray([[9.8...e-01, 1.8...e-02, 1.4...e-08],\n       [9.7...e-01, 2.8...e-02, ...e-08]])\n>>> clf.score(X, y)\n0.97...""
}
```","```json
{
  ""class"": ""LogisticRegression"",
  ""description"": ""Logistic Regression (aka logit, MaxEnt) classifier."",
  ""library"": ""liblinear, newton-cg, sag, saga, lbfgs solvers"",
  ""solvers"": {
    ""lbfgs"": {
      ""penalties"": [""l2"", null],
      ""multinomial_multiclass"": true
    },
    ""liblinear"": {
      ""penalties"": [""l1"", ""l2""],
      ""multinomial_multiclass"": false
    },
    ""newton-cg"": {
      ""penalties"": [""l2"", null],
      ""multinomial_multiclass"": true
    },
    ""newton-cholesky"": {
      ""penalties"": [""l2"", null],
      ""multinomial_multiclass"": false
    },
    ""sag"": {
      ""penalties"": [""l2"", null],
      ""multinomial_multiclass"": true
    },
    ""saga"": {
      ""penalties"": [""elasticnet"", ""l1"", ""l2"", null],
      ""multinomial_multiclass"": true
    }
  },
  ""parameters"": {
    ""penalty"": {
      ""type"": ""enum"",
      ""values"": [""l1"", ""l2"", ""elasticnet"", null],
      ""default"": ""l2"",
      ""description"": ""Specify the norm of the penalty.""
    },
    ""dual"": {
      ""type"": ""boolean"",
      ""default"": false,
      ""description"": ""Dual or primal formulation.""
    },
    ""tol"": {
      ""type"": ""float"",
      ""default"": 0.0001,
      ""description"": ""Tolerance for stopping criteria.""
    },
    ""C"": {
      ""type"": ""float"",
      ""default"": 1.0,
      ""description"": ""Inverse of regularization strength.""
    },
    ""fit_intercept"": {
      ""type"": ""boolean"",
      ""default"": true,
      ""description"": ""Specifies if a constant should be added to the decision function.""
    },
    ""intercept_scaling"": {
      ""type"": ""float"",
      ""default"": 1,
      ""description"": ""Used with solver 'liblinear' and fit_intercept is True.""
    },
    ""class_weight"": {
      ""type"": ""dict or 'balanced'"",
      ""default"": null,
      ""description"": ""Weights associated with classes.""
    },
    ""random_state"": {
      ""type"": ""int, RandomState instance"",
      ""default"": null,
      ""description"": ""Used when solver is 'sag', 'saga' or 'liblinear' to shuffle data.""
    },
    ""solver"": {
      ""type"": ""enum"",
      ""values"": [""lbfgs"", ""liblinear"", ""newton-cg"", ""newton-cholesky"", ""sag"", ""saga""],
      ""default"": ""lbfgs"",
      ""description"": ""Algorithm to use in the optimization problem.""
    },
    ""max_iter"": {
      ""type"": ""int"",
      ""default"": 100,
      ""description"": ""Maximum number of iterations for solvers to converge.""
    },
    ""multi_class"": {
      ""type"": ""enum"",
      ""values"": [""auto"", ""ovr"", ""multinomial""],
      ""default"": ""auto"",
      ""description"": ""Type of the target variable.""
    },
    ""verbose"": {
      ""type"": ""int"",
      ""default"": 0,
      ""description"": ""For liblinear and lbfgs solvers set verbosity.""
    },
    ""warm_start"": {
      ""type"": ""boolean"",
      ""default"": false,
      ""description"": ""Reuses the solution of previous call to fit as initialization.""
    },
    ""n_jobs"": {
      ""type"": ""int"",
      ""default"": null,
      ""description"": ""Number of CPU cores used when parallelizing over classes.""
    },
    ""l1_ratio"": {
      ""type"": ""float"",
      ""default"": null,
      ""description"": ""Elastic-Net mixing parameter, only used if penalty='elasticnet'.""
    }
  },
  ""attributes"": {
    ""classes_"": {
      ""type"": ""ndarray of shape (n_classes, )"",
      ""description"": ""A list of class labels known to the classifier.""
    },
    ""coef_"": {
      ""type"": ""ndarray of shape (1, n_features) or (n_classes, n_features)"",
      ""description"": ""Coefficient of the features in the decision function.""
    },
    ""intercept_"": {
      ""type"": ""ndarray of shape (1,) or (n_classes,)"",
      ""description"": ""Intercept added to the decision function.""
    },
    ""n_features_in_"": {
      ""type"": ""int"",
      ""description"": ""Number of features seen during fit.""
    },
    ""feature_names_in_"": {
      ""type"": ""ndarray of shape (n_features_in_,)"",
      ""description"": ""Names of features seen during fit.""
    },
    ""n_iter_"": {
      ""type"": ""ndarray of shape (n_classes,) or (1, )"",
      ""description"": ""Actual number of iterations for all classes.""
    }
  }
}
```"
make_pipeline,"Construct a :class:`Pipeline` from the given estimators.  This is a shorthand for the :class:`Pipeline` constructor; it does not require, and does not permit, naming the estimators. Instead, their names will be set to the lowercase of their types automatically.  Parameters ---------- *steps : list of Estimator objects     List of the scikit-learn estimators that are chained together.  memory : str or object with the joblib.Memory interface, default=None     Used to cache the fitted transformers of the pipeline. The last step     will never be cached, even if it is a transformer. By default, no     caching is performed. If a string is given, it is the path to the     caching directory. Enabling caching triggers a clone of the transformers     before fitting. Therefore, the transformer instance given to the     pipeline cannot be inspected directly. Use the attribute ``named_steps``     or ``steps`` to inspect estimators within the pipeline. Caching the     transformers is advantageous when fitting is time consuming.  transform_input : list of str, default=None     This enables transforming some input arguments to ``fit`` (other than ``X``)     to be transformed by the steps of the pipeline up to the step which requires     them. Requirement is defined via :ref:`metadata routing <metadata_routing>`.     This can be used to pass a validation set through the pipeline for instance.      You can only set this if metadata routing is enabled, which you     can enable using ``sklearn.set_config(enable_metadata_routing=True)``.      .. versionadded:: 1.6  verbose : bool, default=False     If True, the time elapsed while fitting each step will be printed as it     is completed.  Returns ------- p : Pipeline     Returns a scikit-learn :class:`Pipeline` object.  See Also -------- Pipeline : Class for creating a pipeline of transforms with a final     estimator.  Examples -------- >>> from sklearn.naive_bayes import GaussianNB >>> from sklearn.preprocessing import StandardScaler >>> from sklearn.pipeline import make_pipeline >>> make_pipeline(StandardScaler(), GaussianNB(priors=None)) Pipeline(steps=[('standardscaler', StandardScaler()),                 ('gaussiannb', GaussianNB())])","```json
{
  ""api_element"": {
    ""type"": ""function"",
    ""name"": ""make_pipeline"",
    ""description"": ""Constructs a scikit-learn Pipeline from a list of estimators. This is a shorthand for the Pipeline constructor; it does not require, and does not permit, naming the estimators. Instead, their names will be set to the lowercase of their types automatically."",
    ""parameters"": [
      {
        ""name"": ""steps"",
        ""type"": ""list of Estimator objects"",
        ""description"": ""List of the scikit-learn estimators that are chained together."",
        ""is_variadic"": true
      },
      {
        ""name"": ""memory"",
        ""type"": ""str or object with the joblib.Memory interface"",
        ""description"": ""Used to cache the fitted transformers of the pipeline. The last step will never be cached. If a string is given, it is the path to the caching directory."",
        ""default"": ""None""
      },
      {
        ""name"": ""transform_input"",
        ""type"": ""list of str"",
        ""description"": ""Enables transforming some input arguments (other than X) to fit using pipeline steps up to the step requiring them, based on metadata routing."",
        ""default"": ""None"",
        ""notes"": [
          ""Added in version 1.6"",
          ""Requires metadata routing to be enabled (sklearn.set_config(enable_metadata_routing=True))""
        ]
      },
      {
        ""name"": ""verbose"",
        ""type"": ""bool"",
        ""description"": ""If True, the time elapsed while fitting each step will be printed."",
        ""default"": ""False""
      }
    ],
    ""returns"": {
      ""type"": ""Pipeline"",
      ""description"": ""A scikit-learn Pipeline object.""
    },
    ""relationships"": [
      {
        ""type"": ""SeeAlso"",
        ""target"": ""Pipeline"",
        ""description"": ""Class for creating a pipeline of transforms with a final estimator.""
      },
      {
        ""type"": ""Constructs"",
        ""target"": ""Pipeline"",
        ""description"": ""This function is a shorthand for constructing a Pipeline object.""
      }
    ]
  }
}
```","Here is the structured representation of the API in JSON format:

```json
{
  ""class"": ""Pipeline"",
  ""constructor"": ""make_pipeline"",
  ""parameters"": {
    ""steps"": {
      ""type"": ""list"",
      ""description"": ""List of the scikit-learn estimators that are chained together."",
      ""elements"": ""Estimator objects""
    },
    ""memory"": {
      ""type"": ""str or object with joblib.Memory interface, default=None"",
      ""description"": ""Used to cache the fitted transformers of the pipeline. The last step will never be cached, even if it is a transformer. By default, no caching is performed. If a string is given, it is the path to the caching directory. Enabling caching triggers a clone of the transformers before fitting. Therefore, the transformer instance given to the pipeline cannot be inspected directly. Use the attribute named_steps or steps to inspect estimators within the pipeline. Caching the transformers is advantageous when fitting is time consuming.""
    },
    ""transform_input"": {
      ""type"": ""list of str, default=None"",
      ""description"": ""This enables transforming some input arguments to fit (other than X) to be transformed by the steps of the pipeline up to the step which requires them. Requirement is defined via metadata routing. This can be used to pass a validation set through the pipeline for instance. You can only set this if metadata routing is enabled, which you can enable using sklearn.set_config(enable_metadata_routing=True)."",
      ""versionadded"": ""1.6""
    },
    ""verbose"": {
      ""type"": ""bool, default=False"",
      ""description"": ""If True, the time elapsed while fitting each step will be printed as it is completed.""
    }
  },
  ""returns"": {
    ""type"": ""Pipeline"",
    ""description"": ""Returns a scikit-learn Pipeline object.""
  },
  ""see_also"": [
    {
      ""related_class"": ""Pipeline"",
      ""description"": ""Class for creating a pipeline of transforms with a final estimator.""
    }
  ],
  ""examples"": [
    {
      ""code"": ""from sklearn.naive_bayes import GaussianNB\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\nmake_pipeline(StandardScaler(), GaussianNB(priors=None))"",
      ""output"": ""Pipeline(steps=[('standardscaler', StandardScaler()), ('gaussiannb', GaussianNB())])""
    }
  ]
}
```

This JSON representation outlines the `make_pipeline` factory function, its parameters, return type, related classes, and an example usage with expected output."
PCA,"Principal component analysis (PCA).  Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. The input data is centered but not scaled for each feature before applying the SVD.  It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.  With sparse inputs, the ARPACK implementation of the truncated SVD can be used (i.e. through :func:`scipy.sparse.linalg.svds`). Alternatively, one may consider :class:`TruncatedSVD` where the data are not centered.  Notice that this class only supports sparse inputs for some solvers such as ""arpack"" and ""covariance_eigh"". See :class:`TruncatedSVD` for an alternative with sparse data.  For a usage example, see :ref:`sphx_glr_auto_examples_decomposition_plot_pca_iris.py`  Read more in the :ref:`User Guide <PCA>`.  Parameters ---------- n_components : int, float or 'mle', default=None     Number of components to keep.     if n_components is not set all components are kept::          n_components == min(n_samples, n_features)      If ``n_components == 'mle'`` and ``svd_solver == 'full'``, Minka's     MLE is used to guess the dimension. Use of ``n_components == 'mle'``     will interpret ``svd_solver == 'auto'`` as ``svd_solver == 'full'``.      If ``0 < n_components < 1`` and ``svd_solver == 'full'``, select the     number of components such that the amount of variance that needs to be     explained is greater than the percentage specified by n_components.      If ``svd_solver == 'arpack'``, the number of components must be     strictly less than the minimum of n_features and n_samples.      Hence, the None case results in::          n_components == min(n_samples, n_features) - 1  copy : bool, default=True     If False, data passed to fit are overwritten and running     fit(X).transform(X) will not yield the expected results,     use fit_transform(X) instead.  whiten : bool, default=False     When True (False by default) the `components_` vectors are multiplied     by the square root of n_samples and then divided by the singular values     to ensure uncorrelated outputs with unit component-wise variances.      Whitening will remove some information from the transformed signal     (the relative variance scales of the components) but can sometime     improve the predictive accuracy of the downstream estimators by     making their data respect some hard-wired assumptions.  svd_solver : {'auto', 'full', 'covariance_eigh', 'arpack', 'randomized'},            default='auto'     ""auto"" :         The solver is selected by a default 'auto' policy is based on `X.shape` and         `n_components`: if the input data has fewer than 1000 features and         more than 10 times as many samples, then the ""covariance_eigh""         solver is used. Otherwise, if the input data is larger than 500x500         and the number of components to extract is lower than 80% of the         smallest dimension of the data, then the more efficient         ""randomized"" method is selected. Otherwise the exact ""full"" SVD is         computed and optionally truncated afterwards.     ""full"" :         Run exact full SVD calling the standard LAPACK solver via         `scipy.linalg.svd` and select the components by postprocessing     ""covariance_eigh"" :         Precompute the covariance matrix (on centered data), run a         classical eigenvalue decomposition on the covariance matrix         typically using LAPACK and select the components by postprocessing.         This solver is very efficient for n_samples >> n_features and small         n_features. It is, however, not tractable otherwise for large         n_features (large memory footprint required to materialize the         covariance matrix). Also note that compared to the ""full"" solver,         this solver effectively doubles the condition number and is         therefore less numerical stable (e.g. on input data with a large         range of singular values).     ""arpack"" :         Run SVD truncated to `n_components` calling ARPACK solver via         `scipy.sparse.linalg.svds`. It requires strictly         `0 < n_components < min(X.shape)`     ""randomized"" :         Run randomized SVD by the method of Halko et al.      .. versionadded:: 0.18.0      .. versionchanged:: 1.5         Added the 'covariance_eigh' solver.  tol : float, default=0.0     Tolerance for singular values computed by svd_solver == 'arpack'.     Must be of range [0.0, infinity).      .. versionadded:: 0.18.0  iterated_power : int or 'auto', default='auto'     Number of iterations for the power method computed by     svd_solver == 'randomized'.     Must be of range [0, infinity).      .. versionadded:: 0.18.0  n_oversamples : int, default=10     This parameter is only relevant when `svd_solver=""randomized""`.     It corresponds to the additional number of random vectors to sample the     range of `X` so as to ensure proper conditioning. See     :func:`~sklearn.utils.extmath.randomized_svd` for more details.      .. versionadded:: 1.1  power_iteration_normalizer : {'auto', 'QR', 'LU', 'none'}, default='auto'     Power iteration normalizer for randomized SVD solver.     Not used by ARPACK. See :func:`~sklearn.utils.extmath.randomized_svd`     for more details.      .. versionadded:: 1.1  random_state : int, RandomState instance or None, default=None     Used when the 'arpack' or 'randomized' solvers are used. Pass an int     for reproducible results across multiple function calls.     See :term:`Glossary <random_state>`.      .. versionadded:: 0.18.0  Attributes ---------- components_ : ndarray of shape (n_components, n_features)     Principal axes in feature space, representing the directions of     maximum variance in the data. Equivalently, the right singular     vectors of the centered input data, parallel to its eigenvectors.     The components are sorted by decreasing ``explained_variance_``.  explained_variance_ : ndarray of shape (n_components,)     The amount of variance explained by each of the selected components.     The variance estimation uses `n_samples - 1` degrees of freedom.      Equal to n_components largest eigenvalues     of the covariance matrix of X.      .. versionadded:: 0.18  explained_variance_ratio_ : ndarray of shape (n_components,)     Percentage of variance explained by each of the selected components.      If ``n_components`` is not set then all components are stored and the     sum of the ratios is equal to 1.0.  singular_values_ : ndarray of shape (n_components,)     The singular values corresponding to each of the selected components.     The singular values are equal to the 2-norms of the ``n_components``     variables in the lower-dimensional space.      .. versionadded:: 0.19  mean_ : ndarray of shape (n_features,)     Per-feature empirical mean, estimated from the training set.      Equal to `X.mean(axis=0)`.  n_components_ : int     The estimated number of components. When n_components is set     to 'mle' or a number between 0 and 1 (with svd_solver == 'full') this     number is estimated from input data. Otherwise it equals the parameter     n_components, or the lesser value of n_features and n_samples     if n_components is None.  n_samples_ : int     Number of samples in the training data.  noise_variance_ : float     The estimated noise covariance following the Probabilistic PCA model     from Tipping and Bishop 1999. See ""Pattern Recognition and     Machine Learning"" by C. Bishop, 12.2.1 p. 574 or     http://www.miketipping.com/papers/met-mppca.pdf. It is required to     compute the estimated data covariance and score samples.      Equal to the average of (min(n_features, n_samples) - n_components)     smallest eigenvalues of the covariance matrix of X.  n_features_in_ : int     Number of features seen during :term:`fit`.      .. versionadded:: 0.24  feature_names_in_ : ndarray of shape (`n_features_in_`,)     Names of features seen during :term:`fit`. Defined only when `X`     has feature names that are all strings.      .. versionadded:: 1.0  See Also -------- KernelPCA : Kernel Principal Component Analysis. SparsePCA : Sparse Principal Component Analysis. TruncatedSVD : Dimensionality reduction using truncated SVD. IncrementalPCA : Incremental Principal Component Analysis.  References ---------- For n_components == 'mle', this class uses the method from: `Minka, T. P.. ""Automatic choice of dimensionality for PCA"". In NIPS, pp. 598-604 <https://tminka.github.io/papers/pca/minka-pca.pdf>`_  Implements the probabilistic PCA model from: `Tipping, M. E., and Bishop, C. M. (1999). ""Probabilistic principal component analysis"". Journal of the Royal Statistical Society: Series B (Statistical Methodology), 61(3), 611-622. <http://www.miketipping.com/papers/met-mppca.pdf>`_ via the score and score_samples methods.  For svd_solver == 'arpack', refer to `scipy.sparse.linalg.svds`.  For svd_solver == 'randomized', see: :doi:`Halko, N., Martinsson, P. G., and Tropp, J. A. (2011). ""Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions"". SIAM review, 53(2), 217-288. <10.1137/090771806>` and also :doi:`Martinsson, P. G., Rokhlin, V., and Tygert, M. (2011). ""A randomized algorithm for the decomposition of matrices"". Applied and Computational Harmonic Analysis, 30(1), 47-68. <10.1016/j.acha.2010.02.003>`  Examples -------- >>> import numpy as np >>> from sklearn.decomposition import PCA >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]]) >>> pca = PCA(n_components=2) >>> pca.fit(X) PCA(n_components=2) >>> print(pca.explained_variance_ratio_) [0.9924... 0.0075...] >>> print(pca.singular_values_) [6.30061... 0.54980...]  >>> pca = PCA(n_components=2, svd_solver='full') >>> pca.fit(X) PCA(n_components=2, svd_solver='full') >>> print(pca.explained_variance_ratio_) [0.9924... 0.00755...] >>> print(pca.singular_values_) [6.30061... 0.54980...]  >>> pca = PCA(n_components=1, svd_solver='arpack') >>> pca.fit(X) PCA(n_components=1, svd_solver='arpack') >>> print(pca.explained_variance_ratio_) [0.99244...] >>> print(pca.singular_values_) [6.30061...]","```json
{
  ""class"": ""PCA"",
  ""description"": ""Principal component analysis (PCA).\nLinear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. The input data is centered but not scaled for each feature before applying the SVD.\nIt uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.\nWith sparse inputs, the ARPACK implementation of the truncated SVD can be used (i.e. through :func:`scipy.sparse.linalg.svds`).\nAlternatively, one may consider :class:`TruncatedSVD` where the data are not centered. Notice that this class only supports sparse inputs for some solvers such as \""arpack\"" and \""covariance_eigh\"". See :class:`TruncatedSVD` for an alternative with sparse data."",
  ""parameters"": [
    {
      ""name"": ""n_components"",
      ""type"": ""int | float | 'mle' | None"",
      ""default"": ""None"",
      ""description"": ""Number of components to keep.\nIf n_components is not set all components are kept:\n\nn_components == min(n_samples, n_features)\n\nIf ``n_components == 'mle'`` and ``svd_solver == 'full'``, Minka's MLE is used to guess the dimension. Use of ``n_components == 'mle'`` will interpret ``svd_solver == 'auto'`` as ``svd_solver == 'full'``.\nIf ``0 < n_components < 1`` and ``svd_solver == 'full'``, select the number of components such that the amount of variance that needs to be explained is greater than the percentage specified by n_components.\nIf ``svd_solver == 'arpack'``, the number of components must be strictly less than the minimum of n_features and n_samples.\nHence, the None case results in::\n\nn_components == min(n_samples, n_features) - 1""
    },
    {
      ""name"": ""copy"",
      ""type"": ""bool"",
      ""default"": ""True"",
      ""description"": ""If False, data passed to fit are overwritten and running fit(X).transform(X) will not yield the expected results, use fit_transform(X) instead.""
    },
    {
      ""name"": ""whiten"",
      ""type"": ""bool"",
      ""default"": ""False"",
      ""description"": ""When True (False by default) the `components_` vectors are multiplied by the square root of n_samples and then divided by the singular values to ensure uncorrelated outputs with unit component-wise variances.\nWhitening will remove some information from the transformed signal (the relative variance scales of the components) but can sometime improve the predictive accuracy of the downstream estimators by making their data respect some hard-wired assumptions.""
    },
    {
      ""name"": ""svd_solver"",
      ""type"": ""enum"",
      ""values"": [""auto"", ""full"", ""covariance_eigh"", ""arpack"", ""randomized""],
      ""default"": ""'auto'"",
      ""description"": ""The solver is selected by a default 'auto' policy is based on `X.shape` and `n_components`: if the input data has fewer than 1000 features and more than 10 times as many samples, then the \""covariance_eigh\"" solver is used. Otherwise, if the input data is larger than 500x500 and the number of components to extract is lower than 80% of the smallest dimension of the data, then the more efficient \""randomized\"" method is selected. Otherwise the exact \""full\"" SVD is computed and optionally truncated afterwards.\n\""full\"" : Run exact full SVD calling the standard LAPACK solver via `scipy.linalg.svd` and select the components by postprocessing\n\""covariance_eigh\"" : Precompute the covariance matrix (on centered data), run a classical eigenvalue decomposition on the covariance matrix typically using LAPACK and select the components by postprocessing. This solver is very efficient for n_samples >> n_features and small n_features. It is, however, not tractable otherwise for large n_features (large memory footprint required to materialize the covariance matrix). Also note that compared to the \""full\"" solver, this solver effectively doubles the condition number and is therefore less numerical stable (e.g. on input data with a large range of singular values).\n\""arpack\"" : Run SVD truncated to `n_components` calling ARPACK solver via `scipy.sparse.linalg.svds`. It requires strictly `0 < n_components < min(X.shape)`\n\""randomized\"" : Run randomized SVD by the method of Halko et al.""
    },
    {
      ""name"": ""tol"",
      ""type"": ""float"",
      ""default"": ""0.0"",
      ""description"": ""Tolerance for singular values computed by svd_solver == 'arpack'. Must be of range [0.0, infinity).""
    },
    {
      ""name"": ""iterated_power"",
      ""type"": ""int | 'auto'"",
      ""default"": ""'auto'"",
      ""description"": ""Number of iterations for the power method computed by svd_solver == 'randomized'. Must be of range [0, infinity).""
    },
    {
      ""name"": ""n_oversamples"",
      ""type"": ""int"",
      ""default"": ""10"",
      ""description"": ""This parameter is only relevant when `svd_solver=\""randomized\""`. It corresponds to the additional number of random vectors to sample the range of `X` so as to ensure proper conditioning. See :func:`~sklearn.utils.extmath.randomized_svd` for more details.""
    },
    {
      ""name"": ""power_iteration_normalizer"",
      ""type"": ""enum"",
      ""values"": [""auto"", ""QR"", ""LU"", ""none""],
      ""default"": ""'auto'"",
      ""description"": ""Power iteration normalizer for randomized SVD solver. Not used by ARPACK. See :func:`~sklearn.utils.extmath.randomized_svd` for more details.""
    },
    {
      ""name"": ""random_state"",
      ""type"": ""int | RandomState instance | None"",
      ""default"": ""None"",
      ""description"": ""Used when the 'arpack' or 'randomized' solvers are used. Pass an int for reproducible results across multiple function calls. See :term:`Glossary <random_state>`.""
    }
  ],
  ""attributes"": [
    {
      ""name"": ""components_"",
      ""type"": ""ndarray"",
      ""shape"": ""(n_components, n_features)"",
      ""description"": ""Principal axes in feature space, representing the directions of maximum variance in the data. Equivalently, the right singular vectors of the centered input data, parallel to its eigenvectors. The components are sorted by decreasing ``explained_variance_``.""
    },
    {
      ""name"": ""explained_variance_"",
      ""type"": ""ndarray"",
      ""shape"": ""(n_components,)"",
      ""description"": ""The amount of variance explained by each of the selected components. The variance estimation uses `n_samples - 1` degrees of freedom. Equal to n_components largest eigenvalues of the covariance matrix of X.""
    },
    {
      ""name"": ""explained_variance_ratio_"",
      ""type"": ""ndarray"",
      ""shape"": ""(n_components,)"",
      ""description"": ""Percentage of variance explained by each of the selected components. If ``n_components`` is not set then all components are stored and the sum of the ratios is equal to 1.0.""
    },
    {
      ""name"": ""singular_values_"",
      ""type"": ""ndarray"",
      ""shape"": ""(n_components,)"",
      ""description"": ""The singular values corresponding to each of the selected components. The singular values are equal to the 2-norms of the ``n_components`` variables in the lower-dimensional space.""
    },
    {
      ""name"": ""mean_"",
      ""type"": ""ndarray"",
      ""shape"": ""(n_features,)"",
      ""description"": ""Per-feature empirical mean, estimated from the training set. Equal to `X.mean(axis=0)`.""
    },
    {
      ""name"": ""n_components_"",
      ""type"": ""int"",
      ""description"": ""The estimated number of components. When n_components is set to 'mle' or a number between 0 and 1 (with svd_solver == 'full') this number is estimated from input data. Otherwise it equals the parameter n_components, or the lesser value of n_features and n_samples if n_components is None.""
    },
    {
      ""name"": ""n_samples_"",
      ""type"": ""int"",
      ""description"": ""Number of samples in the training data.""
    },
    {
      ""name"": ""noise_variance_"",
      ""type"": ""float"",
      ""description"": ""The estimated noise covariance following the Probabilistic PCA model from Tipping and Bishop 1999... Equal to the average of (min(n_features, n_samples) - n_components) smallest eigenvalues of the covariance matrix of X.""
    },
    {
      ""name"": ""n_features_in_"",
      ""type"": ""int"",
      ""description"": ""Number of features seen during :term:`fit`.""
    },
    {
      ""name"": ""feature_names_in_"",
      ""type"": ""ndarray"",
      ""shape"": ""(`n_features_in_`,)"",
      ""description"": ""Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.""
    }
  ],
  ""see_also"": [
    {""name"": ""KernelPCA"", ""description"": ""Kernel Principal Component Analysis.""},
    {""name"": ""SparsePCA"", ""description"": ""Sparse Principal Component Analysis.""},
    {""name"": ""TruncatedSVD"", ""description"": ""Dimensionality reduction using truncated SVD.""},
    {""name"": ""IncrementalPCA"", ""description"": ""Incremental Principal Component Analysis.""}
  ]
  // Methods like fit, transform, fit_transform, score, score_samples are standard
  // for scikit-learn estimators but not explicitly documented with parameters/returns
  // in the provided text, so they are omitted from this structured representation
  // which is based *only* on the provided documentation text sections.
}
```","Here is a structured representation of the given API documentation, modeled in JSON format:

```json
{
  ""PCA"": {
    ""description"": ""Principal Component Analysis for linear dimensionality reduction using Singular Value Decomposition."",
    ""methods"": {
      ""fit"": {
        ""description"": ""Fit the model with X."",
        ""parameters"": {
          ""X"": {
            ""type"": ""array-like of shape (n_samples, n_features)"",
            ""description"": ""Training data.""
          }
        }
      },
      ""transform"": {
        ""description"": ""Apply dimensionality reduction to X."",
        ""parameters"": {
          ""X"": {
            ""type"": ""array-like of shape (n_samples, n_features)"",
            ""description"": ""Data to transform.""
          }
        }
      },
      ""fit_transform"": {
        ""description"": ""Fit the model and apply dimensionality reduction on X."",
        ""parameters"": {
          ""X"": {
            ""type"": ""array-like of shape (n_samples, n_features)"",
            ""description"": ""Data to fit and transform.""
          }
        }
      }
    },
    ""parameters"": {
      ""n_components"": {
        ""type"": ""int, float, or 'mle', default=None"",
        ""description"": ""Number of components to keep.""
      },
      ""copy"": {
        ""type"": ""bool, default=True"",
        ""description"": ""If False, data passed to fit are overwritten.""
      },
      ""whiten"": {
        ""type"": ""bool, default=False"",
        ""description"": ""When True, ensures uncorrelated outputs with unit component-wise variances.""
      },
      ""svd_solver"": {
        ""type"": ""{'auto', 'full', 'covariance_eigh', 'arpack', 'randomized'}, default='auto'"",
        ""description"": ""SVD solver to use.""
      },
      ""tol"": {
        ""type"": ""float, default=0.0"",
        ""description"": ""Tolerance for singular values computed by svd_solver == 'arpack'.""
      },
      ""iterated_power"": {
        ""type"": ""int or 'auto', default='auto'"",
        ""description"": ""Number of iterations for the power method.""
      },
      ""n_oversamples"": {
        ""type"": ""int, default=10"",
        ""description"": ""Additional number of random vectors to sample the range of X.""
      },
      ""power_iteration_normalizer"": {
        ""type"": ""{'auto', 'QR', 'LU', 'none'}, default='auto'"",
        ""description"": ""Power iteration normalizer for randomized SVD solver.""
      },
      ""random_state"": {
        ""type"": ""int, RandomState instance or None, default=None"",
        ""description"": ""Used when 'arpack' or 'randomized' solvers are used.""
      }
    },
    ""attributes"": {
      ""components_"": {
        ""type"": ""ndarray of shape (n_components, n_features)"",
        ""description"": ""Principal axes in feature space, representing the directions of maximum variance.""
      },
      ""explained_variance_"": {
        ""type"": ""ndarray of shape (n_components,)"",
        ""description"": ""Variance explained by each of the selected components.""
      },
      ""explained_variance_ratio_"": {
        ""type"": ""ndarray of shape (n_components,)"",
        ""description"": ""Percentage of variance explained by each of the selected components.""
      },
      ""singular_values_"": {
        ""type"": ""ndarray of shape (n_components,)"",
        ""description"": ""Singular values corresponding to each of the selected components.""
      },
      ""mean_"": {
        ""type"": ""ndarray of shape (n_features,)"",
        ""description"": ""Per-feature empirical mean, estimated from the training set.""
      },
      ""n_components_"": {
        ""type"": ""int"",
        ""description"": ""The estimated number of components.""
      },
      ""n_samples_"": {
        ""type"": ""int"",
        ""description"": ""Number of samples in the training data.""
      },
      ""noise_variance_"": {
        ""type"": ""float"",
        ""description"": ""Estimated noise covariance following the Probabilistic PCA model.""
      },
      ""n_features_in_"": {
        ""type"": ""int"",
        ""description"": ""Number of features seen during fit.""
      },
      ""feature_names_in_"": {
        ""type"": ""ndarray of shape (n_features_in_,)"",
        ""description"": ""Names of features seen during fit.""
      }
    },
    ""related"": {
      ""TruncatedSVD"": ""Alternative dimensionality reduction using truncated SVD."",
      ""KernelPCA"": ""Kernel Principal Component Analysis."",
      ""SparsePCA"": ""Sparse Principal Component Analysis."",
      ""IncrementalPCA"": ""Incremental Principal Component Analysis.""
    },
    ""references"": [
      {
        ""name"": ""Minka, T. P."",
        ""title"": ""Automatic choice of dimensionality for PCA"",
        ""url"": ""https://tminka.github.io/papers/pca/minka-pca.pdf""
      },
      {
        ""name"": ""Tipping, M. E., and Bishop, C. M."",
        ""title"": ""Probabilistic principal component analysis"",
        ""url"": ""http://www.miketipping.com/papers/met-mppca.pdf""
      }
    ]
  }
}
```

This JSON representation captures the structure and relationships of the `PCA` API, including its methods, parameters, attributes, and related classes."
torch,"The torch package contains data structures for multi-dimensional tensors and defines mathematical operations over these tensors. Additionally, it provides many utilities for efficient serialization of Tensors and arbitrary types, and other useful utilities.  It has a CUDA counterpart, that enables you to run your tensor computations on an NVIDIA GPU with compute capability >= 3.0.","```json
{
  ""package"": {
    ""name"": ""torch"",
    ""description"": ""Contains data structures for multi-dimensional tensors, defines mathematical operations, and provides serialization and other utilities. Has a CUDA counterpart."",
    ""types"": [
      {
        ""name"": ""Tensor"",
        ""description"": ""Multi-dimensional data structure.""
      }
    ],
    ""function_categories"": [
      {
        ""name"": ""Mathematical Operations"",
        ""description"": ""Operations defined over Tensors.""
        // Specific functions within this category are not detailed in the provided text.
      }
    ],
    ""utility_categories"": [
      {
        ""name"": ""Serialization Utilities"",
        ""description"": ""Utilities for efficient serialization of Tensors and arbitrary types.""
      },
      {
        ""name"": ""General Utilities"",
        ""description"": ""Other useful utilities (specific details not provided).""
      }
    ],
    ""features"": [
      {
        ""name"": ""CUDA Support"",
        ""description"": ""Enables running tensor computations on NVIDIA GPUs with compute capability >= 3.0 via a CUDA counterpart.""
      }
    ]
    // Relationships described are high-level:
    // - Mathematical operations are performed *over* Tensors.
    // - Serialization utilities handle Tensors and arbitrary types.
    // Specific function-to-type relationships (e.g., signature details) cannot be extracted from this text.
  }
}
```","Based on the provided description of the `torch` package, here's a structured representation in JSON format:

```json
{
  ""torch"": {
    ""description"": ""The torch package contains data structures for multi-dimensional tensors and defines mathematical operations over these tensors. Additionally, it provides many utilities for efficient serialization of Tensors and arbitrary types, and other useful utilities."",
    ""features"": [
      ""Data structures for multi-dimensional tensors"",
      ""Mathematical operations over tensors"",
      ""Utilities for efficient serialization of Tensors"",
      ""Utilities for serialization of arbitrary types""
    ],
    ""utilities"": [
      ""Efficient serialization of Tensors"",
      ""Serialization of arbitrary types""
    ],
    ""additional_utilities"": ""Other useful utilities"",
    ""cuda"": {
      ""description"": ""CUDA counterpart enables you to run your tensor computations on an NVIDIA GPU with compute capability >= 3.0."",
      ""requirements"": [
        ""NVIDIA GPU with compute capability >= 3.0""
      ]
    }
  }
}
```

This representation captures the primary components and features of the `torch` package as described in the documentation, including the key functionalities and its CUDA counterpart for GPU acceleration."
torch.nn,No documentation found.,"Based on the input provided (""No documentation found.""), I cannot extract a structured representation of the API.

Please provide the API documentation you would like me to process. Once the documentation is available, I can generate the structured JSON representation of the API, including types, functions, and their relationships.","Unfortunately, without any documentation or details about the API, I can't extract or provide a structured representation of the API. API documentation typically includes details such as endpoint URLs, request methods, parameters, request and response formats, authentication, and examples. 

If you have any additional information, such as code snippets, examples of requests, or even endpoint lists, please share them, and I can help construct a structured representation based on that information."
torch.optim,":mod:`torch.optim` is a package implementing various optimization algorithms.  Most commonly used methods are already supported, and the interface is general enough, so that more sophisticated ones can also be easily integrated in the future.","```json
{
  ""package"": {
    ""name"": ""torch.optim"",
    ""description"": ""A package implementing various optimization algorithms. Most commonly used methods are already supported, and the interface is general enough, so that more sophisticated ones can also be easily integrated in the future."",
    ""contents"": {
      ""types"": [
        {
          ""category"": ""Optimization Algorithm"",
          ""description"": ""Classes that implement specific optimization algorithms (e.g., SGD, Adam). The exact names and specific details of these types are not provided in the source documentation snippet."",
          ""nature"": ""Class (inferred, as optimizers typically maintain state and have methods like step())"",
          ""count"": ""Multiple (implied by 'various', 'most commonly used')""
        }
      ],
      ""functions"": [
        {
          ""category"": ""Interface Functionality"",
          ""description"": ""While specific functions within the package namespace are not named, the description implies a general interface, which would involve methods on the optimizer objects (e.g., step(), zero_grad()). The package itself might expose utility functions, but none are specified."",
          ""details_available"": false
        }
      ]
    },
    ""relationships"": [
      {
        ""type"": ""Adherence to Interface"",
        ""description"": ""The optimization algorithm types/classes within this package adhere to a general interface."",
        ""scope"": ""Internal package components (optimizer types)"",
        ""purpose"": ""Ensures compatibility, ease of use, and allows for integration of new optimizers.""
      },
      {
        ""type"": ""Implementation Provider"",
        ""description"": ""The package provides concrete implementations of optimization algorithms."",
        ""from"": ""torch.optim (package)"",
        ""to"": ""Optimization algorithms (conceptual/types)""
      }
    ]
  }
}
```","To extract a structured representation for the `torch.optim` API described, we will focus on the main components and relationships specified in the documentation. Since only a brief description is provided, the structured representation will be based on typical content expected in such an API, with placeholder values indicative of the expected content structure. If more concrete content was provided, it would enable a more accurate extraction. For now, the representation is general.

```json
{
  ""module"": ""torch.optim"",
  ""description"": ""A package implementing various optimization algorithms with a general interface."",
  ""types"": [],
  ""functions"": [],
  ""classes"": [
    {
      ""name"": ""<Optimizer Name>"",
      ""description"": ""A class implementing a specific optimization algorithm."",
      ""methods"": [],
      ""properties"": [],
      ""relationships"": []
    }
  ]
}
```

This structured representation outlines the basic structure based on the given brief description. Typically, this would be expanded to include actual optimizer names and descriptions along with methods, properties, and relationships, depending on the full content of the module."
torchvision,No documentation found.,"```json
{
  ""status"": ""error"",
  ""message"": ""No API documentation was provided, so no structure could be extracted."",
  ""extracted_api_structure"": null
}
```","To create a structured representation of an API given the absence of documentation, we need to assume some basic structure based on common API conventions. However, since there's no specific data to guide us, the response will be more of a template that outlines what an API structure in JSON format might look like. Heres an example of a generic API structure:

```json
{
  ""api"": {
    ""version"": ""1.0"",
    ""base_url"": ""https://api.example.com"",
    ""resources"": [
      {
        ""name"": ""exampleResource"",
        ""endpoint"": ""/exampleResource"",
        ""methods"": [
          {
            ""method"": ""GET"",
            ""description"": ""Retrieve a list of example resources"",
            ""parameters"": [
              {
                ""name"": ""limit"",
                ""type"": ""integer"",
                ""required"": false,
                ""description"": ""The number of resources to return""
              },
              {
                ""name"": ""offset"",
                ""type"": ""integer"",
                ""required"": false,
                ""description"": ""The number of resources to skip""
              }
            ],
            ""responses"": {
              ""200"": {
                ""description"": ""A list of example resources"",
                ""schema"": {
                  ""type"": ""array"",
                  ""items"": {
                    ""type"": ""object"",
                    ""properties"": {
                      ""id"": {
                        ""type"": ""integer"",
                        ""description"": ""The unique identifier for the resource""
                      },
                      ""name"": {
                        ""type"": ""string"",
                        ""description"": ""The name of the resource""
                      }
                    }
                  }
                }
              },
              ""400"": {
                ""description"": ""Bad request""
              }
            }
          },
          {
            ""method"": ""POST"",
            ""description"": ""Create a new example resource"",
            ""parameters"": [
              {
                ""name"": ""name"",
                ""type"": ""string"",
                ""required"": true,
                ""description"": ""The name of the resource""
              }
            ],
            ""responses"": {
              ""201"": {
                ""description"": ""Resource created successfully"",
                ""schema"": {
                  ""type"": ""object"",
                  ""properties"": {
                    ""id"": {
                      ""type"": ""integer"",
                      ""description"": ""The unique identifier for the new resource""
                    }
                  }
                }
              },
              ""400"": {
                ""description"": ""Bad request""
              }
            }
          }
        ]
      }
    ]
  }
}
```

This is a hypothetical structure and is not based on any specific API. It outlines basic components like resources, methods (GET, POST), parameters, and typical HTTP response codes. In a real scenario, the API's structure would depend on its specific endpoints and functionalities."
torchvision.transforms,No documentation found.,"Based on the input ""No documentation found,"" I cannot extract any information about types, functions, or relationships of an API, as there is no documentation provided to analyze.

Therefore, I cannot generate the requested structured JSON representation.

Please provide the actual API documentation.","To create a structured representation of an API when no documentation is provided, we must rely on assumptions and typical API structure. Below is an example JSON representation of an API, using common RESTful API patterns. If you have more specific information or an actual API specification to provide, I can structure the representation more accurately.

```json
{
  ""endpoints"": [
    {
      ""name"": ""Get User"",
      ""method"": ""GET"",
      ""url"": ""/users/{userId}"",
      ""description"": ""Retrieves user information by user ID."",
      ""parameters"": [
        {
          ""name"": ""userId"",
          ""in"": ""path"",
          ""type"": ""string"",
          ""required"": true,
          ""description"": ""The ID of the user to retrieve.""
        }
      ],
      ""responses"": {
        ""200"": {
          ""description"": ""Successful response"",
          ""content"": {
            ""application/json"": {
              ""schema"": {
                ""type"": ""object"",
                ""properties"": {
                  ""id"": {
                    ""type"": ""string""
                  },
                  ""name"": {
                    ""type"": ""string""
                  },
                  ""email"": {
                    ""type"": ""string""
                  }
                }
              }
            }
          }
        },
        ""404"": {
          ""description"": ""User not found""
        }
      }
    },
    {
      ""name"": ""Create User"",
      ""method"": ""POST"",
      ""url"": ""/users"",
      ""description"": ""Creates a new user."",
      ""requestBody"": {
        ""required"": true,
        ""content"": {
          ""application/json"": {
            ""schema"": {
              ""type"": ""object"",
              ""properties"": {
                ""name"": {
                  ""type"": ""string""
                },
                ""email"": {
                  ""type"": ""string""
                },
                ""password"": {
                  ""type"": ""string""
                }
              },
              ""required"": [""name"", ""email"", ""password""]
            }
          }
        }
      },
      ""responses"": {
        ""201"": {
          ""description"": ""User created successfully"",
          ""content"": {
            ""application/json"": {
              ""schema"": {
                ""type"": ""object"",
                ""properties"": {
                  ""id"": {
                    ""type"": ""string""
                  },
                  ""name"": {
                    ""type"": ""string""
                  },
                  ""email"": {
                    ""type"": ""string""
                  }
                }
              }
            }
          }
        },
        ""400"": {
          ""description"": ""Invalid input""
        }
      }
    },
    {
      ""name"": ""List Users"",
      ""method"": ""GET"",
      ""url"": ""/users"",
      ""description"": ""Retrieves a list of users."",
      ""parameters"": [
        {
          ""name"": ""limit"",
          ""in"": ""query"",
          ""type"": ""integer"",
          ""required"": false,
          ""description"": ""The number of users to return.""
        },
        {
          ""name"": ""offset"",
          ""in"": ""query"",
          ""type"": ""integer"",
          ""required"": false,
          ""description"": ""The number of users to skip before starting to collect the result set.""
        }
      ],
      ""responses"": {
        ""200"": {
          ""description"": ""A list of users"",
          ""content"": {
            ""application/json"": {
              ""schema"": {
                ""type"": ""array"",
                ""items"": {
                  ""type"": ""object"",
                  ""properties"": {
                    ""id"": {
                      ""type"": ""string""
                    },
                    ""name"": {
                      ""type"": ""string""
                    },
                    ""email"": {
                      ""type"": ""string""
                    }
                  }
                }
              }
            }
          }
        }
      }
    }
  ]
}
```

This example JSON illustrates a typical RESTful API structure, with common CRUD operations such as getting, creating, and listing users. Each endpoint is described with its HTTP method, URL, parameters, request bodies (where applicable), and possible responses.

If you have real API details or specific endpoints, methods, and data structures, you can replace the placeholder information above with those actual details."
torch.nn.functional,Functional interface.,"```json
{
  ""api_description"": ""Structured representation based on the concept of a 'Functional interface'. Specific details like name, method signature, or purpose are not provided in the input documentation."",
  ""types"": [
    {
      ""name"": ""FunctionalInterfaceType"",
      ""kind"": ""interface"",
      ""description"": ""A functional interface, which by definition has exactly one abstract method. The specific name and purpose of this interface are not provided."",
      ""is_functional"": true,
      ""methods"": [
        {
          ""name"": ""singleAbstractMethod"",
          ""description"": ""The single abstract method required by this functional interface. Its signature (parameters and return type) and specific behavior are not specified in the documentation."",
          ""parameters"": [],
          ""return_type"": ""unknown"",
          ""is_abstract"": true
        }
      ],
      ""relationships"": [
        {
          ""type"": ""contains"",
          ""target"": ""singleAbstractMethod"",
          ""description"": ""This functional interface contains its single abstract method, which defines its primary function.""
        }
      ]
    }
  ],
  ""functions"": [],
  ""relationships"": []
}
```","To create a structured representation of the API in JSON format, I would need to know the specific details from the API documentation you're referring to, such as the available endpoints, types, and functions. Since you haven't provided those details, I'll provide a general template of how you might structure an API using JSON. Once you provide specific details, we can adjust the template to fit the documentation you're looking at.

Here is a general example of how you might structure an API in JSON format:

```json
{
  ""name"": ""Example API"",
  ""version"": ""1.0"",
  ""baseUrl"": ""https://api.example.com"",
  ""endpoints"": [
    {
      ""path"": ""/users"",
      ""methods"": {
        ""GET"": {
          ""description"": ""Retrieve a list of users"",
          ""parameters"": [
            {
              ""name"": ""page"",
              ""type"": ""integer"",
              ""description"": ""The page of users to retrieve""
            },
            {
              ""name"": ""limit"",
              ""type"": ""integer"",
              ""description"": ""Number of users to retrieve per page""
            }
          ],
          ""responses"": {
            ""200"": {
              ""description"": ""A list of users"",
              ""schema"": {
                ""type"": ""array"",
                ""items"": {
                  ""$ref"": ""#/definitions/User""
                }
              }
            },
            ""400"": {
              ""description"": ""Bad Request""
            }
          }
        },
        ""POST"": {
          ""description"": ""Create a new user"",
          ""parameters"": [
            {
              ""name"": ""user"",
              ""in"": ""body"",
              ""required"": true,
              ""schema"": {
                ""$ref"": ""#/definitions/User""
              }
            }
          ],
          ""responses"": {
            ""201"": {
              ""description"": ""User created""
            },
            ""400"": {
              ""description"": ""Bad Request""
            }
          }
        }
      }
    }
  ],
  ""definitions"": {
    ""User"": {
      ""type"": ""object"",
      ""properties"": {
        ""id"": {
          ""type"": ""integer""
        },
        ""name"": {
          ""type"": ""string""
        },
        ""email"": {
          ""type"": ""string"",
          ""format"": ""email""
        }
      },
      ""required"": [""name"", ""email""]
    }
  }
}
```

Please provide the details from your API documentation if you want to fill in specific information regarding endpoints, methods, parameters, and response schemas."
Dataset,"An abstract class representing a :class:`Dataset`.  All datasets that represent a map from keys to data samples should subclass it. All subclasses should overwrite :meth:`__getitem__`, supporting fetching a data sample for a given key. Subclasses could also optionally overwrite :meth:`__len__`, which is expected to return the size of the dataset by many :class:`~torch.utils.data.Sampler` implementations and the default options of :class:`~torch.utils.data.DataLoader`. Subclasses could also optionally implement :meth:`__getitems__`, for speedup batched samples loading. This method accepts list of indices of samples of batch and returns list of samples.  .. note::   :class:`~torch.utils.data.DataLoader` by default constructs an index   sampler that yields integral indices.  To make it work with a map-style   dataset with non-integral indices/keys, a custom sampler must be provided.","```json
{
  ""types"": [
    {
      ""name"": ""Dataset"",
      ""type"": ""abstract class"",
      ""description"": ""Represents a map from keys to data samples. Subclasses should overwrite __getitem__ and optionally __len__ and __getitems__."",
      ""methods"": [
        {
          ""name"": ""__getitem__"",
          ""description"": ""Fetch a data sample for a given key."",
          ""signature"": ""(key) -> sample"",
          ""required"": true
        },
        {
          ""name"": ""__len__"",
          ""description"": ""Return the size of the dataset."",
          ""signature"": ""() -> int"",
          ""required"": false,
          ""notes"": ""Expected by many Sampler implementations and default DataLoader options.""
        },
        {
          ""name"": ""__getitems__"",
          ""description"": ""Fetch a list of data samples for a list of indices/keys for batched loading speedup."",
          ""signature"": ""(keys: list) -> list[sample]"",
          ""required"": false
        }
      ],
      ""relationships"": [
        {
          ""type"": ""subclassed_by"",
          ""target"": ""User-defined Dataset classes"",
          ""description"": ""All map-style datasets should subclass this class.""
        },
        {
          ""type"": ""interacted_with_by"",
          ""target"": ""Sampler"",
          ""via"": [""__len__""]
        },
        {
          ""type"": ""interacted_with_by"",
          ""target"": ""DataLoader"",
          ""via"": [""__len__"", ""__getitem__"", ""__getitems__""]
        }
      ],
      ""notes"": ""To make it work with a map-style dataset with non-integral indices/keys, a custom Sampler must be provided for DataLoader.""
    },
    {
      ""name"": ""Sampler"",
      ""type"": ""class"",
      ""description"": ""Used by DataLoader to yield indices/keys. Many implementations expect the Dataset to implement __len__."",
      ""relationships"": [
        {
          ""type"": ""uses"",
          ""target"": ""Dataset"",
          ""via"": [""__len__""]
        },
         {
          ""type"": ""used_by"",
          ""target"": ""DataLoader""
        }
      ]
    },
    {
      ""name"": ""DataLoader"",
      ""type"": ""class"",
      ""description"": ""Iterates over a Dataset, typically using a Sampler, to load data."",
      ""relationships"": [
        {
          ""type"": ""uses"",
          ""target"": ""Dataset"",
          ""via"": [""__len__"", ""__getitem__"", ""__getitems__""]
        },
        {
          ""type"": ""uses"",
          ""target"": ""Sampler""
        }
      ],
       ""notes"": ""By default constructs an index sampler that yields integral indices. Requires custom sampler for non-integral Dataset keys.""
    }
  ]
}
```","```json
{
  ""class"": ""Dataset"",
  ""description"": ""An abstract class representing a Dataset that serves as a map from keys to data samples."",
  ""relationships"": {
    ""subclasses"": ""Should be subclassed by all datasets representing a map from keys to data samples."",
    ""interacts_with"": [
      {
        ""class"": ""torch.utils.data.Sampler"",
        ""description"": ""Expects `__len__` to return the size of the dataset by many implementations.""
      },
      {
        ""class"": ""torch.utils.data.DataLoader"",
        ""description"": ""By default, constructs an index sampler that yields integral indices.""
      }
    ]
  },
  ""methods"": [
    {
      ""name"": ""__getitem__"",
      ""description"": ""Fetches a data sample for a given key."",
      ""requirement"": ""Must be overwritten by all subclasses.""
    },
    {
      ""name"": ""__len__"",
      ""description"": ""Returns the size of the dataset."",
      ""requirement"": ""Optionally can be overwritten by subclasses.""
    },
    {
      ""name"": ""__getitems__"",
      ""description"": ""Accepts a list of indices of samples of a batch and returns a list of samples, for speedup batched samples loading."",
      ""requirement"": ""Optionally can be implemented by subclasses.""
    }
  ],
  ""notes"": [
    {
      ""note"": ""torch.utils.data.DataLoader"",
      ""description"": ""To make it work with a map-style dataset with non-integral indices/keys, a custom sampler must be provided.""
    }
  ]
}
```"
DataLoader,"Data loader combines a dataset and a sampler, and provides an iterable over the given dataset.  The :class:`~torch.utils.data.DataLoader` supports both map-style and iterable-style datasets with single- or multi-process loading, customizing loading order and optional automatic batching (collation) and memory pinning.  See :py:mod:`torch.utils.data` documentation page for more details.  Args:     dataset (Dataset): dataset from which to load the data.     batch_size (int, optional): how many samples per batch to load         (default: ``1``).     shuffle (bool, optional): set to ``True`` to have the data reshuffled         at every epoch (default: ``False``).     sampler (Sampler or Iterable, optional): defines the strategy to draw         samples from the dataset. Can be any ``Iterable`` with ``__len__``         implemented. If specified, :attr:`shuffle` must not be specified.     batch_sampler (Sampler or Iterable, optional): like :attr:`sampler`, but         returns a batch of indices at a time. Mutually exclusive with         :attr:`batch_size`, :attr:`shuffle`, :attr:`sampler`,         and :attr:`drop_last`.     num_workers (int, optional): how many subprocesses to use for data         loading. ``0`` means that the data will be loaded in the main process.         (default: ``0``)     collate_fn (Callable, optional): merges a list of samples to form a         mini-batch of Tensor(s).  Used when using batched loading from a         map-style dataset.     pin_memory (bool, optional): If ``True``, the data loader will copy Tensors         into device/CUDA pinned memory before returning them.  If your data elements         are a custom type, or your :attr:`collate_fn` returns a batch that is a custom type,         see the example below.     drop_last (bool, optional): set to ``True`` to drop the last incomplete batch,         if the dataset size is not divisible by the batch size. If ``False`` and         the size of dataset is not divisible by the batch size, then the last batch         will be smaller. (default: ``False``)     timeout (numeric, optional): if positive, the timeout value for collecting a batch         from workers. Should always be non-negative. (default: ``0``)     worker_init_fn (Callable, optional): If not ``None``, this will be called on each         worker subprocess with the worker id (an int in ``[0, num_workers - 1]``) as         input, after seeding and before data loading. (default: ``None``)     multiprocessing_context (str or multiprocessing.context.BaseContext, optional): If         ``None``, the default `multiprocessing context`_ of your operating system will         be used. (default: ``None``)     generator (torch.Generator, optional): If not ``None``, this RNG will be used         by RandomSampler to generate random indexes and multiprocessing to generate         ``base_seed`` for workers. (default: ``None``)     prefetch_factor (int, optional, keyword-only arg): Number of batches loaded         in advance by each worker. ``2`` means there will be a total of         2 * num_workers batches prefetched across all workers. (default value depends         on the set value for num_workers. If value of num_workers=0 default is ``None``.         Otherwise, if value of ``num_workers > 0`` default is ``2``).     persistent_workers (bool, optional): If ``True``, the data loader will not shut down         the worker processes after a dataset has been consumed once. This allows to         maintain the workers `Dataset` instances alive. (default: ``False``)     pin_memory_device (str, optional): the device to :attr:`pin_memory` on if ``pin_memory`` is         ``True``. If not given, the current :ref:`accelerator<accelerators>` will be the         default. This argument is discouraged and subject to deprecated.     in_order (bool, optional): If ``False``, the data loader will not enforce that batches         are returned in a first-in, first-out order. Only applies when ``num_workers > 0``. (default: ``True``)   .. warning:: If the ``spawn`` start method is used, :attr:`worker_init_fn`              cannot be an unpicklable object, e.g., a lambda function. See              :ref:`multiprocessing-best-practices` on more details related              to multiprocessing in PyTorch.  .. warning:: ``len(dataloader)`` heuristic is based on the length of the sampler used.              When :attr:`dataset` is an :class:`~torch.utils.data.IterableDataset`,              it instead returns an estimate based on ``len(dataset) / batch_size``, with proper              rounding depending on :attr:`drop_last`, regardless of multi-process loading              configurations. This represents the best guess PyTorch can make because PyTorch              trusts user :attr:`dataset` code in correctly handling multi-process              loading to avoid duplicate data.               However, if sharding results in multiple workers having incomplete last batches,              this estimate can still be inaccurate, because (1) an otherwise complete batch can              be broken into multiple ones and (2) more than one batch worth of samples can be              dropped when :attr:`drop_last` is set. Unfortunately, PyTorch can not detect such              cases in general.               See `Dataset Types`_ for more details on these two types of datasets and how              :class:`~torch.utils.data.IterableDataset` interacts with              `Multi-process data loading`_.  .. warning:: See :ref:`reproducibility`, and :ref:`dataloader-workers-random-seed`, and              :ref:`data-loading-randomness` notes for random seed related questions.  .. warning:: Setting `in_order` to `False` can harm reproducibility and may lead to a skewed data              distribution being fed to the trainer in cases with imbalanced data.  .. _multiprocessing context:     https://docs.python.org/3/library/multiprocessing.html#contexts-and-start-methods","```json
{
  ""api_description"": ""Data loader for PyTorch datasets."",
  ""types"": [
    {
      ""name"": ""Dataset"",
      ""description"": ""Abstract base class for datasets.""
    },
    {
      ""name"": ""Sampler"",
      ""description"": ""Abstract base class for samplers.""
    },
    {
      ""name"": ""Iterable"",
      ""description"": ""Any object that can be iterated over. Must have __len__ implemented if used for sampler/batch_sampler in this context.""
    },
    {
      ""name"": ""Callable"",
      ""description"": ""A function or other object that can be called.""
    },
    {
      ""name"": ""Tensor"",
      ""description"": ""A multi-dimensional matrix containing elements of a single data type.""
    },
    {
      ""name"": ""numeric"",
      ""description"": ""A number (integer or float).""
    },
    {
      ""name"": ""torch.Generator"",
      ""description"": ""A random number generator.""
    },
    {
      ""name"": ""multiprocessing.context.BaseContext"",
      ""description"": ""Base class for multiprocessing contexts.""
    },
    {
        ""name"": ""str"",
        ""description"": ""A string.""
    }
  ],
  ""classes"": [
    {
      ""name"": ""DataLoader"",
      ""full_name"": ""torch.utils.data.DataLoader"",
      ""description"": ""Combines a dataset and a sampler, and provides an iterable over the given dataset. Supports map-style and iterable-style datasets, single- or multi-process loading, customizing loading order, automatic batching, and memory pinning."",
      ""constructor"": {
        ""description"": ""Initializes the DataLoader."",
        ""parameters"": [
          {
            ""name"": ""dataset"",
            ""type"": ""Dataset"",
            ""description"": ""dataset from which to load the data."",
            ""optional"": false
          },
          {
            ""name"": ""batch_size"",
            ""type"": ""int"",
            ""description"": ""how many samples per batch to load."",
            ""optional"": true,
            ""default"": 1
          },
          {
            ""name"": ""shuffle"",
            ""type"": ""bool"",
            ""description"": ""set to True to have the data reshuffled at every epoch."",
            ""optional"": true,
            ""default"": false,
            ""constraints"": [""Mutually exclusive with sampler and batch_sampler.""]
          },
          {
            ""name"": ""sampler"",
            ""type"": ""Sampler or Iterable"",
            ""description"": ""defines the strategy to draw samples from the dataset."",
            ""optional"": true,
            ""constraints"": [""Can be any Iterable with __len__ implemented."", ""If specified, shuffle must not be specified."", ""Mutually exclusive with batch_sampler.""]
          },
          {
            ""name"": ""batch_sampler"",
            ""type"": ""Sampler or Iterable"",
            ""description"": ""like sampler, but returns a batch of indices at a time."",
            ""optional"": true,
            ""constraints"": [""Mutually exclusive with batch_size, shuffle, sampler, and drop_last.""]
          },
          {
            ""name"": ""num_workers"",
            ""type"": ""int"",
            ""description"": ""how many subprocesses to use for data loading. 0 means main process."",
            ""optional"": true,
            ""default"": 0
          },
          {
            ""name"": ""collate_fn"",
            ""type"": ""Callable"",
            ""description"": ""merges a list of samples to form a mini-batch of Tensor(s). Used when using batched loading from a map-style dataset."",
            ""optional"": true
          },
          {
            ""name"": ""pin_memory"",
            ""type"": ""bool"",
            ""description"": ""If True, copies Tensors into device/CUDA pinned memory before returning them."",
            ""optional"": true
          },
          {
            ""name"": ""drop_last"",
            ""type"": ""bool"",
            ""description"": ""set to True to drop the last incomplete batch."",
            ""optional"": true,
            ""default"": false
          },
          {
            ""name"": ""timeout"",
            ""type"": ""numeric"",
            ""description"": ""timeout value for collecting a batch from workers."",
            ""optional"": true,
            ""default"": 0,
            ""constraints"": [""Should always be non-negative.""]
          },
          {
            ""name"": ""worker_init_fn"",
            ""type"": ""Callable"",
            ""description"": ""Called on each worker subprocess with the worker id as input, after seeding and before data loading."",
            ""optional"": true,
            ""default"": null,
            ""constraints"": [""Cannot be an unpicklable object (e.g., lambda) if 'spawn' start method is used.""]
          },
          {
            ""name"": ""multiprocessing_context"",
            ""type"": ""str or multiprocessing.context.BaseContext"",
            ""description"": ""Multiprocessing context to use."",
            ""optional"": true,
            ""default"": null,
            ""constraints"": [""If None, the default context of the operating system is used.""]
          },
          {
            ""name"": ""generator"",
            ""type"": ""torch.Generator"",
            ""description"": ""RNG used by RandomSampler and multiprocessing workers."",
            ""optional"": true,
            ""default"": null
          },
           {
            ""name"": ""prefetch_factor"",
            ""type"": ""int"",
            ""description"": ""Number of batches loaded in advance by each worker."",
            ""optional"": true,
            ""constraints"": [""Keyword-only argument.""],
            ""default"": ""2 if num_workers > 0, None if num_workers == 0""
          },
          {
            ""name"": ""persistent_workers"",
            ""type"": ""bool"",
            ""description"": ""If True, worker processes are not shut down after a dataset has been consumed."",
            ""optional"": true,
            ""default"": false
          },
          {
            ""name"": ""pin_memory_device"",
            ""type"": ""str"",
            ""description"": ""The device to pin memory on if pin_memory is True."",
            ""optional"": true,
             ""constraints"": [""Requires pin_memory=True."", ""Discouraged and subject to deprecation.""]
          },
           {
            ""name"": ""in_order"",
            ""type"": ""bool"",
            ""description"": ""If False, batches are not enforced to be returned in FIFO order."",
            ""optional"": true,
            ""default"": true,
            ""constraints"": [""Only applies when num_workers > 0.""]
          }
        ]
      },
      ""iterable_behavior"": {
        ""description"": ""Provides an iterable interface. Iterating over a DataLoader instance yields batches of data."",
        ""yields"": ""Batches of data (typically Tensors), structure determined by dataset item and collate_fn.""
      },
      ""relationships"": [
        {
          ""type"": ""combines"",
          ""objects"": [""Dataset"", ""Sampler""]
        }
      ],
      ""constraints"": [
        ""shuffle is mutually exclusive with sampler."",
        ""batch_sampler is mutually exclusive with batch_size, shuffle, sampler, and drop_last."",
        ""sampler/batch_sampler must implement __len__ if they are Iterable but not Sampler subclass."",
        ""timeout must be non-negative."",
        ""worker_init_fn warning regarding unpicklable objects with 'spawn' multiprocessing context."",
        ""in_order setting only applicable when num_workers > 0.""
      ],
       ""warnings"": [
           ""If the 'spawn' start method is used, worker_init_fn cannot be an unpicklable object."",
           ""len(dataloader) heuristic is based on sampler length. For IterableDataset, it's an estimate based on len(dataset) / batch_size."",
           ""Setting in_order to False can harm reproducibility and may lead to a skewed data distribution.""
       ],
       ""notes"": [
           ""See torch.utils.data documentation for more details."",
           ""See Dataset Types for details on map-style vs iterable-style datasets."",
           ""See Multi-process data loading for interactions with IterableDataset."",
           ""See reproducibility, dataloader-workers-random-seed, and data-loading-randomness for random seed related questions.""
       ]
    }
  ]
}
```","Here's a structured representation of the API documentation for the `DataLoader` in JSON format:

```json
{
  ""DataLoader"": {
    ""description"": ""Data loader combines a dataset and a sampler, and provides an iterable over the given dataset. Supports both map-style and iterable-style datasets with single- or multi-process loading, customizing loading order and optional automatic batching (collation) and memory pinning."",
    ""arguments"": {
      ""dataset"": {
        ""type"": ""Dataset"",
        ""description"": ""dataset from which to load the data.""
      },
      ""batch_size"": {
        ""type"": ""int, optional"",
        ""description"": ""how many samples per batch to load (default: 1).""
      },
      ""shuffle"": {
        ""type"": ""bool, optional"",
        ""description"": ""set to True to have the data reshuffled at every epoch (default: False).""
      },
      ""sampler"": {
        ""type"": ""Sampler or Iterable, optional"",
        ""description"": ""defines the strategy to draw samples from the dataset. Can be any Iterable with __len__ implemented. If specified, shuffle must not be specified.""
      },
      ""batch_sampler"": {
        ""type"": ""Sampler or Iterable, optional"",
        ""description"": ""like sampler, but returns a batch of indices at a time. Mutually exclusive with batch_size, shuffle, sampler, and drop_last.""
      },
      ""num_workers"": {
        ""type"": ""int, optional"",
        ""description"": ""how many subprocesses to use for data loading. 0 means that the data will be loaded in the main process (default: 0).""
      },
      ""collate_fn"": {
        ""type"": ""Callable, optional"",
        ""description"": ""merges a list of samples to form a mini-batch of Tensor(s). Used when using batched loading from a map-style dataset.""
      },
      ""pin_memory"": {
        ""type"": ""bool, optional"",
        ""description"": ""If True, the data loader will copy Tensors into device/CUDA pinned memory before returning them.""
      },
      ""drop_last"": {
        ""type"": ""bool, optional"",
        ""description"": ""set to True to drop the last incomplete batch, if the dataset size is not divisible by the batch size (default: False).""
      },
      ""timeout"": {
        ""type"": ""numeric, optional"",
        ""description"": ""if positive, the timeout value for collecting a batch from workers. Should always be non-negative (default: 0).""
      },
      ""worker_init_fn"": {
        ""type"": ""Callable, optional"",
        ""description"": ""If not None, this will be called on each worker subprocess with the worker id as input, after seeding and before data loading (default: None).""
      },
      ""multiprocessing_context"": {
        ""type"": ""str or multiprocessing.context.BaseContext, optional"",
        ""description"": ""If None, the default multiprocessing context of your operating system will be used (default: None).""
      },
      ""generator"": {
        ""type"": ""torch.Generator, optional"",
        ""description"": ""If not None, this RNG will be used by RandomSampler to generate random indexes and multiprocessing to generate base_seed for workers (default: None).""
      },
      ""prefetch_factor"": {
        ""type"": ""int, optional"",
        ""description"": ""Number of batches loaded in advance by each worker. Default value depends on the set value for num_workers. If num_workers=0, default is None. Otherwise, if num_workers > 0, default is 2.""
      },
      ""persistent_workers"": {
        ""type"": ""bool, optional"",
        ""description"": ""If True, the data loader will not shut down the worker processes after a dataset has been consumed once (default: False).""
      },
      ""pin_memory_device"": {
        ""type"": ""str, optional"",
        ""description"": ""the device to pin_memory on if pin_memory is True. If not given, the current accelerator will be the default. This argument is discouraged and subject to deprecation.""
      },
      ""in_order"": {
        ""type"": ""bool, optional"",
        ""description"": ""If False, the data loader will not enforce that batches are returned in a first-in, first-out order when num_workers > 0 (default: True).""
      }
    },
    ""warnings"": [
      ""If the spawn start method is used, worker_init_fn cannot be an unpicklable object, e.g., a lambda function."",
      ""len(dataloader) heuristic is based on the length of the sampler used. When dataset is an IterableDataset, it instead returns an estimate based on len(dataset) / batch_size, with proper rounding depending on drop_last, regardless of multi-process loading configurations."",
      ""See reproducibility and data-loading random seed notes for random seed related questions."",
      ""Setting in_order to False can harm reproducibility and may lead to a skewed data distribution being fed to the trainer in cases with imbalanced data.""
    ],
    ""related_links"": {
      ""multiprocessing context"": ""https://docs.python.org/3/library/multiprocessing.html#contexts-and-start-methods""
    }
  }
}
```
This JSON format captures the key components of the DataLoader API including its arguments, warnings, and other relevant details."
torchvision.datasets,No documentation found.,"```json
{
  ""types"": [],
  ""functions"": [],
  ""relationships"": [],
  ""documentation_source"": ""No documentation found.""
}
```","Since there is no documentation provided, it's impossible to directly extract a structured representation of the API. However, you can create a template JSON structure that you can fill in once you have more information about the API, like its endpoints, methods, data types, and relationships.

Here's a basic template to help you get started:

```json
{
  ""api"": {
    ""name"": ""Unknown API"",
    ""version"": ""N/A"",
    ""endpoints"": [
      {
        ""path"": ""/example-endpoint"",
        ""method"": ""GET/POST/PUT/DELETE"",
        ""description"": ""Description of the endpoint"",
        ""parameters"": [
          {
            ""name"": ""parameterName"",
            ""type"": ""string/integer/boolean/array/object"",
            ""required"": true,
            ""description"": ""Description of the parameter""
          }
        ],
        ""responses"": [
          {
            ""status"": ""200"",
            ""description"": ""OK"",
            ""body"": {
              ""type"": ""object"",
              ""properties"": {
                ""propertyName"": {
                  ""type"": ""string/integer/boolean/array/object"",
                  ""description"": ""Description of the property""
                }
              }
            }
          }
        ]
      }
    ],
    ""dataTypes"": [
      {
        ""typeName"": ""ExampleType"",
        ""description"": ""Description of the type"",
        ""properties"": {
          ""propertyName"": {
            ""type"": ""string/integer/boolean/array/object"",
            ""description"": ""Description of the property""
          }
        }
      }
    ],
    ""security"": {
      ""authentication"": {
        ""type"": ""None/API key/OAuth 2.0"",
        ""description"": ""Description of the authentication mechanism""
      }
    }
  }
}
```

You can modify and populate this template once you have more information or documentation about the actual API. This JSON format provides a clear and organized representation of an API's structure, including endpoints, parameters, responses, data types, and security details."
ToTensor,"Convert a PIL Image or ndarray to tensor and scale the values accordingly.  This transform does not support torchscript.  Converts a PIL Image or numpy.ndarray (H x W x C) in the range [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0] if the PIL Image belongs to one of the modes (L, LA, P, I, F, RGB, YCbCr, RGBA, CMYK, 1) or if the numpy.ndarray has dtype = np.uint8  In the other cases, tensors are returned without scaling.  .. note::     Because the input image is scaled to [0.0, 1.0], this transformation should not be used when     transforming target image masks. See the `references`_ for implementing the transforms for image masks.  .. _references: https://github.com/pytorch/vision/tree/main/references/segmentation","```json
{
  ""api_component"": ""Transform"",
  ""description"": ""A transformation to convert images (PIL or ndarray) into PyTorch tensors with optional scaling."",
  ""transforms"": [
    {
      ""name"": ""ToTensor"",
      ""description"": ""Convert a PIL Image or ndarray to a torch.Tensor. Scales values accordingly for specific input types."",
      ""constraints"": [
        {
          ""type"": ""Compatibility"",
          ""detail"": ""Does not support torchscript.""
        }
      ],
      ""inputs"": [
        {
          ""type"": ""PIL.Image.Image"",
          ""details"": {
            ""modes_triggering_scaling"": [""L"", ""LA"", ""P"", ""I"", ""F"", ""RGB"", ""YCbCr"", ""RGBA"", ""CMYK"", ""1""],
            ""expected_range_for_scaling"": ""[0, 255]""
          }
        },
        {
          ""type"": ""numpy.ndarray"",
          ""details"": {
            ""shape"": ""(H x W x C)"",
            ""dtype_triggering_scaling"": ""np.uint8"",
            ""expected_range_for_scaling"": ""[0, 255]""
          }
        }
      ],
      ""outputs"": [
        {
          ""description"": ""Output when scaling is applied"",
          ""condition"": ""Input is PIL Image with mode in ['L', 'LA', 'P', 'I', 'F', 'RGB', 'YCbCr', 'RGBA', 'CMYK', '1'] OR Input is numpy.ndarray with dtype np.uint8 (assuming input values are in [0, 255])"",
          ""type"": ""torch.FloatTensor"",
          ""shape"": ""(C x H x W)"",
          ""value_range"": ""[0.0, 1.0]""
        },
        {
          ""description"": ""Output when scaling is NOT applied"",
          ""condition"": ""Input is PIL Image with mode NOT in ['L', ..., '1'] OR Input is numpy.ndarray with dtype other than np.uint8"",
          ""type"": ""torch.Tensor"",
          ""shape"": ""(C x H x W)"",
          ""value_range"": ""Depends on input range (no scaling applied)"",
           ""note"": ""Conversion to float might still occur depending on input dtype, but values are not scaled to [0, 1].""
        }
      ],
      ""usage_guidelines"": [
        {
          ""type"": ""Warning"",
          ""detail"": ""Should not be used when transforming target image masks."",
          ""reason"": ""The scaling to [0.0, 1.0] (applied under certain conditions) is typically not desired for discrete mask values.""
        }
      ],
      ""references"": [
        {
          ""description"": ""Reference for implementing transforms for image masks"",
          ""url"": ""https://github.com/pytorch/vision/tree/main/references/segmentation""
        }
      ]
    }
  ]
}
```","```json
{
  ""API"": {
    ""name"": ""ConvertToTensor"",
    ""description"": ""Converts a PIL Image or ndarray to a tensor and scales values accordingly. Does not support torchscript."",
    ""input"": [
      {
        ""type"": ""PIL.Image"",
        ""description"": ""A PIL Image object in one of the modes L, LA, P, I, F, RGB, YCbCr, RGBA, CMYK, 1.""
      },
      {
        ""type"": ""numpy.ndarray"",
        ""description"": ""A numpy array of shape (H x W x C) with dtype = np.uint8.""
      }
    ],
    ""output"": {
      ""type"": ""torch.FloatTensor"",
      ""description"": ""A tensor of shape (C x H x W) in the range [0.0, 1.0] if conditions are met; otherwise, unscaled.""
    },
    ""scalingCondition"": ""The image is scaled to range [0.0, 1.0] if PIL Image belongs to modes L, LA, P, I, F, RGB, YCbCr, RGBA, CMYK, 1, or if numpy.ndarray has dtype = np.uint8."",
    ""note"": ""This transformation should not be used for target image masks as they might get scaled incorrectly."",
    ""references"": ""https://github.com/pytorch/vision/tree/main/references/segmentation""
  }
}
```"
numpy,"NumPy =====  Provides   1. An array object of arbitrary homogeneous items   2. Fast mathematical operations over arrays   3. Linear Algebra, Fourier Transforms, Random Number Generation  How to use the documentation ---------------------------- Documentation is available in two forms: docstrings provided with the code, and a loose standing reference guide, available from `the NumPy homepage <https://numpy.org>`_.  We recommend exploring the docstrings using `IPython <https://ipython.org>`_, an advanced Python shell with TAB-completion and introspection capabilities.  See below for further instructions.  The docstring examples assume that `numpy` has been imported as ``np``::    >>> import numpy as np  Code snippets are indicated by three greater-than signs::    >>> x = 42   >>> x = x + 1  Use the built-in ``help`` function to view a function's docstring::    >>> help(np.sort)   ... # doctest: +SKIP  For some objects, ``np.info(obj)`` may provide additional help.  This is particularly true if you see the line ""Help on ufunc object:"" at the top of the help() page.  Ufuncs are implemented in C, not Python, for speed. The native Python help() does not know how to view their help, but our np.info() function does.  Available subpackages --------------------- lib     Basic functions used by several sub-packages. random     Core Random Tools linalg     Core Linear Algebra Tools fft     Core FFT routines polynomial     Polynomial tools testing     NumPy testing tools distutils     Enhancements to distutils with support for     Fortran compilers support and more (for Python <= 3.11)  Utilities --------- test     Run numpy unittests show_config     Show numpy build configuration __version__     NumPy version string  Viewing documentation using IPython -----------------------------------  Start IPython and import `numpy` usually under the alias ``np``: `import numpy as np`.  Then, directly past or use the ``%cpaste`` magic to paste examples into the shell.  To see which functions are available in `numpy`, type ``np.<TAB>`` (where ``<TAB>`` refers to the TAB key), or use ``np.*cos*?<ENTER>`` (where ``<ENTER>`` refers to the ENTER key) to narrow down the list.  To view the docstring for a function, use ``np.cos?<ENTER>`` (to view the docstring) and ``np.cos??<ENTER>`` (to view the source code).  Copies vs. in-place operation ----------------------------- Most of the functions in `numpy` return a copy of the array argument (e.g., `np.sort`).  In-place versions of these functions are often available as array methods, i.e. ``x = np.array([1,2,3]); x.sort()``. Exceptions to this rule are documented.","```json
{
  ""name"": ""NumPy"",
  ""description"": ""Provides an array object of arbitrary homogeneous items, fast mathematical operations over arrays, Linear Algebra, Fourier Transforms, Random Number Generation."",
  ""structure"": {
    ""types"": [
      {
        ""name"": ""array object"",
        ""description"": ""An object of arbitrary homogeneous items."",
        ""characteristics"": [
          ""homogeneous items"",
          ""supports fast mathematical operations"",
          ""often returned as a copy by top-level functions"",
          ""has in-place methods (e.g., sort())""
        ]
      },
      {
        ""name"": ""ufunc object"",
        ""description"": ""Universal function, typically implemented in C for speed."",
        ""characteristics"": [
          ""implemented in C"",
          ""require np.info() for full documentation viewing via Python's help()""
        ],
        ""related_utilities"": [
          ""np.info""
        ]
      }
    ],
    ""subpackages"": [
      {
        ""name"": ""lib"",
        ""description"": ""Basic functions used by several sub-packages.""
      },
      {
        ""name"": ""random"",
        ""description"": ""Core Random Tools.""
      },
      {
        ""name"": ""linalg"",
        ""description"": ""Core Linear Algebra Tools.""
      },
      {
        ""name"": ""fft"",
        ""description"": ""Core FFT routines.""
      },
      {
        ""name"": ""polynomial"",
        ""description"": ""Polynomial tools.""
      },
      {
        ""name"": ""testing"",
        ""description"": ""NumPy testing tools.""
      },
      {
        ""name"": ""distutils"",
        ""description"": ""Enhancements to distutils with support for Fortran compilers support and more (for Python <= 3.11).""
      }
    ],
    ""top_level_members"": [
      {
        ""name"": ""sort"",
        ""type"": ""function"",
        ""description"": ""Example function. Most such functions return a copy of the array argument."",
        ""mentions"": [
          ""Used in help(np.sort) example"",
          ""Example of a function that returns a copy"",
          ""Corresponding in-place method exists on array objects (x.sort())""
        ]
      },
      {
        ""name"": ""info"",
        ""type"": ""utility"",
        ""description"": ""Provides additional help for an object, particularly useful for ufunc objects."",
        ""mentions"": [
          ""Use np.info(obj)"",
          ""Helpful when help() shows 'Help on ufunc object:'"",
          ""Handles ufunc help where help() does not""
        ],
        ""related_types"": [
          ""ufunc object""
        ]
      },
       {
        ""name"": ""array"",
        ""type"": ""constructor"",
        ""description"": ""Function used to create an array object."",
        ""mentions"": [
          ""Used in example x = np.array([1,2,3])""
        ],
        ""output_type"": ""array object""
      },
       {
        ""name"": ""cos"",
        ""type"": ""function"",
        ""description"": ""Example function used in documentation viewing examples (IPython)."",
        ""mentions"": [
          ""Used in IPython examples (np.cos?, np.cos??)""
        ]
      },
      {
        ""name"": ""test"",
        ""type"": ""utility"",
        ""description"": ""Run numpy unittests.""
      },
      {
        ""name"": ""show_config"",
        ""type"": ""utility"",
        ""description"": ""Show numpy build configuration.""
      },
      {
        ""name"": ""__version__"",
        ""type"": ""attribute"",
        ""description"": ""NumPy version string.""
      }
    ]
  },
   ""relationships"": [
     {""from"": ""NumPy"", ""to"": ""array object"", ""type"": ""provides""},
     {""from"": ""NumPy"", ""to"": ""lib"", ""type"": ""contains subpackage""},
     {""from"": ""NumPy"", ""to"": ""random"", ""type"": ""contains subpackage""},
     {""from"": ""NumPy"", ""to"": ""linalg"", ""type"": ""contains subpackage""},
     {""from"": ""NumPy"", ""to"": ""fft"", ""type"": ""contains subpackage""},
     {""from"": ""NumPy"", ""to"": ""polynomial"", ""type"": ""contains subpackage""},
     {""from"": ""NumPy"", ""to"": ""testing"", ""type"": ""contains subpackage""},
     {""from"": ""NumPy"", ""to"": ""distutils"", ""type"": ""contains subpackage""},
     {""from"": ""NumPy"", ""to"": ""sort"", ""type"": ""contains top-level member""},
     {""from"": ""NumPy"", ""to"": ""info"", ""type"": ""contains top-level member""},
     {""from"": ""NumPy"", ""to"": ""array"", ""type"": ""contains top-level member""},
     {""from"": ""NumPy"", ""to"": ""cos"", ""type"": ""contains top-level member""},
     {""from"": ""NumPy"", ""to"": ""test"", ""type"": ""contains top-level member""},
     {""from"": ""NumPy"", ""to"": ""show_config"", ""type"": ""contains top-level member""},
     {""from"": ""NumPy"", ""to"": ""__version__"", ""type"": ""contains top-level member""},
     {""from"": ""np.info"", ""to"": ""ufunc object"", ""type"": ""provides help for""},
     {""from"": ""np.sort"", ""to"": ""array object"", ""type"": ""operates on""},
     {""from"": ""np.sort"", ""to"": ""array object"", ""type"": ""returns copy of""},
     {""from"": ""array object"", ""to"": ""sort() method"", ""type"": ""has method corresponding to""},
     {""from"": ""np.array"", ""to"": ""array object"", ""type"": ""creates""}
   ]
}
```","Here's a structured representation of the NumPy API documentation in JSON format:

```json
{
  ""API"": {
    ""name"": ""NumPy"",
    ""provides"": [
      ""An array object of arbitrary homogeneous items"",
      ""Fast mathematical operations over arrays"",
      ""Linear Algebra, Fourier Transforms, Random Number Generation""
    ],
    ""usage_instructions"": {
      ""docstring_exploration"": [
        ""Import NumPy as np"",
        ""Use the built-in `help` function to view a function's docstring"",
        ""Use `np.info(obj)` for additional information on ufunc objects""
      ],
      ""IPython_usage"": [
        ""Start IPython and import NumPy as np"",
        ""Use `np.<TAB>` to list available functions"",
        ""Use `np.<function>?` to view the docstring"",
        ""Use `np.<function>??` to view the source code""
      ]
    },
    ""subpackages"": {
      ""lib"": ""Basic functions used by several sub-packages"",
      ""random"": ""Core Random Tools"",
      ""linalg"": ""Core Linear Algebra Tools"",
      ""fft"": ""Core FFT routines"",
      ""polynomial"": ""Polynomial tools"",
      ""testing"": ""NumPy testing tools"",
      ""distutils"": ""Enhancements to distutils with support for Fortran compilers (for Python <= 3.11)""
    },
    ""utilities"": {
      ""test"": ""Run NumPy unittests"",
      ""show_config"": ""Show NumPy build configuration"",
      ""__version__"": ""NumPy version string""
    },
    ""copy_vs_inplace"": [
      ""Most functions return a copy of the array argument"",
      ""In-place versions are available as array methods (e.g., x.sort())"",
      ""Exceptions to this rule are documented""
    ],
    ""documentation"": {
      ""types"": [""docstrings"", ""reference guide""],
      ""access"": ""Available from NumPy homepage""
    }
  }
}
```

This JSON structure captures the main components of the API, its subpackages, utilities, documentation instructions, and notes on function operations (copy vs. in-place)."
pandas,"pandas - a powerful data analysis and manipulation library for Python =====================================================================  **pandas** is a Python package providing fast, flexible, and expressive data structures designed to make working with ""relational"" or ""labeled"" data both easy and intuitive. It aims to be the fundamental high-level building block for doing practical, **real world** data analysis in Python. Additionally, it has the broader goal of becoming **the most powerful and flexible open source data analysis / manipulation tool available in any language**. It is already well on its way toward this goal.  Main Features ------------- Here are just a few of the things that pandas does well:    - Easy handling of missing data in floating point as well as non-floating     point data.   - Size mutability: columns can be inserted and deleted from DataFrame and     higher dimensional objects   - Automatic and explicit data alignment: objects can be explicitly aligned     to a set of labels, or the user can simply ignore the labels and let     `Series`, `DataFrame`, etc. automatically align the data for you in     computations.   - Powerful, flexible group by functionality to perform split-apply-combine     operations on data sets, for both aggregating and transforming data.   - Make it easy to convert ragged, differently-indexed data in other Python     and NumPy data structures into DataFrame objects.   - Intelligent label-based slicing, fancy indexing, and subsetting of large     data sets.   - Intuitive merging and joining data sets.   - Flexible reshaping and pivoting of data sets.   - Hierarchical labeling of axes (possible to have multiple labels per tick).   - Robust IO tools for loading data from flat files (CSV and delimited),     Excel files, databases, and saving/loading data from the ultrafast HDF5     format.   - Time series-specific functionality: date range generation and frequency     conversion, moving window statistics, date shifting and lagging.","```json
{
  ""library"": ""pandas"",
  ""description"": ""A Python package providing fast, flexible, and expressive data structures designed to make working with \""relational\"" or \""labeled\"" data both easy and intuitive. It aims to be the fundamental high-level building block for doing practical, real world data analysis in Python."",
  ""core_components"": {
    ""types"": [
      {
        ""name"": ""Series"",
        ""description"": ""A one-dimensional labeled array.""
      },
      {
        ""name"": ""DataFrame"",
        ""description"": ""A two-dimensional labeled data structure with columns of potentially different types.""
      }
      // Note: ""higher dimensional objects"" mentioned but not explicitly named as types in the text provided.
    ],
    ""main_features"": [
      {
        ""name"": ""Missing Data Handling"",
        ""description"": ""Easy handling of missing data in floating point as well as non-floating point data.""
      },
      {
        ""name"": ""Size Mutability"",
        ""description"": ""Columns can be inserted and deleted from DataFrame and higher dimensional objects.""
      },
      {
        ""name"": ""Data Alignment"",
        ""description"": ""Automatic and explicit data alignment based on labels.""
      },
      {
        ""name"": ""Hierarchical Indexing"",
        ""description"": ""Hierarchical labeling of axes (possible to have multiple labels per tick).""
      }
    ]
  },
  ""functional_categories"": [
    {
      ""category_name"": ""Data Manipulation"",
      ""operations"": [
        {
          ""name"": ""Group By Operations"",
          ""description"": ""Powerful, flexible group by functionality to perform split-apply-combine operations (aggregating and transforming data).""
        },
        {
          ""name"": ""Converting Data Structures"",
          ""description"": ""Easy conversion from ragged, differently-indexed data in other Python and NumPy data structures into DataFrame objects.""
        },
        {
          ""name"": ""Merging and Joining"",
          ""description"": ""Intuitive merging and joining data sets.""
        },
        {
          ""name"": ""Reshaping and Pivoting"",
          ""description"": ""Flexible reshaping and pivoting of data sets.""
        }
      ]
    },
    {
      ""category_name"": ""Indexing and Selection"",
      ""operations"": [
        {
          ""name"": ""Label-based Slicing"",
          ""description"": ""Intelligent label-based slicing of large data sets.""
        },
        {
          ""name"": ""Fancy Indexing"",
          ""description"": ""Fancy indexing of large data sets.""
        },
        {
          ""name"": ""Subsetting"",
          ""description"": ""Subsetting of large data sets.""
        }
      ]
    },
    {
      ""category_name"": ""IO Tools"",
      ""operations"": [
         {
          ""name"": ""Loading/Saving Data"",
          ""description"": ""Robust tools for loading and saving data from various formats.""
        },
        {
          ""name"": ""Reading/Writing Flat Files"",
          ""description"": ""Loading data from flat files (CSV and delimited).""
        },
         {
          ""name"": ""Reading/Writing Excel Files"",
          ""description"": ""Loading data from and saving data to Excel files.""
        },
         {
          ""name"": ""Reading/Writing Databases"",
          ""description"": ""Loading data from and saving data to databases.""
        },
         {
          ""name"": ""Reading/Writing HDF5"",
          ""description"": ""Saving/loading data from the ultrafast HDF5 format.""
        }
      ]
    },
    {
      ""category_name"": ""Time Series Functionality"",
      ""operations"": [
        {
          ""name"": ""Date Range Generation"",
          ""description"": ""Generating sequences of dates.""
        },
        {
          ""name"": ""Frequency Conversion"",
          ""description"": ""Converting time series frequency.""
        },
        {
          ""name"": ""Moving Window Statistics"",
          ""description"": ""Calculating moving window statistics on time series data.""
        },
        {
          ""name"": ""Date Shifting and Lagging"",
          ""description"": ""Shifting and lagging time series data.""
        }
      ]
    }
  ],
  ""relationships"": [
    {
      ""description"": ""DataFrame is typically composed of Series objects (columns).""
    },
    {
      ""description"": ""Most operations (Data Manipulation, Indexing, IO, Time Series) are performed on or return Series or DataFrame objects.""
    },
    {
      ""description"": ""Provides tools to convert data between pandas structures (DataFrame) and other Python/NumPy data structures.""
    },
     {
      ""description"": ""IO tools facilitate interaction with external data sources (files, databases) to load data into or save data from Series/DataFrame.""
    }
  ]
}
```","Certainly! Below is a structured representation of the API documentation for pandas, presented in JSON format. This representation organizes various aspects of the library, including its key functionalities and features.

```json
{
  ""pandas"": {
    ""description"": ""A powerful data analysis and manipulation library for Python, providing fast, flexible, and expressive data structures designed to make working with 'relational' or 'labeled' data both easy and intuitive."",
    ""goal"": ""To be the fundamental high-level building block for doing practical, real world data analysis in Python and to become the most powerful and flexible open source data analysis/manipulation tool available in any language."",
    ""main_features"": [
      {
        ""feature"": ""Handling of missing data"",
        ""description"": ""Easy handling of missing data in both floating point and non-floating point data.""
      },
      {
        ""feature"": ""Size mutability"",
        ""description"": ""Columns can be inserted and deleted from DataFrame and higher dimensional objects.""
      },
      {
        ""feature"": ""Automatic and explicit data alignment"",
        ""description"": ""Objects can be explicitly aligned to a set of labels, or can automatically align the data in computations.""
      },
      {
        ""feature"": ""Group by functionality"",
        ""description"": ""Powerful, flexible group by functionality to perform split-apply-combine operations, for aggregating and transforming data.""
      },
      {
        ""feature"": ""Data conversion"",
        ""description"": ""Easy conversion of ragged, differently-indexed data in other Python and NumPy data structures into DataFrame objects.""
      },
      {
        ""feature"": ""Data slicing and indexing"",
        ""description"": ""Intelligent label-based slicing, fancy indexing, and subsetting of large data sets.""
      },
      {
        ""feature"": ""Merging and joining data sets"",
        ""description"": ""Intuitive merging and joining of data sets.""
      },
      {
        ""feature"": ""Data reshaping and pivoting"",
        ""description"": ""Flexible reshaping and pivoting of data sets.""
      },
      {
        ""feature"": ""Hierarchical labeling"",
        ""description"": ""Hierarchical labeling of axes, allowing multiple labels per tick.""
      },
      {
        ""feature"": ""Robust IO tools"",
        ""description"": ""Robust IO tools for loading data from flat files (CSV and delimited), Excel files, databases, and saving/loading data from HDF5 format.""
      },
      {
        ""feature"": ""Time series functionality"",
        ""description"": ""Time series-specific functionality: date range generation and frequency conversion, moving window statistics, date shifting and lagging.""
      }
    ]
  }
}
```

This JSON structure captures the primary features and high-level description of the pandas library as described in the documentation provided. Each feature is listed with a brief description to explain its capabilities."
matplotlib.pyplot,"`matplotlib.pyplot` is a state-based interface to matplotlib. It provides an implicit,  MATLAB-like, way of plotting.  It also opens figures on your screen, and acts as the figure GUI manager.  pyplot is mainly intended for interactive plots and simple cases of programmatic plot generation::      import numpy as np     import matplotlib.pyplot as plt      x = np.arange(0, 5, 0.1)     y = np.sin(x)     plt.plot(x, y)     plt.show()  The explicit object-oriented API is recommended for complex plots, though pyplot is still usually used to create the figure and often the Axes in the figure. See `.pyplot.figure`, `.pyplot.subplots`, and `.pyplot.subplot_mosaic` to create figures, and :doc:`Axes API </api/axes_api>` for the plotting methods on an Axes::      import numpy as np     import matplotlib.pyplot as plt      x = np.arange(0, 5, 0.1)     y = np.sin(x)     fig, ax = plt.subplots()     ax.plot(x, y)     plt.show()   See :ref:`api_interfaces` for an explanation of the tradeoffs between the implicit and explicit interfaces.","```json
{
  ""module"": {
    ""name"": ""matplotlib.pyplot"",
    ""description"": ""A state-based interface to matplotlib, providing an implicit, MATLAB-like way of plotting. It opens figures and acts as the figure GUI manager. Primarily intended for interactive plots and simple cases. It is also commonly used to create figures and axes for the explicit object-oriented API."",
    ""alias"": ""plt"",
    ""functions"": [
      {
        ""name"": ""plot"",
        ""description"": ""Plot y versus x as lines and/or markers on the current Axes."",
        ""parameters"": [
          {
            ""name"": ""x"",
            ""type"": ""array-like"",
            ""description"": ""The horizontal coordinates of the data points. (Implied by example)""
          },
          {
            ""name"": ""y"",
            ""type"": ""array-like"",
            ""description"": ""The vertical coordinates of the data points. (Implied by example)""
          }
          // More parameters exist, but are not detailed in the provided text.
        ],
        ""returns"": null, // The function modifies the current Axes state rather than returning a primary object handle in this context.
        ""interface_style"": ""implicit"",
        ""relationships"": [
          {
            ""type"": ""draws_on"",
            ""target"": ""Axes"",
            ""details"": ""Draws on the currently active Axes managed by the state-based interface.""
          }
        ]
      },
      {
        ""name"": ""show"",
        ""description"": ""Display all open figures."",
        ""parameters"": [],
        ""returns"": ""None"",
        ""interface_style"": ""general"" // Used in both implicit and explicit examples
      },
      {
        ""name"": ""figure"",
        ""description"": ""Create a new Figure, or activate an existing Figure."",
        ""parameters"": [], // Common parameters like 'num', 'figsize' are not in the text
        ""returns"": {
          ""type"": ""Figure""
        },
        ""interface_style"": ""explicit_helper""
      },
      {
        ""name"": ""subplots"",
        ""description"": ""Create a Figure and a set of Axes."",
        ""parameters"": [], // Common parameters like 'nrows', 'ncols' are not in the text
        ""returns"": {
          ""type"": ""tuple"",
          ""components"": [
            {
              ""type"": ""Figure""
            },
            {
              ""type"": ""Axes"",
              ""details"": ""or array of Axes""
            }
          ]
        },
        ""interface_style"": ""explicit_helper""
      },
      {
        ""name"": ""subplot_mosaic"",
        ""description"": ""Create a Figure with a complex grid of Axes based on a layout specification."",
        ""parameters"": [], // Parameters like 'mosaic' are not in the text
        ""returns"": {
           ""type"": ""Figure"",
           ""details"": ""and a structured access (e.g., dictionary) to the created Axes""
        },
        ""interface_style"": ""explicit_helper""
      }
    ],
    ""types_returned_or_referenced"": [
      {
        ""name"": ""Figure"",
        ""description"": ""The top-level container for all plot elements, returned by figure, subplots, and subplot_mosaic. Represents the window or page that contains plots."",
        ""returned_by"": [""figure"", ""subplots"", ""subplot_mosaic""],
        ""relationships"": [
          {
            ""type"": ""contains"",
            ""target"": ""Axes"",
            ""details"": ""A Figure object contains one or more Axes objects.""
          }
        ]
      },
      {
        ""name"": ""Axes"",
        ""description"": ""The plotting area within a Figure, containing plotting methods (like plot). Often returned by subplots and subplot_mosaic."",
        ""returned_by"": [""subplots"", ""subplot_mosaic""],
        ""relationships"": [
          {
            ""type"": ""contained_by"",
            ""target"": ""Figure""
          },
           {
            ""type"": ""provides_method"",
            ""method_name"": ""plot"",
            ""details"": ""Allows plotting data directly onto this specific Axes object (used in explicit API).""
          }
        ]
      }
    ]
  }
}
```","```json
{
  ""module"": ""matplotlib.pyplot"",
  ""description"": ""A state-based interface to matplotlib, providing a MATLAB-like way of plotting, mainly intended for interactive plots and simple cases of programmatic plot generation. It also acts as the figure GUI manager."",
  ""functions"": [
    {
      ""name"": ""plot"",
      ""description"": ""Plots the data points specified by the given x and y."",
      ""example"": ""plt.plot(x, y)""
    },
    {
      ""name"": ""show"",
      ""description"": ""Opens figures on your screen and displays the created plots."",
      ""example"": ""plt.show()""
    },
    {
      ""name"": ""figure"",
      ""description"": ""Creates a new figure."",
      ""references"": [
        "".pyplot.figure""
      ]
    },
    {
      ""name"": ""subplots"",
      ""description"": ""Creates a figure and a set of subplots (axes)."",
      ""example"": ""fig, ax = plt.subplots()"",
      ""references"": [
        "".pyplot.subplots""
      ]
    },
    {
      ""name"": ""subplot_mosaic"",
      ""description"": ""Creates a figure using a mosaic layout for the subplots."",
      ""references"": [
        "".pyplot.subplot_mosaic""
      ]
    }
  ],
  ""objects"": [
    {
      ""name"": ""figure"",
      ""description"": ""An object representing the whole figure, including all plots, axes, etc.""
    },
    {
      ""name"": ""Axes"",
      ""description"": ""An object representing a single plot, where data can be plotted. It supports various plotting methods."",
      ""references"": [
        ""Axes API </api/axes_api>""
      ]
    }
  ],
  ""references"": {
    ""api_interfaces"": ""A reference to the explanation of tradeoffs between the implicit and explicit interfaces.""
  },
  ""examples"": [
    {
      ""description"": ""Implicit interface example"",
      ""code"": [
        ""import numpy as np"",
        ""import matplotlib.pyplot as plt"",
        ""x = np.arange(0, 5, 0.1)"",
        ""y = np.sin(x)"",
        ""plt.plot(x, y)"",
        ""plt.show()""
      ]
    },
    {
      ""description"": ""Explicit interface example"",
      ""code"": [
        ""import numpy as np"",
        ""import matplotlib.pyplot as plt"",
        ""x = np.arange(0, 5, 0.1)"",
        ""y = np.sin(x)"",
        ""fig, ax = plt.subplots()"",
        ""ax.plot(x, y)"",
        ""plt.show()""
      ]
    }
  ]
}
```"
AutoTokenizer,This is a generic tokenizer class that will be instantiated as one of the tokenizer classes of the library when created with the [`AutoTokenizer.from_pretrained`] class method.  This class cannot be instantiated directly using `__init__()` (throws an error).,"```json
{
  ""classes"": [
    {
      ""name"": ""AutoTokenizer"",
      ""description"": ""A generic tokenizer class that will be instantiated as one of the specific tokenizer classes from the library."",
      ""instantiation_notes"": ""Cannot be instantiated directly using `__init__()` (throws an error). Must be created using the `from_pretrained` class method."",
      ""methods"": [
        {
          ""name"": ""from_pretrained"",
          ""type"": ""class method"",
          ""description"": ""Method used to instantiate the generic AutoTokenizer as a specific tokenizer class.""
        }
      ]
    }
  ],
  ""types"": [],
  ""functions"": []
}
```","Certainly! Based on the provided API documentation, here's a structured JSON representation of the API, capturing the relevant types, functions, and their relationships:

```json
{
  ""classes"": [
    {
      ""name"": ""Tokenizer"",
      ""description"": ""A generic tokenizer class that cannot be instantiated directly."",
      ""constructor"": {
        ""method"": ""__init__"",
        ""instantiable"": false,
        ""error_message"": ""This class cannot be instantiated directly using __init__()""
      },
      ""methods"": [],
      ""instantiation"": {
        ""method"": ""AutoTokenizer.from_pretrained"",
        ""description"": ""Instantiates the tokenizer as one of the library's tokenizer classes.""
      }
    }
  ],
  ""methods"": [
    {
      ""class"": ""AutoTokenizer"",
      ""name"": ""from_pretrained"",
      ""description"": ""Class method to create a tokenizer object"",
      ""type"": ""class_method""
    }
  ]
}
```

This JSON structure captures the key concepts from the API specification: the `Tokenizer` class, the fact that it cannot be instantiated directly, and how it can be instantiated through a class method of `AutoTokenizer`."
DataCollatorWithPadding,"Data collator that will dynamically pad the inputs received.  Args:     tokenizer ([`PreTrainedTokenizer`] or [`PreTrainedTokenizerFast`]):         The tokenizer used for encoding the data.     padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `True`):         Select a strategy to pad the returned sequences (according to the model's padding side and padding index)         among:          - `True` or `'longest'` (default): Pad to the longest sequence in the batch (or no padding if only a single           sequence is provided).         - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum           acceptable input length for the model if that argument is not provided.         - `False` or `'do_not_pad'`: No padding (i.e., can output a batch with sequences of different lengths).     max_length (`int`, *optional*):         Maximum length of the returned list and optionally padding length (see above).     pad_to_multiple_of (`int`, *optional*):         If set will pad the sequence to a multiple of the provided value.          This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=         7.0 (Volta).     return_tensors (`str`, *optional*, defaults to `""pt""`):         The type of Tensor to return. Allowable values are ""np"", ""pt"" and ""tf"".","```json
[
  {
    ""name"": ""DataCollatorWithPadding"",
    ""type"": ""class"",
    ""description"": ""Data collator that will dynamically pad the inputs received."",
    ""methods"": [
      {
        ""name"": ""__init__"",
        ""type"": ""constructor"",
        ""description"": ""Initializes the data collator with padding capabilities."",
        ""parameters"": [
          {
            ""name"": ""tokenizer"",
            ""type"": ""PreTrainedTokenizer | PreTrainedTokenizerFast"",
            ""description"": ""The tokenizer used for encoding the data."",
            ""optional"": false
          },
          {
            ""name"": ""padding"",
            ""type"": ""bool | str | ~utils.PaddingStrategy"",
            ""description"": ""Selects a strategy to pad the returned sequences."",
            ""optional"": true,
            ""default"": ""True"",
            ""allowed_values"": [
              true,
              false,
              ""longest"",
              ""max_length"",
              ""do_not_pad""
            ]
          },
          {
            ""name"": ""max_length"",
            ""type"": ""int"",
            ""description"": ""Maximum length of the returned list and optionally padding length."",
            ""optional"": true
          },
          {
            ""name"": ""pad_to_multiple_of"",
            ""type"": ""int"",
            ""description"": ""If set, will pad the sequence to a multiple of the provided value."",
            ""optional"": true
          },
          {
            ""name"": ""return_tensors"",
            ""type"": ""str"",
            ""description"": ""The type of Tensor to return."",
            ""optional"": true,
            ""default"": ""\""pt\"""",
            ""allowed_values"": [
              ""np"",
              ""pt"",
              ""tf""
            ]
          }
        ]
      }
    ]
  }
]
```","```json
{
  ""DataCollator"": {
    ""description"": ""Data collator that will dynamically pad the inputs received."",
    ""args"": [
      {
        ""name"": ""tokenizer"",
        ""type"": [
          ""PreTrainedTokenizer"",
          ""PreTrainedTokenizerFast""
        ],
        ""description"": ""The tokenizer used for encoding the data.""
      },
      {
        ""name"": ""padding"",
        ""type"": [
          ""bool"",
          ""str"",
          ""utils.PaddingStrategy""
        ],
        ""optional"": true,
        ""default"": true,
        ""description"": ""Select a strategy to pad the returned sequences according to the model's padding side and padding index."",
        ""options"": {
          ""True"": ""Pad to the longest sequence in the batch (or no padding if only a single sequence is provided)."",
          ""longest"": ""Pad to the longest sequence in the batch (or no padding if only a single sequence is provided)."",
          ""max_length"": ""Pad to a maximum length specified with the argument max_length or to the maximum acceptable input length for the model if that argument is not provided."",
          ""False"": ""No padding (i.e., can output a batch with sequences of different lengths)."",
          ""do_not_pad"": ""No padding (i.e., can output a batch with sequences of different lengths).""
        }
      },
      {
        ""name"": ""max_length"",
        ""type"": ""int"",
        ""optional"": true,
        ""description"": ""Maximum length of the returned list and optionally padding length.""
      },
      {
        ""name"": ""pad_to_multiple_of"",
        ""type"": ""int"",
        ""optional"": true,
        ""description"": ""If set will pad the sequence to a multiple of the provided value. This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >= 7.0 (Volta).""
      },
      {
        ""name"": ""return_tensors"",
        ""type"": ""str"",
        ""optional"": true,
        ""default"": ""pt"",
        ""description"": ""The type of Tensor to return."",
        ""allowable_values"": [""np"", ""pt"", ""tf""]
      }
    ]
  }
}
```"
AutoModelForSequenceClassification,This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created with the [`~AutoModelForSequenceClassification.from_pretrained`] class method or the [`~AutoModelForSequenceClassification.from_config`] class method.  This class cannot be instantiated directly using `__init__()` (throws an error).,"```json
{
  ""types"": [
    {
      ""name"": ""AutoModelForSequenceClassification"",
      ""kind"": ""class"",
      ""description"": ""A generic model class for sequence classification. It is instantiated as one of the library's specific model classes (with a sequence classification head)."",
      ""instantiation_info"": {
        ""restriction"": ""Cannot be instantiated directly using __init__() (throws an error)."",
        ""factory_methods"": [
          ""from_pretrained"",
          ""from_config""
        ]
      },
      ""methods"": [
        {
          ""name"": ""from_pretrained"",
          ""kind"": ""class_method"",
          ""description"": ""Creates an instance of the model (or a specific subtype) by loading weights from a pre-trained model identifier or path."",
          ""parent_type"": ""AutoModelForSequenceClassification""
          // Parameters/return type not detailed in the provided text
        },
        {
          ""name"": ""from_config"",
          ""kind"": ""class_method"",
          ""description"": ""Creates an instance of the model (or a specific subtype) from a configuration object."",
          ""parent_type"": ""AutoModelForSequenceClassification""
          // Parameters/return type not detailed in the provided text
        }
        // __init__ method is implied but explicitly restricted for direct use, so not listed as a usable method.
      ]
    }
  ],
  ""functions"": [],
  ""relationships"": [
     // Relationships are primarily implicit in the structure (methods belong to type, instantiation uses factory methods)
  ]
}
```","Based on the provided API documentation, the structured representation of the API in JSON format can be described as follows:

```json
{
  ""classes"": {
    ""AutoModelForSequenceClassification"": {
      ""description"": ""This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head)."",
      ""methods"": {
        ""from_pretrained"": {
          ""type"": ""class method"",
          ""description"": ""Creates an instance of the model from a pretrained model.""
        },
        ""from_config"": {
          ""type"": ""class method"",
          ""description"": ""Creates an instance of the model from a configuration.""
        }
      },
      ""instantiation"": {
        ""direct"": {
          ""allowed"": false,
          ""description"": ""This class cannot be instantiated directly using __init__() (throws an error).""
        }
      }
    }
  }
}
```

This JSON representation outlines the `AutoModelForSequenceClassification` class, its purpose, available methods, and the restriction on direct instantiation."
TrainingArguments,"TrainingArguments is the subset of the arguments we use in our example scripts **which relate to the training loop itself**.  Using [`HfArgumentParser`] we can turn this class into [argparse](https://docs.python.org/3/library/argparse#module-argparse) arguments that can be specified on the command line.  Parameters:     output_dir (`str`, *optional*, defaults to `""trainer_output""`):         The output directory where the model predictions and checkpoints will be written.     overwrite_output_dir (`bool`, *optional*, defaults to `False`):         If `True`, overwrite the content of the output directory. Use this to continue training if `output_dir`         points to a checkpoint directory.     do_train (`bool`, *optional*, defaults to `False`):         Whether to run training or not. This argument is not directly used by [`Trainer`], it's intended to be used         by your training/evaluation scripts instead. See the [example         scripts](https://github.com/huggingface/transformers/tree/main/examples) for more details.     do_eval (`bool`, *optional*):         Whether to run evaluation on the validation set or not. Will be set to `True` if `eval_strategy` is         different from `""no""`. This argument is not directly used by [`Trainer`], it's intended to be used by your         training/evaluation scripts instead. See the [example         scripts](https://github.com/huggingface/transformers/tree/main/examples) for more details.     do_predict (`bool`, *optional*, defaults to `False`):         Whether to run predictions on the test set or not. This argument is not directly used by [`Trainer`], it's         intended to be used by your training/evaluation scripts instead. See the [example         scripts](https://github.com/huggingface/transformers/tree/main/examples) for more details.     eval_strategy (`str` or [`~trainer_utils.IntervalStrategy`], *optional*, defaults to `""no""`):         The evaluation strategy to adopt during training. Possible values are:              - `""no""`: No evaluation is done during training.             - `""steps""`: Evaluation is done (and logged) every `eval_steps`.             - `""epoch""`: Evaluation is done at the end of each epoch.      prediction_loss_only (`bool`, *optional*, defaults to `False`):         When performing evaluation and generating predictions, only returns the loss.     per_device_train_batch_size (`int`, *optional*, defaults to 8):         The batch size per device accelerator core/CPU for training.     per_device_eval_batch_size (`int`, *optional*, defaults to 8):         The batch size per device accelerator core/CPU for evaluation.     gradient_accumulation_steps (`int`, *optional*, defaults to 1):         Number of updates steps to accumulate the gradients for, before performing a backward/update pass.          <Tip warning={true}>          When using gradient accumulation, one step is counted as one step with backward pass. Therefore, logging,         evaluation, save will be conducted every `gradient_accumulation_steps * xxx_step` training examples.          </Tip>      eval_accumulation_steps (`int`, *optional*):         Number of predictions steps to accumulate the output tensors for, before moving the results to the CPU. If         left unset, the whole predictions are accumulated on the device accelerator before being moved to the CPU (faster but         requires more memory).     eval_delay (`float`, *optional*):         Number of epochs or steps to wait for before the first evaluation can be performed, depending on the         eval_strategy.     torch_empty_cache_steps (`int`, *optional*):         Number of steps to wait before calling `torch.<device>.empty_cache()`. If left unset or set to None, cache will not be emptied.          <Tip>          This can help avoid CUDA out-of-memory errors by lowering peak VRAM usage at a cost of about [10% slower performance](https://github.com/huggingface/transformers/issues/31372).          </Tip>      learning_rate (`float`, *optional*, defaults to 5e-5):         The initial learning rate for [`AdamW`] optimizer.     weight_decay (`float`, *optional*, defaults to 0):         The weight decay to apply (if not zero) to all layers except all bias and LayerNorm weights in [`AdamW`]         optimizer.     adam_beta1 (`float`, *optional*, defaults to 0.9):         The beta1 hyperparameter for the [`AdamW`] optimizer.     adam_beta2 (`float`, *optional*, defaults to 0.999):         The beta2 hyperparameter for the [`AdamW`] optimizer.     adam_epsilon (`float`, *optional*, defaults to 1e-8):         The epsilon hyperparameter for the [`AdamW`] optimizer.     max_grad_norm (`float`, *optional*, defaults to 1.0):         Maximum gradient norm (for gradient clipping).     num_train_epochs(`float`, *optional*, defaults to 3.0):         Total number of training epochs to perform (if not an integer, will perform the decimal part percents of         the last epoch before stopping training).     max_steps (`int`, *optional*, defaults to -1):         If set to a positive number, the total number of training steps to perform. Overrides `num_train_epochs`.         For a finite dataset, training is reiterated through the dataset (if all data is exhausted) until         `max_steps` is reached.     lr_scheduler_type (`str` or [`SchedulerType`], *optional*, defaults to `""linear""`):         The scheduler type to use. See the documentation of [`SchedulerType`] for all possible values.     lr_scheduler_kwargs ('dict', *optional*, defaults to {}):         The extra arguments for the lr_scheduler. See the documentation of each scheduler for possible values.     warmup_ratio (`float`, *optional*, defaults to 0.0):         Ratio of total training steps used for a linear warmup from 0 to `learning_rate`.     warmup_steps (`int`, *optional*, defaults to 0):         Number of steps used for a linear warmup from 0 to `learning_rate`. Overrides any effect of `warmup_ratio`.     log_level (`str`, *optional*, defaults to `passive`):         Logger log level to use on the main process. Possible choices are the log levels as strings: 'debug',         'info', 'warning', 'error' and 'critical', plus a 'passive' level which doesn't set anything and keeps the         current log level for the Transformers library (which will be `""warning""` by default).     log_level_replica (`str`, *optional*, defaults to `""warning""`):         Logger log level to use on replicas. Same choices as `log_level`""     log_on_each_node (`bool`, *optional*, defaults to `True`):         In multinode distributed training, whether to log using `log_level` once per node, or only on the main         node.     logging_dir (`str`, *optional*):         [TensorBoard](https://www.tensorflow.org/tensorboard) log directory. Will default to         *output_dir/runs/**CURRENT_DATETIME_HOSTNAME***.     logging_strategy (`str` or [`~trainer_utils.IntervalStrategy`], *optional*, defaults to `""steps""`):         The logging strategy to adopt during training. Possible values are:              - `""no""`: No logging is done during training.             - `""epoch""`: Logging is done at the end of each epoch.             - `""steps""`: Logging is done every `logging_steps`.      logging_first_step (`bool`, *optional*, defaults to `False`):         Whether to log the first `global_step` or not.     logging_steps (`int` or `float`, *optional*, defaults to 500):         Number of update steps between two logs if `logging_strategy=""steps""`. Should be an integer or a float in         range `[0,1)`. If smaller than 1, will be interpreted as ratio of total training steps.     logging_nan_inf_filter (`bool`, *optional*, defaults to `True`):         Whether to filter `nan` and `inf` losses for logging. If set to `True` the loss of every step that is `nan`         or `inf` is filtered and the average loss of the current logging window is taken instead.          <Tip>          `logging_nan_inf_filter` only influences the logging of loss values, it does not change the behavior the         gradient is computed or applied to the model.          </Tip>      save_strategy (`str` or [`~trainer_utils.SaveStrategy`], *optional*, defaults to `""steps""`):         The checkpoint save strategy to adopt during training. Possible values are:              - `""no""`: No save is done during training.             - `""epoch""`: Save is done at the end of each epoch.             - `""steps""`: Save is done every `save_steps`.             - `""best""`: Save is done whenever a new `best_metric` is achieved.              If `""epoch""` or `""steps""` is chosen, saving will also be performed at the             very end of training, always.     save_steps (`int` or `float`, *optional*, defaults to 500):         Number of updates steps before two checkpoint saves if `save_strategy=""steps""`. Should be an integer or a         float in range `[0,1)`. If smaller than 1, will be interpreted as ratio of total training steps.     save_total_limit (`int`, *optional*):         If a value is passed, will limit the total amount of checkpoints. Deletes the older checkpoints in         `output_dir`. When `load_best_model_at_end` is enabled, the ""best"" checkpoint according to         `metric_for_best_model` will always be retained in addition to the most recent ones. For example, for         `save_total_limit=5` and `load_best_model_at_end`, the four last checkpoints will always be retained         alongside the best model. When `save_total_limit=1` and `load_best_model_at_end`, it is possible that two         checkpoints are saved: the last one and the best one (if they are different).     save_safetensors (`bool`, *optional*, defaults to `True`):         Use [safetensors](https://huggingface.co/docs/safetensors) saving and loading for state dicts instead of         default `torch.load` and `torch.save`.     save_on_each_node (`bool`, *optional*, defaults to `False`):         When doing multi-node distributed training, whether to save models and checkpoints on each node, or only on         the main one.          This should not be activated when the different nodes use the same storage as the files will be saved with         the same names for each node.     save_only_model (`bool`, *optional*, defaults to `False`):         When checkpointing, whether to only save the model, or also the optimizer, scheduler & rng state.         Note that when this is true, you won't be able to resume training from checkpoint.         This enables you to save storage by not storing the optimizer, scheduler & rng state.         You can only load the model using `from_pretrained` with this option set to `True`.     restore_callback_states_from_checkpoint (`bool`, *optional*, defaults to `False`):         Whether to restore the callback states from the checkpoint. If `True`, will override         callbacks passed to the `Trainer` if they exist in the checkpoint.""     use_cpu (`bool`, *optional*, defaults to `False`):         Whether or not to use cpu. If set to False, we will use cuda or mps device if available.     seed (`int`, *optional*, defaults to 42):         Random seed that will be set at the beginning of training. To ensure reproducibility across runs, use the         [`~Trainer.model_init`] function to instantiate the model if it has some randomly initialized parameters.     data_seed (`int`, *optional*):         Random seed to be used with data samplers. If not set, random generators for data sampling will use the         same seed as `seed`. This can be used to ensure reproducibility of data sampling, independent of the model         seed.     jit_mode_eval (`bool`, *optional*, defaults to `False`):         Whether or not to use PyTorch jit trace for inference.     use_ipex (`bool`, *optional*, defaults to `False`):         Use Intel extension for PyTorch when it is available. [IPEX         installation](https://github.com/intel/intel-extension-for-pytorch).     bf16 (`bool`, *optional*, defaults to `False`):         Whether to use bf16 16-bit (mixed) precision training instead of 32-bit training. Requires Ampere or higher         NVIDIA architecture or using CPU (use_cpu) or Ascend NPU. This is an experimental API and it may change.     fp16 (`bool`, *optional*, defaults to `False`):         Whether to use fp16 16-bit (mixed) precision training instead of 32-bit training.     fp16_opt_level (`str`, *optional*, defaults to 'O1'):         For `fp16` training, Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3']. See details on         the [Apex documentation](https://nvidia.github.io/apex/amp).     fp16_backend (`str`, *optional*, defaults to `""auto""`):         This argument is deprecated. Use `half_precision_backend` instead.     half_precision_backend (`str`, *optional*, defaults to `""auto""`):         The backend to use for mixed precision training. Must be one of `""auto"", ""apex"", ""cpu_amp""`. `""auto""` will         use CPU/CUDA AMP or APEX depending on the PyTorch version detected, while the other choices will force the         requested backend.     bf16_full_eval (`bool`, *optional*, defaults to `False`):         Whether to use full bfloat16 evaluation instead of 32-bit. This will be faster and save memory but can harm         metric values. This is an experimental API and it may change.     fp16_full_eval (`bool`, *optional*, defaults to `False`):         Whether to use full float16 evaluation instead of 32-bit. This will be faster and save memory but can harm         metric values.     tf32 (`bool`, *optional*):         Whether to enable the TF32 mode, available in Ampere and newer GPU architectures. The default value depends         on PyTorch's version default of `torch.backends.cuda.matmul.allow_tf32`. For more details please refer to         the [TF32](https://huggingface.co/docs/transformers/perf_train_gpu_one#tf32) documentation. This is an         experimental API and it may change.     local_rank (`int`, *optional*, defaults to -1):         Rank of the process during distributed training.     ddp_backend (`str`, *optional*):         The backend to use for distributed training. Must be one of `""nccl""`, `""mpi""`, `""ccl""`, `""gloo""`, `""hccl""`.     tpu_num_cores (`int`, *optional*):         When training on TPU, the number of TPU cores (automatically passed by launcher script).     dataloader_drop_last (`bool`, *optional*, defaults to `False`):         Whether to drop the last incomplete batch (if the length of the dataset is not divisible by the batch size)         or not.     eval_steps (`int` or `float`, *optional*):         Number of update steps between two evaluations if `eval_strategy=""steps""`. Will default to the same         value as `logging_steps` if not set. Should be an integer or a float in range `[0,1)`. If smaller than 1,         will be interpreted as ratio of total training steps.     dataloader_num_workers (`int`, *optional*, defaults to 0):         Number of subprocesses to use for data loading (PyTorch only). 0 means that the data will be loaded in the         main process.     past_index (`int`, *optional*, defaults to -1):         Some models like [TransformerXL](../model_doc/transformerxl) or [XLNet](../model_doc/xlnet) can make use of         the past hidden states for their predictions. If this argument is set to a positive int, the `Trainer` will         use the corresponding output (usually index 2) as the past state and feed it to the model at the next         training step under the keyword argument `mems`.     run_name (`str`, *optional*, defaults to `output_dir`):         A descriptor for the run. Typically used for [wandb](https://www.wandb.com/),         [mlflow](https://www.mlflow.org/), [comet](https://www.comet.com/site) and [swanlab](https://swanlab.cn)         logging. If not specified, will be the same as `output_dir`.     disable_tqdm (`bool`, *optional*):         Whether or not to disable the tqdm progress bars and table of metrics produced by         [`~notebook.NotebookTrainingTracker`] in Jupyter Notebooks. Will default to `True` if the logging level is         set to warn or lower (default), `False` otherwise.     remove_unused_columns (`bool`, *optional*, defaults to `True`):         Whether or not to automatically remove the columns unused by the model forward method.     label_names (`List[str]`, *optional*):         The list of keys in your dictionary of inputs that correspond to the labels.          Will eventually default to the list of argument names accepted by the model that contain the word ""label"",         except if the model used is one of the `XxxForQuestionAnswering` in which case it will also include the         `[""start_positions"", ""end_positions""]` keys.     load_best_model_at_end (`bool`, *optional*, defaults to `False`):         Whether or not to load the best model found during training at the end of training. When this option is         enabled, the best checkpoint will always be saved. See         [`save_total_limit`](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments.save_total_limit)         for more.          <Tip>          When set to `True`, the parameters `save_strategy` needs to be the same as `eval_strategy`, and in         the case it is ""steps"", `save_steps` must be a round multiple of `eval_steps`.          </Tip>      metric_for_best_model (`str`, *optional*):         Use in conjunction with `load_best_model_at_end` to specify the metric to use to compare two different         models. Must be the name of a metric returned by the evaluation with or without the prefix `""eval_""`.          If not specified, this will default to `""loss""` when either `load_best_model_at_end == True`         or `lr_scheduler_type == SchedulerType.REDUCE_ON_PLATEAU` (to use the evaluation loss).          If you set this value, `greater_is_better` will default to `True` unless the name ends with ""loss"".         Don't forget to set it to `False` if your metric is better when lower.     greater_is_better (`bool`, *optional*):         Use in conjunction with `load_best_model_at_end` and `metric_for_best_model` to specify if better models         should have a greater metric or not. Will default to:          - `True` if `metric_for_best_model` is set to a value that doesn't end in `""loss""`.         - `False` if `metric_for_best_model` is not set, or set to a value that ends in `""loss""`.     ignore_data_skip (`bool`, *optional*, defaults to `False`):         When resuming training, whether or not to skip the epochs and batches to get the data loading at the same         stage as in the previous training. If set to `True`, the training will begin faster (as that skipping step         can take a long time) but will not yield the same results as the interrupted training would have.     fsdp (`bool`, `str` or list of [`~trainer_utils.FSDPOption`], *optional*, defaults to `''`):         Use PyTorch Distributed Parallel Training (in distributed training only).          A list of options along the following:          - `""full_shard""`: Shard parameters, gradients and optimizer states.         - `""shard_grad_op""`: Shard optimizer states and gradients.         - `""hybrid_shard""`: Apply `FULL_SHARD` within a node, and replicate parameters across nodes.         - `""hybrid_shard_zero2""`: Apply `SHARD_GRAD_OP` within a node, and replicate parameters across nodes.         - `""offload""`: Offload parameters and gradients to CPUs (only compatible with `""full_shard""` and           `""shard_grad_op""`).         - `""auto_wrap""`: Automatically recursively wrap layers with FSDP using `default_auto_wrap_policy`.     fsdp_config (`str` or `dict`, *optional*):         Config to be used with fsdp (Pytorch Distributed Parallel Training). The value is either a location of         fsdp json config file (e.g., `fsdp_config.json`) or an already loaded json file as `dict`.          A List of config and its options:             - min_num_params (`int`, *optional*, defaults to `0`):                 FSDP's minimum number of parameters for Default Auto Wrapping. (useful only when `fsdp` field is                 passed).             - transformer_layer_cls_to_wrap (`List[str]`, *optional*):                 List of transformer layer class names (case-sensitive) to wrap, e.g, `BertLayer`, `GPTJBlock`,                 `T5Block` .... (useful only when `fsdp` flag is passed).             - backward_prefetch (`str`, *optional*)                 FSDP's backward prefetch mode. Controls when to prefetch next set of parameters (useful only when                 `fsdp` field is passed).                  A list of options along the following:                  - `""backward_pre""` : Prefetches the next set of parameters before the current set of parameter's                   gradient                     computation.                 - `""backward_post""` : This prefetches the next set of parameters after the current set of                   parameters                     gradient computation.             - forward_prefetch (`bool`, *optional*, defaults to `False`)                 FSDP's forward prefetch mode (useful only when `fsdp` field is passed).                  If `""True""`, then FSDP explicitly prefetches the next upcoming all-gather while executing in the                  forward pass.             - limit_all_gathers (`bool`, *optional*, defaults to `False`)                 FSDP's limit_all_gathers (useful only when `fsdp` field is passed).                  If `""True""`, FSDP explicitly synchronizes the CPU thread to prevent too many in-flight                  all-gathers.             - use_orig_params (`bool`, *optional*, defaults to `True`)                 If `""True""`, allows non-uniform `requires_grad` during init, which means support for interspersed                 frozen and trainable parameters. Useful in cases such as parameter-efficient fine-tuning. Please                 refer this                 [blog](https://dev-discuss.pytorch.org/t/rethinking-pytorch-fully-sharded-data-parallel-fsdp-from-first-principles/1019             - sync_module_states (`bool`, *optional*, defaults to `True`)                 If `""True""`, each individually wrapped FSDP unit will broadcast module parameters from rank 0 to                 ensure they are the same across all ranks after initialization             - cpu_ram_efficient_loading (`bool`, *optional*, defaults to `False`)                 If `""True""`, only the first process loads the pretrained model checkpoint while all other processes                 have empty weights.  When this setting as `""True""`, `sync_module_states` also must to be `""True""`,                 otherwise all the processes except the main process would have random weights leading to unexpected                 behaviour during training.             - activation_checkpointing (`bool`, *optional*, defaults to `False`):                 If `""True""`, activation checkpointing is a technique to reduce memory usage by clearing activations of                 certain layers and recomputing them during a backward pass. Effectively, this trades extra                 computation time for reduced memory usage.             - xla (`bool`, *optional*, defaults to `False`):                 Whether to use PyTorch/XLA Fully Sharded Data Parallel Training. This is an experimental feature                 and its API may evolve in the future.             - xla_fsdp_settings (`dict`, *optional*)                 The value is a dictionary which stores the XLA FSDP wrapping parameters.                  For a complete list of options, please see [here](                 https://github.com/pytorch/xla/blob/master/torch_xla/distributed/fsdp/xla_fully_sharded_data_parallel.py).             - xla_fsdp_grad_ckpt (`bool`, *optional*, defaults to `False`):                 Will use gradient checkpointing over each nested XLA FSDP wrapped layer. This setting can only be                 used when the xla flag is set to true, and an auto wrapping policy is specified through                 fsdp_min_num_params or fsdp_transformer_layer_cls_to_wrap.     deepspeed (`str` or `dict`, *optional*):         Use [Deepspeed](https://github.com/deepspeedai/DeepSpeed). This is an experimental feature and its API may         evolve in the future. The value is either the location of DeepSpeed json config file (e.g.,         `ds_config.json`) or an already loaded json file as a `dict`""          <Tip warning={true}>             If enabling any Zero-init, make sure that your model is not initialized until             *after* initializing the `TrainingArguments`, else it will not be applied.         </Tip>      accelerator_config (`str`, `dict`, or `AcceleratorConfig`, *optional*):         Config to be used with the internal `Accelerator` implementation. The value is either a location of         accelerator json config file (e.g., `accelerator_config.json`), an already loaded json file as `dict`,         or an instance of [`~trainer_pt_utils.AcceleratorConfig`].          A list of config and its options:             - split_batches (`bool`, *optional*, defaults to `False`):                 Whether or not the accelerator should split the batches yielded by the dataloaders across the devices. If                 `True` the actual batch size used will be the same on any kind of distributed processes, but it must be a                 round multiple of the `num_processes` you are using. If `False`, actual batch size used will be the one set                 in your script multiplied by the number of processes.             - dispatch_batches (`bool`, *optional*):                 If set to `True`, the dataloader prepared by the Accelerator is only iterated through on the main process                 and then the batches are split and broadcast to each process. Will default to `True` for `DataLoader` whose                 underlying dataset is an `IterableDataset`, `False` otherwise.             - even_batches (`bool`, *optional*, defaults to `True`):                 If set to `True`, in cases where the total batch size across all processes does not exactly divide the                 dataset, samples at the start of the dataset will be duplicated so the batch can be divided equally among                 all workers.             - use_seedable_sampler (`bool`, *optional*, defaults to `True`):                 Whether or not use a fully seedable random sampler ([`accelerate.data_loader.SeedableRandomSampler`]). Ensures                 training results are fully reproducible using a different sampling technique. While seed-to-seed results                 may differ, on average the differences are negligible when using multiple different seeds to compare. Should                 also be ran with [`~utils.set_seed`] for the best results.             - use_configured_state (`bool`, *optional*, defaults to `False`):                 Whether or not to use a pre-configured `AcceleratorState` or `PartialState` defined before calling `TrainingArguments`.                 If `True`, an `Accelerator` or `PartialState` must be initialized. Note that by doing so, this could lead to issues                 with hyperparameter tuning.      label_smoothing_factor (`float`, *optional*, defaults to 0.0):         The label smoothing factor to use. Zero means no label smoothing, otherwise the underlying onehot-encoded         labels are changed from 0s and 1s to `label_smoothing_factor/num_labels` and `1 - label_smoothing_factor +         label_smoothing_factor/num_labels` respectively.     debug (`str` or list of [`~debug_utils.DebugOption`], *optional*, defaults to `""""`):         Enable one or more debug features. This is an experimental feature.          Possible options are:          - `""underflow_overflow""`: detects overflow in model's input/outputs and reports the last frames that led to           the event         - `""tpu_metrics_debug""`: print debug metrics on TPU          The options should be separated by whitespaces.     optim (`str` or [`training_args.OptimizerNames`], *optional*, defaults to `""adamw_torch""`):         The optimizer to use, such as ""adamw_torch"", ""adamw_torch_fused"", ""adamw_apex_fused"", ""adamw_anyprecision"",         ""adafactor"". See `OptimizerNames` in [training_args.py](https://github.com/huggingface/transformers/blob/main/src/transformers/training_args.py)         for a full list of optimizers.     optim_args (`str`, *optional*):         Optional arguments that are supplied to optimizers such as AnyPrecisionAdamW, AdEMAMix, and GaLore.     group_by_length (`bool`, *optional*, defaults to `False`):         Whether or not to group together samples of roughly the same length in the training dataset (to minimize         padding applied and be more efficient). Only useful if applying dynamic padding.     length_column_name (`str`, *optional*, defaults to `""length""`):         Column name for precomputed lengths. If the column exists, grouping by length will use these values rather         than computing them on train startup. Ignored unless `group_by_length` is `True` and the dataset is an         instance of `Dataset`.     report_to (`str` or `List[str]`, *optional*, defaults to `""all""`):         The list of integrations to report the results and logs to. Supported platforms are `""azure_ml""`,         `""clearml""`, `""codecarbon""`, `""comet_ml""`, `""dagshub""`, `""dvclive""`, `""flyte""`, `""mlflow""`, `""neptune""`,         `""swanlab""`, `""tensorboard""`, and `""wandb""`. Use `""all""` to report to all integrations installed, `""none""`         for no integrations.     ddp_find_unused_parameters (`bool`, *optional*):         When using distributed training, the value of the flag `find_unused_parameters` passed to         `DistributedDataParallel`. Will default to `False` if gradient checkpointing is used, `True` otherwise.     ddp_bucket_cap_mb (`int`, *optional*):         When using distributed training, the value of the flag `bucket_cap_mb` passed to `DistributedDataParallel`.     ddp_broadcast_buffers (`bool`, *optional*):         When using distributed training, the value of the flag `broadcast_buffers` passed to         `DistributedDataParallel`. Will default to `False` if gradient checkpointing is used, `True` otherwise.     dataloader_pin_memory (`bool`, *optional*, defaults to `True`):         Whether you want to pin memory in data loaders or not. Will default to `True`.     dataloader_persistent_workers (`bool`, *optional*, defaults to `False`):         If True, the data loader will not shut down the worker processes after a dataset has been consumed once.         This allows to maintain the workers Dataset instances alive. Can potentially speed up training, but will         increase RAM usage. Will default to `False`.     dataloader_prefetch_factor (`int`, *optional*):         Number of batches loaded in advance by each worker.         2 means there will be a total of 2 * num_workers batches prefetched across all workers.     skip_memory_metrics (`bool`, *optional*, defaults to `True`):         Whether to skip adding of memory profiler reports to metrics. This is skipped by default because it slows         down the training and evaluation speed.     push_to_hub (`bool`, *optional*, defaults to `False`):         Whether or not to push the model to the Hub every time the model is saved. If this is activated,         `output_dir` will begin a git directory synced with the repo (determined by `hub_model_id`) and the content         will be pushed each time a save is triggered (depending on your `save_strategy`). Calling         [`~Trainer.save_model`] will also trigger a push.          <Tip warning={true}>          If `output_dir` exists, it needs to be a local clone of the repository to which the [`Trainer`] will be         pushed.          </Tip>      resume_from_checkpoint (`str`, *optional*):         The path to a folder with a valid checkpoint for your model. This argument is not directly used by         [`Trainer`], it's intended to be used by your training/evaluation scripts instead. See the [example         scripts](https://github.com/huggingface/transformers/tree/main/examples) for more details.     hub_model_id (`str`, *optional*):         The name of the repository to keep in sync with the local *output_dir*. It can be a simple model ID in         which case the model will be pushed in your namespace. Otherwise it should be the whole repository name,         for instance `""user_name/model""`, which allows you to push to an organization you are a member of with         `""organization_name/model""`. Will default to `user_name/output_dir_name` with *output_dir_name* being the         name of `output_dir`.          Will default to the name of `output_dir`.     hub_strategy (`str` or [`~trainer_utils.HubStrategy`], *optional*, defaults to `""every_save""`):         Defines the scope of what is pushed to the Hub and when. Possible values are:          - `""end""`: push the model, its configuration, the processing class e.g. tokenizer (if passed along to the [`Trainer`]) and a           draft of a model card when the [`~Trainer.save_model`] method is called.         - `""every_save""`: push the model, its configuration, the processing class e.g. tokenizer (if passed along to the [`Trainer`]) and           a draft of a model card each time there is a model save. The pushes are asynchronous to not block           training, and in case the save are very frequent, a new push is only attempted if the previous one is           finished. A last push is made with the final model at the end of training.         - `""checkpoint""`: like `""every_save""` but the latest checkpoint is also pushed in a subfolder named           last-checkpoint, allowing you to resume training easily with           `trainer.train(resume_from_checkpoint=""last-checkpoint"")`.         - `""all_checkpoints""`: like `""checkpoint""` but all checkpoints are pushed like they appear in the output           folder (so you will get one checkpoint folder per folder in your final repository)      hub_token (`str`, *optional*):         The token to use to push the model to the Hub. Will default to the token in the cache folder obtained with         `huggingface-cli login`.     hub_private_repo (`bool`, *optional*):         Whether to make the repo private. If `None` (default), the repo will be public unless the organization's default is private. This value is ignored if the repo already exists.     hub_always_push (`bool`, *optional*, defaults to `False`):         Unless this is `True`, the `Trainer` will skip pushing a checkpoint when the previous push is not finished.     gradient_checkpointing (`bool`, *optional*, defaults to `False`):         If True, use gradient checkpointing to save memory at the expense of slower backward pass.     gradient_checkpointing_kwargs (`dict`, *optional*, defaults to `None`):         Key word arguments to be passed to the `gradient_checkpointing_enable` method.     include_inputs_for_metrics (`bool`, *optional*, defaults to `False`):         This argument is deprecated. Use `include_for_metrics` instead, e.g, `include_for_metrics = [""inputs""]`.     include_for_metrics (`List[str]`, *optional*, defaults to `[]`):         Include additional data in the `compute_metrics` function if needed for metrics computation.         Possible options to add to `include_for_metrics` list:         - `""inputs""`: Input data passed to the model, intended for calculating input dependent metrics.         - `""loss""`: Loss values computed during evaluation, intended for calculating loss dependent metrics.     eval_do_concat_batches (`bool`, *optional*, defaults to `True`):         Whether to recursively concat inputs/losses/labels/predictions across batches. If `False`,         will instead store them as lists, with each batch kept separate.     auto_find_batch_size (`bool`, *optional*, defaults to `False`)         Whether to find a batch size that will fit into memory automatically through exponential decay, avoiding         CUDA Out-of-Memory errors. Requires accelerate to be installed (`pip install accelerate`)     full_determinism (`bool`, *optional*, defaults to `False`)         If `True`, [`enable_full_determinism`] is called instead of [`set_seed`] to ensure reproducible results in         distributed training. Important: this will negatively impact the performance, so only use it for debugging.     torchdynamo (`str`, *optional*):         If set, the backend compiler for TorchDynamo. Possible choices are `""eager""`, `""aot_eager""`, `""inductor""`,         `""nvfuser""`, `""aot_nvfuser""`, `""aot_cudagraphs""`, `""ofi""`, `""fx2trt""`, `""onnxrt""` and `""ipex""`.     ray_scope (`str`, *optional*, defaults to `""last""`):         The scope to use when doing hyperparameter search with Ray. By default, `""last""` will be used. Ray will         then use the last checkpoint of all trials, compare those, and select the best one. However, other options         are also available. See the [Ray documentation](         https://docs.ray.io/en/latest/tune/api_docs/analysis.html#ray.tune.ExperimentAnalysis.get_best_trial) for         more options.     ddp_timeout (`int`, *optional*, defaults to 1800):         The timeout for `torch.distributed.init_process_group` calls, used to avoid GPU socket timeouts when         performing slow operations in distributed runnings. Please refer the [PyTorch documentation]         (https://pytorch.org/docs/stable/distributed.html#torch.distributed.init_process_group) for more         information.     use_mps_device (`bool`, *optional*, defaults to `False`):         This argument is deprecated.`mps` device will be used if it is available similar to `cuda` device.     torch_compile (`bool`, *optional*, defaults to `False`):         Whether or not to compile the model using PyTorch 2.0         [`torch.compile`](https://pytorch.org/get-started/pytorch-2.0/).          This will use the best defaults for the [`torch.compile`         API](https://pytorch.org/docs/stable/generated/torch.compile.html?highlight=torch+compile#torch.compile).         You can customize the defaults with the argument `torch_compile_backend` and `torch_compile_mode` but we         don't guarantee any of them will work as the support is progressively rolled in in PyTorch.          This flag and the whole compile API is experimental and subject to change in future releases.     torch_compile_backend (`str`, *optional*):         The backend to use in `torch.compile`. If set to any value, `torch_compile` will be set to `True`.          Refer to the PyTorch doc for possible values and note that they may change across PyTorch versions.          This flag is experimental and subject to change in future releases.     torch_compile_mode (`str`, *optional*):         The mode to use in `torch.compile`. If set to any value, `torch_compile` will be set to `True`.          Refer to the PyTorch doc for possible values and note that they may change across PyTorch versions.          This flag is experimental and subject to change in future releases.     include_tokens_per_second (`bool`, *optional*):         Whether or not to compute the number of tokens per second per device for training speed metrics.          This will iterate over the entire training dataloader once beforehand,          and will slow down the entire process.      include_num_input_tokens_seen (`bool`, *optional*):         Whether or not to track the number of input tokens seen throughout training.          May be slower in distributed training as gather operations must be called.      neftune_noise_alpha (`Optional[float]`):         If not `None`, this will activate NEFTune noise embeddings. This can drastically improve model performance         for instruction fine-tuning. Check out the [original paper](https://arxiv.org/abs/2310.05914) and the         [original code](https://github.com/neelsjain/NEFTune). Support transformers `PreTrainedModel` and also         `PeftModel` from peft. The original paper used values in the range [5.0, 15.0].     optim_target_modules (`Union[str, List[str]]`, *optional*):         The target modules to optimize, i.e. the module names that you would like to train.         Currently used for the GaLore algorithm (https://arxiv.org/abs/2403.03507) and APOLLO algorithm (https://arxiv.org/abs/2412.05270).         See GaLore implementation (https://github.com/jiaweizzhao/GaLore) and APOLLO implementation (https://github.com/zhuhanqing/APOLLO) for more details.         You need to make sure to pass a valid GaLore or APOLLO optimizer, e.g., one of: ""apollo_adamw"", ""galore_adamw"", ""galore_adamw_8bit"", ""galore_adafactor"" and make sure that the target modules are `nn.Linear` modules only.      batch_eval_metrics (`Optional[bool]`, defaults to `False`):         If set to `True`, evaluation will call compute_metrics at the end of each batch to accumulate statistics         rather than saving all eval logits in memory. When set to `True`, you must pass a compute_metrics function         that takes a boolean argument `compute_result`, which when passed `True`, will trigger the final global         summary statistics from the batch-level summary statistics you've accumulated over the evaluation set.      eval_on_start (`bool`, *optional*, defaults to `False`):         Whether to perform a evaluation step (sanity check) before the training to ensure the validation steps works correctly.      eval_use_gather_object (`bool`, *optional*, defaults to `False`):         Whether to run recursively gather object in a nested list/tuple/dictionary of objects from all devices. This should only be enabled if users are not just returning tensors, and this is actively discouraged by PyTorch.      use_liger_kernel (`bool`, *optional*, defaults to `False`):         Whether enable [Liger](https://github.com/linkedin/Liger-Kernel) Kernel for LLM model training.         It can effectively increase multi-GPU training throughput by ~20% and reduces memory usage by ~60%, works out of the box with         flash attention, PyTorch FSDP, and Microsoft DeepSpeed. Currently, it supports llama, mistral, mixtral and gemma models.      average_tokens_across_devices (`bool`, *optional*, defaults to `False`):         Whether or not to average tokens across devices. If enabled, will use all_reduce to synchronize         num_tokens_in_batch for precise loss calculation. Reference:         https://github.com/huggingface/transformers/issues/34242","```json
{
  ""class_name"": ""TrainingArguments"",
  ""description"": ""The subset of the arguments we use in our example scripts which relate to the training loop itself. Using [`HfArgumentParser`] we can turn this class into [argparse](https://docs.python.org/3/library/argparse#module-argparse) arguments that can be specified on the command line."",
  ""parameters"": [
    {
      ""name"": ""output_dir"",
      ""type"": ""str"",
      ""optional"": true,
      ""default"": ""\""trainer_output\"""",
      ""description"": ""The output directory where the model predictions and checkpoints will be written.""
    },
    {
      ""name"": ""overwrite_output_dir"",
      ""type"": ""bool"",
      ""optional"": true,
      ""default"": ""False"",
      ""description"": ""If `True`, overwrite the content of the output directory. Use this to continue training if `output_dir` points to a checkpoint directory.""
    },
    {
      ""name"": ""do_train"",
      ""type"": ""bool"",
      ""optional"": true,
      ""default"": ""False"",
      ""description"": ""Whether to run training or not. This argument is not directly used by [`Trainer`], it's intended to be used by your training/evaluation scripts instead.""
    },
    {
      ""name"": ""do_eval"",
      ""type"": ""bool"",
      ""optional"": true,
      ""default"": null,
      ""description"": ""Whether to run evaluation on the validation set or not. Will be set to `True` if `eval_strategy` is different from \""no\"".""
    },
    {
      ""name"": ""do_predict"",
      ""type"": ""bool"",
      ""optional"": true,
      ""default"": ""False"",
      ""description"": ""Whether to run predictions on the test set or not. This argument is not directly used by [`Trainer`], it's intended to be used by your training/evaluation scripts instead.""
    },
    {
      ""name"": ""eval_strategy"",
      ""type"": [""str"", ""~trainer_utils.IntervalStrategy""],
      ""optional"": true,
      ""default"": ""\""no\"""",
      ""description"": ""The evaluation strategy to adopt during training. Possible values are: \""no\"": No evaluation is done during training. \""steps\"": Evaluation is done (and logged) every `eval_steps`. \""epoch\"": Evaluation is done at the end of each epoch.""
    },
    {
      ""name"": ""prediction_loss_only"",
      ""type"": ""bool"",
      ""optional"": true,
      ""default"": ""False"",
      ""description"": ""When performing evaluation and generating predictions, only returns the loss.""
    },
    {
      ""name"": ""per_device_train_batch_size"",
      ""type"": ""int"",
      ""optional"": true,
      ""default"": ""8"",
      ""description"": ""The batch size per device accelerator core/CPU for training.""
    },
    {
      ""name"": ""per_device_eval_batch_size"",
      ""type"": ""int"",
      ""optional"": true,
      ""default"": ""8"",
      ""description"": ""The batch size per device accelerator core/CPU for evaluation.""
    },
    {
      ""name"": ""gradient_accumulation_steps"",
      ""type"": ""int"",
      ""optional"": true,
      ""default"": ""1"",
      ""description"": ""Number of updates steps to accumulate the gradients for, before performing a backward/update pass. When using gradient accumulation, one step is counted as one step with backward pass.""
    },
    {
      ""name"": ""eval_accumulation_steps"",
      ""type"": ""int"",
      ""optional"": true,
      ""default"": null,
      ""description"": ""Number of predictions steps to accumulate the output tensors for, before moving the results to the CPU. If left unset, the whole predictions are accumulated on the device accelerator before being moved to the CPU.""
    },
    {
      ""name"": ""eval_delay"",
      ""type"": ""float"",
      ""optional"": true,
      ""default"": null,
      ""description"": ""Number of epochs or steps to wait for before the first evaluation can be performed, depending on the eval_strategy.""
    },
    {
      ""name"": ""torch_empty_cache_steps"",
      ""type"": ""int"",
      ""optional"": true,
      ""default"": null,
      ""description"": ""Number of steps to wait before calling `torch.<device>.empty_cache()`. If left unset or set to None, cache will not be emptied. Can help avoid CUDA out-of-memory errors by lowering peak VRAM usage at a cost of about 10% slower performance.""
    },
    {
      ""name"": ""learning_rate"",
      ""type"": ""float"",
      ""optional"": true,
      ""default"": ""5e-5"",
      ""description"": ""The initial learning rate for [`AdamW`] optimizer.""
    },
    {
      ""name"": ""weight_decay"",
      ""type"": ""float"",
      ""optional"": true,
      ""default"": ""0"",
      ""description"": ""The weight decay to apply (if not zero) to all layers except all bias and LayerNorm weights in [`AdamW`] optimizer.""
    },
    {
      ""name"": ""adam_beta1"",
      ""type"": ""float"",
      ""optional"": true,
      ""default"": ""0.9"",
      ""description"": ""The beta1 hyperparameter for the [`AdamW`] optimizer.""
    },
    {
      ""name"": ""adam_beta2"",
      ""type"": ""float"",
      ""optional"": true,
      ""default"": ""0.999"",
      ""description"": ""The beta2 hyperparameter for the [`AdamW`] optimizer.""
    },
    {
      ""name"": ""adam_epsilon"",
      ""type"": ""float"",
      ""optional"": true,
      ""default"": ""1e-8"",
      ""description"": ""The epsilon hyperparameter for the [`AdamW`] optimizer.""
    },
    {
      ""name"": ""max_grad_norm"",
      ""type"": ""float"",
      ""optional"": true,
      ""default"": ""1.0"",
      ""description"": ""Maximum gradient norm (for gradient clipping).""
    },
    {
      ""name"": ""num_train_epochs"",
      ""type"": ""float"",
      ""optional"": true,
      ""default"": ""3.0"",
      ""description"": ""Total number of training epochs to perform (if not an integer, will perform the decimal part percents of the last epoch before stopping training).""
    },
    {
      ""name"": ""max_steps"",
      ""type"": ""int"",
      ""optional"": true,
      ""default"": ""-1"",
      ""description"": ""If set to a positive number, the total number of training steps to perform. Overrides `num_train_epochs`. For a finite dataset, training is reiterated through the dataset until `max_steps` is reached.""
    },
    {
      ""name"": ""lr_scheduler_type"",
      ""type"": [""str"", ""SchedulerType""],
      ""optional"": true,
      ""default"": ""\""linear\"""",
      ""description"": ""The scheduler type to use. See the documentation of [`SchedulerType`] for all possible values.""
    },
    {
      ""name"": ""lr_scheduler_kwargs"",
      ""type"": ""dict"",
      ""optional"": true,
      ""default"": ""{}"",
      ""description"": ""The extra arguments for the lr_scheduler. See the documentation of each scheduler for possible values.""
    },
    {
      ""name"": ""warmup_ratio"",
      ""type"": ""float"",
      ""optional"": true,
      ""default"": ""0.0"",
      ""description"": ""Ratio of total training steps used for a linear warmup from 0 to `learning_rate`.""
    },
    {
      ""name"": ""warmup_steps"",
      ""type"": ""int"",
      ""optional"": true,
      ""default"": ""0"",
      ""description"": ""Number of steps used for a linear warmup from 0 to `learning_rate`. Overrides any effect of `warmup_ratio`.""
    },
    {
      ""name"": ""log_level"",
      ""type"": ""str"",
      ""optional"": true,
      ""default"": ""\""passive\"""",
      ""description"": ""Logger log level to use on the main process. Possible choices are the log levels as strings: 'debug', 'info', 'warning', 'error' and 'critical', plus a 'passive' level.""
    },
    {
      ""name"": ""log_level_replica"",
      ""type"": ""str"",
      ""optional"": true,
      ""default"": ""\""warning\"""",
      ""description"": ""Logger log level to use on replicas. Same choices as `log_level`.""
    },
    {
      ""name"": ""log_on_each_node"",
      ""type"": ""bool"",
      ""optional"": true,
      ""default"": ""True"",
      ""description"": ""In multinode distributed training, whether to log using `log_level` once per node, or only on the main node.""
    },
    {
      ""name"": ""logging_dir"",
      ""type"": ""str"",
      ""optional"": true,
      ""default"": null,
      ""description"": ""[TensorBoard](https://www.tensorflow.org/tensorboard) log directory. Will default to *output_dir/runs/**CURRENT_DATETIME_HOSTNAME***.""
    },
    {
      ""name"": ""logging_strategy"",
      ""type"": [""str"", ""~trainer_utils.IntervalStrategy""],
      ""optional"": true,
      ""default"": ""\""steps\"""",
      ""description"": ""The logging strategy to adopt during training. Possible values are: \""no\"", \""epoch\"", \""steps\"".""
    },
    {
      ""name"": ""logging_first_step"",
      ""type"": ""bool"",
      ""optional"": true,
      ""default"": ""False"",
      ""description"": ""Whether to log the first `global_step` or not.""
    },
    {
      ""name"": ""logging_steps"",
      ""type"": [""int"", ""float""],
      ""optional"": true,
      ""default"": ""500"",
      ""description"": ""Number of update steps between two logs if `logging_strategy=\""steps\""`. Should be an integer or a float in range `[0,1)`. If smaller than 1, will be interpreted as ratio of total training steps.""
    },
    {
      ""name"": ""logging_nan_inf_filter"",
      ""type"": ""bool"",
      ""optional"": true,
      ""default"": ""True"",
      ""description"": ""Whether to filter `nan` and `inf` losses for logging. If set to `True` the loss of every step that is `nan` or `inf` is filtered and the average loss of the current logging window is taken instead. This only influences logging, not gradient computation.""
    },
    {
      ""name"": ""save_strategy"",
      ""type"": [""str"", ""~trainer_utils.SaveStrategy""],
      ""optional"": true,
      ""default"": ""\""steps\"""",
      ""description"": ""The checkpoint save strategy to adopt during training. Possible values are: \""no\"", \""epoch\"", \""steps\"", \""best\"". If \""epoch\"" or \""steps\"" is chosen, saving will also be performed at the very end of training.""
    },
    {
      ""name"": ""save_steps"",
      ""type"": [""int"", ""float""],
      ""optional"": true,
      ""default"": ""500"",
      ""description"": ""Number of updates steps before two checkpoint saves if `save_strategy=\""steps\""`. Should be an integer or a float in range `[0,1)`. If smaller than 1, will be interpreted as ratio of total training steps.""
    },
    {
      ""name"": ""save_total_limit"",
      ""type"": ""int"",
      ""optional"": true,
      ""default"": null,
      ""description"": ""If a value is passed, will limit the total amount of checkpoints. Deletes the older checkpoints in `output_dir`. When `load_best_model_at_end` is enabled, the \""best\"" checkpoint according to `metric_for_best_model` will always be retained.""
    },
    {
      ""name"": ""save_safetensors"",
      ""type"": ""bool"",
      ""optional"": true,
      ""default"": ""True"",
      ""description"": ""Use [safetensors](https://huggingface.co/docs/safetensors) saving and loading for state dicts instead of default `torch.load` and `torch.save`.""
    },
    {
      ""name"": ""save_on_each_node"",
      ""type"": ""bool"",
      ""optional"": true,
      ""default"": ""False"",
      ""description"": ""When doing multi-node distributed training, whether to save models and checkpoints on each node, or only on the main one.""
    },
    {
      ""name"": ""save_only_model"",
      ""type"": ""bool"",
      ""optional"": true,
      ""default"": ""False"",
      ""description"": ""When checkpointing, whether to only save the model, or also the optimizer, scheduler & rng state. If true, won't be able to resume training from checkpoint.""
    },
    {
      ""name"": ""restore_callback_states_from_checkpoint"",
      ""type"": ""bool"",
      ""optional"": true,
      ""default"": ""False"",
      ""description"": ""Whether to restore the callback states from the checkpoint. If `True`, will override callbacks passed to the `Trainer` if they exist in the checkpoint.""
    },
    {
      ""name"": ""use_cpu"",
      ""type"": ""bool"",
      ""optional"": true,
      ""default"": ""False"",
      ""description"": ""Whether or not to use cpu. If set to False, we will use cuda or mps device if available.""
    },
    {
      ""name"": ""seed"",
      ""type"": ""int"",
      ""optional"": true,
      ""default"": ""42"",
      ""description"": ""Random seed that will be set at the beginning of training.""
    },
    {
      ""name"": ""data_seed"",
      ""type"": ""int"",
      ""optional"": true,
      ""default"": null,
      ""description"": ""Random seed to be used with data samplers. If not set, random generators for data sampling will use the same seed as `seed`.""
    },
    {
      ""name"": ""jit_mode_eval"",
      ""type"": ""bool"",
      ""optional"": true,
      ""default"": ""False"",
      ""description"": ""Whether or not to use PyTorch jit trace for inference.""
    },
    {
      ""name"": ""use_ipex"",
      ""type"": ""bool"",
      ""optional"": true,
      ""default"": ""False"",
      ""description"": ""Use Intel extension for PyTorch when it is available.""
    },
    {
      ""name"": ""bf16"",
      ""type"": ""bool"",
      ""optional"": true,
      ""default"": ""False"",
      ""description"": ""Whether to use bf16 16-bit (mixed) precision training instead of 32-bit training. Requires Ampere or higher NVIDIA architecture or using CPU (use_cpu) or Ascend NPU. Experimental API.""
    },
    {
      ""name"": ""fp16"",
      ""type"": ""bool"",
      ""optional"": true,
      ""default"": ""False"",
      ""description"": ""Whether to use fp16 16-bit (mixed) precision training instead of 32-bit training.""
    },
    {
      ""name"": ""fp16_opt_level"",
      ""type"": ""str"",
      ""optional"": true,
      ""default"": ""'O1'"",
      ""description"": ""For `fp16` training, Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3']. See details on the [Apex documentation](https://nvidia.github.io/apex/amp).""
    },
    {
      ""name"": ""fp16_backend"",
      ""type"": ""str"",
      ""optional"": true,
      ""default"": ""\""auto\"""",
      ""description"": ""This argument is deprecated. Use `half_precision_backend` instead.""
    },
    {
      ""name"": ""half_precision_backend"",
      ""type"": ""str"",
      ""optional"": true,
      ""default"": ""\""auto\"""",
      ""description"": ""The backend to use for mixed precision training. Must be one of \""auto\"", \""apex\"", \""cpu_amp\"". \""auto\"" will use CPU/CUDA AMP or APEX depending on PyTorch version.""
    },
    {
      ""name"": ""bf16_full_eval"",
      ""type"": ""bool"",
      ""optional"": true,
      ""default"": ""False"",
      ""description"": ""Whether to use full bfloat16 evaluation instead of 32-bit. Faster and saves memory but can harm metric values. Experimental API.""
    },
    {
      ""name"": ""fp16_full_eval"",
      ""type"": ""bool"",
      ""optional"": true,
      ""default"": ""False"",
      ""description"": ""Whether to use full float16 evaluation instead of 32-bit. Faster and saves memory but can harm metric values.""
    },
    {
      ""name"": ""tf32"",
      ""type"": ""bool"",
      ""optional"": true,
      ""default"": null,
      ""description"": ""Whether to enable the TF32 mode, available in Ampere and newer GPU architectures. Default depends on PyTorch's version default. Experimental API.""
    },
    {
      ""name"": ""local_rank"",
      ""type"": ""int"",
      ""optional"": true,
      ""default"": ""-1"",
      ""description"": ""Rank of the process during distributed training.""
    },
    {
      ""name"": ""ddp_backend"",
      ""type"": ""str"",
      ""optional"": true,
      ""default"": null,
      ""description"": ""The backend to use for distributed training. Must be one of \""nccl\"", \""mpi\"", \""ccl\"", \""gloo\"", \""hccl\"".""
    },
    {
      ""name"": ""tpu_num_cores"",
      ""type"": ""int"",
      ""optional"": true,
      ""default"": null,
      ""description"": ""When training on TPU, the number of TPU cores (automatically passed by launcher script).""
    },
    {
      ""name"": ""dataloader_drop_last"",
      ""type"": ""bool"",
      ""optional"": true,
      ""default"": ""False"",
      ""description"": ""Whether to drop the last incomplete batch (if the length of the dataset is not divisible by the batch size) or not.""
    },
    {
      ""name"": ""eval_steps"",
      ""type"": [""int"", ""float""],
      ""optional"": true,
      ""default"": null,
      ""description"": ""Number of update steps between two evaluations if `eval_strategy=\""steps\""`. Will default to the same value as `logging_steps` if not set. Should be an integer or a float in range `[0,1)`. If smaller than 1, interpreted as ratio of total training steps.""
    },
    {
      ""name"": ""dataloader_num_workers"",
      ""type"": ""int"",
      ""optional"": true,
      ""default"": ""0"",
      ""description"": ""Number of subprocesses to use for data loading (PyTorch only). 0 means that the data will be loaded in the main process.""
    },
    {
      ""name"": ""past_index"",
      ""type"": ""int"",
      ""optional"": true,
      ""default"": ""-1"",
      ""description"": ""Some models like [TransformerXL](../model_doc/transformerxl) or [XLNet](../model_doc/xlnet) can make use of the past hidden states for their predictions. If this argument is set to a positive int, the `Trainer` will use the corresponding output as the past state and feed it to the model at the next training step under the keyword argument `mems`.""
    },
    {
      ""name"": ""run_name"",
      ""type"": ""str"",
      ""optional"": true,
      ""default"": ""`output_dir`"",
      ""description"": ""A descriptor for the run. Typically used for logging platforms (wandb, mlflow, comet, swanlab). If not specified, will be the same as `output_dir`.""
    },
    {
      ""name"": ""disable_tqdm"",
      ""type"": ""bool"",
      ""optional"": true,
      ""default"": null,
      ""description"": ""Whether or not to disable the tqdm progress bars and table of metrics produced by [`~notebook.NotebookTrainingTracker`] in Jupyter Notebooks. Will default to `True` if the logging level is set to warn or lower, `False` otherwise.""
    },
    {
      ""name"": ""remove_unused_columns"",
      ""type"": ""bool"",
      ""optional"": true,
      ""default"": ""True"",
      ""description"": ""Whether or not to automatically remove the columns unused by the model forward method.""
    },
    {
      ""name"": ""label_names"",
      ""type"": ""List[str]"",
      ""optional"": true,
      ""default"": null,
      ""description"": ""The list of keys in your dictionary of inputs that correspond to the labels. Will eventually default to argument names containing \""label\"", plus specific keys for QuestionAnswering models.""
    },
    {
      ""name"": ""load_best_model_at_end"",
      ""type"": ""bool"",
      ""optional"": true,
      ""default"": ""False"",
      ""description"": ""Whether or not to load the best model found during training at the end of training. When this option is enabled, the best checkpoint will always be saved. Requires `save_strategy` to be the same as `eval_strategy`, and if \""steps\"", `save_steps` must be a round multiple of `eval_steps`.""
    },
    {
      ""name"": ""metric_for_best_model"",
      ""type"": ""str"",
      ""optional"": true,
      ""default"": null,
      ""description"": ""Use in conjunction with `load_best_model_at_end` to specify the metric to use to compare two different models. Must be the name of a metric returned by the evaluation with or without the prefix \""eval_\"". Defaults to \""loss\"" when applicable.""
    },
    {
      ""name"": ""greater_is_better"",
      ""type"": ""bool"",
      ""optional"": true,
      ""default"": null,
      ""description"": ""Use in conjunction with `load_best_model_at_end` and `metric_for_best_model` to specify if better models should have a greater metric or not. Defaults to `True` if `metric_for_best_model` does not end in \""loss\"", `False` otherwise.""
    },
    {
      ""name"": ""ignore_data_skip"",
      ""type"": ""bool"",
      ""optional"": true,
      ""default"": ""False"",
      ""description"": ""When resuming training, whether or not to skip the epochs and batches to get the data loading at the same stage as in the previous training. If `True`, training begins faster but results may differ slightly.""
    },
    {
      ""name"": ""fsdp"",
      ""type"": [""bool"", ""str"", ""list of ~trainer_utils.FSDPOption""],
      ""optional"": true,
      ""default"": ""''"",
      ""description"": ""Use PyTorch Distributed Parallel Training (in distributed training only). Options include: \""full_shard\"", \""shard_grad_op\"", \""hybrid_shard\"", \""hybrid_shard_zero2\"", \""offload\"", \""auto_wrap\"".""
    },
    {
      ""name"": ""fsdp_config"",
      ""type"": [""str"", ""dict""],
      ""optional"": true,
      ""default"": null,
      ""description"": ""Config to be used with fsdp. Value is file path (str) or dict. Dict options include: min_num_params (int), transformer_layer_cls_to_wrap (List[str]), backward_prefetch (str), forward_prefetch (bool), limit_all_gathers (bool), use_orig_params (bool), sync_module_states (bool), cpu_ram_efficient_loading (bool), activation_checkpointing (bool), xla (bool), xla_fsdp_settings (dict), xla_fsdp_grad_ckpt (bool).""
    },
    {
      ""name"": ""deepspeed"",
      ""type"": [""str"", ""dict""],
      ""optional"": true,
      ""default"": null,
      ""description"": ""Use [Deepspeed](https://github.com/deepspeedai/DeepSpeed). Experimental. Value is file path (str) or dict. If enabling Zero-init, model should not be initialized until *after* initializing `TrainingArguments`.""
    },
    {
      ""name"": ""accelerator_config"",
      ""type"": [""str"", ""dict"", ""AcceleratorConfig""],
      ""optional"": true,
      ""default"": null,
      ""description"": ""Config to be used with the internal `Accelerator` implementation. Value is file path (str), dict, or instance of `~trainer_pt_utils.AcceleratorConfig`. Dict options include: split_batches (bool), dispatch_batches (bool), even_batches (bool), use_seedable_sampler (bool), use_configured_state (bool).""
    },
    {
      ""name"": ""label_smoothing_factor"",
      ""type"": ""float"",
      ""optional"": true,
      ""default"": ""0.0"",
      ""description"": ""The label smoothing factor to use. Zero means no label smoothing.""
    },
    {
      ""name"": ""debug"",
      ""type"": [""str"", ""list of ~debug_utils.DebugOption""],
      ""optional"": true,
      ""default"": ""\""\"""",
      ""description"": ""Enable one or more debug features. Experimental. Options: \""underflow_overflow\"", \""tpu_metrics_debug\"".""
    },
    {
      ""name"": ""optim"",
      ""type"": [""str"", ""training_args.OptimizerNames""],
      ""optional"": true,
      ""default"": ""\""adamw_torch\"""",
      ""description"": ""The optimizer to use, such as \""adamw_torch\"", \""adamw_torch_fused\"", etc. See `OptimizerNames` for full list.""
    },
    {
      ""name"": ""optim_args"",
      ""type"": ""str"",
      ""optional"": true,
      ""default"": null,
      ""description"": ""Optional arguments that are supplied to optimizers such as AnyPrecisionAdamW, AdEMAMix, and GaLore.""
    },
    {
      ""name"": ""group_by_length"",
      ""type"": ""bool"",
      ""optional"": true,
      ""default"": ""False"",
      ""description"": ""Whether or not to group together samples of roughly the same length in the training dataset (to minimize padding). Only useful if applying dynamic padding.""
    },
    {
      ""name"": ""length_column_name"",
      ""type"": ""str"",
      ""optional"": true,
      ""default"": ""\""length\"""",
      ""description"": ""Column name for precomputed lengths. If the column exists, grouping by length will use these values. Ignored unless `group_by_length` is `True` and dataset is a `Dataset` instance.""
    },
    {
      ""name"": ""report_to"",
      ""type"": [""str"", ""List[str]""],
      ""optional"": true,
      ""default"": ""\""all\"""",
      ""description"": ""The list of integrations to report the results and logs to. Supported platforms: \""azure_ml\"", \""clearml\"", \""codecarbon\"", \""comet_ml\"", \""dagshub\"", \""dvclive\"", \""flyte\"", \""mlflow\"", \""neptune\"", \""swanlab\"", \""tensorboard\"", and \""wandb\"". Use \""all\"" or \""none\"".""
    },
    {
      ""name"": ""ddp_find_unused_parameters"",
      ""type"": ""bool"",
      ""optional"": true,
      ""default"": null,
      ""description"": ""When using distributed training, the value of the flag `find_unused_parameters` passed to `DistributedDataParallel`. Defaults to `False` if gradient checkpointing is used, `True` otherwise.""
    },
    {
      ""name"": ""ddp_bucket_cap_mb"",
      ""type"": ""int"",
      ""optional"": true,
      ""default"": null,
      ""description"": ""When using distributed training, the value of the flag `bucket_cap_mb` passed to `DistributedDataParallel`.""
    },
    {
      ""name"": ""ddp_broadcast_buffers"",
      ""type"": ""bool"",
      ""optional"": true,
      ""default"": null,
      ""description"": ""When using distributed training, the value of the flag `broadcast_buffers` passed to `DistributedDataParallel`. Defaults to `False` if gradient checkpointing is used, `True` otherwise.""
    },
    {
      ""name"": ""dataloader_pin_memory"",
      ""type"": ""bool"",
      ""optional"": true,
      ""default"": ""True"",
      ""description"": ""Whether you want to pin memory in data loaders or not.""
    },
    {
      ""name"": ""dataloader_persistent_workers"",
      ""type"": ""bool"",
      ""optional"": true,
      ""default"": ""False"",
      ""description"": ""If True, the data loader will not shut down the worker processes after a dataset has been consumed once. Can potentially speed up training, but will increase RAM usage.""
    },
    {
      ""name"": ""dataloader_prefetch_factor"",
      ""type"": ""int"",
      ""optional"": true,
      ""default"": null,
      ""description"": ""Number of batches loaded in advance by each worker. 2 means there will be a total of 2 * num_workers batches prefetched across all workers.""
    },
    {
      ""name"": ""skip_memory_metrics"",
      ""type"": ""bool"",
      ""optional"": true,
      ""default"": ""True"",
      ""description"": ""Whether to skip adding of memory profiler reports to metrics. Skipped by default as it slows down training/evaluation.""
    },
    {
      ""name"": ""push_to_hub"",
      ""type"": ""bool"",
      ""optional"": true,
      ""default"": ""False"",
      ""description"": ""Whether or not to push the model to the Hub every time the model is saved. If activated, `output_dir` will be a git directory synced with `hub_model_id`.""
    },
    {
      ""name"": ""resume_from_checkpoint"",
      ""type"": ""str"",
      ""optional"": true,
      ""default"": null,
      ""description"": ""The path to a folder with a valid checkpoint for your model. Not directly used by [`Trainer`], intended for training/evaluation scripts.""
    },
    {
      ""name"": ""hub_model_id"",
      ""type"": ""str"",
      ""optional"": true,
      ""default"": ""`user_name/output_dir_name`"",
      ""description"": ""The name of the repository to keep in sync with the local `output_dir`. Can be a simple model ID or full repo name like \""user_name/model\"".""
    },
    {
      ""name"": ""hub_strategy"",
      ""type"": [""str"", ""~trainer_utils.HubStrategy""],
      ""optional"": true,
      ""default"": ""\""every_save\"""",
      ""description"": ""Defines the scope of what is pushed to the Hub and when. Possible values: \""end\"", \""every_save\"", \""checkpoint\"", \""all_checkpoints\"".""
    },
    {
      ""name"": ""hub_token"",
      ""type"": ""str"",
      ""optional"": true,
      ""default"": null,
      ""description"": ""The token to use to push the model to the Hub. Defaults to token in cache folder from `huggingface-cli login`.""
    },
    {
      ""name"": ""hub_private_repo"",
      ""type"": ""bool"",
      ""optional"": true,
      ""default"": null,
      ""description"": ""Whether to make the repo private. If `None` (default), repo is public unless organization's default is private. Ignored if repo exists.""
    },
    {
      ""name"": ""hub_always_push"",
      ""type"": ""bool"",
      ""optional"": true,
      ""default"": ""False"",
      ""description"": ""Unless `True`, the `Trainer` will skip pushing a checkpoint when the previous push is not finished.""
    },
    {
      ""name"": ""gradient_checkpointing"",
      ""type"": ""bool"",
      ""optional"": true,
      ""default"": ""False"",
      ""description"": ""If True, use gradient checkpointing to save memory at the expense of slower backward pass.""
    },
    {
      ""name"": ""gradient_checkpointing_kwargs"",
      ""type"": ""dict"",
      ""optional"": true,
      ""default"": ""null"",
      ""description"": ""Key word arguments to be passed to the `gradient_checkpointing_enable` method.""
    },
    {
      ""name"": ""include_inputs_for_metrics"",
      ""type"": ""bool"",
      ""optional"": true,
      ""default"": ""False"",
      ""description"": ""This argument is deprecated. Use `include_for_metrics` instead, e.g, `include_for_metrics = [\""inputs\""]`.""
    },
    {
      ""name"": ""include_for_metrics"",
      ""type"": ""List[str]"",
      ""optional"": true,
      ""default"": ""[]"",
      ""description"": ""Include additional data in the `compute_metrics` function if needed for metrics computation. Possible options: \""inputs\"", \""loss\"".""
    },
    {
      ""name"": ""eval_do_concat_batches"",
      ""type"": ""bool"",
      ""optional"": true,
      ""default"": ""True"",
      ""description"": ""Whether to recursively concat inputs/losses/labels/predictions across batches. If `False`, will store them as lists with each batch separate.""
    },
    {
      ""name"": ""auto_find_batch_size"",
      ""type"": ""bool"",
      ""optional"": true,
      ""default"": ""False"",
      ""description"": ""Whether to find a batch size that will fit into memory automatically through exponential decay. Requires accelerate.""
    },
    {
      ""name"": ""full_determinism"",
      ""type"": ""bool"",
      ""optional"": true,
      ""default"": ""False"",
      ""description"": ""If `True`, [`enable_full_determinism`] is called instead of [`set_seed`] to ensure reproducible results in distributed training. Negatively impacts performance.""
    },
    {
      ""name"": ""torchdynamo"",
      ""type"": ""str"",
      ""optional"": true,
      ""default"": null,
      ""description"": ""If set, the backend compiler for TorchDynamo. Possible choices: \""eager\"", \""aot_eager\"", \""inductor\"", \""nvfuser\"", \""aot_nvfuser\"", \""aot_cudagraphs\"", \""ofi\"", \""fx2trt\"", \""onnxrt\"" and \""ipex\"".""
    },
    {
      ""name"": ""ray_scope"",
      ""type"": ""str"",
      ""optional"": true,
      ""default"": ""\""last\"""",
      ""description"": ""The scope to use when doing hyperparameter search with Ray. Default \""last\"" uses the last checkpoint of all trials. See [Ray documentation](https://docs.ray.io/en/latest/tune/api_docs/analysis.html#ray.tune.ExperimentAnalysis.get_best_trial) for more options.""
    },
    {
      ""name"": ""ddp_timeout"",
      ""type"": ""int"",
      ""optional"": true,
      ""default"": ""1800"",
      ""description"": ""The timeout for `torch.distributed.init_process_group` calls, used to avoid GPU socket timeouts during distributed runnings. Refer [PyTorch documentation](https://pytorch.org/docs/stable/distributed.html#torch.distributed.init_process_group).""
    },
    {
      ""name"": ""use_mps_device"",
      ""type"": ""bool"",
      ""optional"": true,
      ""default"": ""False"",
      ""description"": ""This argument is deprecated. `mps` device will be used if available similar to `cuda`.""
    },
    {
      ""name"": ""torch_compile"",
      ""type"": ""bool"",
      ""optional"": true,
      ""default"": ""False"",
      ""description"": ""Whether or not to compile the model using PyTorch 2.0 [`torch.compile`](https://pytorch.org/get-started/pytorch-2.0/). Uses best defaults. Can customize with `torch_compile_backend` and `torch_compile_mode`. Experimental.""
    },
    {
      ""name"": ""torch_compile_backend"",
      ""type"": ""str"",
      ""optional"": true,
      ""default"": null,
      ""description"": ""The backend to use in `torch.compile`. If set, `torch_compile` will be set to `True`. Refer to PyTorch doc for possible values. Experimental.""
    },
    {
      ""name"": ""torch_compile_mode"",
      ""type"": ""str"",
      ""optional"": true,
      ""default"": null,
      ""description"": ""The mode to use in `torch.compile`. If set, `torch_compile` will be set to `True`. Refer to PyTorch doc for possible values. Experimental.""
    },
    {
      ""name"": ""include_tokens_per_second"",
      ""type"": ""bool"",
      ""optional"": true,
      ""default"": null,
      ""description"": ""Whether or not to compute the number of tokens per second per device for training speed metrics. Iterates over the entire training dataloader once beforehand, slowing down the process.""
    },
    {
      ""name"": ""include_num_input_tokens_seen"",
      ""type"": ""bool"",
      ""optional"": true,
      ""default"": null,
      ""description"": ""Whether or not to track the number of input tokens seen throughout training. May be slower in distributed training.""
    },
    {
      ""name"": ""neftune_noise_alpha"",
      ""type"": ""Optional[float]"",
      ""optional"": true,
      ""default"": null,
      ""description"": ""If not `None`, activates NEFTune noise embeddings. Can improve performance for instruction fine-tuning. Supports `PreTrainedModel` and `PeftModel`. Original paper used values [5.0, 15.0].""
    },
    {
      ""name"": ""optim_target_modules"",
      ""type"": [""Union[str"", ""List[str]]""],
      ""optional"": true,
      ""default"": null,
      ""description"": ""The target modules to optimize, i.e. the module names that you would like to train. Currently used for GaLore and APOLLO algorithms. Requires passing a valid GaLore or APOLLO optimizer.""
    },
    {
      ""name"": ""batch_eval_metrics"",
      ""type"": ""Optional[bool]"",
      ""optional"": true,
      ""default"": ""False"",
      ""description"": ""If set to `True`, evaluation will call compute_metrics at the end of each batch to accumulate statistics rather than saving all eval logits in memory. Requires compute_metrics function to take a boolean argument `compute_result`.""
    },
    {
      ""name"": ""eval_on_start"",
      ""type"": ""bool"",
      ""optional"": true,
      ""default"": ""False"",
      ""description"": ""Whether to perform an evaluation step before training to ensure validation steps work correctly.""
    },
    {
      ""name"": ""eval_use_gather_object"",
      ""type"": ""bool"",
      ""optional"": true,
      ""default"": ""False"",
      ""description"": ""Whether to run recursively gather object in a nested list/tuple/dictionary of objects from all devices. Should only be enabled if users are not just returning tensors, which is discouraged by PyTorch.""
    },
    {
      ""name"": ""use_liger_kernel"",
      ""type"": ""bool"",
      ""optional"": true,
      ""default"": ""False"",
      ""description"": ""Whether enable [Liger](https://github.com/linkedin/Liger-Kernel) Kernel for LLM model training. Can increase multi-GPU training throughput and reduce memory usage. Works with flash attention, PyTorch FSDP, and DeepSpeed. Supports llama, mistral, mixtral and gemma models.""
    },
    {
      ""name"": ""average_tokens_across_devices"",
      ""type"": ""bool"",
      ""optional"": true,
      ""default"": ""False"",
      ""description"": ""Whether or not to average tokens across devices. If enabled, will use all_reduce to synchronize num_tokens_in_batch for precise loss calculation.""
    }
  ]
}
```","Below is a structured JSON representation of the `TrainingArguments` API. It captures the class name, description, parameters (with type and optional default values), and relationships to other classes and utilities mentioned in the documentation.

```json
{
  ""className"": ""TrainingArguments"",
  ""description"": ""TrainingArguments is the subset of the arguments used in example scripts related to the training loop itself."",
  ""convertWith"": ""HfArgumentParser"",
  ""parameters"": [
    {
      ""name"": ""output_dir"",
      ""type"": ""str"",
      ""optional"": true,
      ""default"": ""trainer_output""
    },
    {
      ""name"": ""overwrite_output_dir"",
      ""type"": ""bool"",
      ""optional"": true,
      ""default"": false
    },
    {
      ""name"": ""do_train"",
      ""type"": ""bool"",
      ""optional"": true,
      ""default"": false
    },
    {
      ""name"": ""do_eval"",
      ""type"": ""bool"",
      ""optional"": true
    },
    {
      ""name"": ""do_predict"",
      ""type"": ""bool"",
      ""optional"": true,
      ""default"": false
    },
    {
      ""name"": ""eval_strategy"",
      ""type"": ""str or ~trainer_utils.IntervalStrategy"",
      ""optional"": true,
      ""default"": ""no""
    },
    {
      ""name"": ""prediction_loss_only"",
      ""type"": ""bool"",
      ""optional"": true,
      ""default"": false
    },
    {
      ""name"": ""per_device_train_batch_size"",
      ""type"": ""int"",
      ""optional"": true,
      ""default"": 8
    },
    {
      ""name"": ""per_device_eval_batch_size"",
      ""type"": ""int"",
      ""optional"": true,
      ""default"": 8
    },
    {
      ""name"": ""gradient_accumulation_steps"",
      ""type"": ""int"",
      ""optional"": true,
      ""default"": 1
    },
    {
      ""name"": ""eval_accumulation_steps"",
      ""type"": ""int"",
      ""optional"": true
    },
    {
      ""name"": ""eval_delay"",
      ""type"": ""float"",
      ""optional"": true
    },
    {
      ""name"": ""torch_empty_cache_steps"",
      ""type"": ""int"",
      ""optional"": true
    },
    {
      ""name"": ""learning_rate"",
      ""type"": ""float"",
      ""optional"": true,
      ""default"": 5e-5
    },
    {
      ""name"": ""weight_decay"",
      ""type"": ""float"",
      ""optional"": true,
      ""default"": 0
    },
    {
      ""name"": ""adam_beta1"",
      ""type"": ""float"",
      ""optional"": true,
      ""default"": 0.9
    },
    {
      ""name"": ""adam_beta2"",
      ""type"": ""float"",
      ""optional"": true,
      ""default"": 0.999
    },
    {
      ""name"": ""adam_epsilon"",
      ""type"": ""float"",
      ""optional"": true,
      ""default"": 1e-8
    },
    {
      ""name"": ""max_grad_norm"",
      ""type"": ""float"",
      ""optional"": true,
      ""default"": 1.0
    },
    {
      ""name"": ""num_train_epochs"",
      ""type"": ""float"",
      ""optional"": true,
      ""default"": 3.0
    },
    {
      ""name"": ""max_steps"",
      ""type"": ""int"",
      ""optional"": true,
      ""default"": -1
    },
    {
      ""name"": ""log_level"",
      ""type"": ""str"",
      ""optional"": true,
      ""default"": ""passive""
    },
    {
      ""name"": ""log_level_replica"",
      ""type"": ""str"",
      ""optional"": true,
      ""default"": ""warning""
    },
    {
      ""name"": ""logging_dir"",
      ""type"": ""str"",
      ""optional"": true
    },
    {
      ""name"": ""logging_strategy"",
      ""type"": ""str or ~trainer_utils.IntervalStrategy"",
      ""optional"": true,
      ""default"": ""steps""
    },
    {
      ""name"": ""logging_first_step"",
      ""type"": ""bool"",
      ""optional"": true,
      ""default"": false
    },
    {
      ""name"": ""logging_steps"",
      ""type"": ""int or float"",
      ""optional"": true,
      ""default"": 500
    },
    {
      ""name"": ""save_strategy"",
      ""type"": ""str or ~trainer_utils.SaveStrategy"",
      ""optional"": true,
      ""default"": ""steps""
    },
    {
      ""name"": ""save_steps"",
      ""type"": ""int or float"",
      ""optional"": true,
      ""default"": 500
    },
    {
      ""name"": ""save_total_limit"",
      ""type"": ""int"",
      ""optional"": true
    },
    {
      ""name"": ""save_safetensors"",
      ""type"": ""bool"",
      ""optional"": true,
      ""default"": true
    },
    {
      ""name"": ""use_cpu"",
      ""type"": ""bool"",
      ""optional"": true,
      ""default"": false
    },
    {
      ""name"": ""seed"",
      ""type"": ""int"",
      ""optional"": true,
      ""default"": 42
    },
    {
      ""name"": ""jit_mode_eval"",
      ""type"": ""bool"",
      ""optional"": true,
      ""default"": false
    },
    {
      ""name"": ""use_ipex"",
      ""type"": ""bool"",
      ""optional"": true,
      ""default"": false
    },
    {
      ""name"": ""fp16"",
      ""type"": ""bool"",
      ""optional"": true,
      ""default"": false
    },
    {
      ""name"": ""fp16_opt_level"",
      ""type"": ""str"",
      ""optional"": true,
      ""default"": ""O1""
    },
    {
      ""name"": ""lr_scheduler_type"",
      ""type"": ""str or SchedulerType"",
      ""optional"": true,
      ""default"": ""linear""
    },
    {
      ""name"": ""ddp_backend"",
      ""type"": ""str"",
      ""optional"": true
    },
    {
      ""name"": ""tpu_num_cores"",
      ""type"": ""int"",
      ""optional"": true
    },
    {
      ""name"": ""run_name"",
      ""type"": ""str"",
      ""optional"": true,
      ""default"": ""output_dir""
    },
    {
      ""name"": ""disable_tqdm"",
      ""type"": ""bool"",
      ""optional"": true
    },
    {
      ""name"": ""remove_unused_columns"",
      ""type"": ""bool"",
      ""optional"": true,
      ""default"": true
    },
    {
      ""name"": ""label_names"",
      ""type"": ""List[str]"",
      ""optional"": true
    },
    {
      ""name"": ""load_best_model_at_end"",
      ""type"": ""bool"",
      ""optional"": true,
      ""default"": false
    },
    {
      ""name"": ""metric_for_best_model"",
      ""type"": ""str"",
      ""optional"": true
    },
    {
      ""name"": ""greater_is_better"",
      ""type"": ""bool"",
      ""optional"": true
    },
    {
      ""name"": ""push_to_hub"",
      ""type"": ""bool"",
      ""optional"": true,
      ""default"": false
    },
    {
      ""name"": ""resume_from_checkpoint"",
      ""type"": ""str"",
      ""optional"": true
    },
    {
      ""name"": ""hub_model_id"",
      ""type"": ""str"",
      ""optional"": true
    },
    {
      ""name"": ""hub_strategy"",
      ""type"": ""str or ~trainer_utils.HubStrategy"",
      ""optional"": true,
      ""default"": ""every_save""
    },
    {
      ""name"": ""hub_token"",
      ""type"": ""str"",
      ""optional"": true
    },
    {
      ""name"": ""hub_private_repo"",
      ""type"": ""bool"",
      ""optional"": true
    },
    {
      ""name"": ""hub_always_push"",
      ""type"": ""bool"",
      ""optional"": true,
      ""default"": false
    },
    {
      ""name"": ""gradient_checkpointing"",
      ""type"": ""bool"",
      ""optional"": true,
      ""default"": false
    },
    {
      ""name"": ""gradient_checkpointing_kwargs"",
      ""type"": ""dict"",
      ""optional"": true,
      ""default"": null
    },
    {
      ""name"": ""include_for_metrics"",
      ""type"": ""List[str]"",
      ""optional"": true,
      ""default"": []
    },
    {
      ""name"": ""group_by_length"",
      ""type"": ""bool"",
      ""optional"": true,
      ""default"": false
    },
    {
      ""name"": ""length_column_name"",
      ""type"": ""str"",
      ""optional"": true,
      ""default"": ""length""
    },
    {
      ""name"": ""torch_compile"",
      ""type"": ""bool"",
      ""optional"": true,
      ""default"": false
    },
    {
      ""name"": ""torch_compile_backend"",
      ""type"": ""str"",
      ""optional"": true
    },
    {
      ""name"": ""torch_compile_mode"",
      ""type"": ""str"",
      ""optional"": true
    }
  ],
  ""relatedClasses"": [
    ""HfArgumentParser"",
    ""Trainer"",
    ""~trainer_utils.IntervalStrategy"",
    ""SchedulerType"",
    ""~trainer_utils.SaveStrategy"",
    ""AdamW"",
    ""Dataset""
  ],
  ""relatedLibraries"": [
    ""argparse"",
    ""TensorBoard"",
    ""wandb"",
    ""mlflow"",
    ""comet"",
    ""swanlab""
  ]
}
```

This JSON provides an overview of the API's structure and parameters, detailing their types, optionality, and default values, while also noting related classes and libraries."
Trainer,"Trainer is a simple but feature-complete training and eval loop for PyTorch, optimized for  Transformers.  Args:     model ([`PreTrainedModel`] or `torch.nn.Module`, *optional*):         The model to train, evaluate or use for predictions. If not provided, a `model_init` must be passed.          <Tip>          [`Trainer`] is optimized to work with the [`PreTrainedModel`] provided by the library. You can still use         your own models defined as `torch.nn.Module` as long as they work the same way as the  Transformers         models.          </Tip>      args ([`TrainingArguments`], *optional*):         The arguments to tweak for training. Will default to a basic instance of [`TrainingArguments`] with the         `output_dir` set to a directory named *tmp_trainer* in the current directory if not provided.     data_collator (`DataCollator`, *optional*):         The function to use to form a batch from a list of elements of `train_dataset` or `eval_dataset`. Will         default to [`default_data_collator`] if no `processing_class` is provided, an instance of         [`DataCollatorWithPadding`] otherwise if the processing_class is a feature extractor or tokenizer.     train_dataset (Union[`torch.utils.data.Dataset`, `torch.utils.data.IterableDataset`, `datasets.Dataset`], *optional*):         The dataset to use for training. If it is a [`~datasets.Dataset`], columns not accepted by the         `model.forward()` method are automatically removed.          Note that if it's a `torch.utils.data.IterableDataset` with some randomization and you are training in a         distributed fashion, your iterable dataset should either use a internal attribute `generator` that is a         `torch.Generator` for the randomization that must be identical on all processes (and the Trainer will         manually set the seed of this `generator` at each epoch) or have a `set_epoch()` method that internally         sets the seed of the RNGs used.     eval_dataset (Union[`torch.utils.data.Dataset`, Dict[str, `torch.utils.data.Dataset`, `datasets.Dataset`]), *optional*):          The dataset to use for evaluation. If it is a [`~datasets.Dataset`], columns not accepted by the          `model.forward()` method are automatically removed. If it is a dictionary, it will evaluate on each          dataset prepending the dictionary key to the metric name.     processing_class (`PreTrainedTokenizerBase` or `BaseImageProcessor` or `FeatureExtractionMixin` or `ProcessorMixin`, *optional*):         Processing class used to process the data. If provided, will be used to automatically process the inputs         for the model, and it will be saved along the model to make it easier to rerun an interrupted training or         reuse the fine-tuned model.         This supersedes the `tokenizer` argument, which is now deprecated.     model_init (`Callable[[], PreTrainedModel]`, *optional*):         A function that instantiates the model to be used. If provided, each call to [`~Trainer.train`] will start         from a new instance of the model as given by this function.          The function may have zero argument, or a single one containing the optuna/Ray Tune/SigOpt trial object, to         be able to choose different architectures according to hyper parameters (such as layer count, sizes of         inner layers, dropout probabilities etc).     compute_loss_func (`Callable`, *optional*):         A function that accepts the raw model outputs, labels, and the number of items in the entire accumulated         batch (batch_size * gradient_accumulation_steps) and returns the loss. For example, see the default [loss function](https://github.com/huggingface/transformers/blob/052e652d6d53c2b26ffde87e039b723949a53493/src/transformers/trainer.py#L3618) used by [`Trainer`].     compute_metrics (`Callable[[EvalPrediction], Dict]`, *optional*):         The function that will be used to compute metrics at evaluation. Must take a [`EvalPrediction`] and return         a dictionary string to metric values. *Note* When passing TrainingArgs with `batch_eval_metrics` set to         `True`, your compute_metrics function must take a boolean `compute_result` argument. This will be triggered         after the last eval batch to signal that the function needs to calculate and return the global summary         statistics rather than accumulating the batch-level statistics     callbacks (List of [`TrainerCallback`], *optional*):         A list of callbacks to customize the training loop. Will add those to the list of default callbacks         detailed in [here](callback).          If you want to remove one of the default callbacks used, use the [`Trainer.remove_callback`] method.     optimizers (`Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR]`, *optional*, defaults to `(None, None)`):         A tuple containing the optimizer and the scheduler to use. Will default to an instance of [`AdamW`] on your         model and a scheduler given by [`get_linear_schedule_with_warmup`] controlled by `args`.     optimizer_cls_and_kwargs (`Tuple[Type[torch.optim.Optimizer], Dict[str, Any]]`, *optional*):         A tuple containing the optimizer class and keyword arguments to use.         Overrides `optim` and `optim_args` in `args`. Incompatible with the `optimizers` argument.          Unlike `optimizers`, this argument avoids the need to place model parameters on the correct devices before initializing the Trainer.     preprocess_logits_for_metrics (`Callable[[torch.Tensor, torch.Tensor], torch.Tensor]`, *optional*):         A function that preprocess the logits right before caching them at each evaluation step. Must take two         tensors, the logits and the labels, and return the logits once processed as desired. The modifications made         by this function will be reflected in the predictions received by `compute_metrics`.          Note that the labels (second parameter) will be `None` if the dataset does not have them.  Important attributes:      - **model** -- Always points to the core model. If using a transformers model, it will be a [`PreTrainedModel`]       subclass.     - **model_wrapped** -- Always points to the most external model in case one or more other modules wrap the       original model. This is the model that should be used for the forward pass. For example, under `DeepSpeed`,       the inner model is wrapped in `DeepSpeed` and then again in `torch.nn.DistributedDataParallel`. If the inner       model hasn't been wrapped, then `self.model_wrapped` is the same as `self.model`.     - **is_model_parallel** -- Whether or not a model has been switched to a model parallel mode (different from       data parallelism, this means some of the model layers are split on different GPUs).     - **place_model_on_device** -- Whether or not to automatically place the model on the device - it will be set       to `False` if model parallel or deepspeed is used, or if the default       `TrainingArguments.place_model_on_device` is overridden to return `False` .     - **is_in_train** -- Whether or not a model is currently running `train` (e.g. when `evaluate` is called while       in `train`)","```json
{
  ""api"": {
    ""name"": ""Hugging Face Trainer"",
    ""description"": ""Structured representation of the Trainer API based on provided documentation."",
    ""classes"": [
      {
        ""name"": ""Trainer"",
        ""type"": ""class"",
        ""description"": ""Trainer is a simple but feature-complete training and eval loop for PyTorch, optimized for  Transformers."",
        ""constructor"": {
          ""arguments"": [
            {
              ""name"": ""model"",
              ""type"": [""PreTrainedModel"", ""torch.nn.Module""],
              ""description"": ""The model to train, evaluate or use for predictions."",
              ""optional"": true,
              ""notes"": ""If not provided, a `model_init` must be passed. Optimized for Hugging Face PreTrainedModel."",
              ""relationships"": [
                 {""type"": ""required_if_not_provided"", ""target_arg"": ""model_init""}
              ]
            },
            {
              ""name"": ""args"",
              ""type"": ""TrainingArguments"",
              ""description"": ""The arguments to tweak for training."",
              ""optional"": true,
              ""default"": ""Basic instance of TrainingArguments with output_dir set to tmp_trainer.""
            },
            {
              ""name"": ""data_collator"",
              ""type"": ""DataCollator"",
              ""description"": ""The function to use to form a batch from a list of elements."",
              ""optional"": true,
              ""default"": ""default_data_collator (if no processing_class provided), otherwise DataCollatorWithPadding (if processing_class is feature extractor/tokenizer)."",
              ""relationships"": [
                 {""type"": ""default_depends_on"", ""target_arg"": ""processing_class""}
              ]
            },
            {
              ""name"": ""train_dataset"",
              ""type"": [""torch.utils.data.Dataset"", ""torch.utils.data.IterableDataset"", ""datasets.Dataset""],
              ""description"": ""The dataset to use for training."",
              ""optional"": true,
              ""notes"": ""Columns not accepted by model.forward() are removed for datasets.Dataset. IterableDataset requires internal generator or set_epoch() method for distributed training randomization.""
            },
             {
              ""name"": ""eval_dataset"",
              ""type"": [""torch.utils.data.Dataset"", ""datasets.Dataset"", ""Dict[str, Union[torch.utils.data.Dataset, datasets.Dataset]]""],
              ""description"": ""The dataset to use for evaluation."",
              ""optional"": true,
              ""notes"": ""Columns not accepted by model.forward() are removed for datasets.Dataset. If a dictionary, evaluates on each dataset prepending the key to the metric name.""
            },
            {
              ""name"": ""processing_class"",
              ""type"": [""PreTrainedTokenizerBase"", ""BaseImageProcessor"", ""FeatureExtractionMixin"", ""ProcessorMixin""],
              ""description"": ""Processing class used to process the data."",
              ""optional"": true,
              ""notes"": ""Supersedes the deprecated 'tokenizer' argument. Used to automatically process inputs and saved along with the model.""
            },
            {
              ""name"": ""model_init"",
              ""type"": ""Callable[[], PreTrainedModel] or Callable[[TrialObject], PreTrainedModel]"",
              ""description"": ""A function that instantiates the model to be used."",
              ""optional"": true,
              ""notes"": ""If provided, each call to train will start from a new instance. Can accept an Optuna/Ray Tune/SigOpt trial object for hyperparameter tuning."",
              ""relationships"": [
                 {""type"": ""required_if_not_provided"", ""target_arg"": ""model""}
              ]
            },
            {
              ""name"": ""compute_loss_func"",
              ""type"": ""Callable[[raw_outputs, labels, accumulated_batch_size], loss]"",
              ""description"": ""A function that accepts the raw model outputs, labels, and the number of items in the entire accumulated batch and returns the loss."",
              ""optional"": true
            },
             {
              ""name"": ""compute_metrics"",
              ""type"": ""Callable[[EvalPrediction], Dict[str, float]] or Callable[[EvalPrediction, bool], Dict[str, float]]"",
              ""description"": ""The function that will be used to compute metrics at evaluation."",
              ""optional"": true,
              ""notes"": ""Must take a EvalPrediction and return a dictionary. Signature includes boolean `compute_result` argument when TrainingArgs with `batch_eval_metrics` is True.""
            },
            {
              ""name"": ""callbacks"",
              ""type"": ""List[TrainerCallback]"",
              ""description"": ""A list of callbacks to customize the training loop."",
              ""optional"": true,
              ""notes"": ""Will be added to the list of default callbacks. Default callbacks can be removed using the `Trainer.remove_callback` method.""
            },
             {
              ""name"": ""optimizers"",
              ""type"": ""Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR]"",
              ""description"": ""A tuple containing the optimizer and the scheduler to use."",
              ""optional"": true,
              ""default"": ""(None, None)"",
              ""notes"": ""Defaults to an instance of AdamW and a scheduler given by get_linear_schedule_with_warmup controlled by args."",
              ""relationships"": [
                 {""type"": ""incompatible_with"", ""target_arg"": ""optimizer_cls_and_kwargs""}
              ]
            },
            {
              ""name"": ""optimizer_cls_and_kwargs"",
              ""type"": ""Tuple[Type[torch.optim.Optimizer], Dict[str, Any]]"",
              ""description"": ""A tuple containing the optimizer class and keyword arguments to use."",
              ""optional"": true,
              ""notes"": ""Overrides `optim` and `optim_args` in `args`. Unlike `optimizers`, avoids the need to place model parameters on correct devices before initialization."",
              ""relationships"": [
                 {""type"": ""incompatible_with"", ""target_arg"": ""optimizers""}
              ]
            },
            {
              ""name"": ""preprocess_logits_for_metrics"",
              ""type"": ""Callable[[torch.Tensor, torch.Tensor], torch.Tensor]"",
              ""description"": ""A function that preprocess the logits right before caching them at each evaluation step."",
              ""optional"": true,
              ""notes"": ""Must take two tensors (logits, labels) and return processed logits. Labels will be None if dataset does not have them. Modifications reflected in compute_metrics predictions.""
            }
          ]
        },
        ""attributes"": [
          {
            ""name"": ""model"",
            ""description"": ""Always points to the core model."",
            ""type"": [""PreTrainedModel"", ""torch.nn.Module""]
          },
          {
            ""name"": ""model_wrapped"",
            ""description"": ""Always points to the most external model in case one or more other modules wrap the original model."",
             ""type"": ""torch.nn.Module"",
            ""notes"": ""This is the model used for the forward pass. If the inner model hasn't been wrapped (e.g., by DeepSpeed or DDP), then it's the same as self.model.""
          },
          {
            ""name"": ""is_model_parallel"",
            ""description"": ""Whether or not a model has been switched to a model parallel mode."",
            ""type"": ""boolean""
          },
          {
            ""name"": ""place_model_on_device"",
            ""description"": ""Whether or not to automatically place the model on the device."",
            ""type"": ""boolean"",
            ""notes"": ""False if model parallel or deepspeed is used, or if the default TrainingArguments.place_model_on_device is overridden to return False.""
          },
          {
            ""name"": ""is_in_train"",
            ""description"": ""Whether or not a model is currently running `train` (e.g. when `evaluate` is called while in `train`)."",
            ""type"": ""boolean""
          }
        ],
         ""methods_mentioned"": [
            {""name"": ""train"", ""description"": ""Method to start the training loop (implied by documentation).""},
            {""name"": ""evaluate"", ""description"": ""Method to start the evaluation loop (implied by documentation).""},
            {""name"": ""remove_callback"", ""description"": ""Method to remove a default TrainerCallback from the list.""}
         ]
      }
    ],
     ""types_mentioned"": [
        {""name"": ""PreTrainedModel"", ""description"": ""Hugging Face Transformers base class for pre-trained models.""},
        {""name"": ""torch.nn.Module"", ""description"": ""PyTorch base class for all neural network modules.""},
        {""name"": ""TrainingArguments"", ""description"": ""Configuration object for Trainer arguments.""},
        {""name"": ""DataCollator"", ""description"": ""Abstract base class or protocol for functions that prepare batches.""},
        {""name"": ""torch.utils.data.Dataset"", ""description"": ""PyTorch abstract class representing a dataset.""},
        {""name"": ""torch.utils.data.IterableDataset"", ""description"": ""PyTorch abstract class for iterable-style datasets.""},
        {""name"": ""datasets.Dataset"", ""description"": ""Hugging Face datasets library dataset object.""},
        {""name"": ""PreTrainedTokenizerBase"", ""description"": ""Base class for Hugging Face Transformers tokenizers.""},
        {""name"": ""BaseImageProcessor"", ""description"": ""Base class for Hugging Face image processors.""},
        {""name"": ""FeatureExtractionMixin"", ""description"": ""Mixin for feature extraction capabilities in Hugging Face libraries.""},
        {""name"": ""ProcessorMixin"", ""description"": ""Mixin for combined processing (e.g., text and images).""},
        {""name"": ""Callable"", ""description"": ""Type hint for a function or other callable object.""},
        {""name"": ""EvalPrediction"", ""description"": ""Object containing predictions and optionally labels for evaluation.""},
        {""name"": ""TrainerCallback"", ""description"": ""Base class for callbacks used within the Trainer training loop.""},
        {""name"": ""torch.optim.Optimizer"", ""description"": ""Base class for PyTorch optimizers.""},
        {""name"": ""torch.optim.lr_scheduler.LambdaLR"", ""description"": ""PyTorch learning rate scheduler that multiplies the learning rate by a factor given by a function.""},
        {""name"": ""torch.Generator"", ""description"": ""PyTorch random number generator object.""},
        {""name"": ""Dict"", ""description"": ""Python dictionary type hint.""},
        {""name"": ""Tuple"", ""description"": ""Python tuple type hint.""},
        {""name"": ""List"", ""description"": ""Python list type hint.""},
        {""name"": ""Type"", ""description"": ""Python type hint for a type object.""},
        {""name"": ""Any"", ""description"": ""Python type hint representing any type.""},
        {""name"": ""torch.Tensor"", ""description"": ""PyTorch tensor object.""},
        {""name"": ""TrialObject"", ""description"": ""Generic type representing a hyperparameter tuning trial object (e.g., from Optuna, Ray Tune, SigOpt).""}
     ],
     ""functions_mentioned"": [
        {""name"": ""default_data_collator"", ""description"": ""Default function to collate data into batches.""},
        {""name"": ""get_linear_schedule_with_warmup"", ""description"": ""Function to create a learning rate scheduler with a linear decay after a warmup phase.""},
        {""name"": ""model.forward"", ""description"": ""Standard method on PyTorch modules for the forward pass (mentioned in context of dataset column handling).""},
        {""name"": ""set_epoch"", ""description"": ""Method on IterableDataset to set the epoch for randomization (mentioned in context of distributed training).""}
     ]
  }
}
```","Here is a structured representation of the API documentation, formatted in JSON to capture the types, functions, parameters, optional attributes, and relationships outlined in the text:

```json
{
  ""Trainer"": {
    ""description"": ""A simple but feature-complete training and evaluation loop for PyTorch, optimized for  Transformers."",
    ""args"": {
      ""model"": {
        ""type"": ""[`PreTrainedModel`] or `torch.nn.Module`"",
        ""optional"": true,
        ""description"": ""The model to train, evaluate or use for predictions. If not provided, a `model_init` must be passed.""
      },
      ""args"": {
        ""type"": ""[`TrainingArguments`]"",
        ""optional"": true,
        ""description"": ""The arguments to tweak for training. Defaults to a basic instance of `TrainingArguments` with the `output_dir` set to *tmp_trainer* in the current directory if not provided.""
      },
      ""data_collator"": {
        ""type"": ""`DataCollator`"",
        ""optional"": true,
        ""description"": ""The function to use to form a batch from a dataset. Defaults to `default_data_collator` if no `processing_class` is provided.""
      },
      ""train_dataset"": {
        ""type"": ""Union[`torch.utils.data.Dataset`, `torch.utils.data.IterableDataset`, `datasets.Dataset`]"",
        ""optional"": true,
        ""description"": ""The dataset to use for training. Automatically removes columns not accepted by `model.forward()` if it's a `datasets.Dataset`.""
      },
      ""eval_dataset"": {
        ""type"": ""Union[`torch.utils.data.Dataset`, Dict[str, `torch.utils.data.Dataset`, `datasets.Dataset`]]"",
        ""optional"": true,
        ""description"": ""The dataset to use for evaluation. Automatically removes columns not accepted by `model.forward()` if it's a `datasets.Dataset`.""
      },
      ""processing_class"": {
        ""type"": ""`PreTrainedTokenizerBase` or `BaseImageProcessor` or `FeatureExtractionMixin` or `ProcessorMixin`"",
        ""optional"": true,
        ""description"": ""Processing class used to process the data and saved with the model. Supersedes the `tokenizer` argument.""
      },
      ""model_init"": {
        ""type"": ""`Callable[[], PreTrainedModel]`"",
        ""optional"": true,
        ""description"": ""A function that instantiates the model to be used. Allows starting from a new instance for each training run.""
      },
      ""compute_loss_func"": {
        ""type"": ""`Callable`"",
        ""optional"": true,
        ""description"": ""Function that computes the loss from model outputs, labels, and batch size.""
      },
      ""compute_metrics"": {
        ""type"": ""`Callable[[EvalPrediction], Dict]`"",
        ""optional"": true,
        ""description"": ""Function to compute metrics at evaluation taking `EvalPrediction` and returning a dictionary of metric values.""
      },
      ""callbacks"": {
        ""type"": ""List of [`TrainerCallback`]"",
        ""optional"": true,
        ""description"": ""List of callbacks to customize the training loop, which adds to the default callbacks.""
      },
      ""optimizers"": {
        ""type"": ""`Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR]`"",
        ""optional"": true,
        ""default"": ""(None, None)"",
        ""description"": ""A tuple containing the optimizer and the scheduler to use, defaults to `AdamW` and a linear scheduler.""
      },
      ""optimizer_cls_and_kwargs"": {
        ""type"": ""`Tuple[Type[torch.optim.Optimizer], Dict[str, Any]]`"",
        ""optional"": true,
        ""description"": ""A tuple containing the optimizer class and keyword arguments. Overrides optimizers and avoids placing model parameters on devices before initializing.""
      },
      ""preprocess_logits_for_metrics"": {
        ""type"": ""`Callable[[torch.Tensor, torch.Tensor], torch.Tensor]`"",
        ""optional"": true,
        ""description"": ""Function that preprocesses the logits before caching them for metrics evaluation.""
      }
    },
    ""important_attributes"": {
      ""model"": ""Always points to the core model."",
      ""model_wrapped"": ""Always points to the most external model when modules wrap the original model."",
      ""is_model_parallel"": ""Indicates if the model has been switched to model parallel mode."",
      ""place_model_on_device"": ""Specifies whether to automatically place the model on the device."",
      ""is_in_train"": ""Indicates if the model is currently running `train`.""
    }
  }
}
```

This JSON captures the structure of the API documentation by detailing the types, optionality, and descriptions of each argument, as well as the attributes associated with the `Trainer`."
notebook_login,"Displays a widget to log in to the HF website and store the token.  This is equivalent to [`login`] without passing a token when run in a notebook. [`notebook_login`] is useful if you want to force the use of the notebook widget instead of a prompt in the terminal.  For more details, see [`login`].  Args:     new_session (`bool`, defaults to `True`):         If `True`, will request a token even if one is already saved on the machine.     write_permission (`bool`):         Ignored and deprecated argument.","```json
{
  ""api"": {
    ""name"": ""Hugging Face Login API (Partial)"",
    ""description"": ""API functions related to user authentication and token management."",
    ""functions"": [
      {
        ""name"": ""notebook_login"",
        ""description"": ""Displays a widget to log in to the HF website and store the token. Equivalent to `login` without passing a token when run in a notebook. Useful if you want to force the use of the notebook widget instead of a prompt in the terminal. For more details, see `login`."",
        ""parameters"": [
          {
            ""name"": ""new_session"",
            ""type"": ""bool"",
            ""description"": ""If `True`, will request a token even if one is already saved on the machine."",
            ""default_value"": true,
            ""required"": false
          },
          {
            ""name"": ""write_permission"",
            ""type"": ""bool"",
            ""description"": ""Ignored and deprecated argument."",
            ""required"": false,
            ""notes"": [""ignored"", ""deprecated""]
          }
        ],
        ""returns"": null,
        ""relationships"": [
          {
            ""type"": ""related_to"",
            ""target"": ""login"",
            ""description"": ""Equivalent to `login` without a token in a notebook context, and provides a widget UI. See `login` for more details.""
          }
        ],
        ""notes"": ""Specifically designed for notebook environments to provide a GUI login experience.""
      }
    ],
    ""types"": [
      {
        ""name"": ""bool"",
        ""description"": ""A boolean value, either true or false.""
      }
      // The type of the ""token"" is not explicitly defined as an input/output type
      // for this specific function's signature, so it's not listed here.
    ]
  }
}
```","```json
{
  ""API"": {
    ""function"": ""notebook_login"",
    ""description"": ""Displays a widget to log in to the HF website and store the token. Equivalent to the login function without passing a token when run in a notebook. Useful to force the use of the notebook widget instead of a prompt in the terminal."",
    ""arguments"": {
      ""new_session"": {
        ""type"": ""bool"",
        ""default"": true,
        ""description"": ""If True, will request a token even if one is already saved on the machine.""
      },
      ""write_permission"": {
        ""type"": ""bool"",
        ""description"": ""Ignored and deprecated argument.""
      }
    },
    ""related_functions"": [""login""]
  }
}
```"
load_dataset,"Load a dataset from the Hugging Face Hub, or a local dataset.  You can find the list of datasets on the [Hub](https://huggingface.co/datasets) or with [`huggingface_hub.list_datasets`].  A dataset is a directory that contains:  - some data files in generic formats (JSON, CSV, Parquet, text, etc.). - and optionally a dataset script, if it requires some code to read the data files. This is used to load any kind of formats or structures.  Note that dataset scripts can also download and read data files from anywhere - in case your data files already exist online.  This function does the following under the hood:      1. Download and import in the library the dataset script from `path` if it's not already cached inside the library.          If the dataset has no dataset script, then a generic dataset script is imported instead (JSON, CSV, Parquet, text, etc.)          Dataset scripts are small python scripts that define dataset builders. They define the citation, info and format of the dataset,         contain the path or URL to the original data files and the code to load examples from the original data files.          You can find the complete list of datasets in the Datasets [Hub](https://huggingface.co/datasets).      2. Run the dataset script which will:          * Download the dataset file from the original URL (see the script) if it's not already available locally or cached.         * Process and cache the dataset in typed Arrow tables for caching.              Arrow table are arbitrarily long, typed tables which can store nested objects and be mapped to numpy/pandas/python generic types.             They can be directly accessed from disk, loaded in RAM or even streamed over the web.      3. Return a dataset built from the requested splits in `split` (default: all).  It also allows to load a dataset from a local directory or a dataset repository on the Hugging Face Hub without dataset script. In this case, it automatically loads all the data files from the directory or the dataset repository.  Args:      path (`str`):         Path or name of the dataset.         Depending on `path`, the dataset builder that is used comes from a generic dataset script (JSON, CSV, Parquet, text etc.) or from the dataset script (a python file) inside the dataset directory.          For local datasets:          - if `path` is a local directory (containing data files only)           -> load a generic dataset builder (csv, json, text etc.) based on the content of the directory           e.g. `'./path/to/directory/with/my/csv/data'`.         - if `path` is a local dataset script or a directory containing a local dataset script (if the script has the same name as the directory)           -> load the dataset builder from the dataset script           e.g. `'./dataset/squad'` or `'./dataset/squad/squad.py'`.          For datasets on the Hugging Face Hub (list all available datasets with [`huggingface_hub.list_datasets`])          - if `path` is a dataset repository on the HF hub (containing data files only)           -> load a generic dataset builder (csv, text etc.) based on the content of the repository           e.g. `'username/dataset_name'`, a dataset repository on the HF hub containing your data files.         - if `path` is a dataset repository on the HF hub with a dataset script (if the script has the same name as the directory)           -> load the dataset builder from the dataset script in the dataset repository           e.g. `glue`, `squad`, `'username/dataset_name'`, a dataset repository on the HF hub containing a dataset script `'dataset_name.py'`.      name (`str`, *optional*):         Defining the name of the dataset configuration.     data_dir (`str`, *optional*):         Defining the `data_dir` of the dataset configuration. If specified for the generic builders (csv, text etc.) or the Hub datasets and `data_files` is `None`,         the behavior is equal to passing `os.path.join(data_dir, **)` as `data_files` to reference all the files in a directory.     data_files (`str` or `Sequence` or `Mapping`, *optional*):         Path(s) to source data file(s).     split (`Split` or `str`):         Which split of the data to load.         If `None`, will return a `dict` with all splits (typically `datasets.Split.TRAIN` and `datasets.Split.TEST`).         If given, will return a single Dataset.         Splits can be combined and specified like in tensorflow-datasets.     cache_dir (`str`, *optional*):         Directory to read/write data. Defaults to `""~/.cache/huggingface/datasets""`.     features (`Features`, *optional*):         Set the features type to use for this dataset.     download_config ([`DownloadConfig`], *optional*):         Specific download configuration parameters.     download_mode ([`DownloadMode`] or `str`, defaults to `REUSE_DATASET_IF_EXISTS`):         Download/generate mode.     verification_mode ([`VerificationMode`] or `str`, defaults to `BASIC_CHECKS`):         Verification mode determining the checks to run on the downloaded/processed dataset information (checksums/size/splits/...).          <Added version=""2.9.1""/>     ignore_verifications (`bool`, defaults to `False`):         Ignore the verifications of the downloaded/processed dataset information (checksums/size/splits/...).          <Deprecated version=""2.9.1"">          `ignore_verifications` was deprecated in version 2.9.1 and will be removed in 3.0.0.         Please use `verification_mode` instead.          </Deprecated>     keep_in_memory (`bool`, defaults to `None`):         Whether to copy the dataset in-memory. If `None`, the dataset         will not be copied in-memory unless explicitly enabled by setting `datasets.config.IN_MEMORY_MAX_SIZE` to         nonzero. See more details in the [improve performance](../cache#improve-performance) section.     save_infos (`bool`, defaults to `False`):         Save the dataset information (checksums/size/splits/...).     revision ([`Version`] or `str`, *optional*):         Version of the dataset script to load.         As datasets have their own git repository on the Datasets Hub, the default version ""main"" corresponds to their ""main"" branch.         You can specify a different version than the default ""main"" by using a commit SHA or a git tag of the dataset repository.     token (`str` or `bool`, *optional*):         Optional string or boolean to use as Bearer token for remote files on the Datasets Hub.         If `True`, or not specified, will get token from `""~/.huggingface""`.     use_auth_token (`str` or `bool`, *optional*):         Optional string or boolean to use as Bearer token for remote files on the Datasets Hub.         If `True`, or not specified, will get token from `""~/.huggingface""`.          <Deprecated version=""2.14.0"">          `use_auth_token` was deprecated in favor of `token` in version 2.14.0 and will be removed in 3.0.0.          </Deprecated>     task (`str`):         The task to prepare the dataset for during training and evaluation. Casts the dataset's [`Features`] to standardized column names and types as detailed in `datasets.tasks`.          <Deprecated version=""2.13.0"">          `task` was deprecated in version 2.13.0 and will be removed in 3.0.0.          </Deprecated>     streaming (`bool`, defaults to `False`):         If set to `True`, don't download the data files. Instead, it streams the data progressively while         iterating on the dataset. An [`IterableDataset`] or [`IterableDatasetDict`] is returned instead in this case.          Note that streaming works for datasets that use data formats that support being iterated over like txt, csv, jsonl for example.         Json files may be downloaded completely. Also streaming from remote zip or gzip files is supported but other compressed formats         like rar and xz are not yet supported. The tgz format doesn't allow streaming.     num_proc (`int`, *optional*, defaults to `None`):         Number of processes when downloading and generating the dataset locally.         Multiprocessing is disabled by default.          <Added version=""2.7.0""/>     storage_options (`dict`, *optional*, defaults to `None`):         **Experimental**. Key/value pairs to be passed on to the dataset file-system backend, if any.          <Added version=""2.11.0""/>     **config_kwargs (additional keyword arguments):         Keyword arguments to be passed to the `BuilderConfig`         and used in the [`DatasetBuilder`].  Returns:     [`Dataset`] or [`DatasetDict`]:     - if `split` is not `None`: the dataset requested,     - if `split` is `None`, a [`~datasets.DatasetDict`] with each split.      or [`IterableDataset`] or [`IterableDatasetDict`]: if `streaming=True`      - if `split` is not `None`, the dataset is requested     - if `split` is `None`, a [`~datasets.streaming.IterableDatasetDict`] with each split.  Example:  Load a dataset from the Hugging Face Hub:  ```py >>> from datasets import load_dataset >>> ds = load_dataset('rotten_tomatoes', split='train')  # Map data files to splits >>> data_files = {'train': 'train.csv', 'test': 'test.csv'} >>> ds = load_dataset('namespace/your_dataset_name', data_files=data_files) ```  Load a local dataset:  ```py # Load a CSV file >>> from datasets import load_dataset >>> ds = load_dataset('csv', data_files='path/to/local/my_dataset.csv')  # Load a JSON file >>> from datasets import load_dataset >>> ds = load_dataset('json', data_files='path/to/local/my_dataset.json')  # Load from a local loading script >>> from datasets import load_dataset >>> ds = load_dataset('path/to/local/loading_script/loading_script.py', split='train') ```  Load an [`~datasets.IterableDataset`]:  ```py >>> from datasets import load_dataset >>> ds = load_dataset('rotten_tomatoes', split='train', streaming=True) ```  Load an image dataset with the `ImageFolder` dataset builder:  ```py >>> from datasets import load_dataset >>> ds = load_dataset('imagefolder', data_dir='/path/to/images', split='train') ```","```json
{
  ""name"": ""load_dataset"",
  ""description"": ""Load a dataset from the Hugging Face Hub or a local dataset. This function handles locating dataset scripts (or using generic ones), downloading and caching data, and returning a Dataset or DatasetDict (or their iterable variants)."",
  ""details"": ""Under the hood, it downloads/imports the dataset script, runs the script to download/process data into Arrow tables, and returns the requested splits. It supports loading from Hub repositories or local directories with or without dataset scripts."",
  ""parameters"": [
    {
      ""name"": ""path"",
      ""type"": ""str"",
      ""description"": ""Path or name of the dataset. Can be a local directory, local dataset script path, or Hugging Face Hub repository identifier. Determines the dataset builder used."",
      ""optional"": false
    },
    {
      ""name"": ""name"",
      ""type"": ""str"",
      ""description"": ""Defining the name of the dataset configuration."",
      ""optional"": true
    },
    {
      ""name"": ""data_dir"",
      ""type"": ""str"",
      ""description"": ""Defining the `data_dir` of the dataset configuration. For generic builders or Hub datasets without `data_files`, defaults to `os.path.join(data_dir, **)`."",
      ""optional"": true
    },
    {
      ""name"": ""data_files"",
      ""type"": [""str"", ""Sequence"", ""Mapping""],
      ""description"": ""Path(s) to source data file(s)."",
      ""optional"": true
    },
    {
      ""name"": ""split"",
      ""type"": [""Split"", ""str""],
      ""description"": ""Which split of the data to load. If `None`, returns a `dict` with all splits. If given, returns a single Dataset. Splits can be combined."",
      ""optional"": true
    },
    {
      ""name"": ""cache_dir"",
      ""type"": ""str"",
      ""description"": ""Directory to read/write data."",
      ""optional"": true,
      ""default"": ""~/.cache/huggingface/datasets""
    },
    {
      ""name"": ""features"",
      ""type"": ""Features"",
      ""description"": ""Set the features type to use for this dataset."",
      ""optional"": true
    },
    {
      ""name"": ""download_config"",
      ""type"": ""DownloadConfig"",
      ""description"": ""Specific download configuration parameters."",
      ""optional"": true
    },
    {
      ""name"": ""download_mode"",
      ""type"": [""DownloadMode"", ""str""],
      ""description"": ""Download/generate mode."",
      ""optional"": true,
      ""default"": ""REUSE_DATASET_IF_EXISTS""
    },
    {
      ""name"": ""verification_mode"",
      ""type"": [""VerificationMode"", ""str""],
      ""description"": ""Verification mode determining the checks to run on the downloaded/processed dataset information."",
      ""optional"": true,
      ""default"": ""BASIC_CHECKS"",
      ""added_version"": ""2.9.1""
    },
    {
      ""name"": ""ignore_verifications"",
      ""type"": ""bool"",
      ""description"": ""Ignore the verifications of the downloaded/processed dataset information."",
      ""optional"": true,
      ""default"": false,
      ""deprecation"": {
        ""version"": ""2.9.1"",
        ""removal_version"": ""3.0.0"",
        ""note"": ""Please use `verification_mode` instead.""
      }
    },
    {
      ""name"": ""keep_in_memory"",
      ""type"": ""bool"",
      ""description"": ""Whether to copy the dataset in-memory."",
      ""optional"": true,
      ""default"": null
    },
    {
      ""name"": ""save_infos"",
      ""type"": ""bool"",
      ""description"": ""Save the dataset information (checksums/size/splits/)."",
      ""optional"": true,
      ""default"": false
    },
    {
      ""name"": ""revision"",
      ""type"": [""Version"", ""str""],
      ""description"": ""Version of the dataset script to load (commit SHA or git tag)."",
      ""optional"": true,
      ""default"": ""main""
    },
    {
      ""name"": ""token"",
      ""type"": [""str"", ""bool""],
      ""description"": ""Bearer token for remote files on the Datasets Hub. If `True` or unspecified, gets token from `~/.huggingface`."",
      ""optional"": true
    },
    {
      ""name"": ""use_auth_token"",
      ""type"": [""str"", ""bool""],
      ""description"": ""Bearer token for remote files on the Datasets Hub. If `True` or unspecified, gets token from `~/.huggingface`."",
      ""optional"": true,
      ""deprecation"": {
        ""version"": ""2.14.0"",
        ""removal_version"": ""3.0.0"",
        ""note"": ""Deprecated in favor of `token`.""
      }
    },
    {
      ""name"": ""task"",
      ""type"": ""str"",
      ""description"": ""The task to prepare the dataset for (casts Features to standardized column names and types)."",
      ""optional"": true,
      ""deprecation"": {
        ""version"": ""2.13.0"",
        ""removal_version"": ""3.0.0""
      }
    },
    {
      ""name"": ""streaming"",
      ""type"": ""bool"",
      ""description"": ""If set to `True`, streams data progressively instead of downloading all files. Returns an `IterableDataset` or `IterableDatasetDict`."",
      ""optional"": true,
      ""default"": false
    },
    {
      ""name"": ""num_proc"",
      ""type"": ""int"",
      ""description"": ""Number of processes when downloading and generating the dataset locally. Multiprocessing is disabled by default."",
      ""optional"": true,
      ""default"": null,
      ""added_version"": ""2.7.0""
    },
    {
      ""name"": ""storage_options"",
      ""type"": ""dict"",
      ""description"": ""Experimental. Key/value pairs to be passed on to the dataset file-system backend."",
      ""optional"": true,
      ""default"": null,
      ""added_version"": ""2.11.0""
    },
    {
      ""name"": ""**config_kwargs"",
      ""type"": ""dict"",
      ""description"": ""Additional keyword arguments to be passed to the `BuilderConfig` and used in the [`DatasetBuilder`]."",
      ""optional"": true,
      ""is_variadic"": true
    }
  ],
  ""returns"": {
    ""type"": [""Dataset"", ""DatasetDict"", ""IterableDataset"", ""IterableDatasetDict""],
    ""description"": ""Returns a `Dataset` if `split` is not `None` and `streaming` is `False`. Returns a `DatasetDict` if `split` is `None` and `streaming` is `False`. Returns an `IterableDataset` if `split` is not `None` and `streaming` is `True`. Returns an `IterableDatasetDict` if `split` is `None` and `streaming` is `True`.""
  },
  ""examples"": [
    ""Load a dataset from the Hugging Face Hub:\n>>> from datasets import load_dataset\n>>> ds = load_dataset('rotten_tomatoes', split='train')\n\n# Map data files to splits\n>>> data_files = {'train': 'train.csv', 'test': 'test.csv'}\n>>> ds = load_dataset('namespace/your_dataset_name', data_files=data_files)"",
    ""Load a local dataset:\n# Load a CSV file\n>>> from datasets import load_dataset\n>>> ds = load_dataset('csv', data_files='path/to/local/my_dataset.csv')\n\n# Load a JSON file\n>>> from datasets import load_dataset\n>>> ds = load_dataset('json', data_files='path/to/local/my_dataset.json')\n\n# Load from a local loading script\n>>> from datasets import load_dataset\n>>> ds = load_dataset('path/to/local/loading_script/loading_script.py', split='train')"",
    ""Load an [`~datasets.IterableDataset`]:\n>>> from datasets import load_dataset\n>>> ds = load_dataset('rotten_tomatoes', split='train', streaming=True)"",
    ""Load an image dataset with the `ImageFolder` dataset builder:\n>>> from datasets import load_dataset\n>>> ds = load_dataset('imagefolder', data_dir='/path/to/images', split='train')""
  ],
  ""referenced_types"": [
    ""Split"",
    ""Sequence"",
    ""Mapping"",
    ""Features"",
    ""DownloadConfig"",
    ""DownloadMode"",
    ""VerificationMode"",
    ""Version"",
    ""Dataset"",
    ""DatasetDict"",
    ""IterableDataset"",
    ""IterableDatasetDict"",
    ""BuilderConfig"",
    ""DatasetBuilder""
  ]
}
```","Below is a structured JSON representation of the API documentation provided:

```json
{
  ""function"": ""load_dataset"",
  ""description"": ""Load a dataset from the Hugging Face Hub, or a local dataset."",
  ""args"": {
    ""path"": {
      ""type"": ""str"",
      ""description"": ""Path or name of the dataset."",
      ""details"": [
        {
          ""condition"": ""local directory containing data files only"",
          ""action"": ""load a generic dataset builder based on the content of the directory""
        },
        {
          ""condition"": ""local dataset script or directory containing a script"",
          ""action"": ""load the dataset builder from the dataset script""
        },
        {
          ""condition"": ""dataset repository on the HF hub containing data files only"",
          ""action"": ""load a generic dataset builder based on the content of the repository""
        },
        {
          ""condition"": ""dataset repository on the HF hub with a dataset script"",
          ""action"": ""load the dataset builder from the dataset script in the repository""
        }
      ]
    },
    ""name"": {
      ""type"": ""str"",
      ""optional"": true,
      ""description"": ""Defining the name of the dataset configuration.""
    },
    ""data_dir"": {
      ""type"": ""str"",
      ""optional"": true,
      ""description"": ""Defining the data_dir of the dataset configuration.""
    },
    ""data_files"": {
      ""type"": ""str or Sequence or Mapping"",
      ""optional"": true,
      ""description"": ""Path(s) to source data file(s).""
    },
    ""split"": {
      ""type"": ""Split or str"",
      ""description"": ""Which split of the data to load.""
    },
    ""cache_dir"": {
      ""type"": ""str"",
      ""optional"": true,
      ""description"": ""Directory to read/write data.""
    },
    ""features"": {
      ""type"": ""Features"",
      ""optional"": true,
      ""description"": ""Set the features type to use for this dataset.""
    },
    ""download_config"": {
      ""type"": ""DownloadConfig"",
      ""optional"": true,
      ""description"": ""Specific download configuration parameters.""
    },
    ""download_mode"": {
      ""type"": ""DownloadMode or str"",
      ""default"": ""REUSE_DATASET_IF_EXISTS"",
      ""description"": ""Download/generate mode.""
    },
    ""verification_mode"": {
      ""type"": ""VerificationMode or str"",
      ""default"": ""BASIC_CHECKS"",
      ""description"": ""Verification mode determining the checks to run on the downloaded/processed dataset information.""
    },
    ""ignore_verifications"": {
      ""type"": ""bool"",
      ""default"": ""False"",
      ""deprecated_in"": ""2.9.1"",
      ""description"": ""Ignore the verifications of the downloaded/processed dataset information.""
    },
    ""keep_in_memory"": {
      ""type"": ""bool"",
      ""default"": ""None"",
      ""description"": ""Whether to copy the dataset in-memory.""
    },
    ""save_infos"": {
      ""type"": ""bool"",
      ""default"": ""False"",
      ""description"": ""Save the dataset information.""
    },
    ""revision"": {
      ""type"": ""Version or str"",
      ""optional"": true,
      ""description"": ""Version of the dataset script to load.""
    },
    ""token"": {
      ""type"": ""str or bool"",
      ""optional"": true,
      ""description"": ""Bearer token for remote files on the Datasets Hub.""
    },
    ""use_auth_token"": {
      ""type"": ""str or bool"",
      ""optional"": true,
      ""deprecated_in"": ""2.14.0"",
      ""description"": ""Bearer token for remote files on the Datasets Hub.""
    },
    ""task"": {
      ""type"": ""str"",
      ""deprecated_in"": ""2.13.0"",
      ""description"": ""The task to prepare the dataset for during training and evaluation.""
    },
    ""streaming"": {
      ""type"": ""bool"",
      ""default"": ""False"",
      ""description"": ""If set to True, stream the data progressively while iterating on the dataset.""
    },
    ""num_proc"": {
      ""type"": ""int"",
      ""optional"": true,
      ""default"": ""None"",
      ""description"": ""Number of processes when downloading and generating the dataset locally.""
    },
    ""storage_options"": {
      ""type"": ""dict"",
      ""optional"": true,
      ""default"": ""None"",
      ""description"": ""Key/value pairs to be passed on to the dataset file-system backend, if any.""
    },
    ""config_kwargs"": {
      ""type"": ""kwargs"",
      ""description"": ""Keyword arguments to be passed to the BuilderConfig and used in the DatasetBuilder.""
    }
  },
  ""returns"": [
    {
      ""type"": ""Dataset or DatasetDict"",
      ""condition"": ""if split is not None"",
      ""description"": ""The dataset requested.""
    },
    {
      ""type"": ""DatasetDict"",
      ""condition"": ""if split is None"",
      ""description"": ""A DatasetDict with each split.""
    },
    {
      ""type"": ""IterableDataset or IterableDatasetDict"",
      ""condition"": ""if streaming=True"",
      ""description"": ""An IterableDataset or IterableDatasetDict.""
    }
  ],
  ""examples"": [
    {
      ""description"": ""Load a dataset from the Hugging Face Hub"",
      ""code"": ""ds = load_dataset('rotten_tomatoes', split='train')""
    },
    {
      ""description"": ""Map data files to splits"",
      ""code"": ""data_files = {'train': 'train.csv', 'test': 'test.csv'}\nds = load_dataset('namespace/your_dataset_name', data_files=data_files)""
    },
    {
      ""description"": ""Load a local CSV file"",
      ""code"": ""ds = load_dataset('csv', data_files='path/to/local/my_dataset.csv')""
    },
    {
      ""description"": ""Load a local JSON file"",
      ""code"": ""ds = load_dataset('json', data_files='path/to/local/my_dataset.json')""
    },
    {
      ""description"": ""Load from a local loading script"",
      ""code"": ""ds = load_dataset('path/to/local/loading_script/loading_script.py', split='train')""
    },
    {
      ""description"": ""Load an IterableDataset"",
      ""code"": ""ds = load_dataset('rotten_tomatoes', split='train', streaming=True)""
    },
    {
      ""description"": ""Load an image dataset with the ImageFolder dataset builder"",
      ""code"": ""ds = load_dataset('imagefolder', data_dir='/path/to/images', split='train')""
    }
  ]
}
```

This JSON represents the structure of the `load_dataset` API function, its arguments, return types, and examples for usage as extracted from the documentation."