{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1e0bXCJD1hX7iSvmcxnRhQaXuSoUxXEth","authorship_tag":"ABX9TyPjb29GzICKoZE7+1XvzGr0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":4,"metadata":{"id":"JNeCGXi7BYO3","executionInfo":{"status":"ok","timestamp":1748991699920,"user_tz":420,"elapsed":5,"user":{"displayName":"Neeharika Gupta","userId":"13434636909198909500"}}},"outputs":[],"source":["import pandas as pd\n","import re"]},{"cell_type":"code","source":["df_gemini = pd.read_csv(\"/content/drive/MyDrive/Stanford NLP/final_generated_code_gemini.csv\")\n","df_gemini.head(2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":112},"id":"9yyzCGh4BgAa","executionInfo":{"status":"ok","timestamp":1748991688099,"user_tz":420,"elapsed":6594,"user":{"displayName":"Neeharika Gupta","userId":"13434636909198909500"}},"outputId":"1556f553-e47d-42bc-e947-ce1f1f57b465"},"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                        final_gemini\n","0  ```python\\nimport tensorflow as tf\\nfrom sklea...\n","1  ```python\\nimport torch\\nimport torch.nn as nn..."],"text/html":["\n","  <div id=\"df-81d205a0-136f-40a2-8cc5-2230e5caddda\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>final_gemini</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>```python\\nimport tensorflow as tf\\nfrom sklea...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>```python\\nimport torch\\nimport torch.nn as nn...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-81d205a0-136f-40a2-8cc5-2230e5caddda')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-81d205a0-136f-40a2-8cc5-2230e5caddda button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-81d205a0-136f-40a2-8cc5-2230e5caddda');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    <div id=\"df-e69e1596-ac63-44b3-9f2e-27ab3613cba6\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e69e1596-ac63-44b3-9f2e-27ab3613cba6')\"\n","                title=\"Suggest charts\"\n","                style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","      <script>\n","        async function quickchart(key) {\n","          const quickchartButtonEl =\n","            document.querySelector('#' + key + ' button');\n","          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","          quickchartButtonEl.classList.add('colab-df-spinner');\n","          try {\n","            const charts = await google.colab.kernel.invokeFunction(\n","                'suggestCharts', [key], {});\n","          } catch (error) {\n","            console.error('Error during call to suggestCharts:', error);\n","          }\n","          quickchartButtonEl.classList.remove('colab-df-spinner');\n","          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","        }\n","        (() => {\n","          let quickchartButtonEl =\n","            document.querySelector('#df-e69e1596-ac63-44b3-9f2e-27ab3613cba6 button');\n","          quickchartButtonEl.style.display =\n","            google.colab.kernel.accessAllowed ? 'block' : 'none';\n","        })();\n","      </script>\n","    </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"df_gemini","summary":"{\n  \"name\": \"df_gemini\",\n  \"rows\": 8,\n  \"fields\": [\n    {\n      \"column\": \"final_gemini\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"```python\\nimport torch\\nimport torch.nn as nn\\nimport torch.optim as optim\\nimport torchvision\\nimport torchvision.transforms as transforms\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\n# 1. Data Loading and Preprocessing\\n# Define transformations for the training and test sets\\ntransform = transforms.Compose([\\n    transforms.ToTensor(),  # Convert PIL Image to PyTorch Tensor\\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) # Normalize pixel values to [-1, 1]\\n])\\n\\n# Load the CIFAR10 training dataset\\ntrainset = torchvision.datasets.CIFAR10(root='./data', train=True,\\n                                        download=True, transform=transform)\\n\\n# Create a DataLoader for the training set\\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=64,\\n                                          shuffle=True, num_workers=2)\\n\\n# Load the CIFAR10 test dataset\\ntestset = torchvision.datasets.CIFAR10(root='./data', train=False,\\n                                       download=True, transform=transform)\\n\\n# Create a DataLoader for the test set\\ntestloader = torch.utils.data.DataLoader(testset, batch_size=64,\\n                                         shuffle=False, num_workers=2)\\n\\n# Define the 10 classes in CIFAR10\\nclasses = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\\n\\n# Optional: Show some images (helper function from PyTorch tutorial)\\ndef imshow(img):\\n    img = img / 2 + 0.5     # unnormalize\\n    npimg = img.numpy()\\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\\n    plt.show()\\n\\n# Get some random training images\\n# dataiter = iter(trainloader)\\n# images, labels = next(dataiter)\\n#\\n# # show images\\n# imshow(torchvision.utils.make_grid(images[:4]))\\n# # print labels\\n# print(' '.join(f'{classes[labels[j]]:5s}' for j in range(4)))\\n\\n\\n# 2. Model Definition (Convolutional Neural Network)\\nclass Net(nn.Module):\\n    def __init__(self):\\n        super().__init__()\\n        # First convolutional block\\n        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1) # 3 input channels (RGB), 32 output channels\\n        self.relu1 = nn.ReLU()\\n        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2) # Max pooling layer\\n\\n        # Second convolutional block\\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1) # 32 input channels, 64 output channels\\n        self.relu2 = nn.ReLU()\\n        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\\n\\n        # Third convolutional block\\n        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1) # 64 input channels, 128 output channels\\n        self.relu3 = nn.ReLU()\\n        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2) # Output size before flatten: 128 channels, (32/2)/2/2 = 4x4\\n\\n        # Fully connected layers\\n        self.fc1 = nn.Linear(128 * 4 * 4, 512) # Flatten the output of conv layers (128 channels * 4x4 size)\\n        self.relu4 = nn.ReLU()\\n        self.fc2 = nn.Linear(512, 10) # Output layer with 10 classes\\n\\n    def forward(self, x):\\n        x = self.pool1(self.relu1(self.conv1(x)))\\n        x = self.pool2(self.relu2(self.conv2(x)))\\n        x = self.pool3(self.relu3(self.conv3(x)))\\n        x = torch.flatten(x, 1) # Flatten all dimensions except batch\\n        x = self.relu4(self.fc1(x))\\n        x = self.fc2(x)\\n        return x\\n\\n# Instantiate the network\\nnet = Net()\\n\\n# Check if CUDA is available and move the model to GPU\\ndevice = torch.device(\\\"cuda:0\\\" if torch.cuda.is_available() else \\\"cpu\\\")\\nprint(f\\\"Using device: {device}\\\")\\nnet.to(device)\\n\\n\\n# 3. Loss Function and Optimizer\\ncriterion = nn.CrossEntropyLoss() # Loss function suitable for classification\\noptimizer = optim.Adam(net.parameters(), lr=0.001) # Adam optimizer\\n\\n\\n# 4. Training Loop\\nprint(\\\"Starting training...\\\")\\nepochs = 10 # Number of training epochs\\nrunning_loss_history = [] # To store loss over batches for plotting/tracking\\nrunning_correct_history = [] # To store number of correct predictions\\n\\nfor epoch in range(epochs):  # loop over the dataset multiple times\\n\\n    running_loss = 0.0\\n    running_correct = 0\\n    total = 0\\n\\n    for i, data in enumerate(trainloader, 0):\\n        # get the inputs; data is a list of [inputs, labels]\\n        inputs, labels = data[0].to(device), data[1].to(device)\\n\\n        # zero the parameter gradients\\n        optimizer.zero_grad()\\n\\n        # forward + backward + optimize\\n        outputs = net(inputs)\\n        loss = criterion(outputs, labels)\\n        loss.backward()\\n        optimizer.step()\\n\\n        # print statistics\\n        running_loss += loss.item()\\n\\n        # Calculate accuracy for the batch\\n        _, predicted = torch.max(outputs.data, 1)\\n        total += labels.size(0)\\n        running_correct += (predicted == labels).sum().item()\\n\\n        # Print every 100 batches\\n        if i % 100 == 99:\\n            print(f'Epoch [{epoch + 1}/{epochs}], Batch [{i + 1}/{len(trainloader)}], Loss: {running_loss / 100:.3f}')\\n            running_loss_history.append(running_loss / 100)\\n            running_correct_history.append(running_correct)\\n            running_loss = 0.0\\n            running_correct = 0\\n            total = 0 # Reset total for this mini-batch accuracy report scope\\n\\n    print(f'End of Epoch {epoch + 1}, Total Epoch Loss: {running_loss_history[-1] if running_loss_history else \\\"N/A\\\"}') # Print last reported loss\\n\\nprint('Finished Training')\\n\\n# Optional: Save the trained model\\n# PATH = './cifar_net.pth'\\n# torch.save(net.state_dict(), PATH)\\n\\n\\n# 5. Evaluation\\nprint(\\\"Starting evaluation...\\\")\\ncorrect = 0\\ntotal = 0\\n# since we're not training, we don't need to calculate the gradients for our outputs\\nwith torch.no_grad(): # Disable gradient calculation\\n    for data in testloader:\\n        images, labels = data[0].to(device), data[1].to(device)\\n        # calculate outputs by running images through the network\\n        outputs = net(images)\\n        # the class with the highest energy is what we choose as prediction\\n        _, predicted = torch.max(outputs.data, 1)\\n        total += labels.size(0)\\n        correct += (predicted == labels).sum().item()\\n\\nprint(f'Accuracy of the network on the 10000 test images: {100 * correct / total:.2f} %')\\n\\n# Optional: Evaluation per class\\nprint(\\\"Starting per-class evaluation...\\\")\\nclass_correct = list(0. for i in range(10))\\nclass_total = list(0. for i in range(10))\\nwith torch.no_grad():\\n    for data in testloader:\\n        images, labels = data[0].to(device), data[1].to(device)\\n        outputs = net(images)\\n        _, predicted = torch.max(outputs, 1)\\n        c = (predicted == labels).squeeze()\\n        for i in range(len(labels)):\\n            label = labels[i]\\n            class_correct[label] += c[i].item()\\n            class_total[label] += 1\\n\\nfor i in range(10):\\n    print(f'Accuracy of {classes[i]:5s} : {100 * class_correct[i] / class_total[i]:.2f} %')\\n\\n# Optional: Plotting loss (example, requires storing loss history)\\n# if running_loss_history:\\n#     plt.figure(figsize=(10, 5))\\n#     plt.plot(running_loss_history)\\n#     plt.title(\\\"Running Training Loss (per 100 batches)\\\")\\n#     plt.xlabel(\\\"Batch (x100)\\\")\\n#     plt.ylabel(\\\"Loss\\\")\\n#     plt.show()\\n\\n```\",\n          \"```python\\nimport torch\\nimport torch.utils.data\\nimport torchvision\\nimport torchvision.transforms as transforms\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\n# Define transformations to apply to the images\\n# FashionMNIST images are PIL Images, transforms.ToTensor() converts them to PyTorch tensors\\n# and scales pixel values from [0, 255] to [0.0, 1.0]\\ntransform = transforms.Compose([\\n    transforms.ToTensor()\\n])\\n\\n# Load the FashionMNIST dataset\\n# torchvision.datasets.FashionMNIST is already a subclass of torch.utils.data.Dataset\\n# It handles downloading the data and loading it.\\n# 'root' specifies the directory to download the data to.\\n# 'train=True' gets the training set, 'train=False' would get the test set.\\n# 'download=True' downloads the dataset if it's not already in the root directory.\\n# 'transform' applies the defined transformations to the images.\\ntrain_dataset = torchvision.datasets.FashionMNIST(\\n    root='./data',\\n    train=True,\\n    download=True,\\n    transform=transform\\n)\\n\\n# Create a DataLoader for batching\\n# The DataLoader wraps the Dataset and provides an iterable over the dataset.\\n# 'dataset' is the Dataset object (in this case, train_dataset).\\n# 'batch_size' specifies the number of samples per batch.\\n# 'shuffle=True' shuffles the data at the beginning of each epoch (good for training).\\n# 'num_workers' can be used to load data in parallel using multiple processes (speeds up loading).\\nbatch_size = 64\\ntrain_loader = torch.utils.data.DataLoader(\\n    dataset=train_dataset,\\n    batch_size=batch_size,\\n    shuffle=True,\\n    num_workers=2 # Use more workers if available and necessary\\n)\\n\\n# --- Demonstrate iterating through the DataLoader ---\\n\\nprint(f\\\"Number of training samples: {len(train_dataset)}\\\")\\nprint(f\\\"Batch size: {batch_size}\\\")\\nprint(f\\\"Number of batches: {len(train_loader)}\\\")\\n\\n# Get one batch from the DataLoader\\n# Iterating the loader yields batches of data (images and labels)\\ndataiter = iter(train_loader)\\nimages, labels = next(dataiter)\\n\\nprint(f\\\"\\\\nShape of images batch: {images.shape}\\\") # [batch_size, channels, height, width] -> [64, 1, 28, 28] for FashionMNIST\\nprint(f\\\"Shape of labels batch: {labels.shape}\\\") # [batch_size] -> [64]\\n\\n# Example: Visualize some images from the batch\\n# We need to remove the channel dimension (which is 1) for plotting grayscale images\\n# and convert the tensor back to a numpy array.\\n# FashionMNIST labels are integers from 0 to 9 corresponding to different clothing types.\\n# Let's define the class names for better understanding.\\nclass_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\\n               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\\n\\n# Helper function to show an image\\ndef imshow(img):\\n    # img has shape [channels, height, width]. For grayscale, channel is 1.\\n    # Convert to numpy array and transpose to [height, width, channels] or [height, width] for grayscale\\n    npimg = img.numpy()\\n    # For grayscale, squeeze the channel dimension\\n    plt.imshow(np.squeeze(npimg), cmap='gray')\\n    plt.axis('off')\\n\\n# Plot the first few images in the batch\\nfig = plt.figure(figsize=(10, 5))\\nfor i in range(6):\\n    plt.subplot(2, 3, i + 1)\\n    imshow(images[i])\\n    plt.title(class_names[labels[i]])\\n\\nplt.suptitle(\\\"Examples from one batch of FashionMNIST\\\")\\nplt.show()\\n\\n# Note: While the request asked to load from a folder using a custom Dataset,\\n# the standard way to use FashionMNIST in PyTorch is via torchvision.datasets.FashionMNIST,\\n# which is *already* a subclass of torch.utils.data.Dataset and handles data loading\\n# from the downloaded files internally, abstracting away the folder structure.\\n# The code above demonstrates the standard usage pattern for FashionMNIST with DataLoader.\\n```\",\n          \"```python\\nimport tensorflow as tf\\nfrom sklearn.datasets import fetch_california_housing\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.preprocessing import StandardScaler\\nimport numpy as np\\n\\ndef train_california_housing_linear_model():\\n    \\\"\\\"\\\"\\n    Loads the California Housing dataset, preprocesses it, builds and\\n    trains a simple linear regression model using TensorFlow Keras.\\n\\n    Returns:\\n        tuple: A tuple containing the trained TensorFlow Keras Model\\n               and the training History object.\\n    \\\"\\\"\\\"\\n    print(\\\"Loading California Housing dataset...\\\")\\n    # 1. Load the dataset\\n    housing = fetch_california_housing()\\n    X, y = housing.data, housing.target\\n    feature_names = housing.feature_names\\n\\n    print(f\\\"Dataset shape: X={X.shape}, y={y.shape}\\\")\\n    print(f\\\"Feature names: {feature_names}\\\")\\n\\n    # 2. Split the data into training and testing sets\\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\\n    print(f\\\"Training data shape: X_train={X_train.shape}, y_train={y_train.shape}\\\")\\n    print(f\\\"Testing data shape: X_test={X_test.shape}, y_test={y_test.shape}\\\")\\n\\n    # 3. Scale the features\\n    # Neural networks benefit from feature scaling\\n    print(\\\"Scaling features...\\\")\\n    scaler = StandardScaler()\\n    X_train_scaled = scaler.fit_transform(X_train)\\n    X_test_scaled = scaler.transform(X_test) # Use transform, not fit_transform, on test data\\n\\n    print(\\\"Building the linear regression model...\\\")\\n    # 4. Build the simple linear regression model (one Dense layer)\\n    # This model will have input shape equal to the number of features,\\n    # and output shape 1 for the regression target.\\n    model = tf.keras.Sequential([\\n        tf.keras.layers.Input(shape=(X_train_scaled.shape[1],)), # Input layer matching number of features\\n        tf.keras.layers.Dense(units=1, name='linear_output') # Single dense layer with 1 unit for regression\\n    ])\\n\\n    # 5. Compile the model\\n    # Use Mean Squared Error (MSE) as the loss function for regression\\n    # Use an optimizer like Adam\\n    # Monitor Mean Absolute Error (MAE) during training\\n    print(\\\"Compiling the model...\\\")\\n    model.compile(optimizer='adam',\\n                  loss='mse',       # Mean Squared Error is common for regression\\n                  metrics=['mae'])  # Mean Absolute Error is also a useful metric\\n\\n    print(\\\"Model Summary:\\\")\\n    model.summary()\\n\\n    # 6. Train the model\\n    print(\\\"Training the model...\\\")\\n    # Train on the scaled training data, validate on the scaled test data\\n    # Set verbose=1 to see training progress per epoch\\n    history = model.fit(X_train_scaled, y_train,\\n                        epochs=50, # Number of training epochs\\n                        validation_data=(X_test_scaled, y_test),\\n                        verbose=1)\\n\\n    print(\\\"\\\\nTraining finished.\\\")\\n\\n    # 7. Evaluate the model on the test data\\n    print(\\\"\\\\nEvaluating the model on the test data...\\\")\\n    loss, mae = model.evaluate(X_test_scaled, y_test, verbose=0)\\n    print(f\\\"Final Test Loss (MSE): {loss:.4f}\\\")\\n    print(f\\\"Final Test MAE: {mae:.4f}\\\")\\n\\n    return model, history\\n\\n# Example usage:\\nif __name__ == \\\"__main__\\\":\\n    # This block will run when the script is executed directly\\n    trained_model, training_history = train_california_housing_linear_model()\\n\\n    # You can now use the trained_model for predictions\\n    # Example: Make a prediction on the first few scaled test samples\\n    # Note: For real prediction, you would need to scale new data using the same scaler\\n    # print(\\\"\\\\nMaking predictions on first 5 test samples:\\\")\\n    # sample_predictions = trained_model.predict(X_test_scaled[:5])\\n    # print(\\\"Predictions:\\\", sample_predictions.flatten())\\n    # print(\\\"Actual values:\\\", y_test[:5])\\n\\n    # You can also access training history (e.g., to plot loss curves)\\n    # print(\\\"\\\\nTraining history keys:\\\", training_history.history.keys())\\n```\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["df_gpt = pd.read_csv(\"/content/drive/MyDrive/Stanford NLP/final_generated_code_gpt.csv\")\n","df_gpt.head(2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":112},"id":"LazDKWphB3q-","executionInfo":{"status":"ok","timestamp":1748991719181,"user_tz":420,"elapsed":455,"user":{"displayName":"Neeharika Gupta","userId":"13434636909198909500"}},"outputId":"adda7230-1540-474a-9b9e-09a64c3cbdf3"},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                           final_gpt\n","0  To create a simple linear regression model usi...\n","1  To solve the task of training a convolutional ..."],"text/html":["\n","  <div id=\"df-019a0eb0-a871-4324-b2ec-abefb1f491c3\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>final_gpt</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>To create a simple linear regression model usi...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>To solve the task of training a convolutional ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-019a0eb0-a871-4324-b2ec-abefb1f491c3')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-019a0eb0-a871-4324-b2ec-abefb1f491c3 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-019a0eb0-a871-4324-b2ec-abefb1f491c3');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    <div id=\"df-d991eb38-bd46-4fc9-b99c-d7538c1181a4\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d991eb38-bd46-4fc9-b99c-d7538c1181a4')\"\n","                title=\"Suggest charts\"\n","                style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","      <script>\n","        async function quickchart(key) {\n","          const quickchartButtonEl =\n","            document.querySelector('#' + key + ' button');\n","          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","          quickchartButtonEl.classList.add('colab-df-spinner');\n","          try {\n","            const charts = await google.colab.kernel.invokeFunction(\n","                'suggestCharts', [key], {});\n","          } catch (error) {\n","            console.error('Error during call to suggestCharts:', error);\n","          }\n","          quickchartButtonEl.classList.remove('colab-df-spinner');\n","          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","        }\n","        (() => {\n","          let quickchartButtonEl =\n","            document.querySelector('#df-d991eb38-bd46-4fc9-b99c-d7538c1181a4 button');\n","          quickchartButtonEl.style.display =\n","            google.colab.kernel.accessAllowed ? 'block' : 'none';\n","        })();\n","      </script>\n","    </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"df_gpt","summary":"{\n  \"name\": \"df_gpt\",\n  \"rows\": 8,\n  \"fields\": [\n    {\n      \"column\": \"final_gpt\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"To solve the task of training a convolutional neural network (CNN) on the CIFAR-10 dataset using PyTorch, you can follow the complete code below. This script covers data loading, model definition, training, and evaluation.\\n\\n```python\\nimport torch\\nimport torch.nn as nn\\nimport torch.optim as optim\\nimport torchvision\\nimport torchvision.transforms as transforms\\nfrom torch.utils.data import DataLoader\\n\\n# Define the CNN model\\nclass SimpleCNN(nn.Module):\\n    def __init__(self):\\n        super(SimpleCNN, self).__init__()\\n        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\\n        self.fc1 = nn.Linear(64 * 8 * 8, 512)\\n        self.fc2 = nn.Linear(512, 10)\\n        self.relu = nn.ReLU()\\n        self.dropout = nn.Dropout(0.5)\\n\\n    def forward(self, x):\\n        x = self.pool(self.relu(self.conv1(x)))\\n        x = self.pool(self.relu(self.conv2(x)))\\n        x = x.view(-1, 64 * 8 * 8)\\n        x = self.relu(self.fc1(x))\\n        x = self.dropout(x)\\n        x = self.fc2(x)\\n        return x\\n\\n# Device configuration\\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\\n\\n# Hyperparameters\\nnum_epochs = 10\\nbatch_size = 100\\nlearning_rate = 0.001\\n\\n# Data augmentations and normalization\\ntransform = transforms.Compose(\\n    [transforms.RandomHorizontalFlip(), \\n     transforms.RandomCrop(32, padding=4),\\n     transforms.ToTensor(),\\n     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))])\\n\\n# Load the CIFAR-10 dataset\\ntrain_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\\ntest_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\\n\\ntrain_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\\ntest_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\\n\\n# Initialize the model, loss function and optimizer\\nmodel = SimpleCNN().to(device)\\ncriterion = nn.CrossEntropyLoss()\\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\\n\\n# Training loop\\nfor epoch in range(num_epochs):\\n    model.train()\\n    running_loss = 0.0\\n    for images, labels in train_loader:\\n        images, labels = images.to(device), labels.to(device)\\n\\n        # Forward pass\\n        outputs = model(images)\\n        loss = criterion(outputs, labels)\\n\\n        # Backward and optimize\\n        optimizer.zero_grad()\\n        loss.backward()\\n        optimizer.step()\\n\\n        running_loss += loss.item()\\n\\n    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}')\\n\\n# Evaluation\\nmodel.eval()\\nwith torch.no_grad():\\n    correct = 0\\n    total = 0\\n    for images, labels in test_loader:\\n        images, labels = images.to(device), labels.to(device)\\n        outputs = model(images)\\n        _, predicted = torch.max(outputs.data, 1)\\n        total += labels.size(0)\\n        correct += (predicted == labels).sum().item()\\n\\n    print(f'Accuracy of the model on the 10000 test images: {100 * correct / total:.2f}%')\\n```\\n\\n### Key Components:\\n- **Model Definition (SimpleCNN):** A simple CNN architecture with two convolutional layers, a max-pooling layer, and two fully-connected layers.\\n- **Data Loading:** The CIFAR-10 dataset is loaded using torchvision, and data augmentations are applied.\\n- **Training and Evaluation:** The training loop iterates over the data, computes the loss, and backpropagates the error. Evaluation is done on the test dataset to determine accuracy.\\n- **Device Management:** The script leverages GPU acceleration if available. \\n\\nThis code should provide a good starting point for more complex experiments or tuning to achieve better performance.\",\n          \"To load images and labels from a folder using a PyTorch `Dataset` class and utilize a `DataLoader` for batching, you can follow these steps. We will use the FashionMNIST dataset, but assume that the dataset has been downloaded and structured as image files in directories named after the class labels.\\n\\nHere's an example implementation:\\n\\n```python\\nimport os\\nfrom PIL import Image\\nfrom torch.utils.data import Dataset, DataLoader\\nimport torchvision.transforms as transforms\\nfrom torchvision.datasets import FashionMNIST\\nimport torch\\n\\n# Define the custom dataset class\\nclass ImageFolderDataset(Dataset):\\n    def __init__(self, root_dir, transform=None):\\n        \\\"\\\"\\\"\\n        Args:\\n            root_dir (string): Directory with all the images, structured in subdirectories named after class labels.\\n            transform (callable, optional): Optional transform to be applied on a sample.\\n        \\\"\\\"\\\"\\n        self.root_dir = root_dir\\n        self.transform = transform\\n        self.classes = os.listdir(root_dir)\\n        self.image_paths = []\\n        self.labels = []\\n\\n        for idx, class_name in enumerate(self.classes):\\n            class_path = os.path.join(root_dir, class_name)\\n            for img_name in os.listdir(class_path):\\n                img_path = os.path.join(class_path, img_name)\\n                self.image_paths.append(img_path)\\n                self.labels.append(idx)\\n\\n    def __len__(self):\\n        return len(self.image_paths)\\n\\n    def __getitem__(self, idx):\\n        img_path = self.image_paths[idx]\\n        image = Image.open(img_path)\\n        label = self.labels[idx]\\n        \\n        if self.transform:\\n            image = self.transform(image)\\n        \\n        return image, label\\n\\n# Use a transform that is commonly used for FashionMNIST\\ntransform = transforms.Compose([\\n    transforms.Grayscale(), # Ensure the image is in grayscale\\n    transforms.ToTensor(),  # Convert images to PyTorch tensors\\n    transforms.Normalize((0.5,), (0.5,))  # Normalize images like FashionMNIST\\n])\\n\\n# Assuming you have downloaded the FashionMNIST data in the folder 'fashion_mnist'\\n# The 'fashion_mnist' directory should have subdirectories named after classes\\ndataset = ImageFolderDataset(root_dir='fashion_mnist', transform=transform)\\n\\n# Create a DataLoader for batching\\ndataloader = DataLoader(dataset, batch_size=32, shuffle=True)\\n\\n# Example of iterating through the DataLoader\\nfor images, labels in dataloader:\\n    print(images.size(), labels)\\n    # Perform training steps here\\n```\\n\\n### Instructions:\\n\\n1. **Dataset Setup**: Ensure that the images are organized in directories inside `fashion_mnist`, with each directory named after the class (e.g., `0`, `1`, ..., `9` for FashionMNIST).\\n\\n2. **Transforms**: The transformations applied to the images ensure they are prepared similarly to the actual FashionMNIST dataset (grayscale and normalized).\\n\\n3. **DataLoader**: The `DataLoader` is utilized to iterate over batches of images and labels during training or evaluation.\\n\\n4. **Training Loop**: Iterate over `dataloader` to retrieve image-label pairs in batches, suitable for training models.\\n\\nNote: Before running code in a real scenario, be sure to have the FashionMNIST dataset (or images structured similarly) stored in the specified directory path.\",\n          \"To create a simple linear regression model using TensorFlow and train it on the California housing dataset, follow the steps below. Ensure you have TensorFlow and Scikit-learn installed in your environment. If not, you can install them using pip:\\n```bash\\npip install tensorflow scikit-learn\\n```\\n\\nHere's a complete code snippet for your task:\\n\\n```python\\nimport numpy as np\\nimport tensorflow as tf\\nfrom sklearn.datasets import fetch_california_housing\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.preprocessing import StandardScaler\\n\\ndef train_linear_regression_model():\\n    # Load the California housing dataset\\n    housing_data = fetch_california_housing()\\n    X, y = housing_data.data, housing_data.target\\n\\n    # Split the dataset into training and testing sets\\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\\n\\n    # Preprocess the dataset by scaling it\\n    scaler = StandardScaler()\\n    X_train_scaled = scaler.fit_transform(X_train)\\n    X_test_scaled = scaler.transform(X_test)\\n\\n    # Build a simple linear regression model using TensorFlow\\n    model = tf.keras.models.Sequential([\\n        tf.keras.layers.Dense(1, input_shape=(X_train.shape[1],), activation='linear')\\n    ])\\n\\n    # Compile the model\\n    model.compile(optimizer='sgd', loss='mean_squared_error', metrics=['mae'])\\n\\n    # Train the model\\n    model.fit(X_train_scaled, y_train, epochs=100, batch_size=32, validation_split=0.2)\\n\\n    # Evaluate the model on the test set\\n    test_loss, test_mae = model.evaluate(X_test_scaled, y_test)\\n    print(f'Test Loss: {test_loss}, Test MAE: {test_mae}')\\n\\ntrain_linear_regression_model()\\n```\\n\\n### Explanation:\\n- **Dataset Loading**: We use the California housing dataset from `sklearn.datasets`.\\n- **Data Splitting**: Split the dataset into training and test subsets using `train_test_split`.\\n- **Data Preprocessing**: Standardize the data using `StandardScaler` for better convergence.\\n- **Model Building**: Create a sequential TensorFlow model with one dense layer, appropriate for linear regression.\\n- **Model Compilation**: Compile the model using Stochastic Gradient Descent (SGD) and the Mean Squared Error (MSE) loss.\\n- **Training**: Train the model with the training set for 100 epochs, using a validation split for monitoring training.\\n- **Evaluation**: Finally, evaluate the trained model's performance on a separate test set.\\n\\nThis code should help you to understand the workflow of creating, compiling, training, and evaluating a simple linear regression model using TensorFlow.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["def extract_python_code(text):\n","    code_blocks = re.findall(r\"```(?:python)?(.*?)```\", text, re.DOTALL)\n","    code_blocks = [code.strip() for code in code_blocks]\n","    return code_blocks[0]"],"metadata":{"id":"e7YuMTAIBuhV","executionInfo":{"status":"ok","timestamp":1748992184580,"user_tz":420,"elapsed":14,"user":{"displayName":"Neeharika Gupta","userId":"13434636909198909500"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["gemini_code = df_gemini['final_gemini'].apply(extract_python_code)\n","gemini_code"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":335},"id":"dTOIjRgjB8lQ","executionInfo":{"status":"ok","timestamp":1748992186924,"user_tz":420,"elapsed":5,"user":{"displayName":"Neeharika Gupta","userId":"13434636909198909500"}},"outputId":"9b40aca8-10ec-424b-b753-40fa688cf68f"},"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0    import tensorflow as tf\\nfrom sklearn.datasets...\n","1    import torch\\nimport torch.nn as nn\\nimport to...\n","2    import numpy as np\\nfrom sklearn.datasets impo...\n","3    import torch\\nfrom datasets import load_datase...\n","4    import tensorflow as tf\\nfrom tensorflow.keras...\n","5    import torch\\nimport torch.utils.data\\nimport ...\n","6    import matplotlib.pyplot as plt\\nimport numpy ...\n","7    java\\nimport java.io.IOException;\\nimport java...\n","Name: final_gemini, dtype: object"],"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>final_gemini</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>import tensorflow as tf\\nfrom sklearn.datasets...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>import torch\\nimport torch.nn as nn\\nimport to...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>import numpy as np\\nfrom sklearn.datasets impo...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>import torch\\nfrom datasets import load_datase...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>import tensorflow as tf\\nfrom tensorflow.keras...</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>import torch\\nimport torch.utils.data\\nimport ...</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>import matplotlib.pyplot as plt\\nimport numpy ...</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>java\\nimport java.io.IOException;\\nimport java...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div><br><label><b>dtype:</b> object</label>"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["gemini_code.to_csv(\"/content/drive/MyDrive/Stanford NLP/final_code_gemini.csv\")"],"metadata":{"id":"WdfeFrRaDzp4","executionInfo":{"status":"ok","timestamp":1748992249208,"user_tz":420,"elapsed":16,"user":{"displayName":"Neeharika Gupta","userId":"13434636909198909500"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["gpt_code = df_gpt['final_gpt'].apply(extract_python_code)\n","gpt_code"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":335},"id":"YaPn56JkCBJZ","executionInfo":{"status":"ok","timestamp":1748992190138,"user_tz":420,"elapsed":2,"user":{"displayName":"Neeharika Gupta","userId":"13434636909198909500"}},"outputId":"899f7712-af38-4d36-c7a7-c91b444392a6"},"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0            bash\\npip install tensorflow scikit-learn\n","1    import torch\\nimport torch.nn as nn\\nimport to...\n","2    from sklearn.datasets import make_classificati...\n","3    from datasets import load_dataset\\nfrom transf...\n","4    import tensorflow as tf\\nfrom tensorflow.keras...\n","5    import os\\nfrom PIL import Image\\nfrom torch.u...\n","6    from sklearn.datasets import load_iris\\nfrom s...\n","7    java\\nimport org.apache.hadoop.conf.Configurat...\n","Name: final_gpt, dtype: object"],"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>final_gpt</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>bash\\npip install tensorflow scikit-learn</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>import torch\\nimport torch.nn as nn\\nimport to...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>from sklearn.datasets import make_classificati...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>from datasets import load_dataset\\nfrom transf...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>import tensorflow as tf\\nfrom tensorflow.keras...</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>import os\\nfrom PIL import Image\\nfrom torch.u...</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>from sklearn.datasets import load_iris\\nfrom s...</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>java\\nimport org.apache.hadoop.conf.Configurat...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div><br><label><b>dtype:</b> object</label>"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["gpt_code.to_csv(\"/content/drive/MyDrive/Stanford NLP/final_code_gpt.csv\")"],"metadata":{"id":"7d4hJNrfCV9D","executionInfo":{"status":"ok","timestamp":1748992316996,"user_tz":420,"elapsed":14,"user":{"displayName":"Neeharika Gupta","userId":"13434636909198909500"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"WP2xPf5WENW_"},"execution_count":null,"outputs":[]}]}