{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"7eb4307a88714f06a63ab71ff3100783":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4e5464d12c2d442097de74e8a7ac51ea","IPY_MODEL_614c00bd427f4104a0075daffbeb8ecf","IPY_MODEL_6364b723ce604422950be8efd9f71cd5"],"layout":"IPY_MODEL_89dfdd4651db47ac88b16dd4a1c4ed2a"}},"4e5464d12c2d442097de74e8a7ac51ea":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2c35bc81b1014971a0712c22f5bee48d","placeholder":"​","style":"IPY_MODEL_062f2cc10ce9497ab6734d61a596eb20","value":"model.safetensors: 100%"}},"614c00bd427f4104a0075daffbeb8ecf":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_44f9d09006dd4e06b413eaa29607bf01","max":267954768,"min":0,"orientation":"horizontal","style":"IPY_MODEL_74ca548efdac4623b1ce0596f8eae619","value":267954768}},"6364b723ce604422950be8efd9f71cd5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_06771cca0a314f90b6d5b963b14ac178","placeholder":"​","style":"IPY_MODEL_1a0d64e4b6724c208381df274be59917","value":" 268M/268M [00:10&lt;00:00, 52.3MB/s]"}},"89dfdd4651db47ac88b16dd4a1c4ed2a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2c35bc81b1014971a0712c22f5bee48d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"062f2cc10ce9497ab6734d61a596eb20":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"44f9d09006dd4e06b413eaa29607bf01":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"74ca548efdac4623b1ce0596f8eae619":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"06771cca0a314f90b6d5b963b14ac178":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1a0d64e4b6724c208381df274be59917":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"13ecc32830da4020a36162ce7fbcb6ac":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_62fde27d6b904dc6b72ee1047f5b5cfb","IPY_MODEL_17064b6c83be49c78b53a1d8520855e3","IPY_MODEL_72afd0236e104fd7a930ab7b47f75cc3"],"layout":"IPY_MODEL_69e04332a1eb46c197e5b1d33d239e3f"}},"62fde27d6b904dc6b72ee1047f5b5cfb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_72254daf93c94937953ab64c52df0b37","placeholder":"​","style":"IPY_MODEL_20b15c8bc1f04ca188114edb33844445","value":"tokenizer_config.json: 100%"}},"17064b6c83be49c78b53a1d8520855e3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1781b17856844f16b22661ed44a37b90","max":48,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3e66456a4815455d94bcbeb9458809f5","value":48}},"72afd0236e104fd7a930ab7b47f75cc3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ba78e72c404349fb96dcef24d9d73c32","placeholder":"​","style":"IPY_MODEL_d8543b42bedc44168e9101c6ec6860b9","value":" 48.0/48.0 [00:00&lt;00:00, 1.39kB/s]"}},"69e04332a1eb46c197e5b1d33d239e3f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"72254daf93c94937953ab64c52df0b37":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"20b15c8bc1f04ca188114edb33844445":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1781b17856844f16b22661ed44a37b90":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3e66456a4815455d94bcbeb9458809f5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ba78e72c404349fb96dcef24d9d73c32":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d8543b42bedc44168e9101c6ec6860b9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ee5ea48ac783492f9001c69e3c498478":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d749c33d1fe4430180be816ac37c669e","IPY_MODEL_1f63183f55904bb1b926622b58b1c454","IPY_MODEL_21d0a5e4e80d437dabac47f58e41c7a7"],"layout":"IPY_MODEL_317394fa447340ec9d7cf7d78a665bb4"}},"d749c33d1fe4430180be816ac37c669e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bd8128b584b14bc89d9b064be3a1a88a","placeholder":"​","style":"IPY_MODEL_8bf5992f399a4c4aa533effeffa72571","value":"vocab.txt: 100%"}},"1f63183f55904bb1b926622b58b1c454":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_be81c802945446fd90afff1ef8ae4300","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a012acba13bf4e54ac36feb1cea4ec6c","value":231508}},"21d0a5e4e80d437dabac47f58e41c7a7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1cdeede1479c4b92b4f12a7e1224dff9","placeholder":"​","style":"IPY_MODEL_4254256c606f4c01987db7bace609dd5","value":" 232k/232k [00:00&lt;00:00, 1.02MB/s]"}},"317394fa447340ec9d7cf7d78a665bb4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bd8128b584b14bc89d9b064be3a1a88a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8bf5992f399a4c4aa533effeffa72571":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"be81c802945446fd90afff1ef8ae4300":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a012acba13bf4e54ac36feb1cea4ec6c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1cdeede1479c4b92b4f12a7e1224dff9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4254256c606f4c01987db7bace609dd5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a5e095472ab74a329b6ae6eca33e9d2a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_929c317a225148cb9f3911fe3925c040","IPY_MODEL_bdba332355ec4bff94f16234a9ed94b5","IPY_MODEL_0d3891d52cf24a5b86a1552e570242e4"],"layout":"IPY_MODEL_0561b064bc1d4263b6f057a8f840226e"}},"929c317a225148cb9f3911fe3925c040":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_29e8a33184494af3ad5b2bc50f0c534a","placeholder":"​","style":"IPY_MODEL_91151ea691c74eac8ae02cbe6a01d567","value":"tokenizer.json: 100%"}},"bdba332355ec4bff94f16234a9ed94b5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_042b22ba4a7d4a9c97f3aca24650c786","max":466062,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d76e3da81efd47c2b80a62de0036eadb","value":466062}},"0d3891d52cf24a5b86a1552e570242e4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ea420509dfbe49c992f53cfb40eaab7e","placeholder":"​","style":"IPY_MODEL_b2ff05fb2df64d79a501fb8282a69f0a","value":" 466k/466k [00:00&lt;00:00, 4.75MB/s]"}},"0561b064bc1d4263b6f057a8f840226e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"29e8a33184494af3ad5b2bc50f0c534a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"91151ea691c74eac8ae02cbe6a01d567":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"042b22ba4a7d4a9c97f3aca24650c786":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d76e3da81efd47c2b80a62de0036eadb":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ea420509dfbe49c992f53cfb40eaab7e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b2ff05fb2df64d79a501fb8282a69f0a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"742f72ce97144839831aca93a71454b1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8420758f2b804c1fa08138cf43ffe2b9","IPY_MODEL_106d33fb5e2a489d9d7c5f0af041eab1","IPY_MODEL_58c627fc9b2c4d08a8d2d5dd7f5f77cf"],"layout":"IPY_MODEL_e5d1a682cd5d4a7dbb036de8ef2cf77e"}},"8420758f2b804c1fa08138cf43ffe2b9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_04af40df51cc4f75b2041d3ea0a7d34e","placeholder":"​","style":"IPY_MODEL_6933e01e194648ce8f3efbb8d1174628","value":"config.json: 100%"}},"106d33fb5e2a489d9d7c5f0af041eab1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7bc2adb12ab742d389469944d514509e","max":483,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4eeaebc336f443a5bf09305e9a893a8f","value":483}},"58c627fc9b2c4d08a8d2d5dd7f5f77cf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dc1df07caeb548acb7d1d6f6635751f7","placeholder":"​","style":"IPY_MODEL_efb64554e0534bdcbee6c8a5714a7bb9","value":" 483/483 [00:00&lt;00:00, 6.10kB/s]"}},"e5d1a682cd5d4a7dbb036de8ef2cf77e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"04af40df51cc4f75b2041d3ea0a7d34e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6933e01e194648ce8f3efbb8d1174628":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7bc2adb12ab742d389469944d514509e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4eeaebc336f443a5bf09305e9a893a8f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"dc1df07caeb548acb7d1d6f6635751f7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"efb64554e0534bdcbee6c8a5714a7bb9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"obAfvGV1l_Ho","executionInfo":{"status":"ok","timestamp":1749025957335,"user_tz":420,"elapsed":913,"user":{"displayName":"Vishesh Agrawal","userId":"16926083185186864017"}}},"outputs":[],"source":["import pandas as pd\n","import re"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QwVGGPL3v2Og","executionInfo":{"status":"ok","timestamp":1749026053291,"user_tz":420,"elapsed":15421,"user":{"displayName":"Vishesh Agrawal","userId":"16926083185186864017"}},"outputId":"a7bf8833-06f5-4d31-82d5-75560b188cbd"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["df = pd.read_csv('/content/drive/MyDrive/Stanford NLP/step2-responses.csv')\n","df"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":893},"id":"CTizWm4Ru5Qg","executionInfo":{"status":"ok","timestamp":1749026064753,"user_tz":420,"elapsed":1398,"user":{"displayName":"Vishesh Agrawal","userId":"16926083185186864017"}},"outputId":"02ebfde0-eb51-4a23-ad08-12407eadae8d"},"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                                Task  \\\n","0  Write a Python function using TensorFlow to cr...   \n","1  Write a Python script using PyTorch to train a...   \n","2  Create a scikit-learn pipeline that generates ...   \n","3  Write a Python function that uses Hugging Face...   \n","4  Build a TensorFlow model for image classificat...   \n","5  Write a PyTorch class that subclasses torch.ut...   \n","6  Write a Python script that loads the Iris data...   \n","7  Write a Java program using the Hadoop MapReduc...   \n","\n","                                 direct_response_gpt  \\\n","0  ```python\\nimport tensorflow as tf\\nfrom sklea...   \n","1  ```python\\nimport torch\\nimport torch.nn as nn...   \n","2  ```python\\nfrom sklearn.datasets import make_c...   \n","3  ```python\\nfrom datasets import load_dataset\\n...   \n","4  ```python\\nimport tensorflow as tf\\nfrom tenso...   \n","5  ```python\\nimport os\\nfrom PIL import Image\\nf...   \n","6  ```python\\nfrom sklearn.datasets import load_i...   \n","7  ```java\\nimport java.io.IOException;\\nimport j...   \n","\n","                                    cot_response_gpt  \\\n","0  To create a simple linear regression model wit...   \n","1  Certainly! Training a convolutional neural net...   \n","2  Certainly! Let's break down the task and under...   \n","3  To train a `distilbert-base-uncased` model usi...   \n","4  To build an image classification model using t...   \n","5  To create a custom PyTorch dataset class for l...   \n","6  Certainly! I'll guide you step-by-step through...   \n","7  To write a Java program using the Hadoop MapRe...   \n","\n","                                        few_shot_gpt  \\\n","0  Sure, I'll guide you through creating and comp...   \n","1  Below is a complete Python script using PyTorc...   \n","2  To create a complete pipeline that includes da...   \n","3  To train a binary classification model using t...   \n","4  To build an image classification model using t...   \n","5  To create a custom `Dataset` class in PyTorch ...   \n","6  Certainly! Here's a Python script that loads t...   \n","7  Below is the Java program using the Hadoop Map...   \n","\n","                              direct_response_gemini  \\\n","0  ```python\\nimport tensorflow as tf\\nimport num...   \n","1  ```python\\nimport torch\\nimport torch.nn as nn...   \n","2  ```python\\nimport numpy as np\\nfrom sklearn.da...   \n","3  ```python\\nimport torch\\nfrom datasets import ...   \n","4  ```python\\nimport tensorflow as tf\\nfrom tenso...   \n","5  ```python\\nimport torch\\nimport os\\nfrom PIL i...   \n","6  ```python\\nfrom sklearn.datasets import load_i...   \n","7  ```java\\nimport java.io.IOException;\\nimport j...   \n","\n","                                 cot_response_gemini  \\\n","0  Okay, let's create a Python function using Ten...   \n","1  Okay, let's build a Python script using PyTorc...   \n","2  Okay, let's build this scikit-learn pipeline s...   \n","3  Okay, let's create a Python function using Hug...   \n","4  Okay, let's build a TensorFlow model for Fashi...   \n","5  Okay, let's create a PyTorch `Dataset` subclas...   \n","6  Okay, let's write a Python script using scikit...   \n","7  Okay, let's break down how to write a Word Cou...   \n","\n","                                     few_shot_gemini  \n","0  Okay, here is the Python function using Tensor...  \n","1  Okay, here is a complete Python script using P...  \n","2  Okay, here is the scikit-learn code to generat...  \n","3  Okay, here is the Python function `train_tense...  \n","4  Okay, here is the TensorFlow code to build an ...  \n","5  Okay, you've provided examples of a custom dat...  \n","6  ```python\\nimport numpy as np\\nimport matplotl...  \n","7  Okay, here is the Java program using the Hadoo...  "],"text/html":["\n","  <div id=\"df-954fc445-1a95-41fc-90ea-3f00f540ad0a\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Task</th>\n","      <th>direct_response_gpt</th>\n","      <th>cot_response_gpt</th>\n","      <th>few_shot_gpt</th>\n","      <th>direct_response_gemini</th>\n","      <th>cot_response_gemini</th>\n","      <th>few_shot_gemini</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Write a Python function using TensorFlow to cr...</td>\n","      <td>```python\\nimport tensorflow as tf\\nfrom sklea...</td>\n","      <td>To create a simple linear regression model wit...</td>\n","      <td>Sure, I'll guide you through creating and comp...</td>\n","      <td>```python\\nimport tensorflow as tf\\nimport num...</td>\n","      <td>Okay, let's create a Python function using Ten...</td>\n","      <td>Okay, here is the Python function using Tensor...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Write a Python script using PyTorch to train a...</td>\n","      <td>```python\\nimport torch\\nimport torch.nn as nn...</td>\n","      <td>Certainly! Training a convolutional neural net...</td>\n","      <td>Below is a complete Python script using PyTorc...</td>\n","      <td>```python\\nimport torch\\nimport torch.nn as nn...</td>\n","      <td>Okay, let's build a Python script using PyTorc...</td>\n","      <td>Okay, here is a complete Python script using P...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Create a scikit-learn pipeline that generates ...</td>\n","      <td>```python\\nfrom sklearn.datasets import make_c...</td>\n","      <td>Certainly! Let's break down the task and under...</td>\n","      <td>To create a complete pipeline that includes da...</td>\n","      <td>```python\\nimport numpy as np\\nfrom sklearn.da...</td>\n","      <td>Okay, let's build this scikit-learn pipeline s...</td>\n","      <td>Okay, here is the scikit-learn code to generat...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Write a Python function that uses Hugging Face...</td>\n","      <td>```python\\nfrom datasets import load_dataset\\n...</td>\n","      <td>To train a `distilbert-base-uncased` model usi...</td>\n","      <td>To train a binary classification model using t...</td>\n","      <td>```python\\nimport torch\\nfrom datasets import ...</td>\n","      <td>Okay, let's create a Python function using Hug...</td>\n","      <td>Okay, here is the Python function `train_tense...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Build a TensorFlow model for image classificat...</td>\n","      <td>```python\\nimport tensorflow as tf\\nfrom tenso...</td>\n","      <td>To build an image classification model using t...</td>\n","      <td>To build an image classification model using t...</td>\n","      <td>```python\\nimport tensorflow as tf\\nfrom tenso...</td>\n","      <td>Okay, let's build a TensorFlow model for Fashi...</td>\n","      <td>Okay, here is the TensorFlow code to build an ...</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>Write a PyTorch class that subclasses torch.ut...</td>\n","      <td>```python\\nimport os\\nfrom PIL import Image\\nf...</td>\n","      <td>To create a custom PyTorch dataset class for l...</td>\n","      <td>To create a custom `Dataset` class in PyTorch ...</td>\n","      <td>```python\\nimport torch\\nimport os\\nfrom PIL i...</td>\n","      <td>Okay, let's create a PyTorch `Dataset` subclas...</td>\n","      <td>Okay, you've provided examples of a custom dat...</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>Write a Python script that loads the Iris data...</td>\n","      <td>```python\\nfrom sklearn.datasets import load_i...</td>\n","      <td>Certainly! I'll guide you step-by-step through...</td>\n","      <td>Certainly! Here's a Python script that loads t...</td>\n","      <td>```python\\nfrom sklearn.datasets import load_i...</td>\n","      <td>Okay, let's write a Python script using scikit...</td>\n","      <td>```python\\nimport numpy as np\\nimport matplotl...</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>Write a Java program using the Hadoop MapReduc...</td>\n","      <td>```java\\nimport java.io.IOException;\\nimport j...</td>\n","      <td>To write a Java program using the Hadoop MapRe...</td>\n","      <td>Below is the Java program using the Hadoop Map...</td>\n","      <td>```java\\nimport java.io.IOException;\\nimport j...</td>\n","      <td>Okay, let's break down how to write a Word Cou...</td>\n","      <td>Okay, here is the Java program using the Hadoo...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-954fc445-1a95-41fc-90ea-3f00f540ad0a')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-954fc445-1a95-41fc-90ea-3f00f540ad0a button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-954fc445-1a95-41fc-90ea-3f00f540ad0a');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    <div id=\"df-6d9e9421-254d-4400-9c47-79b76be16335\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-6d9e9421-254d-4400-9c47-79b76be16335')\"\n","                title=\"Suggest charts\"\n","                style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","      <script>\n","        async function quickchart(key) {\n","          const quickchartButtonEl =\n","            document.querySelector('#' + key + ' button');\n","          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","          quickchartButtonEl.classList.add('colab-df-spinner');\n","          try {\n","            const charts = await google.colab.kernel.invokeFunction(\n","                'suggestCharts', [key], {});\n","          } catch (error) {\n","            console.error('Error during call to suggestCharts:', error);\n","          }\n","          quickchartButtonEl.classList.remove('colab-df-spinner');\n","          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","        }\n","        (() => {\n","          let quickchartButtonEl =\n","            document.querySelector('#df-6d9e9421-254d-4400-9c47-79b76be16335 button');\n","          quickchartButtonEl.style.display =\n","            google.colab.kernel.accessAllowed ? 'block' : 'none';\n","        })();\n","      </script>\n","    </div>\n","\n","  <div id=\"id_36ea1091-8195-4358-8194-35fbfe46e68f\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_36ea1091-8195-4358-8194-35fbfe46e68f button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('df');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"df","summary":"{\n  \"name\": \"df\",\n  \"rows\": 8,\n  \"fields\": [\n    {\n      \"column\": \"Task\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"Write a Python script using PyTorch to train a convolutional neural network (CNN) on the Cifar 10 dataset. Include data loading, model definition, training loop, and evaluation\",\n          \"Write a PyTorch class that subclasses torch.utils.data.Dataset to load image files and labels from a folder. Use this class with a DataLoader for batching. Use FashionMNIST dataset.\",\n          \"Write a Python function using TensorFlow to create and compile a simple linear regression model with one dense layer. Train it on the classic California housing dataset.\\n\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"direct_response_gpt\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"```python\\nimport torch\\nimport torch.nn as nn\\nimport torch.optim as optim\\nimport torchvision\\nimport torchvision.transforms as transforms\\nfrom torch.utils.data import DataLoader\\n\\n# Hyperparameters\\nbatch_size = 32\\nlearning_rate = 0.001\\nnum_epochs = 10\\n\\n# Transformations for the training and test datasets\\ntransform = transforms.Compose([\\n    transforms.ToTensor(),\\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\\n])\\n\\n# Loading the CIFAR-10 dataset\\ntrain_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\\n\\ntest_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\\n\\n# Define a simple CNN model\\nclass SimpleCNN(nn.Module):\\n    def __init__(self):\\n        super(SimpleCNN, self).__init__()\\n        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\\n        self.fc1 = nn.Linear(64 * 8 * 8, 512)\\n        self.fc2 = nn.Linear(512, 10)\\n        self.relu = nn.ReLU()\\n\\n    def forward(self, x):\\n        x = self.pool(self.relu(self.conv1(x)))\\n        x = self.pool(self.relu(self.conv2(x)))\\n        x = x.view(-1, 64 * 8 * 8)\\n        x = self.relu(self.fc1(x))\\n        x = self.fc2(x)\\n        return x\\n\\n# Instantiate the model, define the loss function and the optimizer\\nmodel = SimpleCNN()\\ncriterion = nn.CrossEntropyLoss()\\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\\n\\n# Training loop\\nfor epoch in range(num_epochs):\\n    running_loss = 0.0\\n    for i, (inputs, labels) in enumerate(train_loader):\\n        optimizer.zero_grad()\\n        outputs = model(inputs)\\n        loss = criterion(outputs, labels)\\n        loss.backward()\\n        optimizer.step()\\n\\n        running_loss += loss.item()\\n        if i % 200 == 199:  # Print every 200 mini-batches\\n            print(f'[Epoch {epoch + 1}, Batch {i + 1}] loss: {running_loss / 200:.3f}')\\n            running_loss = 0.0\\n\\n# Evaluation loop\\ncorrect = 0\\ntotal = 0\\nwith torch.no_grad():\\n    for inputs, labels in test_loader:\\n        outputs = model(inputs)\\n        _, predicted = torch.max(outputs.data, 1)\\n        total += labels.size(0)\\n        correct += (predicted == labels).sum().item()\\n\\nprint('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))\\n```\",\n          \"```python\\nimport os\\nfrom PIL import Image\\nfrom torch.utils.data import Dataset, DataLoader\\nfrom torchvision import transforms\\nimport torchvision.datasets as datasets\\n\\nclass FashionMNISTDataset(Dataset):\\n    def __init__(self, root_dir, transform=None):\\n        self.root_dir = root_dir\\n        self.transform = transform\\n        self.data = datasets.FashionMNIST(root=self.root_dir, train=True, download=True)\\n\\n    def __len__(self):\\n        return len(self.data)\\n\\n    def __getitem__(self, idx):\\n        image, label = self.data[idx]\\n        if self.transform:\\n            image = self.transform(image)\\n        return image, label\\n\\n# Define the transform to convert PIL images to tensors\\ntransform = transforms.Compose([\\n    transforms.ToTensor(),\\n])\\n\\n# Initialize the dataset and data loader\\ndataset = FashionMNISTDataset(root_dir='./data', transform=transform)\\ndataloader = DataLoader(dataset, batch_size=64, shuffle=True)\\n\\n# Example of iterating through the DataLoader\\n# for batch in dataloader:\\n#     images, labels = batch\\n#     # process images and labels\\n```\\n\\nThis code defines a `FashionMNISTDataset` class that uses the `FashionMNIST` dataset from `torchvision.datasets`. It then creates an instance of this `Dataset` and wraps it in a `DataLoader` for batching. The data is downloaded and stored in the specified `root_dir`. The transform converts images to tensors, suitable for input into a PyTorch model.\",\n          \"```python\\nimport tensorflow as tf\\nfrom sklearn.datasets import fetch_california_housing\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.preprocessing import StandardScaler\\n\\ndef create_and_train_linear_regression_model():\\n    # Load and split the California housing dataset\\n    housing = fetch_california_housing()\\n    X_train, X_test, y_train, y_test = train_test_split(housing.data, housing.target, test_size=0.2, random_state=42)\\n\\n    # Feature scaling\\n    scaler = StandardScaler()\\n    X_train_scaled = scaler.fit_transform(X_train)\\n    X_test_scaled = scaler.transform(X_test)\\n\\n    # Create a simple linear regression model\\n    model = tf.keras.models.Sequential([\\n        tf.keras.layers.Dense(1, input_shape=[X_train.shape[1]])\\n    ])\\n\\n    # Compile the model\\n    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\\n\\n    # Train the model\\n    model.fit(X_train_scaled, y_train, epochs=20, validation_data=(X_test_scaled, y_test))\\n    \\n    return model\\n\\n# Create and train the model\\nmodel = create_and_train_linear_regression_model()\\n```\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cot_response_gpt\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"Certainly! Training a convolutional neural network (CNN) on the CIFAR-10 dataset using PyTorch involves several steps. I'll guide you through the entire process with detailed explanations.\\n\\n### Step 1: Install Libraries\\n\\nMake sure you have Python and PyTorch installed. You can install PyTorch following the instructions on the [PyTorch website](https://pytorch.org/get-started/locally/). Other libraries we will use can be installed via `pip` if necessary.\\n\\n### Step 2: Import Required Libraries\\n\\nWe need several libraries from PyTorch for handling datasets, defining models, and training. We will also use torchvision, which provides convenient functions for loading datasets like CIFAR-10 and for common network layers.\\n\\n```python\\nimport torch\\nimport torch.nn as nn\\nimport torch.optim as optim\\nimport torchvision\\nimport torchvision.transforms as transforms\\nfrom torch.utils.data import DataLoader\\n```\\n\\n### Step 3: Data Loading\\n\\nThe CIFAR-10 dataset consists of 60,000 32x32 color images in 10 classes, with 6,000 images per class. We\\u2019ll use `torchvision` to load this dataset, applying transformations for normalization.\\n\\n```python\\n# Define transformations for the training and test sets\\ntransform = transforms.Compose([\\n    transforms.ToTensor(),  # Convert images to PyTorch tensors\\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize the images\\n])\\n\\n# Load the dataset\\ntrainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\\ntrainloader = DataLoader(trainset, batch_size=100, shuffle=True)\\n\\ntestset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\\ntestloader = DataLoader(testset, batch_size=100, shuffle=False)\\n```\\n\\n### Step 4: Define a CNN Model\\n\\nA simple CNN consists of convolutional layers, followed by pooling layers (for down-sampling), and finally, fully connected layers.\\n\\n```python\\nclass SimpleCNN(nn.Module):\\n    def __init__(self):\\n        super(SimpleCNN, self).__init__()\\n        self.conv1 = nn.Conv2d(3, 6, 5)  # 3 input channels (RGB), 6 output channels, 5x5 kernel\\n        self.pool = nn.MaxPool2d(2, 2)  # 2x2 pooling\\n        self.conv2 = nn.Conv2d(6, 16, 5)  # 6 input channels, 16 output channels, 5x5 kernel\\n        self.fc1 = nn.Linear(16 * 5 * 5, 120)  # Linear layer for 5x5 feature maps to 120 outputs\\n        self.fc2 = nn.Linear(120, 84)\\n        self.fc3 = nn.Linear(84, 10)  # 10 output neurons for 10 classes\\n\\n    def forward(self, x):\\n        x = self.pool(F.relu(self.conv1(x)))\\n        x = self.pool(F.relu(self.conv2(x)))\\n        x = x.view(-1, 16 * 5 * 5)  # Flatten feature map\\n        x = F.relu(self.fc1(x))\\n        x = F.relu(self.fc2(x))\\n        x = self.fc3(x)\\n        return x\\n```\\n\\n### Step 5: Define the Loss and Optimizer\\n\\nThe CrossEntropyLoss is typically used for classification problems, and we will use Stochastic Gradient Descent (SGD) for optimization.\\n\\n```python\\nnet = SimpleCNN()\\n\\ncriterion = nn.CrossEntropyLoss()  # Loss function\\noptimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)  # Optimizer\\n```\\n\\n### Step 6: Training Loop\\n\\nThe core of the training process involves looping over our data, performing the forward and backward passes, and updating the weights.\\n\\n```python\\n# Training the network\\nfor epoch in range(10):  # Let's train for 10 epochs\\n    running_loss = 0.0\\n    for i, data in enumerate(trainloader, 0):\\n        # Get the inputs and labels\\n        inputs, labels = data\\n        \\n        # Zero the parameter gradients\\n        optimizer.zero_grad()\\n        \\n        # Forward pass\\n        outputs = net(inputs)\\n        \\n        # Compute the loss\\n        loss = criterion(outputs, labels)\\n        \\n        # Backward pass and optimize\\n        loss.backward()\\n        optimizer.step()\\n        \\n        # Print statistics\\n        running_loss += loss.item()\\n        if i % 100 == 99:  # Print every 100 mini-batches\\n            print(f'[Epoch {epoch + 1}, Batch {i + 1}] loss: {running_loss / 100:.3f}')\\n            running_loss = 0.0\\n\\nprint('Finished Training')\\n```\\n\\n### Step 7: Model Evaluation\\n\\nAfter training, we evaluate the model on the test dataset to determine its accuracy.\\n\\n```python\\n# Evaluate the network on the test data\\ncorrect = 0\\ntotal = 0\\nwith torch.no_grad():  # Disable gradient computation for evaluation\\n    for data in testloader:\\n        images, labels = data\\n        outputs = net(images)\\n        _, predicted = torch.max(outputs, 1)\\n        total += labels.size(0)\\n        correct += (predicted == labels).sum().item()\\n\\nprint(f'Accuracy of the network on the 10000 test images: {100 * correct / total:.2f}%')\\n```\\n\\n### Step 8: Save the Model\\n\\nFinally, after training, you might want to save the model for later use.\\n\\n```python\\ntorch.save(net.state_dict(), 'cifar10_net.pth')\\n```\\n\\nThere you have the basic steps with a Python script to train a CNN on the CIFAR-10 dataset using PyTorch, including loading data, defining the model, setting up the loss and optimization, training the model, evaluating its performance, and saving it. You can expand upon this by adding features like learning rate scheduling, data augmentation, more complex architectures, and so on, depending on your needs.\",\n          \"To create a custom PyTorch dataset class for loading images and labels from a local folder structure, we need to subclass `torch.utils.data.Dataset`. This custom dataset class will be used in conjunction with a `DataLoader` to handle batching. Here, we'll target the FashionMNIST dataset, which consists of fashion product images in a similar structure to the MNIST handwritten digit dataset.\\n\\nLet's breakdown the process step-by-step:\\n\\n### Step 1: Import Necessary Libraries\\nFirst, import the necessary PyTorch libraries and utilities.\\n\\n```python\\nimport os\\nfrom PIL import Image\\nimport torch\\nfrom torch.utils.data import Dataset, DataLoader\\nfrom torchvision import transforms\\n```\\n\\n- `os`: For operating system-dependent functionality, like directory traversal.\\n- `PIL`: To handle image processing.\\n- `torch`: To access PyTorch functionalities.\\n- `torch.utils.data.Dataset`: For creating custom datasets.\\n- `DataLoader`: For batching and shuffling data.\\n- `transforms`: For image data pre-processing.\\n\\n### Step 2: Create the Custom Dataset Class\\nWe'll define a `FashionMNISTDataset` class that inherits from `Dataset`.\\n\\n```python\\nclass FashionMNISTDataset(Dataset):\\n    def __init__(self, root_dir, transform=None):\\n        self.root_dir = root_dir\\n        self.transform = transform\\n        self.image_files = []\\n        self.labels = []\\n        \\n        # Assuming the directory structure is:\\n        # root_dir/class_name/image1.png\\n        # root_dir/class_name/image2.png\\n        for label, class_name in enumerate(sorted(os.listdir(root_dir))):\\n            class_dir = os.path.join(root_dir, class_name)\\n            if os.path.isdir(class_dir):\\n                for file_name in os.listdir(class_dir):\\n                    if file_name.endswith(('.png', '.jpg', '.jpeg')):\\n                        self.image_files.append(os.path.join(class_dir, file_name))\\n                        self.labels.append(label)\\n\\n    def __len__(self):\\n        return len(self.image_files)\\n\\n    def __getitem__(self, idx):\\n        image_path = self.image_files[idx]\\n        image = Image.open(image_path).convert('L')  # Convert to grayscale\\n        label = self.labels[idx]\\n\\n        if self.transform:\\n            image = self.transform(image)\\n\\n        return image, label\\n```\\n\\n- `__init__`: Initializes the dataset class. We go through the root directory, listing class subdirectories (each representing a label) and collecting image file paths with corresponding labels. Additionally, we assume the images are stored in a grayscale format representative of FashionMNIST.\\n  \\n- `__len__`: Returns the number of images in the dataset.\\n\\n- `__getitem__`: Loads and returns a single image and its corresponding label by applying any transforms defined during initialization.\\n\\n### Step 3: Use the Dataset with DataLoader\\nNow, use this custom dataset class with a `DataLoader` to manage batching, shuffling, and parallel processing.\\n\\n```python\\n# Define transformation\\ntransform = transforms.Compose([\\n    transforms.Resize((28, 28)),  # Resize images to 28x28 pixels\\n    transforms.ToTensor(),       # Convert image to tensor\\n    transforms.Normalize((0.5,), (0.5,))  # Normalization\\n])\\n\\n# Initialize the dataset\\ndataset = FashionMNISTDataset(root_dir='path/to/fashionmnist', transform=transform)\\n\\n# Create a DataLoader\\ndataloader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=2)\\n\\n# Iterate through the DataLoader\\nfor images, labels in dataloader:\\n    print(images.size(), labels)\\n    # Here, you'd usually pass the data to your model for training or evaluation\\n```\\n\\n- **Transforms**: Resize images to 28x28 pixels (standard for FashionMNIST) and normalize them. Normalization is typically done based on the dataset's mean and standard deviation.\\n\\n- **Dataset Initialization**: `root_dir` should point to the root of the dataset folder.\\n\\n- **DataLoader**: Initializes with batch size, shuffling (to randomize input order each epoch), and multiple workers for parallel data loading.\\n\\n- **Iterating DataLoader**: The data is now ready to be fed into a neural network model.\\n\\nBy structuring our data pipeline this way, we create a flexible structure that allows simple integration with PyTorch's model training and evaluation workflows.\",\n          \"To create a simple linear regression model with TensorFlow and train it on the California housing dataset, we will follow these steps:\\n\\n1. Load and prepare the dataset.\\n2. Define the linear regression model using TensorFlow.\\n3. Compile the model.\\n4. Train the model.\\n5. Evaluate the model.\\n\\nHere's the step-by-step explanation along with the code:\\n\\n### Step 1: Load and Prepare the Dataset\\n\\nThe California housing dataset is included in `sklearn.datasets`. We will use `fetch_california_housing` to load the data and then split it into training and test sets using `train_test_split`.\\n\\n```python\\nfrom sklearn.datasets import fetch_california_housing\\nfrom sklearn.model_selection import train_test_split\\nimport numpy as np\\n\\n# Load the dataset\\nhousing = fetch_california_housing()\\n\\n# Split the dataset into training and testing sets\\nX_train, X_test, y_train, y_test = train_test_split(housing.data, housing.target, test_size=0.2, random_state=42)\\n\\n# Normalize the features (assuming this helps the model converge better)\\nmean = X_train.mean(axis=0)\\nstd = X_train.std(axis=0)\\n\\nX_train = (X_train - mean) / std\\nX_test = (X_test - mean) / std\\n```\\n\\n### Step 2: Define the Linear Regression Model\\n\\nLinear regression can be modeled using a dense layer with a single output neuron. In TensorFlow Keras, you can create a simple sequential model with one dense layer.\\n\\n```python\\nimport tensorflow as tf\\nfrom tensorflow import keras\\nfrom tensorflow.keras import layers\\n\\n# Define the model\\nmodel = keras.Sequential([\\n    layers.Dense(1, input_shape=(X_train.shape[1],))  # Single output neuron\\n])\\n```\\n\\n### Step 3: Compile the Model\\n\\nWe'll compile the model using the mean squared error (MSE) loss function, which is typical for regression tasks, and the Adam optimizer, which is robust and performs well in many scenarios.\\n\\n```python\\n# Compile the model\\nmodel.compile(optimizer='adam',\\n              loss='mse',\\n              metrics=['mae'])  # Mean Absolute Error for evaluation\\n```\\n\\n### Step 4: Train the Model\\n\\nTrain the model with the training data. We'll use a reasonable number of epochs, say 100, but this can be adjusted depending on how well the model performs. We will also specify a validation split to monitor the performance on unseen data.\\n\\n```python\\n# Train the model\\nhistory = model.fit(X_train, y_train,\\n                    epochs=100,\\n                    batch_size=32,\\n                    validation_split=0.2,\\n                    verbose=1)\\n```\\n\\n### Step 5: Evaluate the Model\\n\\nAfter training, we can evaluate the model's performance on the test set.\\n\\n```python\\n# Evaluate the model on the test set\\ntest_loss, test_mae = model.evaluate(X_test, y_test)\\nprint(f\\\"Test MAE: {test_mae}\\\")\\n```\\n\\n### Full Code\\n\\nHere's the complete code combining all steps above:\\n\\n```python\\nfrom sklearn.datasets import fetch_california_housing\\nfrom sklearn.model_selection import train_test_split\\nimport numpy as np\\nimport tensorflow as tf\\nfrom tensorflow import keras\\nfrom tensorflow.keras import layers\\n\\n# Load the dataset\\nhousing = fetch_california_housing()\\n\\n# Split the dataset into training and test sets\\nX_train, X_test, y_train, y_test = train_test_split(housing.data, housing.target, test_size=0.2, random_state=42)\\n\\n# Normalize the features\\nmean = X_train.mean(axis=0)\\nstd = X_train.std(axis=0)\\nX_train = (X_train - mean) / std\\nX_test = (X_test - mean) / std\\n\\n# Define the linear regression model with one dense layer\\nmodel = keras.Sequential([\\n    layers.Dense(1, input_shape=(X_train.shape[1],))\\n])\\n\\n# Compile the model using Mean Squared Error loss and Adam optimizer\\nmodel.compile(optimizer='adam',\\n              loss='mse',\\n              metrics=['mae'])\\n\\n# Train the model for a modest number of epochs\\nhistory = model.fit(X_train, y_train,\\n                    epochs=100,\\n                    batch_size=32,\\n                    validation_split=0.2,\\n                    verbose=1)\\n\\n# Evaluate the model on the test set\\ntest_loss, test_mae = model.evaluate(X_test, y_test)\\nprint(f\\\"Test MAE: {test_mae}\\\")\\n```\\n\\nThis code demonstrates how to set up and train a simple linear regression model using TensorFlow to predict house prices from the California housing dataset. The use of normalization, evaluation, and validation helps ensure the model is trained effectively.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"few_shot_gpt\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"Below is a complete Python script using PyTorch to train a convolutional neural network (CNN) on the CIFAR-10 dataset. This script includes steps for data loading, model definition, training loop, and evaluation.\\n\\n```python\\nimport torch\\nimport torch.nn as nn\\nimport torch.optim as optim\\nimport torchvision\\nimport torchvision.transforms as transforms\\nfrom torch.utils.data import DataLoader\\nfrom torchvision.datasets import CIFAR10\\nimport torch.nn.functional as F\\nfrom tqdm import tqdm\\n\\n# Device configuration\\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\\n\\n# Hyperparameters\\nnum_epochs = 10\\nbatch_size = 100\\nlearning_rate = 0.001\\n\\n# Define a simple CNN\\nclass ConvNet(nn.Module):\\n    def __init__(self):\\n        super(ConvNet, self).__init__()\\n        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=5, stride=1, padding=2)\\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\\n        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=1, padding=2)\\n        self.fc1 = nn.Linear(32 * 8 * 8, 256)\\n        self.fc2 = nn.Linear(256, 10)\\n\\n    def forward(self, x):\\n        x = self.pool(F.relu(self.conv1(x)))\\n        x = self.pool(F.relu(self.conv2(x)))\\n        x = x.view(-1, 32 * 8 * 8)  # Flatten the tensor\\n        x = F.relu(self.fc1(x))\\n        x = self.fc2(x)\\n        return x\\n\\n# Data transformation and dataset loading\\ntransform = transforms.Compose([\\n    transforms.ToTensor(),\\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\\n])\\n\\ntrain_dataset = CIFAR10(root='./data', train=True, download=True, transform=transform)\\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\\n\\ntest_dataset = CIFAR10(root='./data', train=False, download=True, transform=transform)\\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\\n\\n# Initialize the network\\nmodel = ConvNet().to(device)\\n\\n# Loss and optimizer\\ncriterion = nn.CrossEntropyLoss()\\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\\n\\n# Training the network\\ndef train():\\n    model.train()\\n    for epoch in range(num_epochs):\\n        running_loss = 0.0\\n        for images, labels in tqdm(train_loader, desc=f\\\"Epoch {epoch+1}/{num_epochs}\\\"):\\n            images = images.to(device)\\n            labels = labels.to(device)\\n\\n            # Forward pass\\n            outputs = model(images)\\n            loss = criterion(outputs, labels)\\n\\n            # Backward and optimize\\n            optimizer.zero_grad()\\n            loss.backward()\\n            optimizer.step()\\n\\n            running_loss += loss.item()\\n\\n        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}')\\n\\n# Evaluating the network\\ndef evaluate():\\n    model.eval()\\n    with torch.no_grad():\\n        correct = 0\\n        total = 0\\n        for images, labels in test_loader:\\n            images = images.to(device)\\n            labels = labels.to(device)\\n            outputs = model(images)\\n            _, predicted = torch.max(outputs.data, 1)\\n            total += labels.size(0)\\n            correct += (predicted == labels).sum().item()\\n\\n        print(f'Accuracy: {100 * correct / total:.2f}%')\\n\\nif __name__ == \\\"__main__\\\":\\n    print(\\\"Starting training...\\\")\\n    train()\\n    print(\\\"Evaluating model...\\\")\\n    evaluate()\\n```\\n\\nHere is a step-by-step explanation of the script:\\n\\n1. **Imports**: The necessary libraries and modules are imported, including PyTorch libraries and utilities for data loading and transformations.\\n\\n2. **Device Configuration**: The script defines `device` to use a GPU if available for faster training.\\n\\n3. **Hyperparameters**: These define the number of epochs, batch size, and learning rate for training.\\n\\n4. **Define the CNN**: The `ConvNet` class implements a simple CNN with two convolutional layers followed by two fully connected layers.\\n\\n5. **Data Loading**: The CIFAR-10 dataset is loaded using torchvision with transformations applied, including normalization.\\n\\n6. **Initialize Network, Loss, and Optimizer**: The CNN model is created and moved to the appropriate device, with a cross-entropy loss and Adam optimizer defined.\\n\\n7. **Training Function**: The function `train` implements the training loop, iterating over the dataset, calculating loss, and updating model weights.\\n\\n8. **Evaluation Function**: The function `evaluate` assesses the model's performance on the test data.\\n\\n9. **Execution**: The script's main section calls the training and evaluation functions.\\n\\nTo run the script, ensure you have the required dependencies installed (`torch` and `torchvision`) and execute the script in an environment with these dependencies.\",\n          \"To create a custom `Dataset` class in PyTorch that loads image files and labels from a folder, we will subclass `torch.utils.data.Dataset`. For this example, we will simulate loading the FashionMNIST dataset.\\n\\nLet's start with setting up the class, assuming our data is organized as follows:\\n\\n- Images are stored in a directory.\\n- Labels are stored in a CSV file where each row maps an image filename to its label.\\n\\nHere's an example implementation of the custom dataset class and how you can use it with a DataLoader:\\n\\n```python\\nimport os\\nimport pandas as pd\\nimport torch\\nfrom torch.utils.data import Dataset, DataLoader\\nfrom torchvision.io import read_image\\nfrom torchvision import transforms\\nfrom torchvision.datasets import FashionMNIST\\nimport random\\n\\n# Define a transformation for the FashionMNIST dataset (if needed)\\ntransform = transforms.Compose([\\n    transforms.ToTensor(),\\n    # Add additional transformations, e.g., normalization, if needed\\n])\\n\\n# Custom Dataset class\\nclass FashionMNISTDataset(Dataset):\\n    def __init__(self, img_dir, annotations_file, transform=None):\\n        self.img_labels = pd.read_csv(annotations_file)\\n        self.img_dir = img_dir\\n        self.transform = transform\\n\\n    def __len__(self):\\n        return len(self.img_labels)\\n\\n    def __getitem__(self, idx):\\n        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\\n        image = read_image(img_path)\\n        label = self.img_labels.iloc[idx, 1]\\n        if self.transform:\\n            image = self.transform(image)\\n        return image, label\\n\\n# Example usage:\\n# Assume you have the FashionMNIST dataset downloaded somewhere\\ndef create_fashion_mnist_dataset():\\n    # This is just an example to simulate: normally, you'd have a path to your dataset\\n    # You can use torchvision to download FashionMNIST and subsequently use it in practice\\n\\n    # Create a temporary dataset with random filenames and labels\\n    img_dir = 'path_to_images'\\n    annotations_file = 'path_to_annotations.csv'\\n\\n    fashion_mnist_data = FashionMNIST(root='./data', train=True, download=True)\\n    os.makedirs(img_dir, exist_ok=True)\\n    annotations = []\\n\\n    for idx in range(len(fashion_mnist_data)):\\n        image, label = fashion_mnist_data[idx]\\n        image_filename = f'image_{idx}.png'\\n        image.save(os.path.join(img_dir, image_filename))  # Save images locally\\n        annotations.append([image_filename, label])\\n\\n    # Save annotations to a CSV file\\n    annotations_df = pd.DataFrame(annotations, columns=['filename', 'label'])\\n    annotations_df.to_csv(annotations_file, index=False)\\n\\n    return img_dir, annotations_file\\n\\n# Setup your train dataset\\nimg_dir, annotations_file = create_fashion_mnist_dataset()\\n\\n# Instantiate the dataset\\ndataset = FashionMNISTDataset(img_dir=img_dir, annotations_file=annotations_file, transform=transform)\\n\\n# Create DataLoader\\ndataloader = DataLoader(dataset, batch_size=4, shuffle=True, num_workers=2)\\n\\n# Iterate through the dataloader\\nfor images, labels in dataloader:\\n    print(images.size(), labels.size())\\n    # process the batch\\n```\\n\\n### Explanation\\n\\n1. **Dataset Class**: Subclass `torch.utils.data.Dataset` and implement the `__init__`, `__len__`, and `__getitem__` methods. This class will handle loading image paths from a CSV file and applying any specified transforms.\\n\\n2. **DataLoader**: Use PyTorch's `DataLoader` to handle batching, shuffling, and loading the dataset in mini-batches for training.\\n\\n3. **Image Processing**: Make sure images are converted to tensors using `transforms.ToTensor()` and optionally apply more transformations such as normalization or augmentation.\\n\\n4. **Creating and Saving Data**: For illustration, I've shown how you can use the `FashionMNIST` dataset from torchvision to create a mock directory of images and a CSV file with labels. You would typically organize your data similarly in practice.\\n\\nMake sure you modify `img_dir` and `annotations_file` to point to your actual paths where your images and labels CSV are stored.\",\n          \"Sure, I'll guide you through creating and compiling a simple linear regression model using TensorFlow that trains on the California housing dataset. We'll write a function to achieve this.\\n\\nFirst, ensure you have TensorFlow and the necessary libraries installed. If not, use `!pip install tensorflow` in your Jupyter or Colab notebook environment.\\n\\nBelow is a Python function that does this task:\\n\\n```python\\nimport numpy as np\\nimport pandas as pd\\nimport tensorflow as tf\\nfrom tensorflow.keras import layers\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.datasets import fetch_california_housing\\n\\n\\ndef create_and_train_linear_model():\\n    # Fetch California housing dataset\\n    cali_housing = fetch_california_housing()\\n\\n    # Convert to Pandas DataFrame\\n    data = pd.DataFrame(\\n        cali_housing.data, columns=cali_housing.feature_names)\\n    labels = pd.DataFrame(cali_housing.target, columns=[\\\"MedHouseVal\\\"])\\n\\n    # Feature selection\\n    feature = 'AveRooms'  # We will perform linear regression with respect to AveRooms\\n\\n    # Split the data: 80% train, 20% test\\n    train_features, test_features, train_labels, test_labels = train_test_split(\\n        data[[feature]], labels, test_size=0.2, random_state=42)\\n\\n    # Normalization layer\\n    normalizer = layers.Normalization(input_shape=[1,], axis=None)\\n    normalizer.adapt(np.array(train_features))\\n\\n    # Create the sequential model\\n    model = tf.keras.Sequential([\\n        normalizer,\\n        layers.Dense(units=1)\\n    ])\\n\\n    model.summary()\\n\\n    # Compile the model\\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.1),\\n                  loss='mean_absolute_error')\\n\\n    # Train the model\\n    history = model.fit(\\n        train_features,\\n        train_labels,\\n        epochs=100,\\n        verbose=0,\\n        validation_split=0.2\\n    )\\n\\n    # Evaluate the model\\n    loss = model.evaluate(test_features, test_labels, verbose=2)\\n    print(f\\\"Test set Mean Absolute Error: {loss}\\\")\\n\\n# Call the function\\ncreate_and_train_linear_model()\\n```\\n\\n### Explanation:\\n\\n1. **Fetch the Dataset**: We use `fetch_california_housing()` to get the dataset.\\n\\n2. **Data Preparation**: Convert the dataset into Pandas DataFrame and select one feature for linear regression (`AveRooms`).\\n\\n3. **Normalization**: We use a `Normalization` layer adapted to the `AveRooms` data to standardize the inputs.\\n\\n4. **Model Creation**: Build a sequential model with one normalizer layer followed by a dense layer with a single unit. This setup models a linear regression considering one feature.\\n\\n5. **Compile**: The model is compiled using the Adam optimizer and Mean Absolute Error as the loss function.\\n\\n6. **Train**: Train the model with 80% of the training data and validate with 20% using the training set.\\n\\n7. **Evaluate**: The model is evaluated on the test dataset.\\n\\nThis function defines a simple linear regression model focusing on a single feature and demonstrates basic usage of TensorFlow's `Sequential` API to perform regression tasks. Adjust the feature and model complexity as needed depending on your dataset's specific aspects or other variables of interest.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"direct_response_gemini\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"```python\\nimport torch\\nimport torch.nn as nn\\nimport torch.optim as optim\\nimport torchvision\\nimport torchvision.transforms as transforms\\nfrom torch.utils.data import DataLoader\\nfrom tqdm import tqdm\\n\\n# Device configuration\\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\\n\\n# Hyperparameters\\nnum_epochs = 10\\nbatch_size = 64\\nlearning_rate = 0.001\\n\\n# CIFAR10 dataset\\ntransform_train = transforms.Compose([\\n    transforms.RandomCrop(32, padding=4),\\n    transforms.RandomHorizontalFlip(),\\n    transforms.ToTensor(),\\n    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\\n])\\n\\ntransform_test = transforms.Compose([\\n    transforms.ToTensor(),\\n    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\\n])\\n\\ntrain_dataset = torchvision.datasets.CIFAR10(root='./data', train=True,\\n                                             download=True, transform=transform_train)\\n\\ntest_dataset = torchvision.datasets.CIFAR10(root='./data', train=False,\\n                                            download=True, transform=transform_test)\\n\\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size,\\n                          shuffle=True, num_workers=2)\\n\\ntest_loader = DataLoader(test_dataset, batch_size=batch_size,\\n                         shuffle=False, num_workers=2)\\n\\n# Convolutional neural network (simple architecture)\\nclass Net(nn.Module):\\n    def __init__(self):\\n        super(Net, self).__init__()\\n        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\\n        self.relu = nn.ReLU()\\n        self.pool = nn.MaxPool2d(2, 2)\\n        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\\n        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\\n        self.fc1 = nn.Linear(128 * 4 * 4, 512)\\n        self.fc2 = nn.Linear(512, 10)\\n\\n    def forward(self, x):\\n        x = self.pool(self.relu(self.conv1(x)))\\n        x = self.pool(self.relu(self.conv2(x)))\\n        x = self.pool(self.relu(self.conv3(x)))\\n        x = x.view(-1, 128 * 4 * 4) # Flatten the tensor\\n        x = self.relu(self.fc1(x))\\n        x = self.fc2(x)\\n        return x\\n\\nmodel = Net().to(device)\\n\\n# Loss and optimizer\\ncriterion = nn.CrossEntropyLoss()\\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\\n\\n# Training loop\\nprint(\\\"Starting training...\\\")\\nfor epoch in range(num_epochs):\\n    model.train()\\n    running_loss = 0.0\\n    for i, (inputs, labels) in enumerate(tqdm(train_loader, desc=f\\\"Epoch {epoch+1}/{num_epochs}\\\")):\\n        inputs, labels = inputs.to(device), labels.to(device)\\n\\n        # Forward pass\\n        outputs = model(inputs)\\n        loss = criterion(outputs, labels)\\n\\n        # Backward and optimize\\n        optimizer.zero_grad()\\n        loss.backward()\\n        optimizer.step()\\n\\n        running_loss += loss.item()\\n\\n    avg_loss = running_loss / len(train_loader)\\n    print(f\\\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\\\")\\n\\nprint(\\\"Finished Training\\\")\\n\\n# Evaluation\\nprint(\\\"Starting evaluation...\\\")\\nmodel.eval()\\ncorrect = 0\\ntotal = 0\\nwith torch.no_grad():\\n    for inputs, labels in tqdm(test_loader, desc=\\\"Evaluating\\\"):\\n        inputs, labels = inputs.to(device), labels.to(device)\\n        outputs = model(inputs)\\n        _, predicted = torch.max(outputs.data, 1)\\n        total += labels.size(0)\\n        correct += (predicted == labels).sum().item()\\n\\naccuracy = 100 * correct / total\\nprint(f'Accuracy of the network on the 10000 test images: {accuracy:.2f} %')\\n\\n```\",\n          \"```python\\nimport torch\\nimport os\\nfrom PIL import Image\\nfrom torch.utils.data import Dataset, DataLoader\\nimport torchvision.transforms as transforms\\n\\n# Assume the FashionMNIST dataset has been organized into folders\\n# like this:\\n# root_dir/\\n#   train/\\n#     0/\\n#       image1.png\\n#       image2.png\\n#       ...\\n#     1/\\n#       imageX.png\\n#       ...\\n#     ...\\n#     9/\\n#   test/\\n#     0/\\n#       ...\\n#     ...\\n#     9/\\n\\nclass FashionMNISTFolderDataset(Dataset):\\n    \\\"\\\"\\\"\\n    A custom PyTorch Dataset for loading image files from a folder structure.\\n    Assumes the data is organized into subfolders where each subfolder\\n    represents a class.\\n    \\\"\\\"\\\"\\n    def __init__(self, root_dir, transform=None):\\n        \\\"\\\"\\\"\\n        Args:\\n            root_dir (string): Directory with all the class folders.\\n            transform (callable, optional): Optional transform to be applied\\n                on a sample.\\n        \\\"\\\"\\\"\\n        self.root_dir = root_dir\\n        self.transform = transform\\n        self.image_files = []\\n        self.labels = []\\n        self.classes = sorted(entry.name for entry in os.scandir(root_dir) if entry.is_dir())\\n        self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.classes)}\\n\\n        if not self.classes:\\n             raise FileNotFoundError(f\\\"No subdirectories found in {root_dir}. \\\"\\n                                     f\\\"Please organize images into class folders.\\\")\\n\\n        for class_name in self.classes:\\n            class_dir = os.path.join(root_dir, class_name)\\n            if not os.path.isdir(class_dir):\\n                continue # Skip if it's not a directory\\n\\n            for entry in os.scandir(class_dir):\\n                 if entry.is_file() and entry.name.lower().endswith(('.png', '.jpg', '.jpeg')):\\n                    self.image_files.append(entry.path)\\n                    self.labels.append(self.class_to_idx[class_name])\\n\\n        if not self.image_files:\\n             raise FileNotFoundError(f\\\"No image files found in subdirectories of {root_dir}. \\\"\\n                                     f\\\"Supported formats: .png, .jpg, .jpeg.\\\")\\n\\n\\n    def __len__(self):\\n        return len(self.image_files)\\n\\n    def __getitem__(self, idx):\\n        if torch.is_tensor(idx):\\n            idx = idx.tolist()\\n\\n        img_path = self.image_files[idx]\\n        image = Image.open(img_path).convert('L') # Convert to grayscale as FashionMNIST is grayscale\\n        label = self.labels[idx]\\n\\n        if self.transform:\\n            image = self.transform(image)\\n\\n        return image, label\\n\\n# --- Example Usage ---\\n\\n# Define transformations (example: resize, to tensor, normalize)\\n# FashionMNIST images are 28x28\\ndata_transform = transforms.Compose([\\n    transforms.Resize((32, 32)), # Example: resize to 32x32\\n    transforms.ToTensor(),\\n    transforms.Normalize((0.5,), (0.5,)) # FashionMNIST is grayscale, use single mean/std\\n])\\n\\n# Define dummy root directories assuming the data is structured as described\\n# Replace with actual paths if you have the data organized this way\\ntrain_data_root = \\\"./dummy_fashion_mnist_folder/train\\\"\\ntest_data_root = \\\"./dummy_fashion_mnist_folder/test\\\"\\n\\n# --- Create Dummy Folders and Files for Demonstration ---\\n# This part is just to make the code runnable without a real dataset\\n# In a real scenario, you would already have the data organized.\\nif not os.path.exists(train_data_root):\\n    print(f\\\"Creating dummy data structure in {os.path.abspath('./dummy_fashion_mnist_folder')}\\\")\\n    os.makedirs(train_data_root, exist_ok=True)\\n    os.makedirs(test_data_root, exist_ok=True)\\n    # Create a few dummy files for 2 classes\\n    for class_id in [0, 1]:\\n        os.makedirs(os.path.join(train_data_root, str(class_id)), exist_ok=True)\\n        os.makedirs(os.path.join(test_data_root, str(class_id)), exist_ok=True)\\n        # Create dummy images (e.g., blank images)\\n        for i in range(5): # 5 dummy images per class\\n            dummy_img_train = Image.new('L', (28, 28), color='white')\\n            dummy_img_train.save(os.path.join(train_data_root, str(class_id), f'img_{i}.png'))\\n            dummy_img_test = Image.new('L', (28, 28), color='white')\\n            dummy_img_test.save(os.path.join(test_data_root, str(class_id), f'test_img_{i}.png'))\\n    print(\\\"Dummy data structure created.\\\")\\nelse:\\n    print(f\\\"Dummy data structure already exists in {os.path.abspath('./dummy_fashion_mnist_folder')}\\\")\\n# --- End of Dummy Data Creation ---\\n\\n\\n# Instantiate the custom Dataset\\ntry:\\n    fashion_mnist_dataset = FashionMNISTFolderDataset(root_dir=train_data_root, transform=data_transform)\\n\\n    # Check dataset size and classes\\n    print(f\\\"Number of samples in dataset: {len(fashion_mnist_dataset)}\\\")\\n    print(f\\\"Classes found: {fashion_mnist_dataset.classes}\\\")\\n    print(f\\\"Class to index mapping: {fashion_mnist_dataset.class_to_idx}\\\")\\n\\n    # Create a DataLoader\\n    batch_size = 32\\n    dataloader = DataLoader(fashion_mnist_dataset, batch_size=batch_size, shuffle=True)\\n\\n    # Iterate through the DataLoader (example)\\n    print(f\\\"\\\\nIterating through DataLoader with batch size {batch_size}...\\\")\\n    for i, (images, labels) in enumerate(dataloader):\\n        print(f\\\"Batch {i+1}:\\\")\\n        print(f\\\"  Image batch shape: {images.shape}\\\")\\n        print(f\\\"  Label batch shape: {labels.shape}\\\")\\n        # Stop after 2 batches for demonstration\\n        if i >= 1:\\n            break\\n\\n    print(\\\"\\\\nDataLoader usage demonstrated.\\\")\\n\\nexcept FileNotFoundError as e:\\n    print(f\\\"Error: {e}\\\")\\n    print(\\\"Please ensure the root_dir contains subdirectories for each class with image files inside.\\\")\\n    print(f\\\"The dummy data creation code created a structure in {os.path.abspath('./dummy_fashion_mnist_folder')}.\\\")\\n    print(\\\"You can point the `root_dir` to this location.\\\")\\n\\n```\",\n          \"```python\\nimport tensorflow as tf\\nimport numpy as np\\nfrom sklearn.datasets import fetch_california_housing\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.preprocessing import StandardScaler\\n\\ndef train_california_housing_linear_model():\\n    # Load the dataset\\n    housing = fetch_california_housing()\\n    X, y = housing.data, housing.target\\n\\n    # Preprocess the data (scaling)\\n    scaler = StandardScaler()\\n    X_scaled = scaler.fit_transform(X)\\n    y_scaled = y # For simplicity, we won't scale the target here, model will learn direct mapping\\n\\n    # Split data (optional but good practice)\\n    # X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=42)\\n    # Using all data for training as requested implicitly\\n\\n    # Define the model\\n    model = tf.keras.Sequential([\\n        tf.keras.layers.Dense(units=1, input_shape=(X_scaled.shape[1],))\\n    ])\\n\\n    # Compile the model\\n    model.compile(optimizer='adam', loss='mean_squared_error')\\n\\n    # Train the model\\n    model.fit(X_scaled, y_scaled, epochs=50, batch_size=32, verbose=0)\\n\\n    return model\\n\\nif __name__ == '__main__':\\n    # Example usage:\\n    trained_model = train_california_housing_linear_model()\\n    print(\\\"Model training complete.\\\")\\n    # You can now use the trained_model for predictions\\n    # For example: trained_model.predict(some_scaled_data)\\n```\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cot_response_gemini\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"Okay, let's build a Python script using PyTorch to train a Convolutional Neural Network (CNN) on the CIFAR-10 dataset. I will explain each step as we go.\\n\\nCIFAR-10 is a dataset of 60,000 32x32 colour images in 10 classes, with 6,000 images per class. There are 50,000 training images and 10,000 test images.\\n\\nHere's the plan:\\n1.  **Import necessary libraries:** PyTorch core, neural network module, optimizer module, data utilities, and torchvision for dataset and transforms.\\n2.  **Define Hyperparameters:** Set values like batch size, learning rate, number of epochs, etc.\\n3.  **Set up Device:** Check if a GPU is available and use it, otherwise use the CPU.\\n4.  **Load and Preprocess Data:** Download the CIFAR-10 dataset, apply necessary transformations (like converting to tensor and normalization), and create DataLoaders for training and testing.\\n5.  **Define the CNN Model:** Create a PyTorch `nn.Module` class for our CNN architecture.\\n6.  **Instantiate Model, Loss Function, and Optimizer:** Create an instance of the model, define the loss criterion (CrossEntropyLoss for classification), and choose an optimizer (e.g., Adam or SGD).\\n7.  **Define Training Loop:** Write a function or code block that iterates over the epochs, processing data in batches, calculating loss, performing backpropagation, and updating model weights.\\n8.  **Define Evaluation Function:** Write a function to evaluate the model's performance on the test dataset (calculate accuracy).\\n9.  **Run Training and Evaluation:** Execute the training loop and periodically evaluate the model.\\n10. **Optional: Plot Results:** Visualize training loss and test accuracy over epochs.\\n\\nLet's start coding!\\n\\n```python\\n# 1. Import necessary libraries\\nimport torch\\nimport torch.nn as nn\\nimport torch.optim as optim\\nimport torchvision\\nimport torchvision.transforms as transforms\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\nimport os # To create directory for saving model\\nfrom tqdm import tqdm # For progress bar\\n\\n# 2. Define Hyperparameters\\nnum_epochs = 10         # Number of training epochs\\nbatch_size = 100        # Number of images per batch\\nlearning_rate = 0.001   # Learning rate for the optimizer\\ndata_path = './data'    # Directory to download the dataset\\nmodel_path = './models' # Directory to save the trained model\\nmodel_save_name = 'cifar10_cnn.pth' # Name for the saved model file\\n\\n# 3. Set up Device\\n# This checks if a CUDA-enabled GPU is available. If yes, it uses the GPU; otherwise, it uses the CPU.\\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\\nprint(f'Using device: {device}')\\n\\n# 4. Load and Preprocess Data\\n\\n# Data augmentation and normalization for training\\n# We define a sequence of transformations to apply to the images.\\n# transforms.ToTensor(): Converts a PIL Image or NumPy array (H x W x C)\\n#                        to a PyTorch Tensor (C x H x W) in the range [0.0, 1.0].\\n# transforms.Normalize(mean, std): Normalizes a tensor image with mean and standard deviation.\\n# We use the standard mean and std for CIFAR-10 across its channels.\\ntransform_train = transforms.Compose([\\n    transforms.RandomCrop(32, padding=4), # Pad and randomly crop to 32x32\\n    transforms.RandomHorizontalFlip(), # Randomly flip the image horizontally\\n    transforms.ToTensor(),\\n    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\\n])\\n\\n# Normalization for testing (no augmentation)\\ntransform_test = transforms.Compose([\\n    transforms.ToTensor(),\\n    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\\n])\\n\\n# Load the CIFAR-10 training dataset\\n# root: directory where the dataset will be saved or loaded from.\\n# train=True: specifies the training set.\\n# download=True: downloads the dataset if it's not found in the root directory.\\n# transform: applies the specified transformations.\\ntrainset = torchvision.datasets.CIFAR10(root=data_path, train=True,\\n                                        download=True, transform=transform_train)\\n\\n# Create a DataLoader for the training set\\n# DataLoader is an iterator that provides batches of data.\\n# dataset: the dataset to load data from.\\n# batch_size: how many samples per batch to load.\\n# shuffle=True: shuffles the data at the beginning of each epoch (important for training).\\n# num_workers: how many subprocesses to use for data loading (higher values can speed up loading).\\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\\n                                          shuffle=True, num_workers=2)\\n\\n# Load the CIFAR-10 test dataset\\ntestset = torchvision.datasets.CIFAR10(root=data_path, train=False,\\n                                       download=True, transform=transform_test)\\n\\n# Create a DataLoader for the test set\\n# shuffle=False: shuffling is not necessary for testing/evaluation.\\ntestloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\\n                                         shuffle=False, num_workers=2)\\n\\n# Define the 10 class names for CIFAR-10\\nclasses = ('plane', 'car', 'bird', 'cat', 'deer',\\n           'dog', 'frog', 'horse', 'ship', 'truck')\\n\\nprint(f'Training data size: {len(trainset)}')\\nprint(f'Test data size: {len(testset)}')\\n\\n# Optional: Show a few images (for debugging/visualization)\\n# def imshow(img):\\n#     # Unnormalize the image\\n#     img = img / 2 + 0.5     # Assuming the normalization was done around mean 0.5, std 0.5 for simplicity here,\\n#                           # though we used CIFAR10 specific mean/std above.\\n#                           # For correct visualization with CIFAR10 norm:\\n#     mean = torch.tensor((0.4914, 0.4822, 0.4465)).reshape(3, 1, 1)\\n#     std = torch.tensor((0.2023, 0.1994, 0.2010)).reshape(3, 1, 1)\\n#     img = img * std + mean\\n#\\n#     npimg = img.numpy()\\n#     plt.imshow(np.transpose(npimg, (1, 2, 0))) # Convert from Tensor (C, H, W) to NumPy array (H, W, C)\\n#     plt.show()\\n\\n# Get some random training images\\n# dataiter = iter(trainloader)\\n# images, labels = next(dataiter)\\n# print(' '.join(f'{classes[labels[j]]:5s}' for j in range(4)))\\n# imshow(torchvision.utils.make_grid(images))\\n\\n\\n# 5. Define the CNN Model\\n# We create a class MyCNN that inherits from nn.Module.\\n# This is the standard way to define models in PyTorch.\\nclass MyCNN(nn.Module):\\n    def __init__(self):\\n        # Call the constructor of the parent class (nn.Module)\\n        super(MyCNN, self).__init__()\\n\\n        # Define the layers of the network\\n\\n        # Convolutional Layer 1\\n        # nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\\n        # Input image size: 3x32x32 (3 channels, 32x32 pixels)\\n        # Output size after conv1: (32 + 2*1 - 3)/1 + 1 = 32 -> 32x32x32\\n        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1)\\n        # Batch Normalization 1: Applied after convolution, before activation, helps stabilize training.\\n        self.bn1 = nn.BatchNorm2d(32)\\n        # ReLU Activation: Applies the rectified linear unit function element-wise (max(0, x)).\\n        self.relu = nn.ReLU()\\n        # Max Pooling Layer 1: Reduces spatial dimensions.\\n        # nn.MaxPool2d(kernel_size, stride)\\n        # Output size after pool1: 32/2 x 32/2 = 16x16 -> 32x16x16\\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\\n\\n        # Convolutional Layer 2\\n        # Input size: 32x16x16\\n        # Output size after conv2: (64 + 2*1 - 3)/1 + 1 = 16 -> 64x16x16\\n        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\\n        self.bn2 = nn.BatchNorm2d(64)\\n        # Output size after pool: 16/2 x 16/2 = 8x8 -> 64x8x8\\n\\n        # Convolutional Layer 3\\n        # Input size: 64x8x8\\n        # Output size after conv3: (128 + 2*1 - 3)/1 + 1 = 8 -> 128x8x8\\n        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\\n        self.bn3 = nn.BatchNorm2d(128)\\n        # Output size after pool: 8/2 x 8/2 = 4x4 -> 128x4x4\\n\\n        # Fully Connected Layers (also called Dense layers)\\n        # Input size for the first fully connected layer: Flat output from the last pooling layer\\n        # The output tensor from the last pooling layer is 128 channels of size 4x4.\\n        # Flattened size: 128 * 4 * 4 = 2048\\n        self.fc1 = nn.Linear(in_features=128 * 4 * 4, out_features=512)\\n        # Dropout Layer: Randomly sets a fraction of input units to 0 at each update during training. Helps prevent overfitting.\\n        self.dropout = nn.Dropout(p=0.5) # 50% probability of dropping an activation\\n        self.fc2 = nn.Linear(in_features=512, out_features=10) # Output layer: 10 features for 10 classes\\n\\n    # Define the forward pass (how data flows through the network)\\n    def forward(self, x):\\n        # Apply Conv -> BN -> ReLU -> Pool\\n        x = self.pool(self.relu(self.bn1(self.conv1(x))))\\n        x = self.pool(self.relu(self.bn2(self.conv2(x))))\\n        x = self.pool(self.relu(self.bn3(self.conv3(x))))\\n\\n        # Flatten the output from the convolutional layers for the fully connected layers\\n        # x.size(0) is the batch size\\n        x = x.view(-1, 128 * 4 * 4) # -1 lets PyTorch figure out the batch size\\n\\n        # Apply Fully Connected layers\\n        x = self.relu(self.fc1(x))\\n        x = self.dropout(x) # Apply dropout after the first FC layer\\n        x = self.fc2(x)\\n\\n        return x # The output layer doesn't typically have an activation for CrossEntropyLoss,\\n                 # as it includes LogSoftmax internally.\\n\\n# 6. Instantiate Model, Loss Function, and Optimizer\\nmodel = MyCNN().to(device) # Create an instance of the model and move it to the selected device (GPU/CPU)\\n\\n# Loss function: Cross-Entropy Loss is suitable for multi-class classification.\\ncriterion = nn.CrossEntropyLoss()\\n\\n# Optimizer: Adam is an adaptive learning rate optimization algorithm.\\n# model.parameters(): Provides all the learnable parameters of the model.\\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\\n\\n# Optional: Learning Rate Scheduler\\n# Decays the learning rate by a factor of 0.1 every 30 epochs\\n# scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\\n\\n# 7. Define Training Loop\\n# This function trains the model for one epoch\\ndef train_epoch(model, train_loader, criterion, optimizer, device):\\n    model.train() # Set the model to training mode (enables dropout and batch normalization updates)\\n    running_loss = 0.0\\n    # Wrap the DataLoader with tqdm for a progress bar\\n    for i, data in enumerate(tqdm(train_loader, desc='Training', leave=False)):\\n        # Get the inputs; data is a list of [inputs, labels]\\n        inputs, labels = data\\n        # Move data to the appropriate device\\n        inputs, labels = inputs.to(device), labels.to(device)\\n\\n        # Zero the parameter gradients\\n        # It's important to zero the gradients before each backpropagation step,\\n        # as gradients are accumulated by default in PyTorch.\\n        optimizer.zero_grad()\\n\\n        # Forward pass: Compute the predicted outputs by passing inputs through the model\\n        outputs = model(inputs)\\n        # Compute the loss: Compare the predicted outputs with the true labels\\n        loss = criterion(outputs, labels)\\n\\n        # Backward pass: Compute gradients of the loss with respect to model parameters\\n        loss.backward()\\n        # Optimizer step: Update the model parameters using the computed gradients\\n        optimizer.step()\\n\\n        # Print statistics\\n        running_loss += loss.item() # .item() gets the scalar value from a tensor\\n    return running_loss / len(train_loader) # Return average loss for the epoch\\n\\n# 8. Define Evaluation Function\\n# This function evaluates the model on a given data loader (usually the test set)\\ndef evaluate(model, data_loader, device):\\n    model.eval() # Set the model to evaluation mode (disables dropout and batch normalization tracking/updates)\\n    correct = 0\\n    total = 0\\n    # Disable gradient calculations during evaluation\\n    # This reduces memory consumption and speeds up computation as we don't need gradients for evaluation.\\n    with torch.no_grad():\\n        for data in tqdm(data_loader, desc='Evaluating', leave=False):\\n            images, labels = data\\n            # Move data to the appropriate device\\n            images, labels = images.to(device), labels.to(device)\\n            # Forward pass: Get predictions\\n            outputs = model(images)\\n            # Get the predicted class with the highest score\\n            # torch.max returns (values, indices). We need the indices (the class predictions).\\n            _, predicted = torch.max(outputs.data, 1)\\n            # Update total number of images and correct predictions\\n            total += labels.size(0) # Add batch size to total count\\n            correct += (predicted == labels).sum().item() # Count correct predictions in the batch\\n\\n    accuracy = 100 * correct / total\\n    return accuracy\\n\\n# 9. Run Training and Evaluation\\ntrain_losses = []\\ntest_accuracies = []\\n\\n# Ensure model save directory exists\\nos.makedirs(model_path, exist_ok=True)\\nbest_accuracy = 0.0 # To keep track of the best test accuracy achieved\\n\\nprint(\\\"Starting Training...\\\")\\nfor epoch in range(num_epochs):\\n    print(f'\\\\nEpoch {epoch + 1}/{num_epochs}')\\n\\n    # Train for one epoch\\n    avg_train_loss = train_epoch(model, trainloader, criterion, optimizer, device)\\n    train_losses.append(avg_train_loss)\\n\\n    # Evaluate on the test set\\n    test_accuracy = evaluate(model, testloader, device)\\n    test_accuracies.append(test_accuracy)\\n\\n    print(f'Epoch {epoch + 1} finished.')\\n    print(f'Train Loss: {avg_train_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')\\n\\n    # Optional: Save the model checkpoint if it's the best performing so far\\n    if test_accuracy > best_accuracy:\\n        best_accuracy = test_accuracy\\n        save_path = os.path.join(model_path, model_save_name)\\n        # Save the model's state dictionary\\n        # Saving state_dict is recommended as it's more flexible for loading later\\n        torch.save(model.state_dict(), save_path)\\n        print(f'Model saved to {save_path} with improved accuracy: {best_accuracy:.2f}%')\\n\\n    # Optional: Step the learning rate scheduler\\n    # if 'scheduler' in locals():\\n    #     scheduler.step()\\n\\nprint('Finished Training')\\n\\n# 10. Plot Results (Optional)\\nplt.figure(figsize=(10, 5))\\nplt.subplot(1, 2, 1)\\nplt.plot(range(1, num_epochs + 1), train_losses)\\nplt.xlabel('Epoch')\\nplt.ylabel('Training Loss')\\nplt.title('Training Loss per Epoch')\\n\\nplt.subplot(1, 2, 2)\\nplt.plot(range(1, num_epochs + 1), test_accuracies, color='orange')\\nplt.xlabel('Epoch')\\nplt.ylabel('Test Accuracy (%)')\\nplt.title('Test Accuracy per Epoch')\\n\\nplt.tight_layout()\\nplt.show()\\n\\n# Optional: Load the best model and evaluate one last time\\ntry:\\n    loaded_model = MyCNN().to(device)\\n    load_path = os.path.join(model_path, model_save_name)\\n    loaded_model.load_state_dict(torch.load(load_path))\\n    print(f'\\\\nLoaded model from {load_path}')\\n    final_test_accuracy = evaluate(loaded_model, testloader, device)\\n    print(f'Final Test Accuracy (loaded model): {final_test_accuracy:.2f}%')\\nexcept FileNotFoundError:\\n    print(f\\\"\\\\nModel file not found at {load_path}. Skipping final evaluation of loaded model.\\\")\\nexcept Exception as e:\\n    print(f\\\"\\\\nError loading or evaluating model: {e}\\\")\\n\\n```\\n\\n**Explanation of Steps and Reasoning:**\\n\\n1.  **Imports:** We import `torch` for the core framework, `torch.nn` for neural network modules (layers), `torch.optim` for optimizers, `torchvision` for datasets and common transforms, `torchvision.transforms` for data augmentation/preprocessing, `matplotlib.pyplot` for plotting, `numpy` for potential numerical operations (used implicitly by matplotlib), `os` to handle directories, and `tqdm` for progress bars.\\n2.  **Hyperparameters:** These are settings that control the training process. Defining them at the top makes it easy to experiment with different values without changing the core logic.\\n3.  **Device Setup:** Training neural networks is computationally intensive. Using a GPU (`cuda`) if available is crucial for performance. This snippet automatically detects and uses the GPU or falls back to the CPU.\\n4.  **Data Loading and Preprocessing:**\\n    *   We use `torchvision.datasets.CIFAR10` which handles downloading and loading the dataset.\\n    *   `transforms.Compose` chains together multiple transformations.\\n    *   `transforms.RandomCrop` and `RandomHorizontalFlip` are common data *augmentation* techniques. They artificially increase the size and variability of the training data, making the model more robust and reducing overfitting.\\n    *   `transforms.ToTensor()` converts images to PyTorch tensors and scales pixel values from \\\\[0, 255] to \\\\[0.0, 1.0].\\n    *   `transforms.Normalize()` scales pixel values based on the channel-wise mean and standard deviation of the dataset. This helps in speeding up convergence and improving performance.\\n    *   `torch.utils.data.DataLoader` is essential. It wraps the dataset and provides an iterable that yields data in batches. This is necessary because training is typically done using mini-batches, which makes gradient estimation more stable than training on individual samples and is more memory-efficient than training on the entire dataset at once. `shuffle=True` for training is important to prevent the model from learning the order of data.\\n5.  **Define the CNN Model:**\\n    *   We create a class `MyCNN` inheriting from `nn.Module`. The `__init__` method defines the layers, and the `forward` method defines how data passes through these layers.\\n    *   **Convolutional Layers (`nn.Conv2d`):** These are the core of the CNN. They apply filters (kernels) to the input image to extract features. `in_channels` is the number of input feature maps (3 for RGB images), `out_channels` is the number of filters (and thus output feature maps), `kernel_size` is the size of the filter (e.g., 3x3), `stride` is how many pixels the filter moves at each step, and `padding` adds zeros around the input borders (useful for controlling output size).\\n    *   **Batch Normalization (`nn.BatchNorm2d`):** Normalizes the output of a convolutional layer across the batch dimension. This helps in training deeper networks by making the training process more stable and allowing for higher learning rates. It's typically applied after the convolution and before the activation function.\\n    *   **Activation Function (`nn.ReLU`):** Introduces non-linearity into the network. Without non-linearities, a CNN would just be a sequence of linear operations, which cannot model complex patterns.\\n    *   **Pooling Layers (`nn.MaxPool2d`):** Reduces the spatial dimensions (height and width) of the feature maps. This reduces the number of parameters and computation in the network, and helps the network become more invariant to small translations in the input. Max pooling selects the maximum value in a given window.\\n    *   **Flattening:** After the convolutional and pooling layers, the 2D feature maps are flattened into a 1D vector to be fed into the fully connected layers. `x.view(x.size(0), -1)` reshapes the tensor while keeping the batch size the same and automatically calculating the required size for the flattened dimension.\\n    *   **Fully Connected Layers (`nn.Linear`):** These are standard feedforward layers. They take the flattened features and perform classification based on them. The last layer has `out_features=10` corresponding to the 10 classes of CIFAR-10.\\n    *   **Dropout (`nn.Dropout`):** A regularization technique where random connections are temporarily ignored during training. This prevents units from becoming too dependent on each other and helps reduce overfitting.\\n6.  **Instantiate Model, Loss, and Optimizer:**\\n    *   `model = MyCNN().to(device)` creates an instance of our defined network and moves all its parameters to the specified device (GPU or CPU).\\n    *   `nn.CrossEntropyLoss()` is the standard loss function for multi-class classification problems. It measures the difference between the model's output logits and the true class labels.\\n    *   `optim.Adam()` is a popular optimization algorithm that updates the model's weights based on the gradients computed during backpropagation. It's often a good default choice. We pass `model.parameters()` so the optimizer knows which parameters to update.\\n7.  **Training Loop (`train_epoch`):**\\n    *   `model.train()` sets the model to training mode. This is important because layers like Dropout and BatchNorm behave differently during training (e.g., Dropout is active, BatchNorm tracks running mean/variance and uses batch statistics) vs. evaluation.\\n    *   We iterate through the `train_loader`, getting batches of `inputs` and `labels`.\\n    *   `optimizer.zero_grad()` clears the gradients from the *previous* training step. Gradients are accumulated by default, so this reset is crucial for calculating gradients correctly for the *current* batch.\\n    *   `outputs = model(inputs)` performs the forward pass, feeding the inputs through the network to get predictions.\\n    *   `loss = criterion(outputs, labels)` calculates the loss between the predicted outputs and the ground truth labels.\\n    *   `loss.backward()` calculates the gradients of the loss with respect to every trainable parameter in the model using backpropagation.\\n    *   `optimizer.step()` updates the model's parameters using the computed gradients and the chosen optimization algorithm (Adam in this case).\\n    *   `loss.item()` gets the scalar value of the loss tensor.\\n8.  **Evaluation Function (`evaluate`):**\\n    *   `model.eval()` sets the model to evaluation mode. Dropout is disabled, and BatchNorm layers use their saved running statistics instead of batch statistics.\\n    *   `with torch.no_grad():` is a context manager that disables gradient calculation. This is important during evaluation because we don't need to compute gradients, saving memory and computation.\\n    *   We iterate through the `test_loader`.\\n    *   `torch.max(outputs.data, 1)` finds the index (class prediction) with the highest probability/logit score along dimension 1 (the class dimension).\\n    *   We compare the `predicted` classes with the true `labels` to count correct predictions and calculate the accuracy.\\n9.  **Run Training and Evaluation:** The main loop iterates for the specified number of `num_epochs`. In each epoch, it calls the `train_epoch` function and then the `evaluate` function on the test set to track performance. It prints the loss and accuracy for each epoch. Saving the model based on the best test accuracy is a common practice.\\n10. **Plot Results:** `matplotlib` is used to visualize the training loss and test accuracy over the epochs, providing insight into the training process (e.g., if the model is overfitting, if the loss is decreasing).\\n11. **Load and Evaluate Final Model:** After training, we load the `state_dict` of the model that achieved the best accuracy during training and perform a final evaluation on the test set to confirm its performance. Loading just the `state_dict` is standard practice as it's more portable than saving the entire model object.\\n\\nThis script provides a solid foundation for training a CNN on CIFAR-10. You can extend it by adding more complex model architectures, different optimizers or schedulers, more advanced data augmentations, logging, etc.\",\n          \"Okay, let's create a PyTorch `Dataset` subclass to load FashionMNIST images and labels from a folder structure, and then use it with a `DataLoader`.\\n\\nSince FashionMNIST doesn't *natively* come as individual image files in class subfolders, we first need a pre-processing step to download the standard dataset and save it in the desired folder structure. Then, we'll build the custom `Dataset` class that reads from this structure.\\n\\n**Step 1: Understand the Goal and Data Structure**\\n\\nWe want to load data organized like this:\\n\\n```\\nfashion_mnist_folder/\\n\\u251c\\u2500\\u2500 train/\\n\\u2502   \\u251c\\u2500\\u2500 T-shirt_top/\\n\\u2502   \\u2502   \\u251c\\u2500\\u2500 img_00001.png\\n\\u2502   \\u2502   \\u251c\\u2500\\u2500 img_00002.png\\n\\u2502   \\u2502   \\u2514\\u2500\\u2500 ...\\n\\u2502   \\u251c\\u2500\\u2500 Trouser/\\n\\u2502   \\u2502   \\u251c\\u2500\\u2500 img_01001.png\\n\\u2502   \\u2502   \\u2514\\u2500\\u2500 ...\\n\\u2502   \\u2514\\u2500\\u2500 ... (other class folders)\\n\\u2514\\u2500\\u2500 test/\\n    \\u251c\\u2500\\u2500 T-shirt_top/\\n    \\u2502   \\u251c\\u2500\\u2500 img_60001.png\\n    \\u2502   \\u2514\\u2500\\u2500 ...\\n    \\u2514\\u2500\\u2500 ... (other class folders)\\n```\\n\\nOur custom `Dataset` will need to traverse this structure, find all images, associate them with their labels based on the folder name, and provide a way to load individual images and labels when indexed.\\n\\n**Step 2: Pre-processing - Saving FashionMNIST to Folders**\\n\\nThis part is *not* the custom `Dataset` itself, but a necessary setup step before our custom `Dataset` can be used. We will use `torchvision.datasets.FashionMNIST` to easily download the data and then save it into the target folder structure.\\n\\n*   We'll need to map the integer labels (0-9) from the original dataset to class names (e.g., 0 -> 'T-shirt/top').\\n*   We'll iterate through the training and test sets.\\n*   For each image and label, we'll create the appropriate directory (`root_dir/split/class_name/`) if it doesn't exist and save the image (which is a PIL Image) as a file (e.g., PNG).\\n\\n```python\\nimport torch\\nimport torchvision\\nimport torchvision.transforms as transforms\\nimport os\\nfrom PIL import Image\\nimport shutil\\n\\n# --- Pre-processing: Save FashionMNIST to Folder Structure ---\\n# This part downloads the data and organizes it into folders.\\n# You only need to run this once.\\n\\ndef save_fashion_mnist_to_folder(root_output_dir='./fashion_mnist_folder'):\\n    \\\"\\\"\\\"\\n    Downloads FashionMNIST and saves images into a folder structure\\n    like root_output_dir/split/class_name/image.png\\n    \\\"\\\"\\\"\\n    print(f\\\"Saving FashionMNIST to folders in {root_output_dir}...\\\")\\n\\n    # Define class names for FashionMNIST\\n    classes = [\\n        'T-shirt_top', 'Trouser', 'Pullover', 'Dress', 'Coat',\\n        'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle_boot'\\n    ]\\n\\n    # Ensure the root output directory is clean or created\\n    if os.path.exists(root_output_dir):\\n        print(f\\\"Removing existing directory: {root_output_dir}\\\")\\n        shutil.rmtree(root_output_dir)\\n    os.makedirs(root_output_dir)\\n\\n    # Download and process training data\\n    train_dataset_original = torchvision.datasets.FashionMNIST(\\n        root='./data', train=True, download=True\\n    )\\n    save_split(train_dataset_original, 'train', root_output_dir, classes)\\n\\n    # Download and process test data\\n    test_dataset_original = torchvision.datasets.FashionMNIST(\\n        root='./data', train=False, download=True\\n    )\\n    save_split(test_dataset_original, 'test', root_output_dir, classes)\\n\\n    print(\\\"FashionMNIST data saved successfully.\\\")\\n\\ndef save_split(dataset, split_name, root_output_dir, classes):\\n    \\\"\\\"\\\"Saves images from a single split (train/test) to folders.\\\"\\\"\\\"\\n    split_dir = os.path.join(root_output_dir, split_name)\\n    print(f\\\"Processing {split_name} split...\\\")\\n\\n    for class_name in classes:\\n        os.makedirs(os.path.join(split_dir, class_name), exist_ok=True)\\n\\n    for i, (img, label) in enumerate(dataset):\\n        class_name = classes[label]\\n        # FashionMNIST images are 28x28 grayscale PIL Images\\n        img_path = os.path.join(split_dir, class_name, f'img_{i:05d}.png')\\n        img.save(img_path) # PIL Image save\\n\\n# --- Run the pre-processing step ---\\n# Uncomment the line below to generate the folder structure.\\n# save_fashion_mnist_to_folder()\\n# print(\\\"Please run the save_fashion_mnist_to_folder() function to create the data structure.\\\")\\n\\n```\\n\\n**Reasoning for Pre-processing:**\\n\\n*   `torchvision.datasets.FashionMNIST` is the easiest way to get the data initially.\\n*   We need to convert the dataset object (which holds images and labels in memory or arrays) into a *file system* representation.\\n*   Saving as individual image files (like PNG) is a common practice and allows our custom `Dataset` to simulate loading data from scratch, as if it wasn't downloaded via `torchvision`.\\n*   Organizing by `split/class_name/` makes it straightforward for our `Dataset` to determine the label from the file path.\\n*   Using `PIL.Image.save()` is standard for saving image objects loaded or generated by PIL.\\n\\n**Step 3: Create the Custom `Dataset` Class**\\n\\nNow, we define `FashionMNISTFolderDataset` which inherits from `torch.utils.data.Dataset`.\\n\\n*   **`__init__(self, root_dir, split='train', transform=None)`:**\\n    *   Takes the `root_dir` (e.g., `./fashion_mnist_folder`), the `split` ('train' or 'test'), and an optional `transform`.\\n    *   It will build a list of all image file paths and their corresponding integer labels.\\n    *   It needs to map folder names (class names) to integer labels (0-9).\\n*   **`__len__(self)`:**\\n    *   Returns the total number of samples (images) in the dataset. This is simply the length of the list of (path, label) pairs created in `__init__`.\\n*   **`__getitem__(self, idx)`:**\\n    *   Takes an index `idx`.\\n    *   Retrieves the image path and label from the stored list at that index.\\n    *   Loads the image file using `PIL.Image.open()`.\\n    *   Applies the `transform` if provided.\\n    *   Returns the transformed image tensor and the label.\\n\\n```python\\nimport torch\\nfrom torch.utils.data import Dataset\\nimport torchvision.transforms as transforms\\nimport os\\nfrom PIL import Image\\n\\n# --- Custom Dataset Class ---\\n\\nclass FashionMNISTFolderDataset(Dataset):\\n    \\\"\\\"\\\"\\n    Custom Dataset to load FashionMNIST images from a folder structure\\n    like root_dir/split/class_name/image.png\\n    \\\"\\\"\\\"\\n    def __init__(self, root_dir, split='train', transform=None):\\n        \\\"\\\"\\\"\\n        Args:\\n            root_dir (string): Directory with all the split folders (e.g. 'fashion_mnist_folder/').\\n            split (string): 'train' or 'test' indicating the split to load.\\n            transform (callable, optional): Optional transform to be applied\\n                on a sample.\\n        \\\"\\\"\\\"\\n        self.root_dir = root_dir\\n        self.split = split\\n        self.transform = transform\\n        self.split_dir = os.path.join(root_dir, split)\\n\\n        if not os.path.exists(self.split_dir):\\n            raise FileNotFoundError(f\\\"Split directory not found: {self.split_dir}\\\")\\n\\n        # Store class names and create mapping from class name to index\\n        # FashionMNIST classes (sorted alphabetically would match the folder structure)\\n        self.classes = sorted(entry.name for entry in os.scandir(self.split_dir) if entry.is_dir())\\n        self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.classes)}\\n\\n        if not self.classes:\\n             raise RuntimeError(f\\\"No class directories found in {self.split_dir}\\\")\\n\\n\\n        # Build the list of (image_path, label) pairs\\n        self.samples = []\\n        for class_name in self.classes:\\n            class_dir = os.path.join(self.split_dir, class_name)\\n            if not os.path.isdir(class_dir):\\n                continue # Skip if it's not actually a directory\\n\\n            label = self.class_to_idx[class_name]\\n\\n            for img_file in os.listdir(class_dir):\\n                # Basic check for image files (can be expanded)\\n                if img_file.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif')):\\n                    img_path = os.path.join(class_dir, img_file)\\n                    self.samples.append((img_path, label))\\n\\n        if not self.samples:\\n            raise RuntimeError(f\\\"No images found in {self.split_dir}. Make sure images are saved in class subfolders.\\\")\\n\\n        print(f\\\"Found {len(self.samples)} images in the '{split}' split.\\\")\\n        print(f\\\"Classes: {self.classes}\\\")\\n\\n\\n    def __len__(self):\\n        \\\"\\\"\\\"Returns the total number of samples.\\\"\\\"\\\"\\n        return len(self.samples)\\n\\n    def __getitem__(self, idx):\\n        \\\"\\\"\\\"Loads and returns a sample from the dataset at the given index.\\\"\\\"\\\"\\n        if not (0 <= idx < len(self)):\\n            raise IndexError(f\\\"Index {idx} is out of bounds for dataset of size {len(self)}\\\")\\n\\n        img_path, label = self.samples[idx]\\n\\n        # Load image using Pillow (PIL)\\n        # FashionMNIST is grayscale, so convert to 'L'\\n        try:\\n            img = Image.open(img_path).convert('L')\\n        except Exception as e:\\n            print(f\\\"Error loading image {img_path}: {e}\\\")\\n            # Return a dummy or handle error appropriately, e.g., skip sample\\n            # For simplicity here, we'll raise, but a real system might skip\\n            raise\\n\\n        # Apply transforms\\n        if self.transform:\\n            img = self.transform(img)\\n\\n        return img, label # Return the image tensor and the label\\n\\n```\\n\\n**Reasoning for Custom Dataset:**\\n\\n*   **Inheritance:** Subclassing `torch.utils.data.Dataset` is the contract PyTorch expects. It requires implementing `__len__` and `__getitem__`.\\n*   **`__init__`:**\\n    *   It finds all relevant image files and stores their paths along with their corresponding labels. This is done upfront to avoid repeatedly scanning the file system during training/inference.\\n    *   It relies on the folder structure: the parent directory of the image file *is* the class name.\\n    *   It creates `self.classes` and `self.class_to_idx` which are standard attributes for image datasets, providing the mapping between integer labels and human-readable names.\\n    *   `self.samples` stores the list of `(path, label)` tuples that `__getitem__` will use.\\n*   **`__len__`:** Simple lookup of the total number of samples stored in `self.samples`.\\n*   **`__getitem__`:**\\n    *   Takes the index `idx`.\\n    *   Retrieves the pre-calculated `img_path` and `label`.\\n    *   Uses `PIL.Image.open()` to load the image file from disk. `.convert('L')` is used because FashionMNIST is grayscale. This ensures the image is in a consistent format before transformations.\\n    *   Applies the `self.transform`. This is crucial because transforms handle conversion to PyTorch tensors (`ToTensor`) and often include resizing, augmentation, or normalization needed for model input.\\n    *   Returns the processed image tensor and the integer label. This is the format `DataLoader` expects: a tensor for the data and a number for the label.\\n\\n**Step 4: Using the Custom Dataset with `DataLoader`**\\n\\nNow we can instantiate our `FashionMNISTFolderDataset` and pass it to a `torch.utils.data.DataLoader`. The `DataLoader` handles batching, shuffling, and loading data in parallel processes (`num_workers`).\\n\\n```python\\nimport torch\\nfrom torch.utils.data import DataLoader\\nimport torchvision.transforms as transforms\\nimport matplotlib.pyplot as plt # Optional: for visualization\\nimport numpy as np # Optional: for visualization\\n\\n# --- Usage with DataLoader ---\\n\\n# Define the directory where data was saved (modify if you changed it)\\nDATA_FOLDER_ROOT = './fashion_mnist_folder'\\n\\n# Define transformations\\n# Common transforms for image data:\\n# 1. ToTensor: Converts PIL Image (H x W) to PyTorch Tensor (C x H x W)\\n#    and scales pixel values from [0, 255] to [0.0, 1.0].\\n# 2. Normalize: Standardizes pixel values using mean and standard deviation.\\n#    FashionMNIST mean and std for grayscale: mean=0.286, std=0.353 (approx)\\ntransform = transforms.Compose([\\n    transforms.ToTensor(),\\n    transforms.Normalize((0.286,), (0.353,)) # Mean and Std for grayscale FashionMNIST\\n])\\n\\n# --- Instantiate the Dataset ---\\n# Make sure you have run save_fashion_mnist_to_folder() at least once\\n# to create the ./fashion_mnist_folder directory.\\ntry:\\n    train_dataset = FashionMNISTFolderDataset(\\n        root_dir=DATA_FOLDER_ROOT,\\n        split='train',\\n        transform=transform\\n    )\\n\\n    test_dataset = FashionMNISTFolderDataset(\\n        root_dir=DATA_FOLDER_ROOT,\\n        split='test',\\n        transform=transform\\n    )\\n\\nexcept FileNotFoundError as e:\\n    print(f\\\"Data folder not found: {e}\\\")\\n    print(\\\"Please run the save_fashion_mnist_to_folder() function first.\\\")\\n    exit() # Exit if data isn't ready\\n\\n# --- Instantiate the DataLoader ---\\nBATCH_SIZE = 64\\nNUM_WORKERS = 2 # Number of subprocesses for data loading (set to 0 for debugging)\\n\\ntrain_loader = DataLoader(\\n    train_dataset,\\n    batch_size=BATCH_SIZE,\\n    shuffle=True,      # Shuffle training data\\n    num_workers=NUM_WORKERS\\n)\\n\\ntest_loader = DataLoader(\\n    test_dataset,\\n    batch_size=BATCH_SIZE,\\n    shuffle=False,     # No need to shuffle test data\\n    num_workers=NUM_WORKERS\\n)\\n\\n# --- Example: Iterate through a batch ---\\nprint(\\\"\\\\nIterating through a batch from the DataLoader...\\\")\\n\\n# Get one batch\\nimages, labels = next(iter(train_loader))\\n\\nprint(f\\\"Batch of images shape: {images.shape}\\\") # Expected: [batch_size, channels, height, width] -> [64, 1, 28, 28]\\nprint(f\\\"Batch of labels shape: {labels.shape}\\\") # Expected: [batch_size]\\nprint(f\\\"Labels in batch: {labels}\\\")\\n\\n# Optional: Visualize a few images from the batch\\nprint(\\\"\\\\nDisplaying first 4 images from the batch:\\\")\\nfig = plt.figure(figsize=(8, 8))\\nfor i in range(4):\\n    ax = fig.add_subplot(2, 2, i + 1)\\n    ax.set_title(f\\\"Label: {train_dataset.classes[labels[i]]}\\\")\\n    ax.axis('off')\\n    # Undo normalization and permute dimensions for plotting (C, H, W) -> (H, W, C) or (H, W) for grayscale\\n    img_display = images[i] * 0.353 + 0.286 # Undo normalization\\n    img_display = img_display.squeeze() # Remove channel dimension if grayscale\\n    plt.imshow(img_display.numpy(), cmap='gray')\\nplt.tight_layout()\\nplt.show()\\n\\n\\nprint(\\\"\\\\nDataLoader setup complete. You can now use train_loader and test_loader.\\\")\\n```\\n\\n**Reasoning for DataLoader Usage:**\\n\\n*   **Instantiation:** You create instances of your custom `FashionMNISTFolderDataset` for training and testing, applying the desired transformations.\\n*   **`DataLoader(dataset, ...)`:** The `DataLoader` takes the dataset object as its primary argument.\\n*   **`batch_size`:** Specifies how many samples are grouped together in each batch returned by the loader.\\n*   **`shuffle=True`:** Important for training data to randomize the order of samples across epochs, helping the model generalize better. `shuffle=False` is typical for validation/testing.\\n*   **`num_workers`:** Controls how many subprocesses are used for data loading. `0` means the data loading is done in the main process. Using a small number (like 2 or 4) can speed up loading by fetching the next batch while the current one is being processed on the GPU, but setting it too high can consume excessive CPU/memory.\\n*   **Iteration:** You typically iterate through the `DataLoader` in your training loop (`for images, labels in train_loader:`). Each iteration yields a batch of image tensors and a batch of label tensors, ready to be passed to your model.\\n*   **`next(iter(train_loader))`:** A common way to get just *one* batch for inspection, as shown in the example.\\n\\n**Summary of the Full Process:**\\n\\n1.  **(One-time Setup)** Run the `save_fashion_mnist_to_folder()` function to download the original FashionMNIST data and save it as individual image files within a class-based folder structure.\\n2.  Define the `FashionMNISTFolderDataset` class, inheriting from `torch.utils.data.Dataset`. Implement `__init__`, `__len__`, and `__getitem__` to read from the created folder structure.\\n3.  Define any necessary `torchvision.transforms`.\\n4.  Instantiate the `FashionMNISTFolderDataset`, providing the root folder path, split (train/test), and transforms.\\n5.  Instantiate `torch.utils.data.DataLoader`, passing the dataset object, desired batch size, shuffle option, and number of workers.\\n6.  Use the `DataLoader` instances (e.g., `train_loader`, `test_loader`) in your training and evaluation loops.\\n\\nThis setup effectively demonstrates how to handle datasets that are stored as loose files in a directory structure, providing a flexible pattern applicable to many real-world image datasets.\",\n          \"Okay, let's create a Python function using TensorFlow to train a simple linear regression model on the California Housing dataset. We'll go through the steps: loading data, preparing it, defining the model, compiling it, and training it.\\n\\n**Step-by-step Reasoning and Code Generation:**\\n\\n1.  **Import necessary libraries:** We need TensorFlow for the model, `tensorflow_datasets` to load the dataset easily, and scikit-learn for data splitting and scaling (common preprocessing steps).\\n\\n    ```python\\n    import tensorflow as tf\\n    import tensorflow_datasets as tfds\\n    from sklearn.model_selection import train_test_split\\n    from sklearn.preprocessing import StandardScaler\\n    import numpy as np\\n    ```\\n    *   `tensorflow`: The core library.\\n    *   `tensorflow_datasets`: Provides convenient access to many standard datasets like California Housing.\\n    *   `train_test_split`: Essential for dividing our data into a set for training the model and a separate set for evaluating it. This prevents overfitting.\\n    *   `StandardScaler`: A standard preprocessing step for numerical data. It scales features to have zero mean and unit variance, which often helps optimization algorithms (like the one used in training) converge faster and perform better.\\n    *   `numpy`: Useful for handling numerical arrays, which the data will be converted into.\\n\\n2.  **Define the function:** We'll put all the logic inside a function for reusability.\\n\\n    ```python\\n    def train_linear_regression_california_housing(epochs=50, batch_size=32):\\n        # ... rest of the code will go here\\n        pass # Placeholder\\n    ```\\n    *   The function is named descriptively.\\n    *   We add parameters for `epochs` and `batch_size` as these are common hyperparameters for training. Default values are provided.\\n\\n3.  **Load the dataset:** We use `tensorflow_datasets` to load the California Housing dataset. We'll load the 'train' split initially and then manually split it further using scikit-learn, which is often more straightforward for applying preprocessing like scaling.\\n\\n    ```python\\n        # 1. Load the dataset\\n        print(\\\"Loading California Housing dataset...\\\")\\n        ds = tfds.load('california_housing', split='train', as_supervised=False)\\n        print(\\\"Dataset loaded.\\\")\\n    ```\\n    *   `tfds.load('california_housing', ...)`: Loads the dataset by name.\\n    *   `split='train'`: We load the official 'train' split provided by TFDS. Note that the California Housing dataset in TFDS only has a 'train' split, so we'll split this further ourselves.\\n    *   `as_supervised=False`: By default, `as_supervised=True` would return tuples of (features, label). Since the California Housing dataset has multiple features under a single key ('features') and the target ('median_house_value') is separate, `as_supervised=False` returns a dictionary for each example, which is easier to work with here.\\n\\n4.  **Prepare the data (Extract, Convert, Split, Scale):**\\n    *   Convert the `tf.data.Dataset` into NumPy arrays.\\n    *   Separate features (X) and target (y).\\n    *   Split into training and testing sets.\\n    *   Scale the features.\\n\\n    ```python\\n        # 2. Prepare the data\\n        print(\\\"Preparing data (Extracting, Splitting, Scaling)...\\\")\\n        # Convert dataset to numpy arrays\\n        data = list(ds.as_numpy_iterator())\\n        features = np.array([item['features'] for item in data])\\n        labels = np.array([item['median_house_value'] for item in data])\\n\\n        # Split data into training and testing sets\\n        # We'll use 80% for training, 20% for testing\\n        X_train, X_test, y_train, y_test = train_test_split(\\n            features, labels, test_size=0.2, random_state=42\\n        )\\n\\n        # Scale the features\\n        # Scaling is important for many optimization algorithms\\n        scaler = StandardScaler()\\n        X_train_scaled = scaler.fit_transform(X_train)\\n        X_test_scaled = scaler.transform(X_test)\\n\\n        # The target variable (house value) is also numerical,\\n        # and while not strictly necessary for basic linear regression correctness,\\n        # scaling the target can sometimes help training stability or interpretation\\n        # depending on the loss function and metrics used.\\n        # However, for standard MSE, scaling features is usually sufficient\\n        # and scaling the target requires inverse scaling for predictions.\\n        # Let's keep it simple and only scale features for this example.\\n        print(f\\\"Data shapes: X_train={X_train_scaled.shape}, y_train={y_train.shape}, X_test={X_test_scaled.shape}, y_test={y_test.shape}\\\")\\n        print(\\\"Data preparation complete.\\\")\\n\\n    ```\\n    *   `list(ds.as_numpy_iterator())`: Converts the `tf.data.Dataset` into a list of dictionaries, then `np.array([...])` converts the values from those dictionaries into NumPy arrays.\\n    *   `features = np.array([item['features'] for item in data])`: Extracts the feature arrays.\\n    *   `labels = np.array([item['median_house_value'] for item in data])`: Extracts the target variable.\\n    *   `train_test_split(...)`: Shuffles and splits the data. `test_size=0.2` means 20% goes to the test set. `random_state=42` ensures reproducibility.\\n    *   `StandardScaler()`: Creates a scaler object.\\n    *   `scaler.fit_transform(X_train)`: Calculates the mean and standard deviation *only* on the training data (`fit`) and then applies the scaling to the training data (`transform`).\\n    *   `scaler.transform(X_test)`: Applies the *same* scaling (using the mean/std from the training data) to the test data. It's crucial *not* to fit the scaler on the test data to avoid data leakage.\\n\\n5.  **Define the model:** A simple linear regression model can be represented by a single `Dense` layer with one unit and no activation function (or linear activation, which is the default).\\n\\n    ```python\\n        # 3. Define the model\\n        print(\\\"Defining the model...\\\")\\n        # A linear regression model is essentially a single neuron (dense layer with 1 unit)\\n        # with a linear activation function (which is the default).\\n        # The number of input features must match the input data shape.\\n        model = tf.keras.Sequential([\\n            tf.keras.layers.Dense(\\n                units=1,                  # One output unit for the single regression target\\n                input_shape=(X_train_scaled.shape[1],) # Input shape is the number of features\\n            )\\n        ])\\n        print(\\\"Model defined.\\\")\\n        model.summary()\\n    ```\\n    *   `tf.keras.Sequential([...])`: Creates a sequential model, which is a stack of layers.\\n    *   `tf.keras.layers.Dense(...)`: A fully connected layer.\\n    *   `units=1`: This layer has one neuron, which will produce a single output (our predicted house value).\\n    *   `input_shape=(X_train_scaled.shape[1],)`: Tells the first layer the expected shape of the input data. `X_train_scaled.shape[1]` gives the number of features. The comma is important to indicate it's a tuple representing the shape of *one* input sample (e.g., `(8,)` for 8 features).\\n    *   `model.summary()`: Prints a summary of the model's layers and parameters. For this model, the parameters will be `num_features * 1` (weights) + `1` (bias).\\n\\n6.  **Compile the model:** We need to specify the optimizer, the loss function, and any metrics to monitor.\\n\\n    ```python\\n        # 4. Compile the model\\n        print(\\\"Compiling the model...\\\")\\n        # For linear regression:\\n        # Optimizer: Adam is a common and effective choice. SGD or others could also work.\\n        # Loss: Mean Squared Error (MSE) is the standard loss function for regression.\\n        # Metrics: MSE and Mean Absolute Error (MAE) are useful for evaluation.\\n        model.compile(\\n            optimizer='adam', # Or tf.keras.optimizers.Adam()\\n            loss='mse',       # Mean Squared Error\\n            metrics=['mae', 'mse'] # Mean Absolute Error and Mean Squared Error\\n        )\\n        print(\\\"Model compiled.\\\")\\n    ```\\n    *   `model.compile(...)`: Configures the model for training.\\n    *   `optimizer='adam'`: Specifies the Adam optimization algorithm. This algorithm adapts the learning rate during training and often converges quickly.\\n    *   `loss='mse'`: Specifies Mean Squared Error as the loss function. The model will try to minimize this value during training. MSE is calculated as the average of the squared differences between the predicted and actual values.\\n    *   `metrics=['mae', 'mse']`: Specifies additional metrics to calculate and report during training and evaluation. MAE (Mean Absolute Error) is the average of the absolute differences. It's often easier to interpret than MSE as it's in the same units as the target variable. We include MSE again here to see its value directly during training.\\n\\n7.  **Train the model:** Use the `fit` method to train the model on the training data.\\n\\n    ```python\\n        # 5. Train the model\\n        print(f\\\"Training the model for {epochs} epochs with batch size {batch_size}...\\\")\\n        history = model.fit(\\n            X_train_scaled, y_train,\\n            validation_data=(X_test_scaled, y_test), # Monitor performance on the test set during training\\n            epochs=epochs,\\n            batch_size=batch_size,\\n            verbose=1 # Show training progress\\n        )\\n        print(\\\"Model training complete.\\\")\\n    ```\\n    *   `model.fit(...)`: Starts the training process.\\n    *   `X_train_scaled, y_train`: The input features and target values for training.\\n    *   `validation_data=(X_test_scaled, y_test)`: Provides data to evaluate the model on after each epoch. This helps monitor for overfitting (if validation loss starts increasing while training loss decreases).\\n    *   `epochs`: The number of times the model will iterate over the entire training dataset.\\n    *   `batch_size`: The number of samples processed before the model's weights are updated.\\n    *   `verbose=1`: Displays a progress bar and metrics for each epoch.\\n\\n8.  **Return results:** The function can return the trained model and/or the training history. The history object contains the loss and metric values recorded during training.\\n\\n    ```python\\n        # 6. Return the trained model and history\\n        return model, history\\n    ```\\n\\n9.  **Add execution block:** Include a main block to call the function and potentially print some results or evaluate the model.\\n\\n    ```python\\n    if __name__ == \\\"__main__\\\":\\n        # Set a random seed for reproducibility\\n        tf.random.set_seed(42)\\n        np.random.seed(42)\\n\\n        trained_model, training_history = train_linear_regression_california_housing(epochs=100, batch_size=64)\\n\\n        print(\\\"\\\\n--- Training Finished ---\\\")\\n        print(\\\"Trained Model Summary:\\\")\\n        trained_model.summary()\\n\\n        # Optional: Evaluate the model on the test set\\n        print(\\\"\\\\nEvaluating the model on the test set...\\\")\\n        # Need to reload/re-prepare test data for evaluation outside the function call\\n        # (or pass it back from the function, which is cleaner)\\n        # Let's modify the function to return the test data as well for evaluation\\n        # OR just use the history object which contains validation results.\\n        # History contains validation metrics per epoch.\\n        # To get final performance, we can evaluate the model directly.\\n        # Let's re-split/scale outside just for the final evaluation step demonstration.\\n\\n        # --- Re-prepare test data for final evaluation (demonstration) ---\\n        ds_eval = tfds.load('california_housing', split='train', as_supervised=False)\\n        data_eval = list(ds_eval.as_numpy_iterator())\\n        features_eval = np.array([item['features'] for item in data_eval])\\n        labels_eval = np.array([item['median_house_value'] for item in data_eval])\\n        # Use the same split and random_state as inside the function\\n        _, X_test_eval, _, y_test_eval = train_test_split(\\n            features_eval, labels_eval, test_size=0.2, random_state=42\\n        )\\n        # Use the *same* scaler fitted *only* on the training data (if scaler was returned)\\n        # Since the scaler wasn't returned, we need to refit it on training data again,\\n        # which is okay *only* for demonstrating post-training evaluation if we can't\\n        # reuse the scaler object directly. A better design might pass the scaler back.\\n        # For simplicity here, let's just fit a *new* scaler on the original *full* dataset\\n        # for evaluation features - this isn't ideal but works for demo if scaler isn't returned.\\n        # A *better* approach is to return the scaler from the function or evaluate within.\\n        # Let's stick to evaluating *within* the function before returning, or use history.\\n        # History has validation results, which is sufficient for this example.\\n        # Let's print the last validation metrics from history.\\n\\n        print(\\\"\\\\nValidation results from last epoch:\\\")\\n        # history.history is a dictionary containing lists of metric values per epoch\\n        last_epoch_metrics = {metric: values[-1] for metric, values in training_history.history.items()}\\n        print(last_epoch_metrics)\\n\\n        # Example prediction\\n        # Predict on the first few test samples\\n        print(\\\"\\\\nMaking predictions on the first 5 test samples:\\\")\\n        predictions = trained_model.predict(X_test_scaled[:5])\\n        print(\\\"Predicted values:\\\", predictions.flatten())\\n        print(\\\"Actual values:   \\\", y_test[:5])\\n\\n    ```\\n    *   `if __name__ == \\\"__main__\\\":`: Ensures this code runs only when the script is executed directly.\\n    *   `tf.random.set_seed(42)` and `np.random.seed(42)`: Sets seeds for reproducibility of TensorFlow operations and NumPy operations (like splitting).\\n    *   Calls the function.\\n    *   Prints the model summary again.\\n    *   Accesses the `history` object to print the final validation metrics.\\n    *   Demonstrates making predictions on a few samples from the scaled test set.\\n\\n10. **Assemble the final code:** Combine all the pieces into one script.\\n\\nThis detailed process breaks down the problem into manageable steps: data handling, model definition, configuration, training, and evaluation, explaining the purpose of each code segment along the way.\\n\\n```python\\nimport tensorflow as tf\\nimport tensorflow_datasets as tfds\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.preprocessing import StandardScaler\\nimport numpy as np\\n\\ndef train_linear_regression_california_housing(epochs=100, batch_size=64):\\n    \\\"\\\"\\\"\\n    Loads, prepares, and trains a simple linear regression model\\n    on the California Housing dataset using TensorFlow/Keras.\\n\\n    Args:\\n        epochs (int): Number of epochs to train for.\\n        batch_size (int): Batch size for training.\\n\\n    Returns:\\n        tuple: A tuple containing the trained Keras model and the training history object.\\n    \\\"\\\"\\\"\\n\\n    # --- Step-by-step Implementation ---\\n\\n    # 1. Load the dataset\\n    # Reasoning: We need the data to train and evaluate the model.\\n    # tfds provides easy access to standard datasets.\\n    print(\\\"Loading California Housing dataset...\\\")\\n    # Load the 'train' split. as_supervised=False gives a dict for each example.\\n    ds = tfds.load('california_housing', split='train', as_supervised=False)\\n    print(\\\"Dataset loaded.\\\")\\n\\n    # 2. Prepare the data (Extract, Convert, Split, Scale)\\n    # Reasoning:\\n    # - We need features (X) and targets (y) separately.\\n    # - Keras .fit() method typically expects numpy arrays or tf.data.Dataset.\\n    # - Splitting into train/test is crucial to evaluate model performance on unseen data.\\n    # - Scaling features helps the optimizer converge faster.\\n    print(\\\"Preparing data (Extracting, Splitting, Scaling)...\\\")\\n    # Convert dataset to numpy arrays for easier manipulation with sklearn\\n    data = list(ds.as_numpy_iterator())\\n    features = np.array([item['features'] for item in data])\\n    labels = np.array([item['median_house_value'] for item in data])\\n\\n    # Split data into training and testing sets (e.g., 80% train, 20% test)\\n    X_train, X_test, y_train, y_test = train_test_split(\\n        features, labels, test_size=0.2, random_state=42 # random_state for reproducibility\\n    )\\n\\n    # Scale the features using StandardScaler\\n    # Fit on training data, transform both training and testing data.\\n    scaler = StandardScaler()\\n    X_train_scaled = scaler.fit_transform(X_train)\\n    X_test_scaled = scaler.transform(X_test)\\n\\n    # Optional: Inspect shapes\\n    print(f\\\"Data shapes: X_train={X_train_scaled.shape}, y_train={y_train.shape}, X_test={X_test_scaled.shape}, y_test={y_test.shape}\\\")\\n    print(\\\"Data preparation complete.\\\")\\n\\n    # 3. Define the model\\n    # Reasoning:\\n    # - A simple linear regression model predicts y as a linear combination of X: y = w*X + b\\n    # - In Keras, a Dense layer with units=1 and linear activation (the default)\\n    #   performs exactly this calculation.\\n    # - Sequential model is used for a simple stack of layers.\\n    print(\\\"Defining the model (Simple Linear Regression with one Dense layer)...\\\")\\n    model = tf.keras.Sequential([\\n        # The first layer needs input_shape to know the number of features\\n        tf.keras.layers.Dense(\\n            units=1,                  # One output unit for the single regression target\\n            input_shape=(X_train_scaled.shape[1],) # Input shape is the number of features\\n        )\\n    ])\\n    print(\\\"Model defined.\\\")\\n    model.summary() # Print model architecture\\n\\n    # 4. Compile the model\\n    # Reasoning:\\n    # - Compilation configures the model for training.\\n    # - Optimizer: Algorithm to minimize the loss (e.g., Adam).\\n    # - Loss: Function to measure how far predictions are from actual values (MSE for regression).\\n    # - Metrics: Additional metrics to evaluate during training (MAE is often more interpretable than MSE).\\n    print(\\\"Compiling the model...\\\")\\n    model.compile(\\n        optimizer='adam',       # Adam optimizer is a good default\\n        loss='mse',             # Mean Squared Error, standard for regression\\n        metrics=['mae', 'mse']  # Mean Absolute Error and MSE to monitor\\n    )\\n    print(\\\"Model compiled.\\\")\\n\\n    # 5. Train the model\\n    # Reasoning:\\n    # - The .fit() method trains the model using the specified data and parameters.\\n    # - X_train_scaled, y_train: The training data.\\n    # - validation_data: Data to evaluate model performance *during* training (after each epoch).\\n    # - epochs: Number of passes over the entire training dataset.\\n    # - batch_size: Number of samples processed before a weight update.\\n    print(f\\\"Training the model for {epochs} epochs with batch size {batch_size}...\\\")\\n    history = model.fit(\\n        X_train_scaled, y_train,\\n        validation_data=(X_test_scaled, y_test), # Evaluate on test set after each epoch\\n        epochs=epochs,\\n        batch_size=batch_size,\\n        verbose=1 # Show training progress per epoch\\n    )\\n    print(\\\"Model training complete.\\\")\\n\\n    # 6. Return the trained model and history\\n    # Reasoning: The caller might want to use the model for predictions or analyze training progress.\\n    return model, history\\n\\n# --- Example Usage ---\\nif __name__ == \\\"__main__\\\":\\n    # Set seeds for reproducibility\\n    tf.random.set_seed(42)\\n    np.random.seed(42)\\n\\n    # Train the model by calling the function\\n    trained_model, training_history = train_linear_regression_california_housing(epochs=100, batch_size=64)\\n\\n    print(\\\"\\\\n--- Training Summary ---\\\")\\n    print(\\\"Trained Model Summary:\\\")\\n    trained_model.summary()\\n\\n    # Print final validation metrics from the training history\\n    print(\\\"\\\\nValidation results from the last epoch:\\\")\\n    # history.history is a dictionary containing lists of metric values per epoch\\n    last_epoch_metrics = {metric: values[-1] for metric, values in training_history.history.items()}\\n    print(last_epoch_metrics)\\n\\n    # --- Demonstrate Prediction ---\\n    # To make predictions, we need the test data and the scaler used during training.\\n    # A robust function would return the scaler, but for this simple example,\\n    # we'll just show the process assuming we have access to X_test_scaled.\\n    # In a real application, save the scaler or return it from the function.\\n\\n    # Let's reload/resplit the data just to get X_test and y_test again outside the function scope\\n    # (This is not ideal for general use, but demonstrates prediction flow)\\n    ds_eval = tfds.load('california_housing', split='train', as_supervised=False)\\n    data_eval = list(ds_eval.as_numpy_iterator())\\n    features_eval = np.array([item['features'] for item in data_eval])\\n    labels_eval = np.array([item['median_house_value'] for item in data_eval])\\n    _, X_test_raw, _, y_test_raw = train_test_split(\\n        features_eval, labels_eval, test_size=0.2, random_state=42 # Same split\\n    )\\n    # IMPORTANT: You MUST use the SAME scaler fitted on the training data!\\n    # Since we didn't return the scaler, we'd ideally reload it if saved, or refit on original training data here.\\n    # For demonstration, let's just create and fit a new scaler on the *full* data for prediction features.\\n    # ***WARNING: Fitting scaler on full data for evaluation/prediction is technically data leakage.\\n    # A proper flow returns the fitted scaler or saves/loads it.***\\n    # Re-creating the scaler fitted ONLY on original training data:\\n    temp_scaler = StandardScaler()\\n    temp_scaler.fit(train_test_split(features_eval, labels_eval, test_size=0.2, random_state=42)[0]) # Fit only on training part\\n\\n    X_test_scaled_for_pred = temp_scaler.transform(X_test_raw)\\n\\n\\n    print(\\\"\\\\nMaking predictions on the first 5 test samples:\\\")\\n    # Predict expects scaled input features\\n    predictions = trained_model.predict(X_test_scaled_for_pred[:5])\\n    print(\\\"Predicted values:\\\", predictions.flatten())\\n    print(\\\"Actual values:   \\\", y_test_raw[:5])\\n    # Note: Predictions are in the original scale of y because y was not scaled.\\n```\\n\\n**Explanation of Key Steps and Reasoning:**\\n\\n1.  **Import Libraries:** We gather the necessary tools from TensorFlow (for model building), TensorFlow Datasets (for easy data access), and Scikit-learn (for data splitting and scaling).\\n2.  **Load Dataset:** We use `tfds.load` to get the California Housing data. We load the 'train' split and keep it in a dictionary format (`as_supervised=False`) initially, as the features and target are separate keys.\\n3.  **Prepare Data:**\\n    *   We convert the TF dataset into NumPy arrays. This is convenient for using Scikit-learn's data preprocessing utilities.\\n    *   We separate the features (`X`) and the target (`y`). The goal of linear regression is to predict `y` from `X`.\\n    *   `train_test_split` divides the data into two subsets: one for training the model (`X_train`, `y_train`) and one for evaluating its performance (`X_test`, `y_test`). This prevents the model from simply memorizing the training data (overfitting). A `random_state` is used to ensure the split is the same every time the code runs.\\n    *   `StandardScaler` is applied to the features. Scaling brings all features to a similar range (mean 0, variance 1). This is important for gradient-based optimizers like Adam, as it can help them converge faster and more stably, especially when features have different scales. It's fitted *only* on the training data (`fit_transform`) to learn the scaling parameters, and then these parameters are applied to both training and test data (`transform`) to avoid data leakage from the test set into the training process.\\n4.  **Define Model:**\\n    *   We use `tf.keras.Sequential` because our model is a simple linear stack of layers.\\n    *   The model consists of a single `tf.keras.layers.Dense` layer.\\n    *   `units=1`: A `Dense` layer with one unit calculates a weighted sum of its inputs plus a bias term (`output = w1*x1 + w2*x2 + ... + wn*xn + b`). This is exactly the form of a simple linear regression equation.\\n    *   `input_shape=(X_train_scaled.shape[1],)`: This tells the layer how many input features to expect for each sample. `X_train_scaled.shape[1]` is the number of columns (features) in our scaled training data.\\n    *   The `activation` is by default 'linear', which is correct for linear regression (we don't want to apply a non-linear transformation to the output).\\n5.  **Compile Model:**\\n    *   `model.compile` prepares the model for the training process.\\n    *   `optimizer='adam'`: We choose the Adam optimizer. It's an efficient and widely used algorithm for updating the model's weights based on the training data and the calculated loss.\\n    *   `loss='mse'`: We select Mean Squared Error (MSE) as the loss function. This is the standard loss for regression problems. The training process aims to minimize the average of the squared differences between the model's predictions and the actual target values.\\n    *   `metrics=['mae', 'mse']`: We specify Mean Absolute Error (MAE) and MSE as metrics to monitor. Metrics are displayed during training but *not* used to optimize the model (only the loss function is used for optimization). MAE is often easier to interpret than MSE as it's in the same units as the target variable.\\n6.  **Train Model:**\\n    *   `model.fit` starts the training loop.\\n    *   `X_train_scaled` and `y_train` are the input and output data used to train the model.\\n    *   `validation_data=(X_test_scaled, y_test)` provides the test set to evaluate the model's performance after each training epoch. This helps monitor if the model is learning well or starting to overfit (validation loss increases).\\n    *   `epochs` is the number of times the training algorithm iterates through the entire training dataset.\\n    *   `batch_size` determines how many samples are processed at a time within each epoch before the model's weights are updated.\\n    *   `verbose=1` displays training progress.\\n7.  **Return Results:** The function returns the trained `model` object, which can be used for making predictions, and the `history` object, which contains the recorded loss and metric values per epoch for both training and validation sets.\\n\\nThis function provides a clear and well-structured way to perform a simple linear regression task using TensorFlow on a common dataset, following standard machine learning practices like data splitting and scaling.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"few_shot_gemini\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"Okay, here is a complete Python script using PyTorch to train a Convolutional Neural Network (CNN) on the CIFAR-10 dataset.\\n\\nThis script covers:\\n1.  **Imports:** Necessary libraries.\\n2.  **Device Configuration:** Checks for CUDA availability.\\n3.  **Hyperparameters:** Sets learning rate, batch size, epochs, etc.\\n4.  **Data Loading:** Downloads and loads CIFAR-10 with appropriate transformations.\\n5.  **Model Definition:** Defines a simple CNN architecture.\\n6.  **Loss Function and Optimizer:** Sets up `CrossEntropyLoss` and `Adam` optimizer.\\n7.  **Training Loop:** Iterates through epochs, performs forward/backward passes, and updates weights.\\n8.  **Evaluation:** Evaluates the model on the test set after training.\\n\\n```python\\nimport torch\\nimport torch.nn as nn\\nimport torch.optim as optim\\nimport torchvision\\nimport torchvision.transforms as transforms\\nfrom torch.utils.data import DataLoader\\nimport matplotlib.pyplot as plt # For optional plotting\\n\\n# --- 1. Device Configuration ---\\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\\nprint(f'Using device: {device}')\\n\\n# --- 2. Hyperparameters ---\\nnum_epochs = 10\\nbatch_size = 100\\nlearning_rate = 0.001\\n\\n# CIFAR-10 has 10 classes (0-9)\\nnum_classes = 10\\n\\n# Input image dimensions (3 channels for RGB, 32x32 pixels)\\nin_channels = 3\\n\\n# --- 3. Data Loading ---\\n\\n# Transformations for the data\\n# CIFAR-10 images are 32x32.\\n# We convert images to PyTorch tensors and normalize them.\\n# Standard normalization values for CIFAR-10 are often used.\\ntransform = transforms.Compose([\\n    transforms.ToTensor(),\\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) # Normalize to [-1, 1] range\\n])\\n\\n# Load CIFAR-10 training dataset\\ntrain_dataset = torchvision.datasets.CIFAR10(root='./data', train=True,\\n                                             download=True, transform=transform)\\n\\n# Load CIFAR-10 test dataset\\ntest_dataset = torchvision.datasets.CIFAR10(root='./data', train=False,\\n                                            download=True, transform=transform)\\n\\n# Create data loaders\\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False) # No need to shuffle test data\\n\\n# --- 4. Model Definition ---\\n\\n# Simple CNN model\\nclass SimpleCNN(nn.Module):\\n    def __init__(self, in_channels, num_classes):\\n        super(SimpleCNN, self).__init__()\\n        # First convolutional layer\\n        # Input: 3x32x32\\n        # Output: 32 feature maps, 32x32 size (padding=1 preserves size for kernel=3)\\n        self.conv1 = nn.Conv2d(in_channels, 32, kernel_size=3, stride=1, padding=1)\\n        self.relu1 = nn.ReLU()\\n        # First pooling layer\\n        # Input: 32x32x32\\n        # Output: 32 feature maps, 16x16 size (MaxPool2d halves dimensions)\\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\\n\\n        # Second convolutional layer\\n        # Input: 32x16x16\\n        # Output: 64 feature maps, 16x16 size\\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\\n        self.relu2 = nn.ReLU()\\n        # Second pooling layer\\n        # Input: 64x16x16\\n        # Output: 64 feature maps, 8x8 size\\n        # self.pool (reusing the same pooling layer definition)\\n\\n        # Third convolutional layer (optional, but good for slightly deeper networks)\\n        # Input: 64x8x8\\n        # Output: 128 feature maps, 8x8 size\\n        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\\n        self.relu3 = nn.ReLU()\\n        # Third pooling layer\\n        # Input: 128x8x8\\n        # Output: 128 feature maps, 4x4 size\\n        # self.pool (reusing)\\n\\n\\n        # Fully connected layers\\n        # Input size calculation: After 3 conv/relu/pool blocks (with kernel=3, padding=1 and pool=2),\\n        # the spatial dimensions go from 32 -> 16 -> 8 -> 4.\\n        # The last convolutional layer had 128 output channels.\\n        # Flattened size: 128 * 4 * 4 = 2048\\n        self.fc1 = nn.Linear(128 * 4 * 4, 512) # First fully connected layer\\n        self.relu4 = nn.ReLU()\\n        self.fc2 = nn.Linear(512, num_classes) # Output layer\\n\\n    def forward(self, x):\\n        # Conv 1 -> ReLU 1 -> Pool 1\\n        x = self.pool(self.relu1(self.conv1(x)))\\n        # Conv 2 -> ReLU 2 -> Pool 2\\n        x = self.pool(self.relu2(self.conv2(x)))\\n        # Conv 3 -> ReLU 3 -> Pool 3\\n        x = self.pool(self.relu3(self.conv3(x)))\\n\\n        # Flatten the tensor for the fully connected layers\\n        # -1 tells PyTorch to infer the dimension\\n        x = x.view(x.size(0), -1) # or use torch.flatten(x, 1)\\n\\n        # Fully connected layers\\n        x = self.relu4(self.fc1(x))\\n        x = self.fc2(x) # Output layer (logits)\\n        return x\\n\\n# Instantiate the model and move to device\\nmodel = SimpleCNN(in_channels, num_classes).to(device)\\n\\n# --- 5. Loss and Optimizer ---\\ncriterion = nn.CrossEntropyLoss() # Suitable for multi-class classification\\noptimizer = optim.Adam(model.parameters(), lr=learning_rate) # Adam optimizer\\n\\n# --- 7. Training Loop ---\\nprint(\\\"\\\\nStarting training...\\\")\\n\\n# Lists to store loss for plotting (optional)\\n# train_losses = []\\n# test_accuracies = []\\n\\ntotal_steps = len(train_loader)\\nfor epoch in range(num_epochs):\\n    model.train() # Set the model to training mode\\n    running_loss = 0.0\\n    for i, (images, labels) in enumerate(train_loader):\\n        # Move tensors to the configured device\\n        images = images.to(device)\\n        labels = labels.to(device)\\n\\n        # Forward pass\\n        outputs = model(images)\\n        loss = criterion(outputs, labels)\\n\\n        # Backward and optimize\\n        optimizer.zero_grad() # Clear previous gradients\\n        loss.backward()       # Compute gradient of the loss w.r.t. model parameters\\n        optimizer.step()      # Perform a single optimization step (parameter update)\\n\\n        running_loss += loss.item()\\n\\n        # Print training progress (optional)\\n        # if (i+1) % 100 == 0: # Print every 100 steps\\n        #     print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{total_steps}], Loss: {loss.item():.4f}')\\n\\n    avg_train_loss = running_loss / total_steps\\n    print(f'Epoch [{epoch+1}/{num_epochs}], Average Training Loss: {avg_train_loss:.4f}')\\n    # train_losses.append(avg_train_loss) # Store loss\\n\\n    # --- 8. Evaluation (after each epoch) ---\\n    # In evaluation mode, we don't need to track gradients\\n    model.eval()\\n    with torch.no_grad():\\n        correct = 0\\n        total = 0\\n        for images, labels in test_loader:\\n            images = images.to(device)\\n            labels = labels.to(device)\\n            outputs = model(images)\\n            # Get predictions from the maximum value\\n            _, predicted = torch.max(outputs.data, 1)\\n            total += labels.size(0)\\n            correct += (predicted == labels).sum().item()\\n\\n        accuracy = 100 * correct / total\\n        print(f'Test Accuracy of the model on the {total} test images: {accuracy:.2f} %')\\n        # test_accuracies.append(accuracy) # Store accuracy\\n\\nprint(\\\"\\\\nTraining finished.\\\")\\n\\n# Optional: Plotting training loss and test accuracy over epochs\\n# plt.figure(figsize=(10, 5))\\n# plt.subplot(1, 2, 1)\\n# plt.plot(range(1, num_epochs + 1), train_losses)\\n# plt.xlabel(\\\"Epoch\\\")\\n# plt.ylabel(\\\"Training Loss\\\")\\n# plt.title(\\\"Training Loss per Epoch\\\")\\n\\n# plt.subplot(1, 2, 2)\\n# plt.plot(range(1, num_epochs + 1), test_accuracies)\\n# plt.xlabel(\\\"Epoch\\\")\\n# plt.ylabel(\\\"Test Accuracy (%)\\\")\\n# plt.title(\\\"Test Accuracy per Epoch\\\")\\n\\n# plt.tight_layout()\\n# plt.show()\\n\\n# --- Optional: Save the model ---\\n# torch.save(model.state_dict(), 'cifar10_cnn_model.pth')\\n# print(\\\"Model saved to cifar10_cnn_model.pth\\\")\\n\\n```\\n\\n**Explanation:**\\n\\n1.  **Imports:** We import necessary modules from `torch`, `torch.nn`, `torch.optim`, and `torchvision`.\\n2.  **Device Configuration:** This block checks if a CUDA-enabled GPU is available and sets the `device` variable accordingly. It's crucial to move your model and data tensors to the correct device.\\n3.  **Hyperparameters:** Essential parameters like the number of training epochs, batch size, learning rate, and dataset-specific values like the number of classes and input channels are defined.\\n4.  **Data Loading:**\\n    *   `transforms.Compose` chains multiple transformations together. `ToTensor()` converts PIL Images or NumPy arrays to PyTorch tensors (scaling pixel values to \\\\[0, 1]). `Normalize` scales the pixel values to a range around zero (here, \\\\[-1, 1]) using the mean and standard deviation for each channel.\\n    *   `torchvision.datasets.CIFAR10` downloads (if not already present) and loads the CIFAR-10 dataset. `train=True` gets the training set, `train=False` gets the test set.\\n    *   `DataLoader` wraps the dataset to provide an iterable over batches. `shuffle=True` for the training loader is important for better training.\\n5.  **Model Definition (`SimpleCNN`):**\\n    *   This class inherits from `nn.Module`.\\n    *   The `__init__` method defines the layers:\\n        *   `nn.Conv2d`: Performs convolution. `in_channels` is the number of input feature maps (3 for RGB). `out_channels` is the number of feature maps produced by this layer. `kernel_size` is the filter size. `padding=1` with `kernel_size=3` and `stride=1` helps maintain the spatial dimensions after convolution.\\n        *   `nn.ReLU`: The Rectified Linear Unit activation function, applied after each convolutional layer.\\n        *   `nn.MaxPool2d`: Downsamples the spatial dimensions (height and width). `kernel_size=2` and `stride=2` is standard for halving the dimensions.\\n        *   `nn.Linear`: A fully connected layer. The input size is calculated based on the output dimensions and feature maps from the last pooling layer. `128 * 4 * 4` comes from 128 output channels of `conv3` and spatial dimensions of `4x4` after the last pooling layer.\\n    *   The `forward` method defines how data flows through the network: it applies the layers sequentially, flattens the tensor before the linear layers, and returns the raw output (logits).\\n6.  **Loss and Optimizer:**\\n    *   `nn.CrossEntropyLoss` is used because it combines the Softmax activation and the negative log-likelihood loss, which is suitable for multi-class classification.\\n    *   `optim.Adam` is a popular optimization algorithm used to update the model's weights based on the calculated gradients. We pass the model's parameters and the learning rate.\\n7.  **Training Loop:**\\n    *   The outer loop iterates through epochs.\\n    *   `model.train()` sets the model to training mode (enables features like dropout if used, though not in this simple model).\\n    *   The inner loop iterates through batches provided by the `train_loader`.\\n    *   Data and labels are moved to the specified `device`.\\n    *   A forward pass (`model(images)`) computes the predictions.\\n    *   The `criterion` calculates the loss between predictions and true labels.\\n    *   `optimizer.zero_grad()` clears gradients from the previous batch.\\n    *   `loss.backward()` computes gradients for all parameters w.r.t. the loss.\\n    *   `optimizer.step()` updates the model parameters using the gradients and the optimizer's rules.\\n8.  **Evaluation:**\\n    *   `model.eval()` sets the model to evaluation mode (disables features like dropout, ensures batch normalization uses global stats if present).\\n    *   `with torch.no_grad():` disables gradient calculation, which saves memory and speeds up computation during evaluation since we don't need to backpropagate.\\n    *   The loop iterates through the `test_loader`.\\n    *   Predictions are made. `torch.max(outputs.data, 1)` returns the maximum value and its index along dimension 1 (the class dimension). The index is the predicted class.\\n    *   `total` and `correct` counters are updated to calculate the overall accuracy.\\n    *   The test accuracy is printed after each epoch.\\n\\nThis script provides a solid foundation for training CNNs on image classification tasks like CIFAR-10. You can extend it by adding more complex layers (like Batch Normalization, Dropout), implementing learning rate scheduling, or saving the best model checkpoint.\",\n          \"Okay, you've provided examples of a custom dataset class loading from files/CSV and another custom dataset class loading synthetic data.\\n\\nFor the FashionMNIST dataset, PyTorch's `torchvision.datasets` module already provides a `Dataset` subclass specifically designed to load this dataset. It handles downloading the data (if needed) and parsing the files. So, the standard approach is to use this built-in class rather than writing a *new* custom class from scratch to parse the FashionMNIST files yourself.\\n\\nHere's how you use the existing `torchvision.datasets.FashionMNIST` class with a `DataLoader`:\\n\\n```python\\nimport torch\\nfrom torch.utils.data import Dataset, DataLoader # Dataset is implicitly used by FashionMNIST\\nimport torchvision\\nimport torchvision.transforms as transforms\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\n# --- Configuration ---\\nBATCH_SIZE = 64\\n# Directory to save/load the dataset files\\nDATA_DIR = './data'\\n\\n# --- Transformations ---\\n# FashionMNIST images are PIL images. We need to convert them to PyTorch tensors\\n# and normalize the pixel values to the range [0, 1]. ToTensor() does both.\\ntransform = transforms.Compose([\\n    transforms.ToTensor(),\\n    # Optional: Normalize the images.\\n    # FashionMNIST mean: 0.286, std: 0.353 (calculated over training set)\\n    # transforms.Normalize((0.286,), (0.353,))\\n])\\n\\n# --- Load the FashionMNIST Dataset ---\\n# This class IS the torch.utils.data.Dataset subclass for FashionMNIST\\n# It handles finding/downloading the data files and implements __len__ and __getitem__.\\ntry:\\n    train_dataset = torchvision.datasets.FashionMNIST(\\n        root=DATA_DIR,         # Directory to store data\\n        train=True,            # Load training data\\n        download=True,         # Download data if not available\\n        transform=transform    # Apply the defined transformation\\n    )\\n\\n    # You can also load the test set if needed\\n    # test_dataset = torchvision.datasets.FashionMNIST(\\n    #     root=DATA_DIR,\\n    #     train=False,           # Load test data\\n    #     download=True,\\n    #     transform=transform\\n    # )\\n\\n    print(f\\\"Number of training samples: {len(train_dataset)}\\\")\\n    # print(f\\\"Number of test samples: {len(test_dataset)}\\\")\\n\\nexcept Exception as e:\\n    print(f\\\"Error loading FashionMNIST dataset: {e}\\\")\\n    print(\\\"Please check your internet connection and disk space.\\\")\\n    exit() # Exit if data loading fails\\n\\n# --- Create DataLoader ---\\ntrain_dataloader = DataLoader(\\n    dataset=train_dataset,  # Your dataset object\\n    batch_size=BATCH_SIZE, # How many samples per batch\\n    shuffle=True,           # Shuffle data at the beginning of each epoch (good for training)\\n    num_workers=2           # Number of subprocesses to use for data loading (can speed things up)\\n)\\n\\n# --- Demonstrate Using the DataLoader ---\\nprint(\\\"\\\\nDemonstrating data loading with DataLoader:\\\")\\n\\n# Iterate through the first batch\\nfor images, labels in train_dataloader:\\n    print(f\\\"Batch of images shape: {images.shape}\\\") # Expected: [BATCH_SIZE, 1, 28, 28] (Batch, Channels, Height, Width)\\n    print(f\\\"Batch of labels shape: {labels.shape}\\\") # Expected: [BATCH_SIZE]\\n\\n    # Optional: Display the first image in the batch (requires matplotlib)\\n    if images.shape[0] > 0:\\n        # FashionMNIST is grayscale (1 channel)\\n        plt.imshow(images[0].squeeze(), cmap='gray')\\n        plt.title(f\\\"Example Image (Label: {labels[0].item()})\\\")\\n        plt.axis('off')\\n        plt.show()\\n\\n    # We'll just process one batch for demonstration\\n    break\\n\\nprint(\\\"\\\\nDataLoader setup complete. Ready to use in a training loop.\\\")\\n\\n# Example of a simple training loop structure (no actual training)\\n# model = ... # Define your PyTorch model\\n# criterion = ... # Define your loss function\\n# optimizer = ... # Define your optimizer\\n\\n# num_epochs = 5\\n# for epoch in range(num_epochs):\\n#     for i, (images, labels) in enumerate(train_dataloader):\\n#         # Move tensors to the configured device (e.g., GPU)\\n#         # images = images.to(device)\\n#         # labels = labels.to(device)\\n\\n#         # Forward pass\\n#         # outputs = model(images)\\n#         # loss = criterion(outputs, labels)\\n\\n#         # Backward and optimize\\n#         # optimizer.zero_grad()\\n#         # loss.backward()\\n#         # optimizer.step()\\n\\n#         # Print progress (optional)\\n#         # if (i+1) % 100 == 0:\\n#         #     print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_dataloader)}], Loss: {loss.item():.4f}')\\n\\n```\\n\\n**Explanation:**\\n\\n1.  **`torchvision.datasets.FashionMNIST`:** This class is already a subclass of `torch.utils.data.Dataset`. When you instantiate it, you point it to a directory (`root`), specify whether you want the training or test split (`train=True`), tell it to download the data if it's not found (`download=True`), and provide any transformations (`transform`). It handles all the details of loading the image and label files.\\n2.  **`transforms.ToTensor()`:** FashionMNIST images are loaded as PIL Images by `torchvision.datasets`. Neural networks work with tensors. `ToTensor()` is a crucial transform that converts a PIL Image or NumPy array (`H x W x C`) into a PyTorch Tensor (`C x H x W`) and automatically scales the pixel values from [0, 255] (uint8) to [0.0, 1.0] (float32).\\n3.  **`DataLoader`:** You then wrap the `train_dataset` object (which is an instance of the `FashionMNIST` `Dataset` subclass) in a `DataLoader`. You specify the `batch_size`, whether to `shuffle` the data (important for training to prevent the model from learning the order), and `num_workers` (how many separate processes to use for data loading, which can speed up fetching batches).\\n4.  **Iteration:** The `DataLoader` becomes an iterable. You can loop through it (e.g., `for images, labels in train_dataloader:`) to get batches of data and labels that are ready to be fed into a PyTorch model.\\n\\nThis is the standard and efficient way to work with well-known datasets like FashionMNIST in PyTorch. You don't need to write a *new* `Dataset` class from scratch unless you are working with a custom dataset that `torchvision.datasets` doesn't cover.\",\n          \"Okay, here is the Python function using TensorFlow/Keras to create, compile, and train a simple linear regression model on the California Housing dataset, including a normalization layer and one dense layer.\\n\\n```python\\nimport numpy as np\\nimport tensorflow as tf\\nfrom tensorflow import keras\\nfrom tensorflow.keras import layers\\nfrom sklearn.datasets import fetch_california_housing\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.preprocessing import StandardScaler # Optional: alternative to Keras Normalization\\n\\ndef create_and_train_california_housing_model():\\n    \\\"\\\"\\\"\\n    Creates, compiles, and trains a simple linear regression model\\n    with a Normalization layer and one Dense layer on the California\\n    Housing dataset.\\n\\n    Returns:\\n        tuple: A tuple containing the trained Keras model and\\n               the training history object.\\n    \\\"\\\"\\\"\\n    print(\\\"Loading California Housing dataset...\\\")\\n    # Load the dataset\\n    # as_frame=True loads it into a pandas DataFrame\\n    housing = fetch_california_housing(as_frame=True)\\n    X, y = housing.data, housing.target\\n\\n    print(f\\\"Dataset loaded. Features shape: {X.shape}, Target shape: {y.shape}\\\")\\n\\n    # Split data into training and testing sets\\n    # Using a test set allows evaluating the final model performance\\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\\n\\n    print(f\\\"Data split: Train size {X_train.shape[0]}, Test size {X_test.shape[0]}\\\")\\n\\n    # --- Keras Model Definition ---\\n\\n    # 1. Create a Normalization Layer\\n    # This layer will normalize the input features.\\n    # input_shape needs to be the number of features (8 for California Housing).\\n    # axis=-1 normalizes each feature independently across the batch dimension.\\n    normalizer = layers.Normalization(input_shape=[X_train.shape[1],], axis=-1)\\n\\n    # Adapt the normalizer to the training data\\n    # This calculates the mean and variance for normalization\\n    print(\\\"Adapting normalization layer to training data...\\\")\\n    normalizer.adapt(X_train)\\n    print(\\\"Normalization layer adapted.\\\")\\n\\n\\n    # 2. Create the Sequential Model\\n    # The model has two layers:\\n    # - The normalization layer (to scale features)\\n    # - A Dense layer with 1 unit (for linear regression output)\\n    model = keras.Sequential([\\n        normalizer,               # Normalize input features\\n        layers.Dense(units=1)     # Single dense layer for linear output\\n    ])\\n\\n    # Display the model summary\\n    model.summary()\\n\\n    # 3. Compile the Model\\n    # Define the optimizer and the loss function\\n    model.compile(\\n        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), # Adam optimizer\\n        loss='mean_squared_error'                              # MSE loss for regression\\n    )\\n    print(\\\"Model compiled with Adam optimizer and MSE loss.\\\")\\n\\n    # 4. Train the Model\\n    print(\\\"Starting model training...\\\")\\n    history = model.fit(\\n        X_train,            # Training features\\n        y_train,            # Training labels (target)\\n        epochs=100,         # Number of training epochs\\n        verbose=1,          # Show training progress\\n        # Use a validation split from the training data to monitor performance\\n        # during training without touching the separate test set.\\n        validation_split=0.2\\n    )\\n    print(\\\"Training finished.\\\")\\n\\n    # 5. Evaluate the Model on the Test Set (Optional but recommended)\\n    print(\\\"\\\\nEvaluating model on the test set...\\\")\\n    loss = model.evaluate(X_test, y_test, verbose=0)\\n    print(f\\\"Test Loss (Mean Squared Error): {loss:.4f}\\\")\\n\\n    # 6. Make Predictions (Optional)\\n    print(\\\"\\\\nMaking predictions on the first 5 test samples...\\\")\\n    predictions = model.predict(X_test[:5])\\n    print(\\\"Predictions:\\\")\\n    print(predictions)\\n    print(\\\"Actual values:\\\")\\n    print(y_test[:5].values.reshape(-1, 1)) # Reshape for comparison\\n\\n    return model, history\\n\\n# --- How to use the function ---\\nif __name__ == \\\"__main__\\\":\\n    trained_model, training_history = create_and_train_california_housing_model()\\n\\n    # You can access training history like this:\\n    # print(\\\"\\\\nTraining History Keys:\\\", training_history.history.keys())\\n\\n    # You can plot the training history (e.g., loss vs. epoch)\\n    # import matplotlib.pyplot as plt\\n    # plt.plot(training_history.history['loss'], label='loss')\\n    # plt.plot(training_history.history['val_loss'], label='val_loss')\\n    # plt.xlabel('Epoch')\\n    # plt.ylabel('Loss (MSE)')\\n    # plt.legend()\\n    # plt.grid(True)\\n    # plt.title('Training and Validation Loss')\\n    # plt.show()\\n```\\n\\n**Explanation:**\\n\\n1.  **Import necessary libraries:** TensorFlow, Keras layers, NumPy, and specific parts from scikit-learn for loading and splitting the dataset.\\n2.  **`create_and_train_california_housing_model()` function:** Encapsulates the entire process.\\n3.  **Load Data:** `fetch_california_housing()` loads the dataset. `as_frame=True` provides it as a Pandas DataFrame, which is convenient. We separate features (`X`) and target (`y`).\\n4.  **Split Data:** `train_test_split` divides the data into training and testing sets. It's crucial to evaluate the final model on data it hasn't seen during training.\\n5.  **Normalization Layer:** `layers.Normalization` is created. `input_shape` is set to `[X_train.shape[1],]` because the California Housing dataset has 8 features, and we are feeding all of them to the model. `axis=-1` ensures each feature is normalized independently. `normalizer.adapt(X_train)` calculates the necessary statistics (mean and variance) *only* from the training data.\\n6.  **Model Definition:** A `keras.Sequential` model is created. It contains two layers:\\n    *   The `normalizer` layer takes the raw input features and outputs their normalized versions.\\n    *   A `layers.Dense(units=1)` layer takes the normalized features and computes a single output value using a linear transformation (`output = sum(weights * inputs) + bias`). This single dense layer with one output unit *is* the core of the linear regression model.\\n7.  **Compile Model:**\\n    *   `optimizer='adam'` (or `tf.keras.optimizers.Adam(learning_rate=...)`) is a popular and usually effective optimizer.\\n    *   `loss='mean_squared_error'` is the standard loss function for linear regression problems (minimizing MSE is equivalent to maximizing likelihood under the assumption of Gaussian noise).\\n8.  **Train Model:** `model.fit()` performs the training.\\n    *   `X_train`, `y_train`: The data to train on.\\n    *   `epochs=100`: The model iterates through the training data 100 times.\\n    *   `verbose=1`: Prints progress during training.\\n    *   `validation_split=0.2`: Automatically holds back 20% of the *training data* to use as a validation set. The model is *not* trained on this portion, but the loss on this portion is calculated and reported at the end of each epoch, which helps monitor for overfitting.\\n9.  **Evaluate (Optional):** `model.evaluate()` calculates the loss on the separate `X_test`, `y_test` set. This gives a final, unbiased estimate of the model's performance on unseen data.\\n10. **Predict (Optional):** `model.predict()` generates predictions for new input data (here, the first 5 test samples).\\n\\nThis script effectively implements a simple linear regression model using the Keras API, including the recommended practice of normalizing input features.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["def extract_python_code(text):\n","    code_blocks = re.findall(r\"```(?:python)?(.*?)```\", text, re.DOTALL)\n","    code_blocks = [code.strip() for code in code_blocks]\n","    return code_blocks"],"metadata":{"id":"SVvCaqPyu9Nx","executionInfo":{"status":"ok","timestamp":1749026066125,"user_tz":420,"elapsed":1,"user":{"displayName":"Vishesh Agrawal","userId":"16926083185186864017"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["extract_python_code(df['few_shot_gemini'][7])[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":191},"id":"APUdMMf6vEcs","executionInfo":{"status":"ok","timestamp":1749027097376,"user_tz":420,"elapsed":23,"user":{"displayName":"Vishesh Agrawal","userId":"16926083185186864017"}},"outputId":"c5205d85-c1ff-406c-9ded-8b4f14566f97"},"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'java\\n// Required imports for Hadoop MapReduce\\nimport java.io.IOException;\\nimport java.util.StringTokenizer; // Used for splitting lines into words\\n\\nimport org.apache.hadoop.conf.Configuration; // For job configuration\\nimport org.apache.hadoop.fs.Path;         // For specifying file paths\\nimport org.apache.hadoop.io.IntWritable;    // Hadoop\\'s integer type\\nimport org.apache.hadoop.io.Text;         // Hadoop\\'s string type\\nimport org.apache.hadoop.mapreduce.Job;     // Represents a MapReduce job\\nimport org.apache.hadoop.mapreduce.Mapper;  // Base class for the Mapper\\nimport org.apache.hadoop.mapreduce.Reducer; // Base class for the Reducer\\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat; // Helper for setting input paths\\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat; // Helper for setting output paths\\n\\n// Main class for the Word Count job\\npublic class WordCount {\\n\\n  // The Mapper class\\n  // Takes (Object, Text) as input (default for text files: line number, line content)\\n  // Outputs (Text, IntWritable) as intermediate key-value pairs (word, count=1)\\n  public static class TokenizerMapper\\n       extends Mapper<Object, Text, Text, IntWritable>{\\n\\n    // A constant Writable for the value \\'1\\', used for counting\\n    private final static IntWritable one = new IntWritable(1);\\n    // A Writable Text object to hold the current word, reused for efficiency\\n    private Text word = new Text();\\n\\n    // The map method processes each line of input\\n    @Override // Good practice to use @Override annotation\\n    public void map(Object key, Text value, Context context\\n                    ) throws IOException, InterruptedException {\\n      // Convert the line (Text) to a Java String\\n      String line = value.toString();\\n      // Use StringTokenizer to split the line into tokens (words)\\n      // Note: StringTokenizer is basic; for real-world scenarios, consider\\n      // using regex split or more sophisticated tokenizers to handle punctuation,\\n      // case sensitivity, etc.\\n      StringTokenizer itr = new StringTokenizer(line);\\n\\n      // Iterate through the tokens\\n      while (itr.hasMoreTokens()) {\\n        // Get the next token (word)\\n        String token = itr.nextToken();\\n        // Set the Text object \\'word\\' to the current token\\n        word.set(token);\\n        // Emit the word as the key and \\'1\\' as the value.\\n        // This means for every occurrence of a word, we emit (word, 1).\\n        context.write(word, one);\\n      }\\n    }\\n  }\\n\\n  // The Reducer class\\n  // Takes (Text, Iterable<IntWritable>) as input (word, list of 1s for that word)\\n  // Outputs (Text, IntWritable) as final key-value pairs (word, total count)\\n  public static class IntSumReducer\\n       extends Reducer<Text,IntWritable,Text,IntWritable> {\\n\\n    // A Writable Int object to hold the sum, reused for efficiency\\n    private IntWritable result = new IntWritable();\\n\\n    // The reduce method processes all values for a given key (word)\\n    @Override // Good practice to use @Override annotation\\n    public void reduce(Text key, Iterable<IntWritable> values,\\n                       Context context\\n                       ) throws IOException, InterruptedException {\\n      // Initialize a counter for the total sum of counts for this word\\n      int sum = 0;\\n      // Iterate through the list of values (all the \\'1\\'s emitted by mappers for this word)\\n      for (IntWritable val : values) {\\n        // Add the value (which is 1) to the sum\\n        sum += val.get();\\n      }\\n      // Set the result IntWritable to the calculated total sum\\n      result.set(sum);\\n      // Emit the word (key) and its total count (result)\\n      context.write(key, result);\\n    }\\n  }\\n\\n  // The main method to configure and run the MapReduce job\\n  public static void main(String[] args) throws Exception {\\n    // Create a new Hadoop configuration\\n    Configuration conf = new Configuration();\\n\\n    // Create a new Job instance, giving it a name\\n    Job job = Job.getInstance(conf, \"word count\");\\n\\n    // Specify the main class of the job (this class)\\n    job.setJarByClass(WordCount.class);\\n\\n    // Specify the Mapper class to use\\n    job.setMapperClass(TokenizerMapper.class);\\n\\n    // Specify the Combiner class. For Word Count, the reducer logic (summing)\\n    // can be applied locally within each mapper task to partially aggregate counts\\n    // before sending data to reducers, which reduces network traffic.\\n    // The Reducer class works perfectly as a Combiner here.\\n    job.setCombinerClass(IntSumReducer.class);\\n\\n    // Specify the Reducer class to use\\n    job.setReducerClass(IntSumReducer.class);\\n\\n    // Specify the output key type of the job (which is the Reducer\\'s output key type)\\n    job.setOutputKeyClass(Text.class);\\n\\n    // Specify the output value type of the job (which is the Reducer\\'s output value type)\\n    job.setOutputValueClass(IntWritable.class);\\n\\n    // Specify the input path(s) for the job. It expects the path as the first command-line argument.\\n    FileInputFormat.addInputPath(job, new Path(args[0]));\\n\\n    // Specify the output path for the job. It expects the path as the second command-line argument.\\n    // Note: The output directory must NOT exist before running the job.\\n    FileOutputFormat.setOutputPath(job, new Path(args[1]));\\n\\n    // Submit the job and wait for its completion.\\n    // \\'true\\' means print status updates to the console.\\n    // System.exit(0) on success, System.exit(1) on failure.\\n    System.exit(job.waitForCompletion(true) ? 0 : 1);\\n  }\\n}'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":21}]},{"cell_type":"code","source":["len(extract_python_code(df['few_shot_gpt'][2]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"liqHmlQJ53MG","executionInfo":{"status":"ok","timestamp":1749023186574,"user_tz":420,"elapsed":11,"user":{"displayName":"Vishesh Agrawal","userId":"16926083185186864017"}},"outputId":"fb709821-429e-485a-b09a-3c448c6678fc"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1"]},"metadata":{},"execution_count":28}]},{"cell_type":"code","source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from sklearn.datasets import fetch_california_housing\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler # Optional: alternative to Keras Normalization\n","\n","def create_and_train_california_housing_model():\n","    \"\"\"\n","    Creates, compiles, and trains a simple linear regression model\n","    with a Normalization layer and one Dense layer on the California\n","    Housing dataset.\n","\n","    Returns:\n","        tuple: A tuple containing the trained Keras model and\n","               the training history object.\n","    \"\"\"\n","    print(\"Loading California Housing dataset...\")\n","    # Load the dataset\n","    # as_frame=True loads it into a pandas DataFrame\n","    housing = fetch_california_housing(as_frame=True)\n","    X, y = housing.data, housing.target\n","\n","    print(f\"Dataset loaded. Features shape: {X.shape}, Target shape: {y.shape}\")\n","\n","    # Split data into training and testing sets\n","    # Using a test set allows evaluating the final model performance\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","    print(f\"Data split: Train size {X_train.shape[0]}, Test size {X_test.shape[0]}\")\n","\n","    # --- Keras Model Definition ---\n","\n","    # 1. Create a Normalization Layer\n","    # This layer will normalize the input features.\n","    # input_shape needs to be the number of features (8 for California Housing).\n","    # axis=-1 normalizes each feature independently across the batch dimension.\n","    normalizer = layers.Normalization(input_shape=[X_train.shape[1],], axis=-1)\n","\n","    # Adapt the normalizer to the training data\n","    # This calculates the mean and variance for normalization\n","    print(\"Adapting normalization layer to training data...\")\n","    normalizer.adapt(X_train)\n","    print(\"Normalization layer adapted.\")\n","\n","\n","    # 2. Create the Sequential Model\n","    # The model has two layers:\n","    # - The normalization layer (to scale features)\n","    # - A Dense layer with 1 unit (for linear regression output)\n","    model = keras.Sequential([\n","        normalizer,               # Normalize input features\n","        layers.Dense(units=1)     # Single dense layer for linear output\n","    ])\n","\n","    # Display the model summary\n","    model.summary()\n","\n","    # 3. Compile the Model\n","    # Define the optimizer and the loss function\n","    model.compile(\n","        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), # Adam optimizer\n","        loss='mean_squared_error'                              # MSE loss for regression\n","    )\n","    print(\"Model compiled with Adam optimizer and MSE loss.\")\n","\n","    # 4. Train the Model\n","    print(\"Starting model training...\")\n","    history = model.fit(\n","        X_train,            # Training features\n","        y_train,            # Training labels (target)\n","        epochs=1,         # Number of training epochs\n","        verbose=1,          # Show training progress\n","        # Use a validation split from the training data to monitor performance\n","        # during training without touching the separate test set.\n","        validation_split=0.2\n","    )\n","    print(\"Training finished.\")\n","\n","    # 5. Evaluate the Model on the Test Set (Optional but recommended)\n","    print(\"\\nEvaluating model on the test set...\")\n","    loss = model.evaluate(X_test, y_test, verbose=0)\n","    print(f\"Test Loss (Mean Squared Error): {loss:.4f}\")\n","\n","    # 6. Make Predictions (Optional)\n","    print(\"\\nMaking predictions on the first 5 test samples...\")\n","    predictions = model.predict(X_test[:5])\n","    print(\"Predictions:\")\n","    print(predictions)\n","    print(\"Actual values:\")\n","    print(y_test[:5].values.reshape(-1, 1)) # Reshape for comparison\n","\n","    return model, history\n","\n","# --- How to use the function ---\n","if __name__ == \"__main__\":\n","    trained_model, training_history = create_and_train_california_housing_model()\n","\n","    # You can access training history like this:\n","    # print(\"\\nTraining History Keys:\", training_history.history.keys())\n","\n","    # You can plot the training history (e.g., loss vs. epoch)\n","    # import matplotlib.pyplot as plt\n","    # plt.plot(training_history.history['loss'], label='loss')\n","    # plt.plot(training_history.history['val_loss'], label='val_loss')\n","    # plt.xlabel('Epoch')\n","    # plt.ylabel('Loss (MSE)')\n","    # plt.legend()\n","    # plt.grid(True)\n","    # plt.title('Training and Validation Loss')\n","    # plt.show()"],"metadata":{"id":"yEp4AGLv1-qU","colab":{"base_uri":"https://localhost:8080/","height":498},"executionInfo":{"status":"error","timestamp":1749026097730,"user_tz":420,"elapsed":2632,"user":{"displayName":"Vishesh Agrawal","userId":"16926083185186864017"}},"outputId":"a26508c1-9d0a-4f28-e769-9c2d8d73e8fd"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading California Housing dataset...\n","Dataset loaded. Features shape: (20640, 8), Target shape: (20640,)\n","Data split: Train size 16512, Test size 4128\n","Adapting normalization layer to training data...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/keras/src/layers/preprocessing/tf_data_layer.py:19: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(**kwargs)\n"]},{"output_type":"error","ename":"UnboundLocalError","evalue":"cannot access local variable 'input_shape' where it is not associated with a value","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-4cc7d1eec7f4>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;31m# --- How to use the function ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m     \u001b[0mtrained_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_and_train_california_housing_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;31m# You can access training history like this:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-7-4cc7d1eec7f4>\u001b[0m in \u001b[0;36mcreate_and_train_california_housing_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;31m# This calculates the mean and variance for normalization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Adapting normalization layer to training data...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mnormalizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madapt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Normalization layer adapted.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/layers/preprocessing/normalization.py\u001b[0m in \u001b[0;36madapt\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_keep_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mUnboundLocalError\u001b[0m: cannot access local variable 'input_shape' where it is not associated with a value"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision\n","import torchvision.transforms as transforms\n","from torch.utils.data import DataLoader\n","import matplotlib.pyplot as plt # For optional plotting\n","\n","# --- 1. Device Configuration ---\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(f'Using device: {device}')\n","\n","# --- 2. Hyperparameters ---\n","num_epochs = 1\n","batch_size = 100\n","learning_rate = 0.001\n","\n","# CIFAR-10 has 10 classes (0-9)\n","num_classes = 10\n","\n","# Input image dimensions (3 channels for RGB, 32x32 pixels)\n","in_channels = 3\n","\n","# --- 3. Data Loading ---\n","\n","# Transformations for the data\n","# CIFAR-10 images are 32x32.\n","# We convert images to PyTorch tensors and normalize them.\n","# Standard normalization values for CIFAR-10 are often used.\n","transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) # Normalize to [-1, 1] range\n","])\n","\n","# Load CIFAR-10 training dataset\n","train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True,\n","                                             download=True, transform=transform)\n","\n","# Load CIFAR-10 test dataset\n","test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False,\n","                                            download=True, transform=transform)\n","\n","# Create data loaders\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False) # No need to shuffle test data\n","\n","# --- 4. Model Definition ---\n","\n","# Simple CNN model\n","class SimpleCNN(nn.Module):\n","    def __init__(self, in_channels, num_classes):\n","        super(SimpleCNN, self).__init__()\n","        # First convolutional layer\n","        # Input: 3x32x32\n","        # Output: 32 feature maps, 32x32 size (padding=1 preserves size for kernel=3)\n","        self.conv1 = nn.Conv2d(in_channels, 32, kernel_size=3, stride=1, padding=1)\n","        self.relu1 = nn.ReLU()\n","        # First pooling layer\n","        # Input: 32x32x32\n","        # Output: 32 feature maps, 16x16 size (MaxPool2d halves dimensions)\n","        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n","\n","        # Second convolutional layer\n","        # Input: 32x16x16\n","        # Output: 64 feature maps, 16x16 size\n","        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n","        self.relu2 = nn.ReLU()\n","        # Second pooling layer\n","        # Input: 64x16x16\n","        # Output: 64 feature maps, 8x8 size\n","        # self.pool (reusing the same pooling layer definition)\n","\n","        # Third convolutional layer (optional, but good for slightly deeper networks)\n","        # Input: 64x8x8\n","        # Output: 128 feature maps, 8x8 size\n","        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n","        self.relu3 = nn.ReLU()\n","        # Third pooling layer\n","        # Input: 128x8x8\n","        # Output: 128 feature maps, 4x4 size\n","        # self.pool (reusing)\n","\n","\n","        # Fully connected layers\n","        # Input size calculation: After 3 conv/relu/pool blocks (with kernel=3, padding=1 and pool=2),\n","        # the spatial dimensions go from 32 -> 16 -> 8 -> 4.\n","        # The last convolutional layer had 128 output channels.\n","        # Flattened size: 128 * 4 * 4 = 2048\n","        self.fc1 = nn.Linear(128 * 4 * 4, 512) # First fully connected layer\n","        self.relu4 = nn.ReLU()\n","        self.fc2 = nn.Linear(512, num_classes) # Output layer\n","\n","    def forward(self, x):\n","        # Conv 1 -> ReLU 1 -> Pool 1\n","        x = self.pool(self.relu1(self.conv1(x)))\n","        # Conv 2 -> ReLU 2 -> Pool 2\n","        x = self.pool(self.relu2(self.conv2(x)))\n","        # Conv 3 -> ReLU 3 -> Pool 3\n","        x = self.pool(self.relu3(self.conv3(x)))\n","\n","        # Flatten the tensor for the fully connected layers\n","        # -1 tells PyTorch to infer the dimension\n","        x = x.view(x.size(0), -1) # or use torch.flatten(x, 1)\n","\n","        # Fully connected layers\n","        x = self.relu4(self.fc1(x))\n","        x = self.fc2(x) # Output layer (logits)\n","        return x\n","\n","# Instantiate the model and move to device\n","model = SimpleCNN(in_channels, num_classes).to(device)\n","\n","# --- 5. Loss and Optimizer ---\n","criterion = nn.CrossEntropyLoss() # Suitable for multi-class classification\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate) # Adam optimizer\n","\n","# --- 7. Training Loop ---\n","print(\"\\nStarting training...\")\n","\n","# Lists to store loss for plotting (optional)\n","# train_losses = []\n","# test_accuracies = []\n","\n","total_steps = len(train_loader)\n","for epoch in range(num_epochs):\n","    model.train() # Set the model to training mode\n","    running_loss = 0.0\n","    for i, (images, labels) in enumerate(train_loader):\n","        # Move tensors to the configured device\n","        images = images.to(device)\n","        labels = labels.to(device)\n","\n","        # Forward pass\n","        outputs = model(images)\n","        loss = criterion(outputs, labels)\n","\n","        # Backward and optimize\n","        optimizer.zero_grad() # Clear previous gradients\n","        loss.backward()       # Compute gradient of the loss w.r.t. model parameters\n","        optimizer.step()      # Perform a single optimization step (parameter update)\n","\n","        running_loss += loss.item()\n","\n","        # Print training progress (optional)\n","        # if (i+1) % 100 == 0: # Print every 100 steps\n","        #     print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{total_steps}], Loss: {loss.item():.4f}')\n","\n","    avg_train_loss = running_loss / total_steps\n","    print(f'Epoch [{epoch+1}/{num_epochs}], Average Training Loss: {avg_train_loss:.4f}')\n","    # train_losses.append(avg_train_loss) # Store loss\n","\n","    # --- 8. Evaluation (after each epoch) ---\n","    # In evaluation mode, we don't need to track gradients\n","    model.eval()\n","    with torch.no_grad():\n","        correct = 0\n","        total = 0\n","        for images, labels in test_loader:\n","            images = images.to(device)\n","            labels = labels.to(device)\n","            outputs = model(images)\n","            # Get predictions from the maximum value\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","        accuracy = 100 * correct / total\n","        print(f'Test Accuracy of the model on the {total} test images: {accuracy:.2f} %')\n","        # test_accuracies.append(accuracy) # Store accuracy\n","\n","print(\"\\nTraining finished.\")\n","\n","# Optional: Plotting training loss and test accuracy over epochs\n","# plt.figure(figsize=(10, 5))\n","# plt.subplot(1, 2, 1)\n","# plt.plot(range(1, num_epochs + 1), train_losses)\n","# plt.xlabel(\"Epoch\")\n","# plt.ylabel(\"Training Loss\")\n","# plt.title(\"Training Loss per Epoch\")\n","\n","# plt.subplot(1, 2, 2)\n","# plt.plot(range(1, num_epochs + 1), test_accuracies)\n","# plt.xlabel(\"Epoch\")\n","# plt.ylabel(\"Test Accuracy (%)\")\n","# plt.title(\"Test Accuracy per Epoch\")\n","\n","# plt.tight_layout()\n","# plt.show()\n","\n","# --- Optional: Save the model ---\n","# torch.save(model.state_dict(), 'cifar10_cnn_model.pth')\n","# print(\"Model saved to cifar10_cnn_model.pth\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6pPxXSUXFKRs","executionInfo":{"status":"ok","timestamp":1749026288694,"user_tz":420,"elapsed":149318,"user":{"displayName":"Vishesh Agrawal","userId":"16926083185186864017"}},"outputId":"8d8b908b-f51c-4c75-944c-5a6521add6bb"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cpu\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 170M/170M [00:02<00:00, 80.1MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Starting training...\n","Epoch [1/1], Average Training Loss: 1.3942\n","Test Accuracy of the model on the 10000 test images: 59.57 %\n","\n","Training finished.\n"]}]},{"cell_type":"code","source":["import numpy as np\n","from sklearn.datasets import make_classification\n","from sklearn.model_selection import train_test_split\n","from sklearn.pipeline import Pipeline\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.linear_model import LogisticRegression # Or any other classifier like KNeighborsClassifier, SVC, etc.\n","\n","# --- 1. Generate a random n-class classification problem ---\n","# Let's generate a 3-class problem (n=3)\n","n_samples = 1000\n","n_features = 20\n","n_informative = 15\n","n_redundant = 5\n","n_classes = 3 # n-class where n=3\n","random_state = 42 # for reproducibility\n","\n","X, y = make_classification(n_samples=n_samples,\n","                           n_features=n_features,\n","                           n_informative=n_informative,\n","                           n_redundant=n_redundant,\n","                           n_classes=n_classes,\n","                           random_state=random_state)\n","\n","print(f\"Generated dataset shape: X={X.shape}, y={y.shape}\")\n","print(f\"Number of classes: {np.unique(y).shape[0]}\")\n","\n","# --- 2. Split the dataset into training and testing sets ---\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=random_state, stratify=y)\n","# stratify=y ensures that the proportion of classes is the same in both the training and testing sets.\n","\n","print(f\"Training set shape: X_train={X_train.shape}, y_train={y_train.shape}\")\n","print(f\"Testing set shape: X_test={X_test.shape}, y_test={y_test.shape}\")\n","\n","# --- 3. Create a pipeline for scaling and classification ---\n","# The pipeline consists of:\n","# - StandardScaler: Computes mean and std on the training data (during fit)\n","#                   and applies the scaling using these computed values (during transform).\n","#                   Crucially, when score/predict is called on the pipeline with test data,\n","#                   it uses the *already fitted* scaler from the training step.\n","# - LogisticRegression: A simple classifier that works well with scaled data.\n","\n","pipeline = Pipeline([\n","    ('scaler', StandardScaler()),          # Step 1: Scale features\n","    ('classifier', LogisticRegression()) # Step 2: Train a classifier\n","])\n","\n","print(\"\\nPipeline created:\")\n","print(pipeline)\n","\n","# --- 4. Train the model using the pipeline on the training set ---\n","# Calling fit on the pipeline performs the following steps sequentially:\n","# 1. Calls fit_transform on the first step ('scaler') using X_train.\n","#    The scaler learns the mean and std from X_train and transforms it.\n","# 2. Calls fit on the second step ('classifier') using the *transformed* X_train\n","#    and the original y_train.\n","print(\"\\nFitting the pipeline on the training data...\")\n","pipeline.fit(X_train, y_train)\n","print(\"Pipeline fitting complete.\")\n","\n","# --- 5. Compute score on the test set using the same pipeline ---\n","# Calling score on the pipeline performs the following steps sequentially:\n","# 1. Calls transform on the first step ('scaler') using X_test.\n","#    The scaler uses the mean and std *learned from X_train* during the fit step\n","#    to transform X_test. This prevents data leakage from the test set into the scaling.\n","# 2. Calls score on the second step ('classifier') using the *transformed* X_test\n","#    and the original y_test.\n","print(\"\\nComputing score on the test data...\")\n","test_score = pipeline.score(X_test, y_test)\n","\n","print(f\"\\nTest set accuracy score: {test_score:.4f}\")\n","\n","# You can also make predictions:\n","# y_pred = pipeline.predict(X_test)\n","# print(f\"\\nSample predictions on test set: {y_pred[:10]}\")\n","# print(f\"Corresponding true labels:        {y_test[:10]}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0UgV4Nb6HkcK","executionInfo":{"status":"ok","timestamp":1749026759524,"user_tz":420,"elapsed":358,"user":{"displayName":"Vishesh Agrawal","userId":"16926083185186864017"}},"outputId":"77341255-b494-4d9d-8bfe-ace40f89c9a2"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Generated dataset shape: X=(1000, 20), y=(1000,)\n","Number of classes: 3\n","Training set shape: X_train=(700, 20), y_train=(700,)\n","Testing set shape: X_test=(300, 20), y_test=(300,)\n","\n","Pipeline created:\n","Pipeline(steps=[('scaler', StandardScaler()),\n","                ('classifier', LogisticRegression())])\n","\n","Fitting the pipeline on the training data...\n","Pipeline fitting complete.\n","\n","Computing score on the test data...\n","\n","Test set accuracy score: 0.6600\n"]}]},{"cell_type":"code","source":["from typing import Callable, Dict, Sequence, Text\n","\n","import numpy\n","import tensorflow\n","import toolz\n","import transformers\n","from toolz import curried\n","\n","# Assuming 'gamla' library is available in your environment\n","# If not, you might need to install it or replace its usage\n","# gamla.just is used to create a function that always returns a fixed value\n","# toolz.curried.map is a curried version of map\n","# toolz.curried.merge_with is a curried merge_with\n","# toolz.compose_left is left-to-right function composition\n","# toolz.pipe is functional piping\n","# toolz.concat is used to flatten iterables\n","# toolz.valmap applies a function to values of a dictionary\n","# toolz.identity is a function that returns its input\n","# toolz.take takes a specified number of items from an iterable\n","# gamla.star applies tuple elements as positional arguments to a function\n","# filter(toolz.identity) filters out falsy values like empty strings or None\n","try:\n","    import gamla\n","except ImportError:\n","    print(\"Please install gamla: pip install gamla\")\n","    # Provide dummy functions if gamla is not available, just for structure\n","    # This won't work for actual execution without gamla\n","    class DummyGamla:\n","        def just(self, value):\n","            return lambda *args, **kwargs: value\n","        def star(self, func):\n","             return lambda args: func(*args)\n","    gamla = DummyGamla()\n","\n","\n","def _make_model_from_input_ids_and_masks(\n","    input_ids_in: tensorflow.keras.layers.Input,\n","    input_masks_in: tensorflow.keras.layers.Input,\n","    transformer_model,\n","):\n","    \"\"\"Builds a Keras model on top of the transformer output.\"\"\"\n","    model = toolz.pipe(\n","        transformer_model(input_ids_in, attention_mask=input_masks_in)[0], # Use the sequence output\n","        tensorflow.keras.layers.Bidirectional(\n","            tensorflow.keras.layers.LSTM(\n","                50, return_sequences=True, dropout=0.1, recurrent_dropout=0.1\n","            )\n","        ),\n","        tensorflow.keras.layers.GlobalMaxPool1D(),\n","        tensorflow.keras.layers.Dense(50, activation=\"relu\"),\n","        tensorflow.keras.layers.Dropout(0.2),\n","        tensorflow.keras.layers.Dense(1, activation=\"sigmoid\"), # Sigmoid for binary classification\n","        lambda layers: tensorflow.keras.Model(\n","            inputs=[input_ids_in, input_masks_in], outputs=layers\n","        ),\n","    )\n","\n","    # Freeze the transformer and LSTM layers\n","    for layer in model.layers[:3]: # Assuming the transformer layer is the first, followed by Bidirectional LSTM\n","         layer.trainable = False\n","    return model\n","\n","\n","def _make_model(max_sequence_length: int, transformer_model):\n","    \"\"\"Creates Keras Input layers and builds the model.\"\"\"\n","    return _make_model_from_input_ids_and_masks(\n","        tensorflow.keras.layers.Input(\n","            shape=(max_sequence_length,), name=\"input_ids\", dtype=\"int32\"\n","        ),\n","        tensorflow.keras.layers.Input(\n","            shape=(max_sequence_length,), name=\"attention_mask\", dtype=\"int32\"\n","        ),\n","        transformer_model,\n","    )\n","\n","\n","_DISTILBERT_MODEL = \"distilbert-base-uncased\"\n","\n","\n","def _make_model_and_encoder(max_sequence_length: int):\n","    \"\"\"Loads the DistilBERT model and tokenizer.\"\"\"\n","    config = transformers.DistilBertConfig(dropout=0.2, attention_dropout=0.2)\n","    config.output_hidden_states = False # We only need the last hidden state\n","\n","    # Load the TF Keras DistilBERT model\n","    transformer_model = transformers.TFDistilBertModel.from_pretrained(\n","        _DISTILBERT_MODEL, config=config\n","    )\n","\n","    # Load the DistilBERT tokenizer\n","    # Note: max_length and pad_to_max_length here are defaults for the tokenizer itself,\n","    # the actual encoding uses values passed to encode_plus.\n","    tokenizer = transformers.DistilBertTokenizer.from_pretrained(\n","        _DISTILBERT_MODEL,\n","        do_lower_case=True,\n","        add_special_tokens=True,\n","        max_length=128, # This default might be overridden by encode_plus\n","        pad_to_max_length=True, # This default might be overridden by encode_plus\n","    )\n","\n","    # Create the Keras model using the transformer model and the calculated max sequence length\n","    model = _make_model(\n","        max_sequence_length,\n","        transformer_model,\n","    )\n","\n","    return model, tokenizer\n","\n","\n","_iterable_to_numpy_array = toolz.compose_left(\n","    tuple, lambda data: numpy.asarray(data, dtype=\"int32\"),\n",")\n","\n","\n","def _encode(encoder: Callable[[Text], Dict[Text, numpy.array]], max_length: int):\n","    \"\"\"Encodes an iterable of texts using the provided encoder and merges results.\"\"\"\n","    # Warning - due to weirdness of `merge_with` this might behave unexpectedly if iterable has only one element\n","    # or if the encoding keys differ (e.g., 'input_ids', 'attention_mask').\n","    return toolz.compose_left(\n","        curried.map(\n","            lambda sentence: encoder.encode_plus(\n","                sentence,\n","                add_special_tokens=True,\n","                max_length=max_length, # Use the specified max length for padding/truncation\n","                pad_to_max_length=True,\n","                return_attention_mask=True,\n","                return_token_type_ids=False, # token_type_ids not strictly needed for single sequence classification\n","            )\n","        ),\n","        # Merge the dictionaries returned by encode_plus (e.g., {'input_ids': [...], 'attention_mask': [...]})\n","        # into a single dictionary where values are lists of the results.\n","        gamla.star(curried.merge_with(toolz.identity)),\n","        # Convert lists of results into NumPy arrays.\n","        curried.valmap(_iterable_to_numpy_array),\n","    )\n","\n","\n","def train_tense_classifier(positive_examples: Sequence[Text], negative_examples: Sequence[Text], epochs: int):\n","    \"\"\"\n","    Trains a binary classifier using DistilBERT to distinguish past vs present tense.\n","\n","    Args:\n","        positive_examples: A sequence of strings representing the positive class (e.g., present tense verbs).\n","        negative_examples: A sequence of strings representing the negative class (e.g., past tense verbs).\n","        epochs: The number of training epochs.\n","\n","    Returns:\n","        A tuple containing:\n","            - The trained TensorFlow Keras model.\n","            - A callable function to encode new text inputs for prediction.\n","    \"\"\"\n","    # Calculate max sequence length based on *character* length as in the example\n","    # Note: This is potentially different from token length, but following the provided code structure.\n","    # The actual tokenization will pad/truncate based on this value.\n","    max_sentence_length = max(map(len, toolz.concat([positive_examples, negative_examples])))\n","    # Add a small buffer, or maybe a fixed reasonable max_length is better.\n","    # Let's use a fixed small length reasonable for single words + special tokens\n","    # Max token length for simple words like \"walked\" is likely small (e.g., 3-4 tokens).\n","    # Let's set a reasonable max_length like 32, which is more standard for tokenization\n","    # than using character length.\n","    # Update: Sticking strictly to the prompt's provided train function logic\n","    # which calculates max_sentence_length from char length and passes it for encoding.\n","    # Revert to character length calculation:\n","    max_sequence_length = max(map(len, toolz.concat([positive_examples, negative_examples]))) + 2 # Add buffer for [CLS] and [SEP] tokens\n","\n","    print(f\"Calculated max sequence length (based on character length + 2): {max_sequence_length}\")\n","\n","    # Create the model and tokenizer\n","    model, tokenizer = _make_model_and_encoder(max_sequence_length)\n","\n","    # Compile the model (using mean_squared_error as in the example)\n","    model.compile(loss=\"mean_squared_error\", optimizer=\"adam\", metrics=[\"accuracy\"]) # Added optimizer and metric for clarity\n","\n","    # Create the function to encode sentences using the tokenizer\n","    sentence_encoder = _encode(\n","        lambda sentence: tokenizer.encode_plus(\n","            sentence,\n","            add_special_tokens=True,\n","            max_length=max_sequence_length, # Use the calculated max_length for encoding\n","            pad_to_max_length=True,\n","            return_attention_mask=True,\n","            return_token_type_ids=False, # token_type_ids not strictly needed\n","        ),\n","        max_length=max_sequence_length # Pass max_length to the _encode helper\n","    )\n","\n","    # Prepare the data\n","    all_texts = list(toolz.concat([positive_examples, negative_examples]))\n","    X = sentence_encoder(all_texts) # Encoded inputs (dictionary of numpy arrays)\n","    y = _iterable_to_numpy_array(\n","        list(toolz.concat([map(gamla.just(1), positive_examples), map(gamla.just(0), negative_examples)]))\n","    ) # Labels (1 for positive, 0 for negative)\n","\n","    # Split data into training and validation sets (90/10 split)\n","    total_samples = len(all_texts)\n","    n_train = int(total_samples * 0.9)\n","\n","    trainX = toolz.valmap(lambda d: d[:n_train, ...], X) # Use ... for all subsequent dimensions\n","    testX = toolz.valmap(lambda d: d[n_train:, ...], X)\n","    trainy, testy = y[:n_train], y[n_train:]\n","\n","    print(f\"Training on {n_train} samples, validating on {total_samples - n_train} samples.\")\n","    print(f\"Input shape (input_ids): {trainX['input_ids'].shape}\")\n","    print(f\"Output shape (labels): {trainy.shape}\")\n","\n","    # Train the model\n","    history = model.fit(\n","        x=trainX,\n","        y=trainy,\n","        validation_data=(testX, testy),\n","        epochs=epochs,\n","        batch_size=32 # Added batch size\n","    )\n","\n","    print(\"Training finished.\")\n","    return model, sentence_encoder\n","\n","\n","# Example Usage (based on your provided snippet)\n","if __name__ == \"__main__\":\n","    # Ensure TensorFlow is using eager execution (usually default)\n","    # tensorflow.config.run_modes.run_eagerly(True)\n","\n","    positive_examples = [\n","        \"go\", \"walk\", \"bring\", \"think\", \"build\", \"drink\", \"say\", \"clean\", \"do\",\n","        \"bring\", \"place\", \"break\", \"kick\", \"code\", \"type\", \"kill\", \"scare\",\n","        \"make\", \"bake\", \"run\", \"laugh\", \"smoke\"\n","    ]\n","    negative_examples = [\n","        \"placed\", \"did\", \"brought\", \"said\", \"drank\", \"went\", \"walked\", \"brought\",\n","        \"thought\", \"built\", \"cleaned\", \"broke\", \"kicked\", \"coded\", \"typed\",\n","        \"killed\", \"scared\", \"made\", \"baked\", \"ran\", \"laughed\", \"smoked\"\n","    ]\n","    num_epochs = 1 # Reduced for quicker example run\n","\n","    print(\"Starting model training...\")\n","    trained_model, text_encoder = train_tense_classifier(\n","        positive_examples=positive_examples,\n","        negative_examples=negative_examples,\n","        epochs=num_epochs,\n","    )\n","\n","    print(\"\\nModel trained. Making predictions...\")\n","\n","    # Example predictions\n","    prediction_texts = [\"run\", \"ran\", \"go\", \"went\", \"laugh\", \"laughed\", \"smoke\", \"smoked\", \"eat\", \"ate\", \"jump\", \"jumped\"]\n","    encoded_predictions = text_encoder(prediction_texts)\n","\n","    # Predict probabilities\n","    predictions = trained_model.predict(encoded_predictions)\n","\n","    print(\"\\nPredictions:\")\n","    for text, prediction in zip(prediction_texts, predictions):\n","        # Sigmoid output is a probability. Close to 1 means positive (present), close to 0 means negative (past).\n","        predicted_class = \"Present\" if prediction > 0.5 else \"Past\"\n","        print(f\"'{text}': Probability(Present) = {prediction[0]:.4f} -> Predicted: {predicted_class}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":814,"referenced_widgets":["7eb4307a88714f06a63ab71ff3100783","4e5464d12c2d442097de74e8a7ac51ea","614c00bd427f4104a0075daffbeb8ecf","6364b723ce604422950be8efd9f71cd5","89dfdd4651db47ac88b16dd4a1c4ed2a","2c35bc81b1014971a0712c22f5bee48d","062f2cc10ce9497ab6734d61a596eb20","44f9d09006dd4e06b413eaa29607bf01","74ca548efdac4623b1ce0596f8eae619","06771cca0a314f90b6d5b963b14ac178","1a0d64e4b6724c208381df274be59917","13ecc32830da4020a36162ce7fbcb6ac","62fde27d6b904dc6b72ee1047f5b5cfb","17064b6c83be49c78b53a1d8520855e3","72afd0236e104fd7a930ab7b47f75cc3","69e04332a1eb46c197e5b1d33d239e3f","72254daf93c94937953ab64c52df0b37","20b15c8bc1f04ca188114edb33844445","1781b17856844f16b22661ed44a37b90","3e66456a4815455d94bcbeb9458809f5","ba78e72c404349fb96dcef24d9d73c32","d8543b42bedc44168e9101c6ec6860b9","ee5ea48ac783492f9001c69e3c498478","d749c33d1fe4430180be816ac37c669e","1f63183f55904bb1b926622b58b1c454","21d0a5e4e80d437dabac47f58e41c7a7","317394fa447340ec9d7cf7d78a665bb4","bd8128b584b14bc89d9b064be3a1a88a","8bf5992f399a4c4aa533effeffa72571","be81c802945446fd90afff1ef8ae4300","a012acba13bf4e54ac36feb1cea4ec6c","1cdeede1479c4b92b4f12a7e1224dff9","4254256c606f4c01987db7bace609dd5","a5e095472ab74a329b6ae6eca33e9d2a","929c317a225148cb9f3911fe3925c040","bdba332355ec4bff94f16234a9ed94b5","0d3891d52cf24a5b86a1552e570242e4","0561b064bc1d4263b6f057a8f840226e","29e8a33184494af3ad5b2bc50f0c534a","91151ea691c74eac8ae02cbe6a01d567","042b22ba4a7d4a9c97f3aca24650c786","d76e3da81efd47c2b80a62de0036eadb","ea420509dfbe49c992f53cfb40eaab7e","b2ff05fb2df64d79a501fb8282a69f0a","742f72ce97144839831aca93a71454b1","8420758f2b804c1fa08138cf43ffe2b9","106d33fb5e2a489d9d7c5f0af041eab1","58c627fc9b2c4d08a8d2d5dd7f5f77cf","e5d1a682cd5d4a7dbb036de8ef2cf77e","04af40df51cc4f75b2041d3ea0a7d34e","6933e01e194648ce8f3efbb8d1174628","7bc2adb12ab742d389469944d514509e","4eeaebc336f443a5bf09305e9a893a8f","dc1df07caeb548acb7d1d6f6635751f7","efb64554e0534bdcbee6c8a5714a7bb9"]},"id":"HfJPcx2oHqU_","executionInfo":{"status":"error","timestamp":1749026806834,"user_tz":420,"elapsed":26663,"user":{"displayName":"Vishesh Agrawal","userId":"16926083185186864017"}},"outputId":"c565f382-24dd-4c89-a8f0-bc59a4416bff"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Please install gamla: pip install gamla\n","Starting model training...\n","Calculated max sequence length (based on character length + 2): 9\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7eb4307a88714f06a63ab71ff3100783"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertModel: ['vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight']\n","- This IS expected if you are initializing TFDistilBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFDistilBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n","All the weights of TFDistilBertModel were initialized from the PyTorch model.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"13ecc32830da4020a36162ce7fbcb6ac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee5ea48ac783492f9001c69e3c498478"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a5e095472ab74a329b6ae6eca33e9d2a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"742f72ce97144839831aca93a71454b1"}},"metadata":{}},{"output_type":"error","ename":"AttributeError","evalue":"add_special_tokens conflicts with the method add_special_tokens in DistilBertTokenizer","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-758a66f34c33>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting model training...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m     trained_model, text_encoder = train_tense_classifier(\n\u001b[0m\u001b[1;32m    238\u001b[0m         \u001b[0mpositive_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpositive_examples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0mnegative_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnegative_examples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-14-758a66f34c33>\u001b[0m in \u001b[0;36mtrain_tense_classifier\u001b[0;34m(positive_examples, negative_examples, epochs)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;31m# Create the model and tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_model_and_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_sequence_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;31m# Compile the model (using mean_squared_error as in the example)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-14-758a66f34c33>\u001b[0m in \u001b[0;36m_make_model_and_encoder\u001b[0;34m(max_sequence_length)\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;31m# Note: max_length and pad_to_max_length here are defaults for the tokenizer itself,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;31m# the actual encoding uses values passed to encode_plus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m     tokenizer = transformers.DistilBertTokenizer.from_pretrained(\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[0m_DISTILBERT_MODEL\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mdo_lower_case\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2023\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"loading file {file_path} from cache at {resolved_vocab_files[file_id]}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2024\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2025\u001b[0;31m         return cls._from_pretrained(\n\u001b[0m\u001b[1;32m   2026\u001b[0m             \u001b[0mresolved_vocab_files\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2027\u001b[0m             \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m_from_pretrained\u001b[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2276\u001b[0m         \u001b[0;31m# Instantiate the tokenizer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2277\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2278\u001b[0;31m             \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minit_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0minit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2279\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mimport_protobuf_decode_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2280\u001b[0m             logger.info(\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/distilbert/tokenization_distilbert.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vocab_file, do_lower_case, do_basic_tokenize, never_split, unk_token, sep_token, pad_token, cls_token, mask_token, tokenize_chinese_chars, strip_accents, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwordpiece_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWordpieceTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munk_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munk_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m    135\u001b[0m             \u001b[0mdo_lower_case\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdo_lower_case\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[0mdo_basic_tokenize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdo_basic_tokenize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;31m# 4 init the parent class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m         \u001b[0;31m# 4. If some of the special tokens are not part of the vocab, we add them, at the end.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m   1408\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1410\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{key} conflicts with the method {key} in {self.__class__.__name__}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1412\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: add_special_tokens conflicts with the method add_special_tokens in DistilBertTokenizer"]}]},{"cell_type":"code","source":["import tensorflow as tf\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import os # Needed for creating checkpoint directory\n","\n","from tensorflow.keras.layers import Input, Conv2D, Dense, Flatten, MaxPooling2D\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n","\n","print(\"TensorFlow version:\", tf.__version__)\n","\n","# --- Load the Fashion MNIST dataset ---\n","fashion_mnist = tf.keras.datasets.fashion_mnist\n","# The data is split into training and testing sets\n","(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n","\n","# --- Data Preprocessing ---\n","\n","# Scale pixel values to be between 0 and 1\n","# Images are grayscale, pixel values range from 0 to 255\n","x_train, x_test = x_train / 255.0, x_test / 255.0\n","\n","# Add a channel dimension\n","# Keras Conv2D layers expect input shape (batch_size, height, width, channels)\n","# Fashion MNIST images are 28x28 (grayscale), so we need to add the channel dimension (1)\n","x_train = np.expand_dims(x_train, axis=-1)\n","x_test = np.expand_dims(x_test, axis=-1)\n","\n","print(\"\\n--- Dataset Shapes ---\")\n","print(\"x_train shape:\", x_train.shape) # (num_samples, 28, 28, 1)\n","print(\"y_train shape:\", y_train.shape) # (num_samples,)\n","print(\"x_test shape:\", x_test.shape)   # (num_samples, 28, 28, 1)\n","print(\"y_test shape:\", y_test.shape)   # (num_samples,)\n","\n","# Get the number of classes\n","num_classes = len(np.unique(y_train))\n","print(\"Number of classes:\", num_classes) # Should be 10 (0-9)\n","\n","# Define image dimensions for the input layer\n","IMG_HEIGHT = x_train.shape[1]\n","IMG_WIDTH = x_train.shape[2]\n","IMG_CHANNELS = x_train.shape[3]\n","INPUT_SHAPE = (IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS) # (28, 28, 1)\n","\n","# --- Build the Model (using Sequential API) ---\n","\n","model = Sequential([\n","    # Input layer - specify the shape of the images\n","    Input(shape=INPUT_SHAPE),\n","\n","    # Convolutional Block 1: Conv + Max Pooling\n","    Conv2D(32, (3, 3), activation='relu', padding='same'),\n","    MaxPooling2D((2, 2)),\n","\n","    # Convolutional Block 2: Conv + Max Pooling\n","    Conv2D(64, (3, 3), activation='relu', padding='same'),\n","    MaxPooling2D((2, 2)),\n","\n","    # Convolutional Block 3: Conv + Max Pooling\n","    Conv2D(128, (3, 3), activation='relu', padding='same'),\n","    MaxPooling2D((2, 2)),\n","\n","    # Flatten the output from the convolutional layers to feed into dense layers\n","    Flatten(),\n","\n","    # Dense (fully connected) layers\n","    Dense(128, activation='relu'),\n","    # Output layer: 10 units for 10 classes.\n","    # No activation here if using SparseCategoricalCrossentropy with from_logits=True\n","    Dense(num_classes)\n","])\n","\n","# --- Compile the Model ---\n","\n","model.compile(optimizer='adam',\n","              # Use SparseCategoricalCrossentropy for integer labels (0-9)\n","              # from_logits=True because the last layer has no activation (linear output)\n","              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","              metrics=['accuracy'])\n","\n","# Display model summary to see the layers and parameter counts\n","print(\"\\n--- Model Summary ---\")\n","model.summary()\n","\n","# --- Setup Callbacks: Early Stopping and Model Checkpoint ---\n","\n","# Define the directory where checkpoints will be saved\n","checkpoint_dir = './fashion_mnist_checkpoints'\n","os.makedirs(checkpoint_dir, exist_ok=True) # Create the directory if it doesn't exist\n","\n","# Define the file path pattern for saving checkpoints\n","# Includes epoch number and validation loss in the filename\n","checkpoint_filepath = os.path.join(checkpoint_dir, 'weights_best_val_loss_{epoch:02d}_{val_loss:.4f}.weights.h5')\n","\n","# Model Checkpoint Callback:\n","# This callback saves the model's weights whenever the validation loss improves.\n","model_checkpoint_callback = ModelCheckpoint(\n","    filepath=checkpoint_filepath,\n","    save_weights_only=True, # Save only the model's weights, not the whole model\n","    monitor='val_loss',      # Metric to monitor for improvement\n","    mode='min',              # In 'min' mode, training will stop when the monitored quantity stops decreasing\n","    save_best_only=True,     # Only save the weights when the monitored metric is the best seen so far\n","    verbose=1                # Print a message when a checkpoint is saved\n",")\n","\n","# Early Stopping Callback:\n","# This callback stops training when the monitored metric (validation loss)\n","# has not improved for a specified number of epochs (patience).\n","early_stopping_callback = EarlyStopping(\n","    monitor='val_loss',      # Metric to monitor for lack of improvement\n","    patience=10,             # Number of epochs with no improvement after which training will be stopped\n","    mode='min',              # In 'min' mode, training will stop when the monitored quantity stops decreasing\n","    restore_best_weights=True, # Restore model weights from the epoch with the best value of the monitored metric.\n","    verbose=1                # Print a message when early stopping is triggered\n",")\n","\n","# Create a list of callbacks to pass to model.fit\n","callbacks_list = [\n","    early_stopping_callback,\n","    model_checkpoint_callback\n","]\n","\n","# --- Train the Model ---\n","\n","# Set a reasonably high number of epochs. Early Stopping will typically stop it sooner.\n","EPOCHS = 1\n","\n","print(\"\\n--- Starting Training ---\")\n","history = model.fit(\n","    x_train, y_train,\n","    epochs=EPOCHS,\n","    validation_data=(x_test, y_test), # Provide validation data - crucial for callbacks\n","    callbacks=callbacks_list          # Pass the list of callbacks here\n",")\n","print(\"\\n--- Training Finished ---\")\n","\n","# --- Evaluate the Model ---\n","\n","print(\"\\n--- Evaluating the model on the test set ---\")\n","loss, accuracy = model.evaluate(x_test, y_test, verbose=2)\n","print(f\"Test Loss: {loss:.4f}\")\n","print(f\"Test Accuracy: {accuracy:.4f}\")\n","\n","# --- Plot Training History (Optional) ---\n","# We only plot up to the number of epochs actually trained (due to early stopping)\n","\n","# Get training and validation history from the 'history' object\n","# history.history contains metrics logged during training\n","acc = history.history['accuracy']\n","val_acc = history.history['val_accuracy']\n","loss = history.history['loss']\n","val_loss = history.history['val_loss']\n","\n","# The number of epochs actually trained might be less than EPOCHS due to early stopping\n","epochs_trained = len(acc)\n","epochs_range = range(epochs_trained)\n","\n","plt.figure(figsize=(12, 4))\n","\n","# Plot Accuracy\n","plt.subplot(1, 2, 1) # 1 row, 2 columns, 1st plot\n","plt.plot(epochs_range, acc, label='Training Accuracy')\n","plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n","plt.xlabel('Epoch')\n","plt.ylabel('Accuracy')\n","plt.title('Training and Validation Accuracy')\n","plt.legend(loc='lower right')\n","plt.grid(True)\n","\n","# Plot Loss\n","plt.subplot(1, 2, 2) # 1 row, 2 columns, 2nd plot\n","plt.plot(epochs_range, loss, label='Training Loss')\n","plt.plot(epochs_range, val_loss, label='Validation Loss')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.title('Training and Validation Loss')\n","plt.legend(loc='upper right')\n","plt.grid(True)\n","\n","plt.tight_layout()\n","plt.show()\n","\n","print(f\"\\nModel weights saved to '{checkpoint_dir}' directory whenever validation loss improved.\")\n","print(f\"Training stopped early after {epochs_trained} epochs due to Early Stopping.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"KLvp74YHH4fE","executionInfo":{"status":"ok","timestamp":1749026992713,"user_tz":420,"elapsed":151801,"user":{"displayName":"Vishesh Agrawal","userId":"16926083185186864017"}},"outputId":"23fe6c7e-335b-463a-a4c1-89fb1a22beb1"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["TensorFlow version: 2.18.0\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n","\u001b[1m29515/29515\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n","\u001b[1m26421880/26421880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n","\u001b[1m5148/5148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1us/step\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n","\u001b[1m4422102/4422102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n","\n","--- Dataset Shapes ---\n","x_train shape: (60000, 28, 28, 1)\n","y_train shape: (60000,)\n","x_test shape: (10000, 28, 28, 1)\n","y_test shape: (10000,)\n","Number of classes: 10\n","\n","--- Model Summary ---\n"]},{"output_type":"display_data","data":{"text/plain":["\u001b[1mModel: \"sequential\"\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m320\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │        \u001b[38;5;34m73,856\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1152\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m147,584\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m1,290\u001b[0m │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1152</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">147,584</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,290</span> │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m241,546\u001b[0m (943.54 KB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">241,546</span> (943.54 KB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m241,546\u001b[0m (943.54 KB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">241,546</span> (943.54 KB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","--- Starting Training ---\n","\u001b[1m1874/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.7834 - loss: 0.5958\n","Epoch 1: val_loss improved from inf to 0.32171, saving model to ./fashion_mnist_checkpoints/weights_best_val_loss_01_0.3217.weights.h5\n","\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 60ms/step - accuracy: 0.7835 - loss: 0.5956 - val_accuracy: 0.8830 - val_loss: 0.3217\n","Restoring model weights from the end of the best epoch: 1.\n","\n","--- Training Finished ---\n","\n","--- Evaluating the model on the test set ---\n","313/313 - 5s - 15ms/step - accuracy: 0.8830 - loss: 0.3217\n","Test Loss: 0.3217\n","Test Accuracy: 0.8830\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 1200x400 with 2 Axes>"],"image/png":"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\n"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Model weights saved to './fashion_mnist_checkpoints' directory whenever validation loss improved.\n","Training stopped early after 1 epochs due to Early Stopping.\n"]}]},{"cell_type":"code","source":["import torch\n","from torch.utils.data import Dataset, DataLoader # Dataset is implicitly used by FashionMNIST\n","import torchvision\n","import torchvision.transforms as transforms\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","# --- Configuration ---\n","BATCH_SIZE = 64\n","# Directory to save/load the dataset files\n","DATA_DIR = './data'\n","\n","# --- Transformations ---\n","# FashionMNIST images are PIL images. We need to convert them to PyTorch tensors\n","# and normalize the pixel values to the range [0, 1]. ToTensor() does both.\n","transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    # Optional: Normalize the images.\n","    # FashionMNIST mean: 0.286, std: 0.353 (calculated over training set)\n","    # transforms.Normalize((0.286,), (0.353,))\n","])\n","\n","# --- Load the FashionMNIST Dataset ---\n","# This class IS the torch.utils.data.Dataset subclass for FashionMNIST\n","# It handles finding/downloading the data files and implements __len__ and __getitem__.\n","try:\n","    train_dataset = torchvision.datasets.FashionMNIST(\n","        root=DATA_DIR,         # Directory to store data\n","        train=True,            # Load training data\n","        download=True,         # Download data if not available\n","        transform=transform    # Apply the defined transformation\n","    )\n","\n","    # You can also load the test set if needed\n","    # test_dataset = torchvision.datasets.FashionMNIST(\n","    #     root=DATA_DIR,\n","    #     train=False,           # Load test data\n","    #     download=True,\n","    #     transform=transform\n","    # )\n","\n","    print(f\"Number of training samples: {len(train_dataset)}\")\n","    # print(f\"Number of test samples: {len(test_dataset)}\")\n","\n","except Exception as e:\n","    print(f\"Error loading FashionMNIST dataset: {e}\")\n","    print(\"Please check your internet connection and disk space.\")\n","    exit() # Exit if data loading fails\n","\n","# --- Create DataLoader ---\n","train_dataloader = DataLoader(\n","    dataset=train_dataset,  # Your dataset object\n","    batch_size=BATCH_SIZE, # How many samples per batch\n","    shuffle=True,           # Shuffle data at the beginning of each epoch (good for training)\n","    num_workers=2           # Number of subprocesses to use for data loading (can speed things up)\n",")\n","\n","# --- Demonstrate Using the DataLoader ---\n","print(\"\\nDemonstrating data loading with DataLoader:\")\n","\n","# Iterate through the first batch\n","for images, labels in train_dataloader:\n","    print(f\"Batch of images shape: {images.shape}\") # Expected: [BATCH_SIZE, 1, 28, 28] (Batch, Channels, Height, Width)\n","    print(f\"Batch of labels shape: {labels.shape}\") # Expected: [BATCH_SIZE]\n","\n","    # Optional: Display the first image in the batch (requires matplotlib)\n","    if images.shape[0] > 0:\n","        # FashionMNIST is grayscale (1 channel)\n","        plt.imshow(images[0].squeeze(), cmap='gray')\n","        plt.title(f\"Example Image (Label: {labels[0].item()})\")\n","        plt.axis('off')\n","        plt.show()\n","\n","    # We'll just process one batch for demonstration\n","    break\n","\n","print(\"\\nDataLoader setup complete. Ready to use in a training loop.\")\n","\n","# Example of a simple training loop structure (no actual training)\n","# model = ... # Define your PyTorch model\n","# criterion = ... # Define your loss function\n","# optimizer = ... # Define your optimizer\n","\n","# num_epochs = 5\n","# for epoch in range(num_epochs):\n","#     for i, (images, labels) in enumerate(train_dataloader):\n","#         # Move tensors to the configured device (e.g., GPU)\n","#         # images = images.to(device)\n","#         # labels = labels.to(device)\n","\n","#         # Forward pass\n","#         # outputs = model(images)\n","#         # loss = criterion(outputs, labels)\n","\n","#         # Backward and optimize\n","#         # optimizer.zero_grad()\n","#         # loss.backward()\n","#         # optimizer.step()\n","\n","#         # Print progress (optional)\n","#         # if (i+1) % 100 == 0:\n","#         #     print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_dataloader)}], Loss: {loss.item():.4f}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":619},"id":"e7dyrRSaIlke","executionInfo":{"status":"ok","timestamp":1749027043603,"user_tz":420,"elapsed":5158,"user":{"displayName":"Vishesh Agrawal","userId":"16926083185186864017"}},"outputId":"c5cfabea-d812-4ff7-cf51-d061144ae3be"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 26.4M/26.4M [00:01<00:00, 19.0MB/s]\n","100%|██████████| 29.5k/29.5k [00:00<00:00, 298kB/s]\n","100%|██████████| 4.42M/4.42M [00:00<00:00, 5.60MB/s]\n","100%|██████████| 5.15k/5.15k [00:00<00:00, 8.79MB/s]"]},{"output_type":"stream","name":"stdout","text":["Number of training samples: 60000\n","\n","Demonstrating data loading with DataLoader:\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["Batch of images shape: torch.Size([64, 1, 28, 28])\n","Batch of labels shape: torch.Size([64])\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","DataLoader setup complete. Ready to use in a training loop.\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt # Import for potential visualization\n","from sklearn.datasets import load_iris\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.decomposition import PCA\n","from sklearn.model_selection import train_test_split\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.metrics import accuracy_score, classification_report\n","\n","# Set random state for reproducibility\n","random_state = 42\n","n_neighbors = 5 # Number of neighbors for k-NN\n","\n","# 1. Load the Iris dataset\n","print(\"Loading Iris dataset...\")\n","iris = load_iris()\n","X = iris.data\n","y = iris.target\n","target_names = iris.target_names\n","\n","print(f\"Original dataset shape: {X.shape}\")\n","\n","# 2. Split data into training and testing sets\n","# Splitting BEFORE scaling and PCA to prevent data leakage\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.3, random_state=random_state, stratify=y # Stratify to maintain class distribution\n",")\n","print(f\"Training set shape: {X_train.shape}\")\n","print(f\"Testing set shape: {X_test.shape}\")\n","\n","# 3. Standardize the features\n","# Scaling is important before PCA and distance-based methods like k-NN\n","print(\"Scaling features...\")\n","scaler = StandardScaler()\n","# Fit only on the training data, then transform both training and testing data\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_test_scaled = scaler.transform(X_test)\n","\n","# 4. Apply PCA for dimensionality reduction\n","# Let's reduce to 2 components for easy visualization (and often effective for Iris)\n","n_components = 2\n","print(f\"Reducing dimensionality to {n_components} using PCA...\")\n","pca = PCA(n_components=n_components, random_state=random_state)\n","\n","# Fit PCA only on the scaled training data, then transform both\n","X_train_pca = pca.fit_transform(X_train_scaled)\n","X_test_pca = pca.transform(X_test_scaled)\n","\n","print(f\"Shape after PCA: {X_train_pca.shape}\")\n","print(f\"Explained variance ratio by components: {pca.explained_variance_ratio_}\")\n","print(f\"Total explained variance: {np.sum(pca.explained_variance_ratio_):.4f}\")\n","\n","\n","# 5. Fit a k-nearest neighbors classifier\n","print(f\"\\nFitting k-NN classifier with {n_neighbors} neighbors on PCA-transformed data...\")\n","knn = KNeighborsClassifier(n_neighbors=n_neighbors)\n","\n","# Train the model using the PCA-transformed training data\n","knn.fit(X_train_pca, y_train)\n","\n","# 6. Make predictions on the PCA-transformed test data\n","y_pred = knn.predict(X_test_pca)\n","\n","# 7. Evaluate the model\n","print(\"\\nEvaluating the model:\")\n","accuracy = accuracy_score(y_test, y_pred)\n","print(f\"Accuracy: {accuracy:.4f}\")\n","\n","# Print classification report for more detailed evaluation\n","print(\"\\nClassification Report:\")\n","print(classification_report(y_test, y_pred, target_names=target_names))\n","\n","# Optional: Visualize the PCA-transformed data and the decision boundary (simplified)\n","# This part requires matplotlib\n","try:\n","    print(\"\\nGenerating visualization of PCA components...\")\n","    plt.figure(figsize=(10, 8))\n","    colors = ['navy', 'turquoise', 'darkorange']\n","    lw = 2\n","\n","    for color, i, target_name in zip(colors, [0, 1, 2], target_names):\n","        plt.scatter(X_train_pca[y_train == i, 0], X_train_pca[y_train == i, 1], color=color, alpha=.8, lw=lw,\n","                    label=target_name)\n","    plt.legend(loc='best', shadow=False, scatterpoints=1)\n","    plt.title('PCA of IRIS dataset (2 components)')\n","    plt.xlabel('Principal Component 1')\n","    plt.ylabel('Principal Component 2')\n","\n","    # You can also plot the test data on the same plot if desired, with different markers\n","    # for color, i, target_name in zip(colors, [0, 1, 2], target_names):\n","    #     plt.scatter(X_test_pca[y_test == i, 0], X_test_pca[y_test == i, 1], color=color, alpha=.5, lw=lw,\n","    #                 marker='x', label=f'Test {target_name}')\n","    # plt.legend(loc='best', shadow=False, scatterpoints=1) # Update legend if adding test data\n","\n","    plt.show()\n","\n","except ImportError:\n","    print(\"\\nMatplotlib not installed. Skipping visualization.\")\n","except Exception as e:\n","    print(f\"\\nAn error occurred during visualization: {e}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"5yUkamWOIyHJ","executionInfo":{"status":"ok","timestamp":1749027085261,"user_tz":420,"elapsed":1183,"user":{"displayName":"Vishesh Agrawal","userId":"16926083185186864017"}},"outputId":"feedc0eb-ef49-40a5-dc5b-d0ca2a4db5c0"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading Iris dataset...\n","Original dataset shape: (150, 4)\n","Training set shape: (105, 4)\n","Testing set shape: (45, 4)\n","Scaling features...\n","Reducing dimensionality to 2 using PCA...\n","Shape after PCA: (105, 2)\n","Explained variance ratio by components: [0.7264421  0.23378786]\n","Total explained variance: 0.9602\n","\n","Fitting k-NN classifier with 5 neighbors on PCA-transformed data...\n","\n","Evaluating the model:\n","Accuracy: 0.9111\n","\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","      setosa       1.00      1.00      1.00        15\n","  versicolor       0.82      0.93      0.88        15\n","   virginica       0.92      0.80      0.86        15\n","\n","    accuracy                           0.91        45\n","   macro avg       0.92      0.91      0.91        45\n","weighted avg       0.92      0.91      0.91        45\n","\n","\n","Generating visualization of PCA components...\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 1000x800 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":["java\n","// Required imports for Hadoop MapReduce\n","import java.io.IOException;\n","import java.util.StringTokenizer; // Used for splitting lines into words\n","\n","import org.apache.hadoop.conf.Configuration; // For job configuration\n","import org.apache.hadoop.fs.Path;         // For specifying file paths\n","import org.apache.hadoop.io.IntWritable;    // Hadoop's integer type\n","import org.apache.hadoop.io.Text;         // Hadoop's string type\n","import org.apache.hadoop.mapreduce.Job;     // Represents a MapReduce job\n","import org.apache.hadoop.mapreduce.Mapper;  // Base class for the Mapper\n","import org.apache.hadoop.mapreduce.Reducer; // Base class for the Reducer\n","import org.apache.hadoop.mapreduce.lib.input.FileInputFormat; // Helper for setting input paths\n","import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat; // Helper for setting output paths\n","\n","// Main class for the Word Count job\n","public class WordCount {\n","\n","  // The Mapper class\n","  // Takes (Object, Text) as input (default for text files: line number, line content)\n","  // Outputs (Text, IntWritable) as intermediate key-value pairs (word, count=1)\n","  public static class TokenizerMapper\n","       extends Mapper<Object, Text, Text, IntWritable>{\n","\n","    // A constant Writable for the value '1', used for counting\n","    private final static IntWritable one = new IntWritable(1);\n","    // A Writable Text object to hold the current word, reused for efficiency\n","    private Text word = new Text();\n","\n","    // The map method processes each line of input\n","    @Override // Good practice to use @Override annotation\n","    public void map(Object key, Text value, Context context\n","                    ) throws IOException, InterruptedException {\n","      // Convert the line (Text) to a Java String\n","      String line = value.toString();\n","      // Use StringTokenizer to split the line into tokens (words)\n","      // Note: StringTokenizer is basic; for real-world scenarios, consider\n","      // using regex split or more sophisticated tokenizers to handle punctuation,\n","      // case sensitivity, etc.\n","      StringTokenizer itr = new StringTokenizer(line);\n","\n","      // Iterate through the tokens\n","      while (itr.hasMoreTokens()) {\n","        // Get the next token (word)\n","        String token = itr.nextToken();\n","        // Set the Text object 'word' to the current token\n","        word.set(token);\n","        // Emit the word as the key and '1' as the value.\n","        // This means for every occurrence of a word, we emit (word, 1).\n","        context.write(word, one);\n","      }\n","    }\n","  }\n","\n","  // The Reducer class\n","  // Takes (Text, Iterable<IntWritable>) as input (word, list of 1s for that word)\n","  // Outputs (Text, IntWritable) as final key-value pairs (word, total count)\n","  public static class IntSumReducer\n","       extends Reducer<Text,IntWritable,Text,IntWritable> {\n","\n","    // A Writable Int object to hold the sum, reused for efficiency\n","    private IntWritable result = new IntWritable();\n","\n","    // The reduce method processes all values for a given key (word)\n","    @Override // Good practice to use @Override annotation\n","    public void reduce(Text key, Iterable<IntWritable> values,\n","                       Context context\n","                       ) throws IOException, InterruptedException {\n","      // Initialize a counter for the total sum of counts for this word\n","      int sum = 0;\n","      // Iterate through the list of values (all the '1's emitted by mappers for this word)\n","      for (IntWritable val : values) {\n","        // Add the value (which is 1) to the sum\n","        sum += val.get();\n","      }\n","      // Set the result IntWritable to the calculated total sum\n","      result.set(sum);\n","      // Emit the word (key) and its total count (result)\n","      context.write(key, result);\n","    }\n","  }\n","\n","  // The main method to configure and run the MapReduce job\n","  public static void main(String[] args) throws Exception {\n","    // Create a new Hadoop configuration\n","    Configuration conf = new Configuration();\n","\n","    // Create a new Job instance, giving it a name\n","    Job job = Job.getInstance(conf, \"word count\");\n","\n","    // Specify the main class of the job (this class)\n","    job.setJarByClass(WordCount.class);\n","\n","    // Specify the Mapper class to use\n","    job.setMapperClass(TokenizerMapper.class);\n","\n","    // Specify the Combiner class. For Word Count, the reducer logic (summing)\n","    // can be applied locally within each mapper task to partially aggregate counts\n","    // before sending data to reducers, which reduces network traffic.\n","    // The Reducer class works perfectly as a Combiner here.\n","    job.setCombinerClass(IntSumReducer.class);\n","\n","    // Specify the Reducer class to use\n","    job.setReducerClass(IntSumReducer.class);\n","\n","    // Specify the output key type of the job (which is the Reducer's output key type)\n","    job.setOutputKeyClass(Text.class);\n","\n","    // Specify the output value type of the job (which is the Reducer's output value type)\n","    job.setOutputValueClass(IntWritable.class);\n","\n","    // Specify the input path(s) for the job. It expects the path as the first command-line argument.\n","    FileInputFormat.addInputPath(job, new Path(args[0]));\n","\n","    // Specify the output path for the job. It expects the path as the second command-line argument.\n","    // Note: The output directory must NOT exist before running the job.\n","    FileOutputFormat.setOutputPath(job, new Path(args[1]));\n","\n","    // Submit the job and wait for its completion.\n","    // 'true' means print status updates to the console.\n","    // System.exit(0) on success, System.exit(1) on failure.\n","    System.exit(job.waitForCompletion(true) ? 0 : 1);\n","  }\n","}"],"metadata":{"id":"UJkJ6n2OI6bg"},"execution_count":null,"outputs":[]}]}